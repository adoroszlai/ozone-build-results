Attaching to restart_dn1_1, restart_s3g_1, restart_recon_1, restart_dn2_1, restart_om_1, restart_dn3_1, restart_scm_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-07-06 17:31:43,269 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = fb1b01524648/10.9.0.11
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn1_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-07-06 17:31:43,314 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-07-06 17:31:43,585 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-07-06 17:31:44,094 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-07-06 17:31:45,097 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-07-06 17:31:45,097 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-07-06 17:31:45,505 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:fb1b01524648 ip:10.9.0.11
dn1_1    | 2023-07-06 17:31:46,426 [main] INFO reflections.Reflections: Reflections took 648 ms to scan 2 urls, producing 107 keys and 231 values 
dn1_1    | 2023-07-06 17:31:48,451 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-07-06 17:31:48,676 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn1_1    | 2023-07-06 17:31:49,297 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn1_1    | 2023-07-06 17:31:49,434 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-07-06 17:31:49,443 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-07-06 17:31:49,488 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-07-06 17:31:49,608 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-07-06 17:31:49,724 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-06 17:31:49,731 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn1_1    | 2023-07-06 17:31:49,737 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-07-06 17:31:49,744 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-07-06 17:31:49,750 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-07-06 17:31:49,880 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-07-06 17:31:49,881 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-07-06 17:31:58,415 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-07-06 17:31:58,995 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-06 17:31:59,395 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-07-06 17:32:00,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-06 17:32:00,343 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-07-06 17:32:00,363 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-06 17:32:00,365 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-07-06 17:32:00,365 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-07-06 17:32:00,366 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-07-06 17:32:00,384 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-07-06 17:32:00,420 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:00,428 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-07-06 17:32:00,429 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:32:00,515 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-07-06 17:32:00,546 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-07-06 17:32:00,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-07-06 17:32:02,746 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-07-06 17:32:02,784 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-07-06 17:32:02,788 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-07-06 17:32:02,789 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:02,789 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:32:02,814 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:32:02,902 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-07-06 17:32:03,149 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-07-06 17:32:04,066 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-07-06 17:32:04,159 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-07-06 17:32:04,268 [main] INFO util.log: Logging initialized @28859ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-07-06 17:32:04,906 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-07-06 17:32:04,932 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-07-06 17:32:04,973 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-07-06 17:32:04,985 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-07-06 17:32:04,989 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-07-06 17:32:04,989 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-07-06 17:32:05,257 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-07-06 17:32:05,303 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-07-06 17:32:05,305 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn1_1    | 2023-07-06 17:32:05,490 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-07-06 17:32:05,507 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-07-06 17:32:05,509 [main] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-07-06 17:32:05,551 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61b65d54{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-07-06 17:32:05,556 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6403a4a5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-07-06 17:32:06,338 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@586cc15d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2782897483214348778/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-07-06 17:32:06,413 [main] INFO server.AbstractConnector: Started ServerConnector@5ec6fede{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-07-06 17:32:06,416 [main] INFO server.Server: Started @31006ms
dn1_1    | 2023-07-06 17:32:06,431 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-07-06 17:32:06,436 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-07-06 17:32:06,439 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-07-06 17:32:06,679 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-07-06 17:32:06,953 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn1_1    | 2023-07-06 17:32:06,980 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-07-06 17:32:08,546 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 2023-07-06 17:32:08,548 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 2023-07-06 17:32:08,551 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-07-06 17:32:08,554 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 2023-07-06 17:32:08,563 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-07-06 17:32:08,861 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn1_1    | 2023-07-06 17:32:09,065 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-07-06 17:32:11,705 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:12,706 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:13,707 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:14,708 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:15,709 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:15,743 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From fb1b01524648/10.9.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:47348 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:47348 remote=recon/10.9.0.15:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-06 17:32:16,710 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:17,712 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:18,712 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:32:23,729 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From fb1b01524648/10.9.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:59442 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:59442 remote=scm/10.9.0.17:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-06 17:32:24,506 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-c43c1815-e08e-44ec-acac-dc5c81b07827/container.db to cache
dn1_1    | 2023-07-06 17:32:24,517 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-c43c1815-e08e-44ec-acac-dc5c81b07827/container.db for volume DS-c43c1815-e08e-44ec-acac-dc5c81b07827
dn1_1    | 2023-07-06 17:32:24,537 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-07-06 17:32:24,546 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-07-06 17:32:24,753 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-07-06 17:32:24,753 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:32:24,840 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start RPC server
dn1_1    | 2023-07-06 17:32:24,849 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9858
dn1_1    | 2023-07-06 17:32:24,857 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9856
dn1_1    | 2023-07-06 17:32:24,857 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9857
dn1_1    | 2023-07-06 17:32:24,891 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9858 for RATIS
dn1_1    | 2023-07-06 17:32:24,892 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-07-06 17:32:24,892 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-07-06 17:32:24,891 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: Started
dn1_1    | 2023-07-06 17:32:24,964 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-06 17:32:29,858 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-8C1413A610AC:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-8C1413A610AC:java.util.concurrent.CompletableFuture@3ad23cc3[Not completed]
dn1_1    | 2023-07-06 17:32:29,949 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-8C1413A610AC:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:32:29,950 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:32:29,957 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:32:29,957 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:32:29,957 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:29,958 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:32:29,958 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:32:30,016 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: ConfigurationManager, init=-1: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:32:30,020 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:32:30,048 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:32:30,049 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:32:30,136 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:30,161 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:30,190 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:32:30,196 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:32:30,308 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:32:30,380 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:32:30,389 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:32:30,389 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:32:30,391 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-06 17:32:30,393 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:32:30,394 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:32:30,398 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac does not exist. Creating ...
dn1_1    | 2023-07-06 17:32:30,429 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/in_use.lock acquired by nodename 6@fb1b01524648
dn1_1    | 2023-07-06 17:32:30,457 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac has been successfully formatted.
dn1_1    | 2023-07-06 17:32:30,536 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO ratis.ContainerStateMachine: group-8C1413A610AC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-07-06 17:32:30,549 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:32:30,598 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-06 17:32:30,599 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:30,602 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:32:30,607 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:32:30,618 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:30,645 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:32:30,647 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:32:30,648 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:30,671 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac
dn1_1    | 2023-07-06 17:32:30,683 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:32:30,683 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:32:30,686 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:30,689 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:32:30,689 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:32:30,699 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:32:30,699 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:30,702 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:32:30,737 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:30,740 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:30,768 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:30,769 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:30,771 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:30,798 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:32:30,798 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-07-06 17:31:42,730 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 5805399cd9bd/10.9.0.12
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn2_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | 2023-07-06 17:32:30,804 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: start as a follower, conf=-1: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:30,807 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-07-06 17:32:30,811 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState
dn1_1    | 2023-07-06 17:32:30,824 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:32:30,825 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:32:30,825 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C1413A610AC,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:32:30,827 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:30,833 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-06 17:32:30,835 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:32:30,838 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:32:30,943 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=7176bee7-3bd9-4464-8f69-8c1413a610ac
dn1_1    | 2023-07-06 17:32:30,945 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7176bee7-3bd9-4464-8f69-8c1413a610ac.
dn1_1    | 2023-07-06 17:32:30,947 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@dbf4119[Not completed]
dn1_1    | 2023-07-06 17:32:30,977 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:32:30,979 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:32:30,982 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:32:30,986 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:32:30,987 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:30,987 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:32:30,987 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:32:30,988 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:32:30,988 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:32:30,995 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:32:31,004 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:32:31,010 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:31,010 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:31,010 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:32:31,013 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:32:31,013 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:32:31,019 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:32:31,027 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:32:31,028 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:32:31,028 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-07-06 17:31:42,804 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-07-06 17:31:43,007 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-07-06 17:31:43,509 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-07-06 17:31:44,573 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-07-06 17:31:44,573 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-07-06 17:31:45,005 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5805399cd9bd ip:10.9.0.12
dn2_1    | 2023-07-06 17:31:46,056 [main] INFO reflections.Reflections: Reflections took 813 ms to scan 2 urls, producing 107 keys and 231 values 
dn2_1    | 2023-07-06 17:31:48,233 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-07-06 17:31:48,527 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn2_1    | 2023-07-06 17:31:49,229 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn2_1    | 2023-07-06 17:31:49,363 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-07-06 17:31:49,365 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-07-06 17:31:49,366 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-07-06 17:31:49,517 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-07-06 17:31:49,552 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-06 17:31:49,553 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn2_1    | 2023-07-06 17:31:49,599 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-07-06 17:31:49,599 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-07-06 17:31:49,601 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-07-06 17:31:49,713 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-07-06 17:31:49,720 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-07-06 17:31:58,070 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-07-06 17:31:58,557 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-06 17:31:58,904 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-07-06 17:31:59,741 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-06 17:31:59,752 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-07-06 17:31:59,753 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-06 17:31:59,753 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-07-06 17:31:59,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-07-06 17:31:59,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-07-06 17:31:59,829 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-07-06 17:31:59,829 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:31:59,830 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-07-06 17:31:59,830 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:31:59,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:31:59,985 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-07-06 17:31:59,986 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-07-06 17:32:01,645 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-07-06 17:32:01,681 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-07-06 17:32:01,684 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-07-06 17:32:01,684 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:01,685 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:32:01,717 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:32:02,140 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-07-06 17:32:02,630 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn2_1    | 2023-07-06 17:32:03,204 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-07-06 17:32:03,324 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-07-06 17:32:03,492 [main] INFO util.log: Logging initialized @28023ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-07-06 17:32:04,242 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-07-06 17:32:04,300 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-07-06 17:32:04,343 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-07-06 17:32:04,352 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-07-06 17:32:04,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-07-06 17:32:04,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-07-06 17:32:04,653 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-07-06 17:32:04,705 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-07-06 17:32:04,713 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn2_1    | 2023-07-06 17:32:04,946 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-07-06 17:32:04,946 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-07-06 17:32:04,989 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-07-06 17:32:05,145 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@70b6db83{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-07-06 17:32:05,156 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@89caf47{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-07-06 17:32:05,783 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6889f56f{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2140577035484972862/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-07-06 17:32:05,861 [main] INFO server.AbstractConnector: Started ServerConnector@5e599100{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-07-06 17:32:05,868 [main] INFO server.Server: Started @30393ms
dn2_1    | 2023-07-06 17:32:05,870 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-07-06 17:32:05,870 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-07-06 17:32:05,881 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-07-06 17:32:06,063 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-07-06 17:32:06,280 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn2_1    | 2023-07-06 17:32:06,308 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn2_1    | 2023-07-06 17:32:07,722 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn2_1    | 2023-07-06 17:32:07,732 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn2_1    | 2023-07-06 17:32:07,736 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 2023-07-06 17:32:07,773 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn2_1    | 2023-07-06 17:32:07,808 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-07-06 17:32:08,275 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn2_1    | 2023-07-06 17:32:08,558 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-07-06 17:32:11,072 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.15:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:11,075 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:12,076 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:13,077 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:14,078 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:15,079 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:16,080 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:16,112 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 5805399cd9bd/10.9.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:49126 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:49126 remote=recon/10.9.0.15:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-06 17:32:17,081 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:18,082 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:32:23,096 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 5805399cd9bd/10.9.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:33470 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 2023-07-06 17:32:31,029 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:32:31,032 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:32:31,033 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 does not exist. Creating ...
dn1_1    | 2023-07-06 17:32:31,041 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 6@fb1b01524648
dn1_1    | 2023-07-06 17:32:31,052 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 has been successfully formatted.
dn1_1    | 2023-07-06 17:32:31,054 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-07-06 17:32:31,071 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:32:31,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-06 17:32:31,079 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:31,092 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:32:31,095 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:32:31,098 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:31,099 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:32:31,108 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:32:31,112 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:31,112 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn1_1    | 2023-07-06 17:32:31,115 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:32:31,115 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:32:31,116 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:31,116 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:32:31,117 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:32:31,124 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:32:31,127 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:31,128 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:32:31,129 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:31,140 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:31,877 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: Detected pause in JVM or host machine approximately 0.458s with 0.723s GC time.
dn1_1    | GC pool 'ParNew' had collection(s): count=1 time=35ms
dn1_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=688ms
dn1_1    | 2023-07-06 17:32:31,892 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:31,902 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:31,902 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:31,903 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:32:31,903 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:32:31,908 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:31,924 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-07-06 17:32:31,925 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:32:31,927 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:32:31,927 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:31,927 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1     | 2023-07-06 17:31:41,763 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1     | /************************************************************
om_1     | STARTUP_MSG: Starting OzoneManager
om_1     | STARTUP_MSG:   host = 130e1cdd9ed4/10.9.0.14
om_1     | STARTUP_MSG:   args = [--init]
om_1     | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1     | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
om_1     | STARTUP_MSG:   java = 11.0.19
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:33470 remote=scm/10.9.0.17:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-06 17:32:24,679 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b/container.db to cache
dn2_1    | 2023-07-06 17:32:24,681 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b/container.db for volume DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b
dn2_1    | 2023-07-06 17:32:24,694 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-07-06 17:32:24,706 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-07-06 17:32:24,943 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-07-06 17:32:24,943 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:25,000 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start RPC server
dn2_1    | 2023-07-06 17:32:25,008 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9858
dn2_1    | 2023-07-06 17:32:25,011 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9856
dn2_1    | 2023-07-06 17:32:25,013 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9857
dn2_1    | 2023-07-06 17:32:25,017 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: Started
dn2_1    | 2023-07-06 17:32:25,018 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9858 for RATIS
dn2_1    | 2023-07-06 17:32:25,018 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-07-06 17:32:25,019 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-07-06 17:32:25,045 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-06 17:32:29,133 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-262CDF97B151:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-262CDF97B151:java.util.concurrent.CompletableFuture@3f373ae2[Not completed]
dn2_1    | 2023-07-06 17:32:29,209 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-262CDF97B151:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:32:29,213 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:32:29,218 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:32:29,218 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:32:29,219 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:29,222 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:32:29,222 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1     | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1     | ************************************************************/
om_1     | 2023-07-06 17:31:41,844 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1     | 2023-07-06 17:31:48,063 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1     | 2023-07-06 17:31:50,824 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1     | 2023-07-06 17:31:50,959 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/10.9.0.14:9862
om_1     | 2023-07-06 17:31:50,959 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1     | 2023-07-06 17:31:50,963 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1     | 2023-07-06 17:31:51,038 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:31:52,105 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863]
om_1     | 2023-07-06 17:31:55,265 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1     | 2023-07-06 17:31:57,266 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1     | 2023-07-06 17:31:59,283 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1     | 2023-07-06 17:32:01,285 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1     | 2023-07-06 17:32:03,287 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1     | 2023-07-06 17:32:05,289 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1     | 2023-07-06 17:32:07,292 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1     | 2023-07-06 17:32:09,293 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1     | 2023-07-06 17:32:11,296 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1     | 2023-07-06 17:32:13,297 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1     | 2023-07-06 17:32:15,299 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1     | 2023-07-06 17:32:17,302 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 130e1cdd9ed4/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1     | 2023-07-06 17:32:19,686 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
om_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1     | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1     | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1     | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1     | 2023-07-06 17:32:21,694 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
om_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1     | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1     | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1     | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1     | 2023-07-06 17:32:23,707 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
om_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1     | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1     | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn2_1    | 2023-07-06 17:32:29,261 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:32:29,264 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:32:29,298 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:32:29,302 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:32:29,388 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:29,407 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:29,430 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:32:29,432 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:32:29,516 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:32:29,607 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:32:29,629 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:32:29,632 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-06 17:32:29,633 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:32:29,635 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:32:29,637 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:32:29,644 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151 does not exist. Creating ...
dn2_1    | 2023-07-06 17:32:29,672 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/in_use.lock acquired by nodename 7@5805399cd9bd
dn2_1    | 2023-07-06 17:32:29,706 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151 has been successfully formatted.
dn2_1    | 2023-07-06 17:32:29,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO ratis.ContainerStateMachine: group-262CDF97B151: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-06 17:32:29,737 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:32:29,818 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:32:29,820 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:29,826 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:32:29,832 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:32:29,864 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:29,887 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:32:29,921 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:32:29,921 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:29,952 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151
dn2_1    | 2023-07-06 17:32:29,960 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:32:29,964 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:29,974 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:29,978 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:32:29,981 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:32:29,986 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:32:29,988 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:29,989 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:32:30,059 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:30,067 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:30,141 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:30,147 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:30,149 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:30,194 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1     | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1     | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c;layoutVersion=6
om_1     | 2023-07-06 17:32:25,782 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1     | /************************************************************
om_1     | SHUTDOWN_MSG: Shutting down OzoneManager at 130e1cdd9ed4/10.9.0.14
om_1     | ************************************************************/
om_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1     | 2023-07-06 17:32:27,820 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1     | /************************************************************
om_1     | STARTUP_MSG: Starting OzoneManager
om_1     | STARTUP_MSG:   host = 130e1cdd9ed4/10.9.0.14
om_1     | STARTUP_MSG:   args = []
om_1     | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-07-06 17:32:30,194 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:32:30,203 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:30,206 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-07-06 17:32:30,212 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState
dn2_1    | 2023-07-06 17:32:30,228 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:30,231 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:30,239 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-262CDF97B151,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:30,248 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:30,255 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:32:30,256 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:32:30,262 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:32:30,329 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151
dn2_1    | 2023-07-06 17:32:30,334 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151.
dn2_1    | 2023-07-06 17:32:30,334 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@4a8f85de[Not completed]
dn2_1    | 2023-07-06 17:32:30,358 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:32:30,360 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:32:30,360 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:32:30,361 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:32:30,361 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:30,361 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:32:30,362 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-06 17:32:30,363 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:32:30,371 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:32:30,386 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:32:30,393 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:32:30,393 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:30,395 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:30,395 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:32:30,405 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:32:30,406 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:32:30,411 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:32:30,411 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:32:30,412 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1     | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
om_1     | STARTUP_MSG:   java = 11.0.19
dn1_1    | 2023-07-06 17:32:31,927 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:32:31,927 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:32:31,928 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:32:31,930 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:32:31,930 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn1_1    | 2023-07-06 17:32:34,958 [grpc-default-executor-0] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@43f6f261[Not completed]
dn1_1    | 2023-07-06 17:32:34,960 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:32:34,960 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:32:34,960 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:32:34,960 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:32:34,961 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:34,961 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:32:34,961 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:32:34,961 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:32:34,961 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:32:34,962 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:32:34,962 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:32:34,962 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:32:34,962 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:34,963 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:32:34,963 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:32:34,963 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:32:34,964 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:32:34,964 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:32:34,964 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:32:34,965 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-06 17:32:34,965 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:32:34,966 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:32:34,966 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e does not exist. Creating ...
dn1_1    | 2023-07-06 17:32:34,969 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 6@fb1b01524648
dn1_1    | 2023-07-06 17:32:34,973 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e has been successfully formatted.
dn1_1    | 2023-07-06 17:32:34,979 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO ratis.ContainerStateMachine: group-6C471268B20E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-07-06 17:32:34,987 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:32:34,988 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:32:30,415 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:32:30,416 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:32:30,416 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:32:30,417 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 does not exist. Creating ...
dn2_1    | 2023-07-06 17:32:30,420 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 7@5805399cd9bd
dn2_1    | 2023-07-06 17:32:30,423 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 has been successfully formatted.
dn2_1    | 2023-07-06 17:32:30,436 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-06 17:32:30,442 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:32:30,442 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:32:30,442 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:30,443 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:32:30,445 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:32:30,446 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:30,451 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:32:30,455 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:32:30,456 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:30,457 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn2_1    | 2023-07-06 17:32:30,457 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:32:30,458 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:30,458 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:30,461 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:32:30,462 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:32:30,462 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:32:30,463 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:30,466 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:32:30,472 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:30,482 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:31,244 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: Detected pause in JVM or host machine approximately 0.710s with 0.757s GC time.
dn2_1    | GC pool 'ParNew' had collection(s): count=1 time=121ms
dn2_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=636ms
dn2_1    | 2023-07-06 17:32:31,263 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:31,264 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:31,264 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:31,267 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:32:31,267 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:32:31,275 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:31,275 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-07-06 17:32:31,276 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:32:31,282 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:31,285 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:31,285 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:32:31,285 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:32:31,285 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:32:31,286 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:31,292 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn2_1    | 2023-07-06 17:32:31,293 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:34,070 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68.
dn2_1    | 2023-07-06 17:32:34,071 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@5433f217[Not completed]
dn2_1    | 2023-07-06 17:32:34,079 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:32:34,079 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:32:34,079 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:32:34,079 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:32:34,080 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:34,081 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:32:34,081 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-06 17:32:34,081 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:32:34,081 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:32:34,083 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:32:34,083 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:32:34,084 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:32:34,084 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:34,084 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:32:34,085 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:32:34,085 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:32:34,087 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:32:34,087 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:32:34,088 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-06 17:32:34,088 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:32:34,088 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:32:34,099 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:32:34,100 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e does not exist. Creating ...
dn2_1    | 2023-07-06 17:32:34,122 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 7@5805399cd9bd
dn2_1    | 2023-07-06 17:32:34,134 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e has been successfully formatted.
dn2_1    | 2023-07-06 17:32:34,148 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO ratis.ContainerStateMachine: group-6C471268B20E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-06 17:32:34,152 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:32:34,153 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:32:34,156 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:34,157 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:32:34,157 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:32:34,161 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:34,163 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:32:34,168 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:32:34,168 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:34,168 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn2_1    | 2023-07-06 17:32:34,169 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:32:34,170 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:34,170 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:34,171 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:32:34,171 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:32:34,171 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:32:34,171 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:32:34,557 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:32:34,559 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: Detected pause in JVM or host machine approximately 0.300s with 0.377s GC time.
dn2_1    | GC pool 'ParNew' had collection(s): count=1 time=377ms
dn2_1    | 2023-07-06 17:32:34,559 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:32:34,592 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:34,700 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:34,700 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:32:34,700 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:34,703 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:32:34,703 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:32:34,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:34,720 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-07-06 17:32:34,737 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState
dn2_1    | 2023-07-06 17:32:34,738 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:34,738 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:32:34,738 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:32:34,738 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:32:34,738 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:32:34,739 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:34,745 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:34,740 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=2ab0c656-6867-4094-952d-6c471268b20e
dn1_1    | 2023-07-06 17:32:34,988 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:34,990 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:32:34,990 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:32:34,991 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:34,991 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:32:34,996 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:32:34,997 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:32:34,998 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:32:34,998 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:32:34,998 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:32:35,001 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:32:35,003 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:32:35,011 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:35,014 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:32:35,014 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:35,015 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:32:35,015 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:32:35,016 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:35,019 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-07-06 17:32:35,020 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:32:35,022 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:32:35,024 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:32:35,024 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-06 17:32:35,024 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:32:35,024 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:32:35,025 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:32:35,028 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:32:35,229 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68.
dn1_1    | 2023-07-06 17:32:35,851 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5042441826ns, electionTimeout:5025ms
dn1_1    | 2023-07-06 17:32:35,851 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState
dn1_1    | 2023-07-06 17:32:35,852 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-07-06 17:32:35,871 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-07-06 17:32:35,873 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1
dn1_1    | 2023-07-06 17:32:35,885 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:35,888 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-07-06 17:32:35,894 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:35,895 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-07-06 17:32:35,895 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1
dn1_1    | 2023-07-06 17:32:35,912 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-07-06 17:32:35,912 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C1413A610AC with new leaderId: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:32:35,914 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: change Leader from null to 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 at term 1 for becomeLeader, leader elected after 5780ms
dn1_1    | 2023-07-06 17:32:35,967 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-07-06 17:32:35,992 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:32:35,993 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-07-06 17:32:36,017 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-07-06 17:32:36,021 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-07-06 17:32:36,022 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-07-06 17:32:36,069 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:32:36,076 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-07-06 17:32:36,079 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderStateImpl
dn1_1    | 2023-07-06 17:32:36,115 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-07-06 17:32:36,180 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: set configuration 0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:36,242 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/log_inprogress_0
dn1_1    | 2023-07-06 17:32:36,578 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-D4A90FBE2E68, 0, (t:0, i:0))
dn1_1    | 2023-07-06 17:32:36,582 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FOLLOWER: accept PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 0
dn1_1    | 2023-07-06 17:32:36,602 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t0, leader=null, voted=, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:36,715 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 0, (t:0, i:0))
dn1_1    | 2023-07-06 17:32:36,716 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FOLLOWER: accept PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:32:36,716 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t0, leader=null, voted=, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:36,784 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(ELECTION, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 1, (t:0, i:0))
dn1_1    | 2023-07-06 17:32:36,784 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FOLLOWER: accept ELECTION from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:32:36,791 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:32:36,793 [grpc-default-executor-0] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:32:36,793 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState was interrupted
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-07-06 17:31:42,987 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = b474d983ce39/10.9.0.15
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
recon_1  | STARTUP_MSG:   java = 11.0.19
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-07-06 17:31:42,527 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 5022da3b84cd/10.9.0.13
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn3_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-07-06 17:31:42,602 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-07-06 17:31:42,945 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-07-06 17:31:43,382 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-07-06 17:31:44,468 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-07-06 17:31:44,468 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-07-06 17:31:44,991 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5022da3b84cd ip:10.9.0.13
dn3_1    | 2023-07-06 17:31:46,027 [main] INFO reflections.Reflections: Reflections took 847 ms to scan 2 urls, producing 107 keys and 231 values 
dn3_1    | 2023-07-06 17:31:48,071 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn3_1    | 2023-07-06 17:31:48,348 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn3_1    | 2023-07-06 17:31:48,925 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn3_1    | 2023-07-06 17:31:49,035 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-07-06 17:31:49,048 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-07-06 17:31:49,056 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-07-06 17:31:49,240 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-07-06 17:31:49,272 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-07-06 17:31:49,298 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn3_1    | 2023-07-06 17:31:49,308 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-07-06 17:31:49,312 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-07-06 17:31:49,312 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-07-06 17:31:49,504 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-07-06 17:31:49,504 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-07-06 17:31:58,079 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-07-06 17:31:58,550 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-07-06 17:31:58,912 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-07-06 17:31:59,716 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-06 17:31:59,757 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-07-06 17:32:35,298 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5086491794ns, electionTimeout:5065ms
dn2_1    | 2023-07-06 17:32:35,300 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState
dn2_1    | 2023-07-06 17:32:35,308 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-07-06 17:32:35,314 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:32:35,314 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1
dn2_1    | 2023-07-06 17:32:35,351 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:35,352 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
dn2_1    | 2023-07-06 17:32:35,376 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:35,392 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-07-06 17:32:35,394 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1
dn2_1    | 2023-07-06 17:32:35,402 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-07-06 17:32:35,402 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-262CDF97B151 with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:35,405 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 1 for becomeLeader, leader elected after 6019ms
dn2_1    | 2023-07-06 17:32:35,523 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-07-06 17:32:35,571 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:35,578 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-07-06 17:32:35,585 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e.
dn2_1    | 2023-07-06 17:32:35,597 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-07-06 17:32:35,597 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-07-06 17:32:35,599 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-07-06 17:32:35,621 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:35,626 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-07-06 17:32:35,636 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderStateImpl
dn2_1    | 2023-07-06 17:32:35,720 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-07-06 17:32:35,830 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:36,092 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/log_inprogress_0
dn2_1    | 2023-07-06 17:32:36,477 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200928104ns, electionTimeout:5183ms
dn2_1    | 2023-07-06 17:32:36,479 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:32:36,479 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-07-06 17:32:36,479 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:32:36,479 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2
om_1     | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1     | ************************************************************/
om_1     | 2023-07-06 17:32:27,827 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1     | 2023-07-06 17:32:31,170 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1     | 2023-07-06 17:32:33,056 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1     | 2023-07-06 17:32:33,212 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/10.9.0.14:9862
om_1     | 2023-07-06 17:32:33,212 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1     | 2023-07-06 17:32:33,212 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1     | 2023-07-06 17:32:33,293 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:32:33,447 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1     | 2023-07-06 17:32:34,684 [main] INFO reflections.Reflections: Reflections took 862 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1     | 2023-07-06 17:32:34,753 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1     | 2023-07-06 17:32:34,826 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:32:35,957 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863]
om_1     | 2023-07-06 17:32:36,333 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863]
om_1     | 2023-07-06 17:32:37,988 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1     | 2023-07-06 17:32:38,021 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:32:38,262 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1     | 2023-07-06 17:32:38,799 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1     | 2023-07-06 17:32:38,817 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1     | 2023-07-06 17:32:38,823 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:32:38,883 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1     | 2023-07-06 17:32:38,884 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1     | 2023-07-06 17:32:39,350 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1     | 2023-07-06 17:32:39,436 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1     | 2023-07-06 17:32:39,438 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1     | 2023-07-06 17:32:39,491 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1     | 2023-07-06 17:32:39,516 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1     | 2023-07-06 17:32:39,604 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1     | 2023-07-06 17:32:39,619 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
dn2_1    | 2023-07-06 17:32:36,486 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:36,524 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:36,533 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn2_1    | 2023-07-06 17:32:36,542 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:36,545 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:32:36,658 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:32:36,660 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0
dn2_1    | 2023-07-06 17:32:36,661 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 1: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:FAIL-t0
dn2_1    | 2023-07-06 17:32:36,663 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: result REJECTED
dn2_1    | 2023-07-06 17:32:36,666 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn2_1    | 2023-07-06 17:32:36,666 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2
dn2_1    | 2023-07-06 17:32:36,667 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:32:36,730 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 0, (t:0, i:0))
dn2_1    | 2023-07-06 17:32:36,737 [grpc-default-executor-1] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FOLLOWER: accept PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn2_1    | 2023-07-06 17:32:36,746 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t0. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68:t0, leader=null, voted=, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:36,764 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: receive requestVote(ELECTION, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 1, (t:0, i:0))
dn2_1    | 2023-07-06 17:32:36,764 [grpc-default-executor-0] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FOLLOWER: accept ELECTION from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn2_1    | 2023-07-06 17:32:36,766 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:32:36,766 [grpc-default-executor-0] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:32:36,766 [grpc-default-executor-0] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:32:36,768 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState was interrupted
dn2_1    | 2023-07-06 17:32:36,798 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68 replies to ELECTION vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t1. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68:t1, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:37,064 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:32:37,064 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 1 for appendEntries, leader elected after 6670ms
dn1_1    | 2023-07-06 17:32:36,795 [grpc-default-executor-0] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:32:36,814 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to ELECTION vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t1. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t1, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:37,128 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:32:37,128 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 1 for appendEntries, leader elected after 6119ms
dn1_1    | 2023-07-06 17:32:37,135 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:37,142 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-07-06 17:32:37,147 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn1_1    | 2023-07-06 17:32:39,795 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 0, (t:0, i:0))
dn1_1    | 2023-07-06 17:32:39,796 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FOLLOWER: accept PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:32:39,796 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E:t0, leader=null, voted=, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:39,836 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: receive requestVote(ELECTION, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 1, (t:0, i:0))
dn1_1    | 2023-07-06 17:32:39,836 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FOLLOWER: accept ELECTION from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:32:39,836 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn1_1    | 2023-07-06 17:32:39,837 [grpc-default-executor-0] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:32:39,837 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState was interrupted
dn1_1    | 2023-07-06 17:32:39,837 [grpc-default-executor-0] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-07-06 17:31:43,046 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-07-06 17:31:46,530 [main] INFO reflections.Reflections: Reflections took 402 ms to scan 1 urls, producing 20 keys and 75 values 
dn1_1    | 2023-07-06 17:32:39,841 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E replies to ELECTION vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t1. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:40,074 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn1_1    | 2023-07-06 17:32:40,074 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 1 for appendEntries, leader elected after 5111ms
dn1_1    | 2023-07-06 17:32:40,075 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:32:40,075 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-07-06 17:32:40,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn1_1    | 2023-07-06 17:33:24,966 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-07-06 17:31:49,503 [main] INFO reflections.Reflections: Reflections took 430 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1  | 2023-07-06 17:31:49,844 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-07-06 17:31:51,511 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-07-06 17:31:57,957 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-07-06 17:31:59,917 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-07-06 17:31:59,935 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-07-06 17:32:00,203 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-07-06 17:32:00,390 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-07-06 17:32:00,391 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-07-06 17:32:03,133 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-07-06 17:32:03,207 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-07-06 17:32:03,262 [main] INFO util.log: Logging initialized @27766ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-07-06 17:32:03,708 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-07-06 17:32:03,770 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1  | 2023-07-06 17:32:03,818 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-07-06 17:32:03,825 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-07-06 17:32:03,844 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-07-06 17:32:03,846 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-07-06 17:32:04,098 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-07-06 17:32:04,119 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-07-06 17:32:05,297 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-07-06 17:32:05,333 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1  | 2023-07-06 17:32:05,392 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-07-06 17:32:05,586 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-07-06 17:32:05,596 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1  | 2023-07-06 17:32:08,348 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:32:08,975 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:32:09,133 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1  | 2023-07-06 17:32:09,137 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-07-06 17:32:09,303 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:32:09,585 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1  | 2023-07-06 17:32:09,611 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-07-06 17:32:09,653 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-07-06 17:32:09,702 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1  | 2023-07-06 17:32:09,710 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 2023-07-06 17:32:10,089 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-07-06 17:32:10,113 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-07-06 17:32:10,151 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1  | 2023-07-06 17:32:10,156 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-07-06 17:32:10,194 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 2023-07-06 17:32:10,329 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1  | 2023-07-06 17:32:10,331 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-07-06 17:32:10,332 [main] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-07-06 17:32:10,414 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-07-06 17:32:10,425 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-07-06 17:32:10,425 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-07-06 17:32:10,805 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-07-06 17:32:10,809 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1  | 2023-07-06 17:32:10,880 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-07-06 17:32:10,881 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-07-06 17:32:10,884 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 2023-07-06 17:32:10,903 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47044f7d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1    | 2023-07-06 17:31:49,577 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1    | /************************************************************
scm_1    | STARTUP_MSG: Starting StorageContainerManager
scm_1    | STARTUP_MSG:   host = 27de9db37231/10.9.0.17
scm_1    | STARTUP_MSG:   args = [--init]
scm_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-07-06 17:31:43,451 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-07-06 17:31:43,468 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-07-06 17:31:43,661 [main] INFO util.log: Logging initialized @8225ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-07-06 17:31:44,330 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-07-06 17:31:44,458 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-07-06 17:31:44,511 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-07-06 17:31:44,529 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-07-06 17:31:44,532 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-07-06 17:31:44,532 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-07-06 17:31:44,766 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir8723967060339753491
s3g_1    | 2023-07-06 17:31:45,487 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = 1e1dbbc8f57a/10.9.0.16
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
s3g_1    | STARTUP_MSG:   java = 11.0.19
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir8723967060339753491, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-07-06 17:31:45,538 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-07-06 17:31:45,655 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-07-06 17:31:46,213 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-07-06 17:31:47,115 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-07-06 17:31:47,116 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-07-06 17:31:47,352 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1    | 2023-07-06 17:31:47,378 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-07-06 17:31:47,380 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1    | 2023-07-06 17:31:47,657 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-07-06 17:31:47,660 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-07-06 17:31:47,685 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1    | 2023-07-06 17:31:47,791 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7ae42ce3{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-07-06 17:31:47,804 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@e57b96d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 2023-07-06 17:32:07,104 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@56ec6960{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir8723967060339753491/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-5755517963312950037/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-07-06 17:32:07,165 [main] INFO server.AbstractConnector: Started ServerConnector@3a5ecce3{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-07-06 17:32:07,168 [main] INFO server.Server: Started @31731ms
s3g_1    | 2023-07-06 17:32:07,196 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-07-06 17:32:07,196 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-07-06 17:32:07,198 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
dn2_1    | 2023-07-06 17:32:37,100 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:37,108 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread2] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-07-06 17:32:37,110 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn2_1    | 2023-07-06 17:32:39,777 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5040544230ns, electionTimeout:5032ms
dn2_1    | 2023-07-06 17:32:39,778 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState
dn2_1    | 2023-07-06 17:32:39,778 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-07-06 17:32:39,778 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:32:39,778 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3
dn2_1    | 2023-07-06 17:32:39,788 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:39,811 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:39,812 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:39,812 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:32:39,812 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0
dn2_1    | 2023-07-06 17:32:39,813 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3 PRE_VOTE round 0: result PASSED
dn2_1    | 2023-07-06 17:32:39,818 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:32:39,829 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:32:39,829 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:32:39,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:32:39,871 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t1
dn2_1    | 2023-07-06 17:32:39,872 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3 ELECTION round 0: result PASSED
dn2_1    | 2023-07-06 17:32:39,873 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3
dn2_1    | 2023-07-06 17:32:39,876 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-07-06 17:32:39,877 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:32:39,880 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 1 for becomeLeader, leader elected after 5792ms
dn2_1    | 2023-07-06 17:32:39,890 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-07-06 17:32:39,891 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:39,892 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-07-06 17:32:39,896 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-07-06 17:32:39,897 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-07-06 17:32:39,898 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
scm_1    | STARTUP_MSG:   java = 11.0.19
scm_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1    | ************************************************************/
scm_1    | 2023-07-06 17:31:49,648 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1    | 2023-07-06 17:31:50,203 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:31:52,476 [main] INFO reflections.Reflections: Reflections took 1820 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1    | 2023-07-06 17:31:53,299 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1    | 2023-07-06 17:31:53,476 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1    | 2023-07-06 17:31:54,962 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1    | 2023-07-06 17:31:56,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:31:56,200 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:31:56,256 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:31:56,287 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:31:56,287 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1    | 2023-07-06 17:31:56,288 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1    | 2023-07-06 17:31:56,289 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1    | 2023-07-06 17:31:56,314 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:31:56,335 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1    | 2023-07-06 17:31:56,349 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:31:56,515 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1    | 2023-07-06 17:31:56,565 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1    | 2023-07-06 17:31:56,578 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1    | 2023-07-06 17:31:59,427 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1    | 2023-07-06 17:31:59,536 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1    | 2023-07-06 17:31:59,537 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1    | 2023-07-06 17:31:59,553 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:31:59,556 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:31:59,612 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:31:59,736 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: addNew group-8CA20BA5223C:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER] returns group-8CA20BA5223C:java.util.concurrent.CompletableFuture@63998bf4[Not completed]
scm_1    | 2023-07-06 17:32:00,175 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: new RaftServerImpl for group-8CA20BA5223C:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1    | 2023-07-06 17:32:00,187 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1    | 2023-07-06 17:32:00,196 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1    | 2023-07-06 17:32:00,196 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1    | 2023-07-06 17:32:00,227 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:32:00,228 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:32:00,229 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1    | 2023-07-06 17:32:00,289 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: ConfigurationManager, init=-1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1    | 2023-07-06 17:32:00,318 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:32:00,355 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1    | 2023-07-06 17:32:00,384 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1    | 2023-07-06 17:32:00,597 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1    | 2023-07-06 17:32:00,662 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:00,750 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1    | 2023-07-06 17:32:00,750 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1    | 2023-07-06 17:32:01,029 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1    | 2023-07-06 17:32:01,143 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1    | 2023-07-06 17:32:02,962 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:32:03,012 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1    | 2023-07-06 17:32:03,043 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1    | 2023-07-06 17:32:03,043 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1    | 2023-07-06 17:32:03,044 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1    | 2023-07-06 17:32:03,044 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1    | 2023-07-06 17:32:03,045 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c does not exist. Creating ...
scm_1    | 2023-07-06 17:32:03,098 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/in_use.lock acquired by nodename 13@27de9db37231
scm_1    | 2023-07-06 17:32:03,192 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c has been successfully formatted.
scm_1    | 2023-07-06 17:32:03,223 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1    | 2023-07-06 17:32:03,316 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1    | 2023-07-06 17:32:03,327 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:39,900 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:32:39,901 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-07-06 17:32:39,945 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-07-06 17:32:39,947 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:39,947 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-07-06 17:32:39,952 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:32:39,955 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:32:39,957 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:32:39,958 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderStateImpl
dn2_1    | 2023-07-06 17:32:39,959 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-07-06 17:32:39,960 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn2_1    | 2023-07-06 17:32:39,982 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:33:25,046 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1    | 2023-07-06 17:32:03,329 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1    | 2023-07-06 17:32:03,352 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1    | 2023-07-06 17:32:03,354 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:32:03,449 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1    | 2023-07-06 17:32:03,455 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1    | 2023-07-06 17:32:03,468 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:03,502 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c
scm_1    | 2023-07-06 17:32:03,525 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:32:03,532 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:03,568 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:32:03,568 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1    | 2023-07-06 17:32:03,572 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1    | 2023-07-06 17:32:03,584 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1    | 2023-07-06 17:32:03,588 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:03,588 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1    | 2023-07-06 17:32:03,669 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1    | 2023-07-06 17:32:03,680 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:03,800 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1    | 2023-07-06 17:32:03,808 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1    | 2023-07-06 17:32:03,812 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1    | 2023-07-06 17:32:03,877 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1    | 2023-07-06 17:32:03,877 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1    | 2023-07-06 17:32:03,942 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: start as a follower, conf=-1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:03,943 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1    | 2023-07-06 17:32:03,945 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:32:03,992 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-07-06 17:32:04,008 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1    | 2023-07-06 17:32:04,014 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8CA20BA5223C,id=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:32:04,029 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1    | 2023-07-06 17:32:04,031 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1    | 2023-07-06 17:32:04,032 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1    | 2023-07-06 17:32:04,033 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1    | 2023-07-06 17:32:04,128 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start RPC server
scm_1    | 2023-07-06 17:32:04,913 [main] INFO server.GrpcService: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: GrpcService started, listening on 9894
scm_1    | 2023-07-06 17:32:04,940 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: Started
scm_1    | 2023-07-06 17:32:09,129 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.FollowerState: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5184392574ns, electionTimeout:5115ms
scm_1    | 2023-07-06 17:32:09,131 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:32:09,133 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1    | 2023-07-06 17:32:09,144 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1    | 2023-07-06 17:32:09,145 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
recon_1  | 2023-07-06 17:32:10,904 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a642e4b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-07-06 17:32:13,822 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d1eb214{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-3791029217210721986/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1  | 2023-07-06 17:32:13,840 [main] INFO server.AbstractConnector: Started ServerConnector@482c351d{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-07-06 17:32:13,840 [main] INFO server.Server: Started @38344ms
recon_1  | 2023-07-06 17:32:13,856 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-07-06 17:32:13,856 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-07-06 17:32:13,858 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-07-06 17:32:13,858 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-07-06 17:32:13,880 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1  | 2023-07-06 17:32:13,889 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1  | 2023-07-06 17:32:13,889 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-07-06 17:32:13,889 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:32:13,889 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-07-06 17:32:13,896 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-07-06 17:32:16,105 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b474d983ce39/10.9.0.15 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1  | 2023-07-06 17:32:18,106 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b474d983ce39/10.9.0.15 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1  | 2023-07-06 17:32:20,180 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1  | 2023-07-06 17:32:22,189 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1  | 2023-07-06 17:32:24,999 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1  | 2023-07-06 17:32:25,000 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1  | 2023-07-06 17:32:25,000 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-07-06 17:32:25,011 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-07-06 17:32:25,018 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-07-06 17:32:25,022 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
scm_1    | 2023-07-06 17:32:09,152 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:09,156 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1    | 2023-07-06 17:32:09,165 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:09,166 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1    | 2023-07-06 17:32:09,166 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
scm_1    | 2023-07-06 17:32:09,166 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1    | 2023-07-06 17:32:09,167 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: change Leader from null to 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 at term 1 for becomeLeader, leader elected after 8570ms
scm_1    | 2023-07-06 17:32:09,200 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1    | 2023-07-06 17:32:09,217 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:09,241 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:32:09,272 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:09,272 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1    | 2023-07-06 17:32:09,273 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1    | 2023-07-06 17:32:09,299 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:09,312 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1    | 2023-07-06 17:32:09,313 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderStateImpl
scm_1    | 2023-07-06 17:32:09,405 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: Starting segment from index:0
scm_1    | 2023-07-06 17:32:09,587 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:09,628 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_0
scm_1    | 2023-07-06 17:32:10,952 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: close
scm_1    | 2023-07-06 17:32:10,953 [main] INFO server.GrpcService: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown server GrpcServerProtocolService now
scm_1    | 2023-07-06 17:32:10,953 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: shutdown
scm_1    | 2023-07-06 17:32:10,953 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8CA20BA5223C,id=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:32:10,953 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderStateImpl
scm_1    | 2023-07-06 17:32:10,961 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.PendingRequests: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-PendingRequests: sendNotLeaderResponses
scm_1    | 2023-07-06 17:32:10,977 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.StateMachineUpdater: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater: set stopIndex = 0
scm_1    | 2023-07-06 17:32:10,978 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO impl.StateMachineUpdater: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater: Took a snapshot at index 0
scm_1    | 2023-07-06 17:32:10,990 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO impl.StateMachineUpdater: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1    | 2023-07-06 17:32:10,997 [main] INFO server.GrpcService: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown server GrpcServerProtocolService successfully
scm_1    | 2023-07-06 17:32:11,000 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: closes. applyIndex: 0
scm_1    | 2023-07-06 17:32:11,640 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker close()
scm_1    | 2023-07-06 17:32:11,642 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: Stopped
scm_1    | 2023-07-06 17:32:11,642 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:32:11,646 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c; layoutVersion=7; scmId=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:32:11,658 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1    | /************************************************************
recon_1  | 2023-07-06 17:32:25,306 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:49126 / 10.9.0.12:49126: output error
recon_1  | 2023-07-06 17:32:25,307 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:25,309 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:50236 / 10.9.0.13:50236: output error
recon_1  | 2023-07-06 17:32:25,309 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:25,310 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:50242 / 10.9.0.13:50242: output error
recon_1  | 2023-07-06 17:32:25,310 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:25,308 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:49138 / 10.9.0.12:49138: output error
recon_1  | 2023-07-06 17:32:25,311 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | SHUTDOWN_MSG: Shutting down StorageContainerManager at 27de9db37231/10.9.0.17
scm_1    | ************************************************************/
scm_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1    | 2023-07-06 17:32:14,422 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1    | /************************************************************
scm_1    | STARTUP_MSG: Starting StorageContainerManager
scm_1    | STARTUP_MSG:   host = 27de9db37231/10.9.0.17
scm_1    | STARTUP_MSG:   args = []
scm_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:25,308 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:47348 / 10.9.0.11:47348: output error
recon_1  | 2023-07-06 17:32:25,311 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-07-06 17:31:59,761 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-06 17:31:59,776 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-07-06 17:31:59,776 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-07-06 17:31:59,777 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-07-06 17:31:59,809 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-07-06 17:31:59,840 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:31:59,844 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-07-06 17:31:59,868 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:31:59,960 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:32:00,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-07-06 17:32:00,020 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-07-06 17:32:01,658 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-07-06 17:32:01,697 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-07-06 17:32:01,697 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-07-06 17:32:01,704 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:01,704 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:32:01,726 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:32:02,024 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-07-06 17:32:02,509 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-07-06 17:32:03,239 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-07-06 17:32:03,315 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-07-06 17:32:03,518 [main] INFO util.log: Logging initialized @28948ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-07-06 17:32:04,264 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-07-06 17:32:04,279 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-07-06 17:32:04,320 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-07-06 17:32:04,335 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-07-06 17:32:04,335 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-07-06 17:32:04,335 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-07-06 17:32:04,615 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-07-06 17:32:04,673 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-07-06 17:32:04,700 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn3_1    | 2023-07-06 17:32:04,941 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-07-06 17:32:04,942 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-07-06 17:32:04,948 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-07-06 17:32:05,030 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@18709cb2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-07-06 17:32:05,042 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b87760e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-07-06 17:32:06,094 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2407a36c{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-9942049164873000357/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-07-06 17:32:06,209 [main] INFO server.AbstractConnector: Started ServerConnector@4a642e4b{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-07-06 17:32:06,209 [main] INFO server.Server: Started @31639ms
dn3_1    | 2023-07-06 17:32:06,212 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-07-06 17:32:06,212 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-07-06 17:32:06,248 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-07-06 17:32:06,452 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-07-06 17:32:06,647 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn3_1    | 2023-07-06 17:32:06,684 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn3_1    | 2023-07-06 17:32:07,975 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn3_1    | 2023-07-06 17:32:07,975 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-07-06 17:32:07,976 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-07-06 17:32:07,977 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn3_1    | 2023-07-06 17:32:08,020 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-07-06 17:32:08,299 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn3_1    | 2023-07-06 17:32:08,591 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-07-06 17:32:11,291 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:12,292 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:13,293 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:14,294 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:15,295 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:15,323 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 5022da3b84cd/10.9.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:50236 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
scm_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
scm_1    | STARTUP_MSG:   java = 11.0.19
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:25,308 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:47354 / 10.9.0.11:47354: output error
recon_1  | 2023-07-06 17:32:25,312 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:32:26,080 [IPC Server handler 20 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
recon_1  | 2023-07-06 17:32:26,099 [IPC Server handler 20 on default port 9891] INFO node.SCMNodeManager: Registered Data node : b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5{ip: 10.9.0.12, host: restart_dn2_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:32:26,273 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95d60ec2-4839-4bc7-b249-86264ebe00e9
recon_1  | 2023-07-06 17:32:26,278 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 95d60ec2-4839-4bc7-b249-86264ebe00e9{ip: 10.9.0.13, host: restart_dn3_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:32:26,440 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 to Node DB.
recon_1  | 2023-07-06 17:32:26,441 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 95d60ec2-4839-4bc7-b249-86264ebe00e9 to Node DB.
recon_1  | 2023-07-06 17:32:26,656 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
recon_1  | 2023-07-06 17:32:26,657 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8{ip: 10.9.0.11, host: restart_dn1_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:32:26,658 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 to Node DB.
recon_1  | 2023-07-06 17:32:29,782 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151. Trying to get from SCM.
recon_1  | 2023-07-06 17:32:29,800 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 526c1993-adf9-4c08-bd3e-262cdf97b151, Nodes: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.146Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-06 17:32:29,895 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151 reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:29,976 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2. Trying to get from SCM.
recon_1  | 2023-07-06 17:32:29,985 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 661977b4-3795-4a95-9edc-87b51e7ae7a2, Nodes: 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:95d60ec2-4839-4bc7-b249-86264ebe00e9, CreationTimestamp2023-07-06T17:32:26.386Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-06 17:32:29,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2 reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:30,459 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68. Trying to get from SCM.
recon_1  | 2023-07-06 17:32:30,475 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: c56ee59e-00e2-4ece-bcce-d4a90fbe2e68, Nodes: 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.682Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-06 17:32:30,478 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:30,656 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=7176bee7-3bd9-4464-8f69-8c1413a610ac. Trying to get from SCM.
recon_1  | 2023-07-06 17:32:30,685 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 7176bee7-3bd9-4464-8f69-8c1413a610ac, Nodes: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, CreationTimestamp2023-07-06T17:32:26.662Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-06 17:32:30,755 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:31,080 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)
recon_1  | 2023-07-06 17:32:34,145 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=2ab0c656-6867-4094-952d-6c471268b20e. Trying to get from SCM.
recon_1  | 2023-07-06 17:32:34,147 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 2ab0c656-6867-4094-952d-6c471268b20e, Nodes: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.697Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-06 17:32:34,149 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:34,149 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:34,982 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)
recon_1  | 2023-07-06 17:32:34,982 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)
recon_1  | 2023-07-06 17:32:35,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:35,153 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:35,417 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:35,418 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
recon_1  | 2023-07-06 17:32:35,628 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:35,629 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:35,926 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)
recon_1  | 2023-07-06 17:32:35,927 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)
recon_1  | 2023-07-06 17:32:36,809 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:36,809 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 reported by 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)
recon_1  | 2023-07-06 17:32:39,891 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e reported by b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)
om_1     | 2023-07-06 17:32:39,764 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1     | 2023-07-06 17:32:39,788 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1     | 2023-07-06 17:32:39,796 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1     | 2023-07-06 17:32:39,797 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1     | 2023-07-06 17:32:39,799 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1     | 2023-07-06 17:32:39,800 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1     | 2023-07-06 17:32:39,800 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1     | 2023-07-06 17:32:39,800 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1     | 2023-07-06 17:32:39,807 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:32:39,808 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1     | 2023-07-06 17:32:39,812 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1     | 2023-07-06 17:32:39,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1     | 2023-07-06 17:32:39,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1     | 2023-07-06 17:32:39,872 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1     | 2023-07-06 17:32:40,616 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1     | 2023-07-06 17:32:40,622 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1     | 2023-07-06 17:32:40,624 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1     | 2023-07-06 17:32:40,624 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1     | 2023-07-06 17:32:40,625 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1     | 2023-07-06 17:32:40,629 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1     | 2023-07-06 17:32:40,640 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@b379bc6[Not completed]
om_1     | 2023-07-06 17:32:40,641 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1     | 2023-07-06 17:32:40,644 [main] INFO om.OzoneManager: Creating RPC Server
om_1     | 2023-07-06 17:32:40,674 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1     | 2023-07-06 17:32:40,682 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1     | 2023-07-06 17:32:40,682 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1     | 2023-07-06 17:32:40,683 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1     | 2023-07-06 17:32:40,685 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1     | 2023-07-06 17:32:40,687 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1     | 2023-07-06 17:32:40,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1     | 2023-07-06 17:32:40,716 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1     | 2023-07-06 17:32:40,719 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1     | 2023-07-06 17:32:40,736 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1     | 2023-07-06 17:32:40,743 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1     | 2023-07-06 17:32:40,802 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1     | 2023-07-06 17:32:40,824 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1     | 2023-07-06 17:32:40,845 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1     | 2023-07-06 17:32:40,845 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1     | 2023-07-06 17:32:41,046 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1     | 2023-07-06 17:32:41,263 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1     | 2023-07-06 17:32:41,277 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1     | 2023-07-06 17:32:41,277 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1     | 2023-07-06 17:32:41,282 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1     | 2023-07-06 17:32:41,299 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1     | 2023-07-06 17:32:41,300 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1     | 2023-07-06 17:32:42,052 [main] INFO reflections.Reflections: Reflections took 1330 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1     | 2023-07-06 17:32:42,309 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1     | 2023-07-06 17:32:42,324 [main] INFO ipc.Server: Listener at om:9862
om_1     | 2023-07-06 17:32:42,329 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1     | 2023-07-06 17:32:43,549 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1     | 2023-07-06 17:32:43,563 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1     | 2023-07-06 17:32:43,563 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1     | 2023-07-06 17:32:43,671 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/10.9.0.14:9862
om_1     | 2023-07-06 17:32:43,671 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1     | 2023-07-06 17:32:43,674 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1     | 2023-07-06 17:32:43,679 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@130e1cdd9ed4
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:50236 remote=recon/10.9.0.15:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn3_1    | 2023-07-06 17:32:16,300 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:17,302 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:18,303 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:32:23,313 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 5022da3b84cd/10.9.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:36106 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om_1     | 2023-07-06 17:32:43,686 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1     | 2023-07-06 17:32:43,689 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1     | 2023-07-06 17:32:43,701 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1     | 2023-07-06 17:32:43,701 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:32:43,705 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1     | 2023-07-06 17:32:43,706 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1     | 2023-07-06 17:32:43,711 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1     | 2023-07-06 17:32:43,722 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1     | 2023-07-06 17:32:43,723 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1     | 2023-07-06 17:32:43,724 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:32:43,736 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1     | 2023-07-06 17:32:43,737 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1     | 2023-07-06 17:32:43,738 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1     | 2023-07-06 17:32:43,740 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1     | 2023-07-06 17:32:43,741 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1     | 2023-07-06 17:32:43,741 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1     | 2023-07-06 17:32:43,743 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1     | 2023-07-06 17:32:43,745 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1     | 2023-07-06 17:32:43,745 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1     | 2023-07-06 17:32:43,763 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1     | 2023-07-06 17:32:43,764 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:32:43,794 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1     | 2023-07-06 17:32:43,796 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1     | 2023-07-06 17:32:43,797 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1     | 2023-07-06 17:32:43,811 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1     | 2023-07-06 17:32:43,812 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1     | 2023-07-06 17:32:43,819 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:32:43,823 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1     | 2023-07-06 17:32:43,825 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1     | 2023-07-06 17:32:43,831 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1     | 2023-07-06 17:32:43,833 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1     | 2023-07-06 17:32:43,833 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1     | 2023-07-06 17:32:43,835 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1     | 2023-07-06 17:32:43,835 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1     | 2023-07-06 17:32:43,836 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1     | 2023-07-06 17:32:43,837 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1     | 2023-07-06 17:32:43,840 [main] INFO server.RaftServer: om1: start RPC server
om_1     | 2023-07-06 17:32:43,899 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1     | 2023-07-06 17:32:43,906 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1     | 2023-07-06 17:32:43,912 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1     | 2023-07-06 17:32:44,000 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1     | 2023-07-06 17:32:44,001 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1     | 2023-07-06 17:32:44,054 [main] INFO util.log: Logging initialized @17603ms to org.eclipse.jetty.util.log.Slf4jLog
om_1     | 2023-07-06 17:32:44,384 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1     | 2023-07-06 17:32:44,425 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1     | 2023-07-06 17:32:44,457 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1     | 2023-07-06 17:32:44,464 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1     | 2023-07-06 17:32:44,467 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1     | 2023-07-06 17:32:44,468 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1     | 2023-07-06 17:32:44,610 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1     | 2023-07-06 17:32:44,616 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1     | 2023-07-06 17:32:44,619 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1     | 2023-07-06 17:32:44,744 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1     | 2023-07-06 17:32:44,745 [main] INFO server.session: No SessionScavenger set, using defaults
om_1     | 2023-07-06 17:32:44,751 [main] INFO server.session: node0 Scavenging every 660000ms
om_1     | 2023-07-06 17:32:44,808 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7827d7b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1     | 2023-07-06 17:32:44,809 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5b0e9e0c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1     | 2023-07-06 17:32:45,058 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@54687fd0{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12335216699925808180/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1     | 2023-07-06 17:32:45,083 [main] INFO server.AbstractConnector: Started ServerConnector@1c8e8fed{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1     | 2023-07-06 17:32:45,085 [main] INFO server.Server: Started @18635ms
om_1     | 2023-07-06 17:32:45,097 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1     | 2023-07-06 17:32:45,097 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1     | 2023-07-06 17:32:45,104 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1     | 2023-07-06 17:32:45,116 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1     | 2023-07-06 17:32:45,173 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1     | 2023-07-06 17:32:45,265 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1     | 2023-07-06 17:32:45,419 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1     | 2023-07-06 17:32:48,951 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126127414ns, electionTimeout:5116ms
om_1     | 2023-07-06 17:32:48,952 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1     | 2023-07-06 17:32:48,953 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1     | 2023-07-06 17:32:48,956 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1     | 2023-07-06 17:32:48,956 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
recon_1  | 2023-07-06 17:32:53,040 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from restart_dn3_1.restart_net.
recon_1  | 2023-07-06 17:32:53,121 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1  | 2023-07-06 17:32:55,111 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1  | 2023-07-06 17:32:55,111 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1  | 2023-07-06 17:32:55,118 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1  | 2023-07-06 17:32:55,118 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-07-06 17:32:55,121 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1  | 2023-07-06 17:32:55,122 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-07-06 17:32:55,131 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1  | 2023-07-06 17:32:55,139 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 19 milliseconds.
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:36106 remote=scm/10.9.0.17:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn3_1    | 2023-07-06 17:32:24,536 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b/container.db to cache
dn3_1    | 2023-07-06 17:32:24,536 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b/container.db for volume DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b
dn3_1    | 2023-07-06 17:32:24,559 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-07-06 17:32:24,567 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-07-06 17:32:24,750 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-07-06 17:32:24,750 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:24,839 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start RPC server
dn3_1    | 2023-07-06 17:32:24,847 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9858
dn3_1    | 2023-07-06 17:32:24,850 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9856
dn3_1    | 2023-07-06 17:32:24,855 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9857
dn3_1    | 2023-07-06 17:32:24,893 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9858 for RATIS
dn3_1    | 2023-07-06 17:32:24,893 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-07-06 17:32:24,893 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-07-06 17:32:24,895 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-95d60ec2-4839-4bc7-b249-86264ebe00e9: Started
dn3_1    | 2023-07-06 17:32:24,973 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-07-06 17:32:29,362 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-87B51E7AE7A2:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-87B51E7AE7A2:java.util.concurrent.CompletableFuture@15afbfe7[Not completed]
dn3_1    | 2023-07-06 17:32:29,461 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-87B51E7AE7A2:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:32:29,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:32:29,471 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:32:29,473 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:32:29,476 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:29,478 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:32:29,479 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:32:29,515 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: ConfigurationManager, init=-1: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:32:29,520 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1    | ************************************************************/
scm_1    | 2023-07-06 17:32:14,431 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1    | 2023-07-06 17:32:14,468 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:32:14,762 [main] INFO reflections.Reflections: Reflections took 253 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1    | 2023-07-06 17:32:14,822 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1    | 2023-07-06 17:32:14,828 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1    | 2023-07-06 17:32:15,565 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:32:15,769 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:32:55,163 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1  | 2023-07-06 17:32:55,209 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 74 milliseconds to process 0 existing database records.
recon_1  | 2023-07-06 17:32:55,254 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 44 milliseconds for processing 1 containers.
recon_1  | 2023-07-06 17:33:13,891 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-07-06 17:33:13,893 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1  | 2023-07-06 17:33:15,292 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688664793893
recon_1  | 2023-07-06 17:33:15,307 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:33:15,881 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688664793893.
recon_1  | 2023-07-06 17:33:16,006 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1  | 2023-07-06 17:33:16,538 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1  | 2023-07-06 17:33:16,576 [pool-52-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1  | 2023-07-06 17:33:16,580 [pool-52-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1  | 2023-07-06 17:33:16,580 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1  | 2023-07-06 17:33:16,581 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-07-06 17:33:16,581 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-07-06 17:33:16,656 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1  | 2023-07-06 17:33:16,657 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.075 seconds to process 2 keys.
recon_1  | 2023-07-06 17:33:16,700 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1  | 2023-07-06 17:33:16,748 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
dn3_1    | 2023-07-06 17:32:29,544 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:32:29,546 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:32:29,631 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:29,659 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:29,670 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:32:29,670 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:32:29,728 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:32:29,806 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:32:29,818 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:32:29,819 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:32:29,820 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-06 17:32:29,821 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:32:29,837 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:32:29,837 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2 does not exist. Creating ...
dn3_1    | 2023-07-06 17:32:29,865 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/in_use.lock acquired by nodename 7@5022da3b84cd
dn3_1    | 2023-07-06 17:32:29,892 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2 has been successfully formatted.
dn3_1    | 2023-07-06 17:32:29,901 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO ratis.ContainerStateMachine: group-87B51E7AE7A2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-06 17:32:29,907 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:32:29,930 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:32:29,933 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:29,984 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:32:30,005 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:32:30,032 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,059 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:32:30,059 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:32:30,085 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:30,119 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2
dn3_1    | 2023-07-06 17:32:30,120 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:32:30,122 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:30,124 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,128 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:32:30,129 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:32:30,131 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:32:30,131 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:30,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:32:30,162 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,163 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:30,198 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:30,199 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:30,200 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:30,219 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:32:30,219 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1     | 2023-07-06 17:32:48,969 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:32:48,970 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1     | 2023-07-06 17:32:48,972 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:32:48,973 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1     | 2023-07-06 17:32:48,973 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1     | 2023-07-06 17:32:48,973 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1     | 2023-07-06 17:32:48,981 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 8174ms
om_1     | 2023-07-06 17:32:49,000 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1     | 2023-07-06 17:32:49,017 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1     | 2023-07-06 17:32:49,018 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1     | 2023-07-06 17:32:49,028 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1     | 2023-07-06 17:32:49,028 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1     | 2023-07-06 17:32:49,029 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1     | 2023-07-06 17:32:49,034 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1     | 2023-07-06 17:32:49,036 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1     | 2023-07-06 17:32:49,037 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1     | 2023-07-06 17:32:49,054 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1     | 2023-07-06 17:32:49,141 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:32:49,213 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1     | 2023-07-06 17:32:49,323 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1     | [id: "om1"
om_1     | address: "om:9872"
om_1     | startupRole: FOLLOWER
om_1     | ]
om_1     | 2023-07-06 17:32:50,772 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1     | 2023-07-06 17:32:50,830 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout FILE_SYSTEM_OPTIMIZED in volume: vol1
om_1     | 2023-07-06 17:33:02,990 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombgpre0 of layout LEGACY in volume: vol1
om_1     | 2023-07-06 17:33:14,260 [qtp267527788-51] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1     | 2023-07-06 17:33:15,044 [qtp267527788-51] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1688664794996 in 47 milliseconds
om_1     | 2023-07-06 17:33:15,070 [qtp267527788-51] INFO db.RDBCheckpointUtils: Waited for 23 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1688664794996 availability.
om_1     | 2023-07-06 17:33:15,122 [qtp267527788-51] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 50 milliseconds
dn3_1    | 2023-07-06 17:32:30,223 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: start as a follower, conf=-1: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:30,224 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-07-06 17:32:30,227 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState
dn3_1    | 2023-07-06 17:32:30,253 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-87B51E7AE7A2,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:30,257 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:32:30,258 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:32:30,339 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:30,346 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-06 17:32:30,350 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:32:30,357 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:32:30,557 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2
dn3_1    | 2023-07-06 17:32:30,558 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2.
dn3_1    | 2023-07-06 17:32:30,566 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@6f47a742[Not completed]
dn3_1    | 2023-07-06 17:32:30,653 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-D4A90FBE2E68:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:32:30,654 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:32:30,654 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:32:30,655 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:32:30,659 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:30,659 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:32:30,662 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:32:30,662 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:32:30,664 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:32:30,669 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:32:30,669 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:32:30,670 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:30,670 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:30,673 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:32:30,673 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:32:30,675 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:32:30,695 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:32:30,699 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:32:30,702 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:32:30,705 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1    | 2023-07-06 17:32:16,035 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1    | 2023-07-06 17:32:16,036 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1    | 2023-07-06 17:32:16,116 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1    | 2023-07-06 17:32:16,262 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:32:16,365 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1    | 2023-07-06 17:32:16,383 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:32:16,385 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:32:16,387 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:32:16,395 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:32:16,395 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1    | 2023-07-06 17:32:16,396 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1    | 2023-07-06 17:32:16,397 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1    | 2023-07-06 17:32:16,400 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:16,401 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1    | 2023-07-06 17:32:16,409 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:32:16,431 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1    | 2023-07-06 17:32:16,435 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1    | 2023-07-06 17:32:16,435 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1    | 2023-07-06 17:32:16,627 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1    | 2023-07-06 17:32:16,629 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1    | 2023-07-06 17:32:16,630 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1    | 2023-07-06 17:32:16,630 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:32:16,630 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:32:16,633 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:32:16,636 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: found a subdirectory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c
scm_1    | 2023-07-06 17:32:16,648 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: addNew group-8CA20BA5223C:[] returns group-8CA20BA5223C:java.util.concurrent.CompletableFuture@859ea42[Not completed]
scm_1    | 2023-07-06 17:32:16,669 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: new RaftServerImpl for group-8CA20BA5223C:[] with SCMStateMachine:uninitialized
scm_1    | 2023-07-06 17:32:16,671 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1    | 2023-07-06 17:32:16,672 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1    | 2023-07-06 17:32:16,672 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1    | 2023-07-06 17:32:16,672 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:32:16,673 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:32:16,673 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1    | 2023-07-06 17:32:16,680 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1    | 2023-07-06 17:32:16,680 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:32:16,683 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1    | 2023-07-06 17:32:16,684 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1    | 2023-07-06 17:32:16,700 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1    | 2023-07-06 17:32:16,703 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:16,707 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1    | 2023-07-06 17:32:16,708 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1    | 2023-07-06 17:32:16,733 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1    | 2023-07-06 17:32:16,832 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:32:16,835 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1    | 2023-07-06 17:32:16,836 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1    | 2023-07-06 17:32:16,838 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1    | 2023-07-06 17:32:16,838 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1    | 2023-07-06 17:32:16,839 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1    | 2023-07-06 17:32:16,841 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1    | 2023-07-06 17:32:16,841 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1    | 2023-07-06 17:32:16,842 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
dn3_1    | 2023-07-06 17:32:30,706 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:32:30,706 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:32:30,709 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 does not exist. Creating ...
dn3_1    | 2023-07-06 17:32:30,715 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 7@5022da3b84cd
dn3_1    | 2023-07-06 17:32:30,718 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 has been successfully formatted.
dn3_1    | 2023-07-06 17:32:30,763 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-06 17:32:30,763 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:32:30,769 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:32:30,775 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:30,775 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:32:30,780 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:32:30,784 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,790 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:32:30,790 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:32:30,796 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:30,797 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn3_1    | 2023-07-06 17:32:30,801 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:32:30,801 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:30,802 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,802 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:32:30,803 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:32:30,803 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:32:30,806 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:30,806 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:32:30,815 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:30,825 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:31,423 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-95d60ec2-4839-4bc7-b249-86264ebe00e9: Detected pause in JVM or host machine approximately 0.485s with 0.590s GC time.
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=27ms
dn3_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=563ms
dn3_1    | 2023-07-06 17:32:31,464 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:31,465 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:31,465 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:31,465 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:32:31,465 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:32:31,467 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:31,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-07-06 17:32:31,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState
dn3_1    | 2023-07-06 17:32:31,476 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:31,476 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:31,477 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om_1     | 2023-07-06 17:33:15,126 [qtp267527788-51] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1     | 2023-07-06 17:33:15,144 [qtp267527788-51] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688664794996
scm_1    | 2023-07-06 17:32:16,866 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1    | 2023-07-06 17:32:16,902 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1    | 2023-07-06 17:32:16,904 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1    | 2023-07-06 17:32:16,909 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1    | 2023-07-06 17:32:16,911 [main] INFO ha.SequenceIdGenerator: upgrade rootCertificateId to 1
scm_1    | 2023-07-06 17:32:16,913 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1    | 2023-07-06 17:32:16,967 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1    | 2023-07-06 17:32:16,984 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1    | 2023-07-06 17:32:16,986 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-07-06 17:32:16,992 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1    | 2023-07-06 17:32:17,005 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1    | 2023-07-06 17:32:17,006 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-07-06 17:32:17,014 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1    | 2023-07-06 17:32:17,014 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:32:17,017 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1    | 2023-07-06 17:32:17,019 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1    | 2023-07-06 17:32:17,025 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1    | 2023-07-06 17:32:17,026 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1    | 2023-07-06 17:32:17,061 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-07-06 17:32:17,061 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-07-06 17:32:17,086 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1    | 2023-07-06 17:32:17,146 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1    | 2023-07-06 17:32:17,176 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1    | 2023-07-06 17:32:17,176 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1    | WARNING: An illegal reflective access operation has occurred
scm_1    | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm_1    | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1    | WARNING: All illegal access operations will be denied in a future release
scm_1    | 2023-07-06 17:32:17,198 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1    | 2023-07-06 17:32:17,201 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:17,204 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1    | 2023-07-06 17:32:17,289 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1    | 2023-07-06 17:32:17,901 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:32:17,960 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:32:18,077 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1    | 2023-07-06 17:32:18,079 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1    | 2023-07-06 17:32:18,189 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:32:18,196 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:32:18,198 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1    | 2023-07-06 17:32:18,203 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1    | 2023-07-06 17:32:18,243 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:32:18,251 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:32:18,252 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1    | 2023-07-06 17:32:18,253 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1    | 2023-07-06 17:32:18,360 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1    | 2023-07-06 17:32:18,361 [main] INFO server.StorageContainerManager: 
scm_1    | Container Balancer status:
scm_1    | Key                            Value
scm_1    | Running                        false
scm_1    | Container Balancer Configuration values:
scm_1    | Key                                                Value
scm_1    | Threshold                                          10
scm_1    | Max Datanodes to Involve per Iteration(percent)    20
scm_1    | Max Size to Move per Iteration                     500GB
scm_1    | Max Size Entering Target per Iteration             26GB
scm_1    | Max Size Leaving Source per Iteration              26GB
scm_1    | 
scm_1    | 2023-07-06 17:32:18,361 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1    | 2023-07-06 17:32:18,368 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1    | 2023-07-06 17:32:18,375 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1    | 2023-07-06 17:32:18,383 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/in_use.lock acquired by nodename 7@27de9db37231
dn3_1    | 2023-07-06 17:32:31,477 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:32:31,477 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:32:31,478 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:32:31,512 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn3_1    | 2023-07-06 17:32:31,531 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:32:35,118 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68.
dn3_1    | 2023-07-06 17:32:35,119 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@4e332164[Not completed]
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-6C471268B20E:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:35,126 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:32:35,127 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:32:35,127 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: ConfigurationManager, init=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:32:35,131 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:32:35,132 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:32:35,135 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:32:35,141 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e does not exist. Creating ...
dn3_1    | 2023-07-06 17:32:35,143 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 7@5022da3b84cd
dn3_1    | 2023-07-06 17:32:35,146 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e has been successfully formatted.
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO ratis.ContainerStateMachine: group-6C471268B20E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:32:35,147 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:32:35,148 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:32:35,156 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:32:35,159 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:35,173 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:35,173 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:32:35,173 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:35,174 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:32:35,174 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:32:35,175 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: start as a follower, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:35,175 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-07-06 17:32:35,176 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:32:35,177 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:32:35,177 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:35,196 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-06 17:32:35,196 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-06 17:32:35,196 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:32:35,196 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:32:35,197 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:32:35,197 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=2ab0c656-6867-4094-952d-6c471268b20e
dn3_1    | 2023-07-06 17:32:35,524 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5294345336ns, electionTimeout:5193ms
dn3_1    | 2023-07-06 17:32:35,525 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState
dn3_1    | 2023-07-06 17:32:35,525 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-07-06 17:32:35,570 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-06 17:32:35,570 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1
dn3_1    | 2023-07-06 17:32:35,584 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=2ab0c656-6867-4094-952d-6c471268b20e.
dn3_1    | 2023-07-06 17:32:35,596 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:35,597 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
dn3_1    | 2023-07-06 17:32:35,605 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:35,605 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-07-06 17:32:35,605 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1
dn3_1    | 2023-07-06 17:32:35,608 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-07-06 17:32:35,609 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-87B51E7AE7A2 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:35,617 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 1 for becomeLeader, leader elected after 5982ms
dn3_1    | 2023-07-06 17:32:35,661 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-06 17:32:35,687 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:35,688 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-06 17:32:35,698 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-06 17:32:35,701 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-06 17:32:35,721 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-06 17:32:35,765 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:35,780 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-06 17:32:35,786 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderStateImpl
dn3_1    | 2023-07-06 17:32:35,853 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-07-06 17:32:35,953 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: set configuration 0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:36,168 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/log_inprogress_0
dn3_1    | 2023-07-06 17:32:36,595 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-D4A90FBE2E68, 0, (t:0, i:0))
dn3_1    | 2023-07-06 17:32:36,596 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5127673000ns, electionTimeout:5064ms
dn3_1    | 2023-07-06 17:32:36,596 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState
dn3_1    | 2023-07-06 17:32:36,596 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-07-06 17:32:36,596 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-06 17:32:36,597 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2
dn3_1    | 2023-07-06 17:32:36,609 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-CANDIDATE: reject PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 1 > candidate's priority 0
scm_1    | 2023-07-06 17:32:18,390 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5} from /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/raft-meta
scm_1    | 2023-07-06 17:32:18,475 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:18,483 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1    | 2023-07-06 17:32:18,513 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1    | 2023-07-06 17:32:18,514 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:18,518 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1    | 2023-07-06 17:32:18,519 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1    | 2023-07-06 17:32:18,525 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:32:18,537 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1    | 2023-07-06 17:32:18,538 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1    | 2023-07-06 17:32:18,539 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:18,545 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c
scm_1    | 2023-07-06 17:32:18,546 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:32:18,547 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:18,549 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:32:18,550 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1    | 2023-07-06 17:32:18,551 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1    | 2023-07-06 17:32:18,552 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1    | 2023-07-06 17:32:18,552 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:18,553 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1    | 2023-07-06 17:32:18,563 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1    | 2023-07-06 17:32:18,564 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:18,577 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1    | 2023-07-06 17:32:18,579 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1    | 2023-07-06 17:32:18,580 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1    | 2023-07-06 17:32:18,609 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:18,610 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_0
scm_1    | 2023-07-06 17:32:18,618 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1    | 2023-07-06 17:32:18,689 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: start as a follower, conf=0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:18,689 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1    | 2023-07-06 17:32:18,690 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:32:18,696 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-07-06 17:32:18,697 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1    | 2023-07-06 17:32:18,703 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8CA20BA5223C,id=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:32:18,705 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1    | 2023-07-06 17:32:18,705 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1    | 2023-07-06 17:32:18,706 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1    | 2023-07-06 17:32:18,706 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1    | 2023-07-06 17:32:18,711 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start RPC server
scm_1    | 2023-07-06 17:32:18,778 [main] INFO server.GrpcService: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: GrpcService started, listening on 9894
dn3_1    | 2023-07-06 17:32:36,634 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68 replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:FAIL-t0. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68:t0, leader=null, voted=, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:36,635 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:36,642 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:32:36,642 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:32:36,667 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:32:36,652 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn3_1    | 2023-07-06 17:32:36,736 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-06 17:32:36,736 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 0: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t0
dn3_1    | 2023-07-06 17:32:36,738 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-07-06 17:32:36,742 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:36,760 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:32:36,771 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:32:36,803 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 0: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t1
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 ELECTION round 0: result PASSED
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 1 for becomeLeader, leader elected after 6134ms
dn3_1    | 2023-07-06 17:32:36,804 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:32:36,805 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-06 17:32:36,866 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-06 17:32:36,868 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:32:18,781 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: Started
scm_1    | 2023-07-06 17:32:18,792 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1    | 2023-07-06 17:32:18,792 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1    | 2023-07-06 17:32:18,795 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1    | 2023-07-06 17:32:18,795 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1    | 2023-07-06 17:32:18,795 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1    | 2023-07-06 17:32:18,953 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1    | 2023-07-06 17:32:18,974 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1    | 2023-07-06 17:32:18,974 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1    | 2023-07-06 17:32:19,254 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1    | 2023-07-06 17:32:19,255 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:32:19,273 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1    | 2023-07-06 17:32:19,372 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1    | 2023-07-06 17:32:19,373 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1    | 2023-07-06 17:32:19,373 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1    | 2023-07-06 17:32:19,373 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:32:19,506 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1    | 2023-07-06 17:32:19,506 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1    | 2023-07-06 17:32:19,608 [main] INFO util.log: Logging initialized @7291ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1    | 2023-07-06 17:32:19,807 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1    | 2023-07-06 17:32:19,814 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1    | 2023-07-06 17:32:19,823 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1    | 2023-07-06 17:32:19,825 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1    | 2023-07-06 17:32:19,825 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1    | 2023-07-06 17:32:19,825 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1    | 2023-07-06 17:32:19,863 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1    | 2023-07-06 17:32:19,864 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1    | 2023-07-06 17:32:19,865 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1    | 2023-07-06 17:32:19,910 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1    | 2023-07-06 17:32:19,910 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1    | 2023-07-06 17:32:19,911 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1    | 2023-07-06 17:32:19,928 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1182413a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1    | 2023-07-06 17:32:19,929 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@362a561e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1    | 2023-07-06 17:32:20,076 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27afbf14{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-18139776971458420669/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1    | 2023-07-06 17:32:20,099 [main] INFO server.AbstractConnector: Started ServerConnector@77ccded4{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1    | 2023-07-06 17:32:20,099 [main] INFO server.Server: Started @7785ms
scm_1    | 2023-07-06 17:32:20,101 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1    | 2023-07-06 17:32:20,101 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1    | 2023-07-06 17:32:20,103 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1    | 2023-07-06 17:32:23,888 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.FollowerState: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5198049326ns, electionTimeout:5190ms
scm_1    | 2023-07-06 17:32:23,889 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:32:23,890 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1    | 2023-07-06 17:32:23,894 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1    | 2023-07-06 17:32:23,894 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
scm_1    | 2023-07-06 17:32:23,896 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:23,898 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1    | 2023-07-06 17:32:23,912 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:23,912 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: result PASSED (term=2)
dn3_1    | 2023-07-06 17:32:36,870 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-06 17:32:36,874 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-06 17:32:36,878 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-06 17:32:36,878 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:32:36,880 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-06 17:32:36,884 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-06 17:32:36,884 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:32:36,890 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:32:36,901 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-06 17:32:36,903 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:32:36,904 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-06 17:32:36,904 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-06 17:32:36,904 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-06 17:32:36,904 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:32:36,904 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-06 17:32:36,921 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-06 17:32:36,921 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:32:36,924 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:32:36,927 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderStateImpl
dn3_1    | 2023-07-06 17:32:36,933 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-07-06 17:32:36,938 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn3_1    | 2023-07-06 17:32:36,961 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:39,824 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 0, (t:0, i:0))
dn3_1    | 2023-07-06 17:32:39,824 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FOLLOWER: accept PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-06 17:32:39,826 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t0. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E:t0, leader=null, voted=, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:39,846 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: receive requestVote(ELECTION, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 1, (t:0, i:0))
dn3_1    | 2023-07-06 17:32:39,846 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FOLLOWER: accept ELECTION from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-06 17:32:39,847 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:32:39,847 [grpc-default-executor-1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:32:39,847 [grpc-default-executor-1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:32:39,847 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState was interrupted
scm_1    | 2023-07-06 17:32:23,913 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
scm_1    | 2023-07-06 17:32:23,913 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1    | 2023-07-06 17:32:23,914 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1    | 2023-07-06 17:32:23,914 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1    | 2023-07-06 17:32:23,923 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: change Leader from null to 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 at term 2 for becomeLeader, leader elected after 7214ms
scm_1    | 2023-07-06 17:32:23,933 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1    | 2023-07-06 17:32:23,938 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:23,938 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:32:23,943 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1    | 2023-07-06 17:32:23,943 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1    | 2023-07-06 17:32:23,944 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1    | 2023-07-06 17:32:23,951 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:32:23,953 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1    | 2023-07-06 17:32:23,954 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderStateImpl
scm_1    | 2023-07-06 17:32:23,959 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1    | 2023-07-06 17:32:23,966 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_0 to /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_0-0
scm_1    | 2023-07-06 17:32:23,967 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:32:23,993 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_1
scm_1    | 2023-07-06 17:32:24,003 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1    | 2023-07-06 17:32:24,006 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1    | 2023-07-06 17:32:24,009 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:24,010 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1    | 2023-07-06 17:32:24,011 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1    | 2023-07-06 17:32:24,012 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1    | 2023-07-06 17:32:24,022 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1    | 2023-07-06 17:32:24,036 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:32:24,238 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:59442 / 10.9.0.11:59442: output error
scm_1    | 2023-07-06 17:32:24,238 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn3_1    | 2023-07-06 17:32:39,861 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E replies to ELECTION vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t1. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:40,057 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:32:40,059 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 1 for appendEntries, leader elected after 4925ms
dn3_1    | 2023-07-06 17:32:40,138 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:32:40,140 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread2] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-07-06 17:32:40,142 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn3_1    | 2023-07-06 17:32:54,637 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-95d60ec2-4839-4bc7-b249-86264ebe00e9: Detected pause in JVM or host machine approximately 0.142s with 0.162s GC time.
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=162ms
dn3_1    | 2023-07-06 17:33:24,986 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1    | 2023-07-06 17:32:24,246 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:36106 / 10.9.0.13:36106: output error
scm_1    | 2023-07-06 17:32:24,255 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1    | 2023-07-06 17:32:24,257 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:33470 / 10.9.0.12:33470: output error
scm_1    | 2023-07-06 17:32:24,257 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1    | 2023-07-06 17:32:26,088 [IPC Server handler 42 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
scm_1    | 2023-07-06 17:32:26,096 [IPC Server handler 42 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5{ip: 10.9.0.12, host: restart_dn2_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:32:26,104 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:32:26,108 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1    | 2023-07-06 17:32:26,109 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:32:26,129 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1    | 2023-07-06 17:32:26,147 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151 to datanode:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
scm_1    | 2023-07-06 17:32:26,276 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:32:26,282 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 95d60ec2-4839-4bc7-b249-86264ebe00e9{ip: 10.9.0.13, host: restart_dn3_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:32:26,283 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:32:26,283 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1    | 2023-07-06 17:32:26,363 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:26,380 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 526c1993-adf9-4c08-bd3e-262cdf97b151, Nodes: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.146991Z[UTC]]
scm_1    | 2023-07-06 17:32:26,386 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2 to datanode:95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:32:26,398 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:26,402 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 661977b4-3795-4a95-9edc-87b51e7ae7a2, Nodes: 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.386565Z[UTC]]
scm_1    | 2023-07-06 17:32:26,660 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
scm_1    | 2023-07-06 17:32:26,660 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8{ip: 10.9.0.11, host: restart_dn1_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:32:26,661 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:32:26,662 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1    | 2023-07-06 17:32:26,662 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7176bee7-3bd9-4464-8f69-8c1413a610ac to datanode:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
scm_1    | 2023-07-06 17:32:26,662 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:32:26,663 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1    | 2023-07-06 17:32:26,663 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1    | 2023-07-06 17:32:26,663 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:32:26,671 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:26,673 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 7176bee7-3bd9-4464-8f69-8c1413a610ac, Nodes: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.662582Z[UTC]]
scm_1    | 2023-07-06 17:32:26,682 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 to datanode:95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:32:26,682 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 to datanode:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
scm_1    | 2023-07-06 17:32:26,682 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 to datanode:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
scm_1    | 2023-07-06 17:32:26,692 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:26,693 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: c56ee59e-00e2-4ece-bcce-d4a90fbe2e68, Nodes: 95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13)9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.682003Z[UTC]]
scm_1    | 2023-07-06 17:32:26,698 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2ab0c656-6867-4094-952d-6c471268b20e to datanode:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
scm_1    | 2023-07-06 17:32:26,698 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2ab0c656-6867-4094-952d-6c471268b20e to datanode:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
scm_1    | 2023-07-06 17:32:26,698 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2ab0c656-6867-4094-952d-6c471268b20e to datanode:95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:32:26,706 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:26,709 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=2ab0c656-6867-4094-952d-6c471268b20e contains same datanodes as previous pipelines: PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68 nodeIds: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, 95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:32:26,710 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 2ab0c656-6867-4094-952d-6c471268b20e, Nodes: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8(restart_dn1_1.restart_net/10.9.0.11)b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5(restart_dn2_1.restart_net/10.9.0.12)95d60ec2-4839-4bc7-b249-86264ebe00e9(restart_dn3_1.restart_net/10.9.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-06T17:32:26.697968Z[UTC]]
scm_1    | 2023-07-06 17:32:26,711 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:32:26,712 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:32:29,872 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:29,873 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=526c1993-adf9-4c08-bd3e-262cdf97b151
scm_1    | 2023-07-06 17:32:29,878 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:29,996 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:29,997 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=661977b4-3795-4a95-9edc-87b51e7ae7a2
scm_1    | 2023-07-06 17:32:30,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:30,438 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:30,598 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:30,602 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=7176bee7-3bd9-4464-8f69-8c1413a610ac
scm_1    | 2023-07-06 17:32:30,609 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:30,752 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:31,082 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:34,143 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:34,984 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:35,154 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:35,417 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:35,619 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:35,940 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:36,815 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:36,829 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:32:36,832 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
scm_1    | 2023-07-06 17:32:36,833 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:32:36,834 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:32:36,834 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1    | 2023-07-06 17:32:36,835 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1    | 2023-07-06 17:32:36,835 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-07-06 17:32:36,835 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1    | 2023-07-06 17:32:36,838 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1    | 2023-07-06 17:32:36,841 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1    | 2023-07-06 17:32:36,857 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1    | 2023-07-06 17:32:36,858 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1    | 2023-07-06 17:32:39,924 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=2ab0c656-6867-4094-952d-6c471268b20e
scm_1    | 2023-07-06 17:32:50,921 [IPC Server handler 5 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1    | 2023-07-06 17:32:50,947 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1    | 2023-07-06 17:32:50,953 [IPC Server handler 5 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1    | 2023-07-06 17:32:56,714 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:33:10,440 [IPC Server handler 85 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:33:10,602 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:33:12,422 [IPC Server handler 40 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:33:26,715 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
Attaching to restart_om_1, restart_s3g_1, restart_scm_1, restart_dn2_1, restart_recon_1, restart_dn3_1, restart_dn1_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-07-06 17:34:01,866 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = f487a00cfd86/10.9.0.11
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn1_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-07-06 17:34:01,936 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-07-06 17:34:02,164 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-07-06 17:34:02,651 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-07-06 17:34:03,380 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-07-06 17:34:03,380 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-07-06 17:34:04,346 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:f487a00cfd86 ip:10.9.0.11
dn1_1    | 2023-07-06 17:34:05,243 [main] INFO reflections.Reflections: Reflections took 698 ms to scan 2 urls, producing 107 keys and 231 values 
dn1_1    | 2023-07-06 17:34:07,163 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-07-06 17:34:07,458 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn1_1    | 2023-07-06 17:34:08,433 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 116736 at 2023-07-06T17:33:43.589Z
dn1_1    | 2023-07-06 17:34:08,537 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-07-06 17:34:08,546 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-07-06 17:34:08,562 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-07-06 17:34:08,614 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-07-06 17:34:08,708 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-06 17:34:08,711 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-06T17:33:43.602Z
dn1_1    | 2023-07-06 17:34:08,732 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-07-06 17:34:08,740 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-07-06 17:34:08,740 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-07-06 17:34:10,737 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-c43c1815-e08e-44ec-acac-dc5c81b07827/container.db to cache
dn1_1    | 2023-07-06 17:34:10,737 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-c43c1815-e08e-44ec-acac-dc5c81b07827/container.db for volume DS-c43c1815-e08e-44ec-acac-dc5c81b07827
dn1_1    | 2023-07-06 17:34:10,815 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-07-06 17:34:11,728 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-07-06 17:34:11,728 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-07-06 17:34:18,877 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-07-06 17:34:19,401 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-06 17:34:19,814 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-07-06 17:34:19,920 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-06 17:34:19,922 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-07-06 17:34:19,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-06 17:34:19,955 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-07-06 17:34:19,956 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-07-06 17:34:19,956 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-07-06 17:34:19,957 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-07-06 17:34:19,958 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:19,964 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-07-06 17:34:19,974 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:34:20,032 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-07-06 17:34:20,054 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-07-06 17:34:20,073 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-07-06 17:34:21,746 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-07-06 17:34:21,782 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-07-06 17:34:21,799 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-07-06 17:34:21,804 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:21,815 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:34:21,866 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:34:21,887 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: found a subdirectory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn1_1    | 2023-07-06 17:34:21,977 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-6C471268B20E:[] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@212d080b[Not completed]
dn1_1    | 2023-07-06 17:34:21,980 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: found a subdirectory /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac
dn1_1    | 2023-07-06 17:34:21,980 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-8C1413A610AC:[] returns group-8C1413A610AC:java.util.concurrent.CompletableFuture@366089ba[Not completed]
dn1_1    | 2023-07-06 17:34:21,980 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: found a subdirectory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn1_1    | 2023-07-06 17:34:21,982 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: addNew group-D4A90FBE2E68:[] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@6bcf93f6[Not completed]
dn1_1    | 2023-07-06 17:34:22,177 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-07-06 17:34:22,379 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-6C471268B20E:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:34:22,456 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:34:22,481 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:34:22,482 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:34:22,482 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:22,483 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:34:22,483 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:34:22,588 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:34:22,591 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:34:22,697 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:34:22,720 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:34:22,735 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-07-06 17:34:22,908 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:22,915 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:22,983 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:34:22,996 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:34:23,246 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:34:23,573 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:34:23,585 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:34:23,595 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,598 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,628 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,652 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,654 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-8C1413A610AC:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:34:23,688 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:34:23,688 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:34:23,688 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:34:23,688 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:23,689 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:34:23,689 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:34:23,697 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:34:23,698 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:34:23,699 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:34:23,702 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:34:23,702 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:23,703 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:23,708 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:34:23,708 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:34:23,708 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:34:23,710 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:34:23,738 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:34:23,738 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,739 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,744 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,744 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,754 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: new RaftServerImpl for group-D4A90FBE2E68:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-06 17:34:23,760 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-06 17:34:23,760 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-06 17:34:23,764 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-06 17:34:23,766 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:23,767 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-06 17:34:23,779 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-06 17:34:23,784 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-06 17:34:23,784 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-06 17:34:23,784 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-06 17:34:23,784 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-06 17:34:23,785 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-06 17:34:23,785 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:23,786 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-06 17:34:23,786 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-06 17:34:23,786 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-06 17:34:23,794 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-06 17:34:23,800 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-06 17:34:23,800 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,800 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,803 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-06 17:34:23,803 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-06 17:34:23,813 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-07-06 17:34:23,906 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-07-06 17:34:24,030 [main] INFO util.log: Logging initialized @29665ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-07-06 17:34:24,886 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-07-06 17:34:24,905 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-07-06 17:34:24,952 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-07-06 17:34:24,963 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-07-06 17:34:24,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-07-06 17:34:24,974 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-07-06 17:34:25,319 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-07-06 17:34:25,350 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-07-06 17:34:25,351 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn1_1    | 2023-07-06 17:34:25,653 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-07-06 17:34:25,672 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-07-06 17:34:25,673 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-07-06 17:34:25,800 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@72ecbcb3{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-07-06 17:34:25,805 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e85c21b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-07-06 17:34:26,594 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15d236fd{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17389927851269474633/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-07-06 17:34:26,688 [main] INFO server.AbstractConnector: Started ServerConnector@3a11c0eb{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-07-06 17:34:26,689 [main] INFO server.Server: Started @32324ms
dn1_1    | 2023-07-06 17:34:26,701 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-07-06 17:34:26,702 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-07-06 17:34:26,704 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-07-06 17:34:27,109 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-07-06 17:34:27,353 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn1_1    | 2023-07-06 17:34:27,384 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-07-06 17:34:28,789 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 2023-07-06 17:34:28,789 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 2023-07-06 17:34:28,793 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-07-06 17:34:28,866 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 2023-07-06 17:34:28,903 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-07-06 17:34:29,216 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn1_1    | 2023-07-06 17:34:29,268 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-07-06 17:34:32,056 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:34:33,057 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:34:34,058 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-06 17:34:36,141 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From f487a00cfd86/10.9.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:35760 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:35760 remote=recon/10.9.0.15:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-06 17:34:39,084 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From f487a00cfd86/10.9.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:59494 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.11:59494 remote=scm/10.9.0.17:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-06 17:34:40,459 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-07-06 17:34:40,479 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-07-06 17:34:40,732 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-07-06 17:34:40,734 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:34:40,805 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 7@f487a00cfd86
dn1_1    | 2023-07-06 17:34:40,843 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5} from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/raft-meta
dn1_1    | 2023-07-06 17:34:40,868 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/in_use.lock acquired by nodename 7@f487a00cfd86
dn1_1    | 2023-07-06 17:34:40,899 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8} from /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/raft-meta
dn1_1    | 2023-07-06 17:34:40,910 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 7@f487a00cfd86
dn1_1    | 2023-07-06 17:34:40,911 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=95d60ec2-4839-4bc7-b249-86264ebe00e9} from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/raft-meta
dn1_1    | 2023-07-06 17:34:41,029 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: set configuration 0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:41,012 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:41,030 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:41,088 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO ratis.ContainerStateMachine: group-8C1413A610AC: Setting the last applied index to (t:1, i:0)
dn1_1    | 2023-07-06 17:34:41,091 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO ratis.ContainerStateMachine: group-6C471268B20E: Setting the last applied index to (t:1, i:200)
dn1_1    | 2023-07-06 17:34:41,099 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: Setting the last applied index to (t:1, i:4)
dn1_1    | 2023-07-06 17:34:41,598 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:34:41,598 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:34:41,616 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-06 17:34:41,665 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-06 17:34:41,665 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,670 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:34:41,665 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-06 17:34:41,670 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,671 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:34:41,671 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:34:41,674 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,669 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-06 17:34:41,704 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,709 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-06 17:34:41,709 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:34:41,680 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-06 17:34:41,735 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,736 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,741 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:34:41,746 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:34:41,746 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,744 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:34:41,770 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:34:41,771 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,764 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn1_1    | 2023-07-06 17:34:41,785 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-06 17:34:41,772 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac
dn1_1    | 2023-07-06 17:34:41,789 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:34:41,789 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:34:41,790 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,793 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:34:41,802 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:34:41,803 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:34:41,809 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:41,810 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:34:41,809 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-06 17:34:41,812 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,816 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:34:41,817 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:34:41,818 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:41,818 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:34:41,820 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-06 17:34:41,820 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:34:41,844 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,844 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-06 17:34:41,844 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-06 17:34:41,849 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-06 17:34:41,851 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-06 17:34:41,852 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-06 17:34:41,872 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,873 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:41,873 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:41,894 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:42,735 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:34:42,742 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:34:42,738 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-06 17:34:42,747 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:34:42,751 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:42,811 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:34:42,813 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:34:42,813 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:42,887 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-07-06 17:34:02,598 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 1ffe21382799/10.9.0.12
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn2_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-07-06 17:34:02,318 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = e99cd87b06ef/10.9.0.13
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
dn3_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-07-06 17:34:02,423 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-07-06 17:34:02,656 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-07-06 17:34:03,206 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-07-06 17:34:04,268 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-07-06 17:34:04,269 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-07-06 17:34:05,513 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e99cd87b06ef ip:10.9.0.13
dn3_1    | 2023-07-06 17:34:06,496 [main] INFO reflections.Reflections: Reflections took 737 ms to scan 2 urls, producing 107 keys and 231 values 
dn3_1    | 2023-07-06 17:34:08,652 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn3_1    | 2023-07-06 17:34:08,889 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn3_1    | 2023-07-06 17:34:09,695 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 116736 at 2023-07-06T17:33:43.533Z
dn3_1    | 2023-07-06 17:34:09,767 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-07-06 17:34:09,789 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-07-06 17:34:09,793 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-07-06 17:34:09,856 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-07-06 17:34:09,921 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-07-06 17:34:09,928 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-06T17:33:43.535Z
dn3_1    | 2023-07-06 17:34:09,940 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-07-06 17:34:09,940 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-07-06 17:34:02,638 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-07-06 17:34:02,933 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-07-06 17:34:03,497 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-07-06 17:34:04,177 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-07-06 17:34:04,177 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-07-06 17:34:05,107 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1ffe21382799 ip:10.9.0.12
dn2_1    | 2023-07-06 17:34:06,085 [main] INFO reflections.Reflections: Reflections took 751 ms to scan 2 urls, producing 107 keys and 231 values 
dn2_1    | 2023-07-06 17:34:08,072 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-07-06 17:34:08,343 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn2_1    | 2023-07-06 17:34:09,191 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 116736 at 2023-07-06T17:33:43.539Z
dn2_1    | 2023-07-06 17:34:09,311 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-07-06 17:34:09,322 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-07-06 17:34:09,323 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-07-06 17:34:09,531 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-07-06 17:34:09,549 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-06 17:34:09,574 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-06T17:33:43.543Z
dn2_1    | 2023-07-06 17:34:09,596 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-07-06 17:34:09,597 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-07-06 17:34:42,887 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-06 17:34:42,887 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:42,947 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:42,949 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:42,949 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: set configuration 0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:42,964 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/log_inprogress_0
dn1_1    | 2023-07-06 17:34:42,992 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:34:43,005 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn1_1    | 2023-07-06 17:34:43,022 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-07-06 17:34:43,024 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:34:43,219 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO segmented.LogSegment: Successfully read 201 entries from segment file /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn1_1    | 2023-07-06 17:34:43,221 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 200
dn1_1    | 2023-07-06 17:34:43,223 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-06 17:34:43,328 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:43,334 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn1_1    | 2023-07-06 17:34:43,341 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:34:43,382 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:34:43,385 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:34:43,401 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:43,413 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn1_1    | 2023-07-06 17:34:43,395 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:43,393 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: start as a follower, conf=0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:43,416 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn1_1    | 2023-07-06 17:34:43,414 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:34:43,413 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:34:43,417 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState
dn1_1    | 2023-07-06 17:34:43,436 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-06 17:34:43,438 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:34:43,442 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:34:43,465 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:34:43,478 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C1413A610AC,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:34:44,040 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:44,076 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-06 17:34:44,076 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:34:44,076 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:34:43,478 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:34:44,036 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:34:44,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-06 17:34:44,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-06 17:34:44,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-06 17:34:44,078 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-06 17:34:44,034 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:34:44,091 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:34:44,151 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start RPC server
dn1_1    | 2023-07-06 17:34:44,172 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9858
dn1_1    | 2023-07-06 17:34:44,176 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9856
dn1_1    | 2023-07-06 17:34:44,179 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: GrpcService started, listening on 9857
dn1_1    | 2023-07-06 17:34:44,194 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9858 for RATIS
dn1_1    | 2023-07-06 17:34:44,196 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-07-06 17:34:44,196 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-07-06 17:34:44,197 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: Started
dn1_1    | 2023-07-06 17:34:44,368 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-06 17:34:48,562 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5221955652ns, electionTimeout:5143ms
dn1_1    | 2023-07-06 17:34:48,564 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:34:48,564 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn1_1    | 2023-07-06 17:34:48,567 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-07-06 17:34:48,568 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1
dn1_1    | 2023-07-06 17:34:48,588 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:48,666 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn1_1    | 2023-07-06 17:34:48,684 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:34:48,695 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:34:48,711 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:34:49,221 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5804592067ns, electionTimeout:5124ms
dn1_1    | 2023-07-06 17:34:49,223 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState
dn1_1    | 2023-07-06 17:34:49,226 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn1_1    | 2023-07-06 17:34:49,226 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-07-06 17:34:49,226 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2
dn1_1    | 2023-07-06 17:34:49,250 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5836414235ns, electionTimeout:5170ms
dn1_1    | 2023-07-06 17:34:49,250 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:34:49,250 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn1_1    | 2023-07-06 17:34:49,250 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-07-06 17:34:49,252 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3
dn1_1    | 2023-07-06 17:34:49,278 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:49,293 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:49,296 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2 PRE_VOTE round 0: result PASSED (term=1)
dn1_1    | 2023-07-06 17:34:49,368 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-06 17:34:49,370 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-06 17:34:49,400 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for 0: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:49,400 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2 ELECTION round 0: result PASSED (term=2)
dn1_1    | 2023-07-06 17:34:49,401 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2
dn1_1    | 2023-07-06 17:34:49,402 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn1_1    | 2023-07-06 17:34:49,403 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C1413A610AC with new leaderId: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn1_1    | 2023-07-06 17:34:49,406 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: change Leader from null to 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8 at term 2 for becomeLeader, leader elected after 25700ms
dn1_1    | 2023-07-06 17:34:49,523 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-07-06 17:34:49,699 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:34:49,723 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-07-06 17:34:49,828 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-07-06 17:34:49,831 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-07-06 17:34:49,838 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-07-06 17:34:49,919 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-06 17:34:49,952 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-07-06 17:34:49,977 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderStateImpl
dn2_1    | 2023-07-06 17:34:09,597 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-07-06 17:34:11,480 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b/container.db to cache
dn2_1    | 2023-07-06 17:34:11,480 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b/container.db for volume DS-54ed26fc-1cc8-42ce-a5ee-55ecd149669b
dn2_1    | 2023-07-06 17:34:11,546 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-07-06 17:34:12,881 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-07-06 17:34:12,881 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn2_1    | 2023-07-06 17:34:20,132 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-07-06 17:34:20,442 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-06 17:34:20,746 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-07-06 17:34:20,785 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-06 17:34:20,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-07-06 17:34:20,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-06 17:34:20,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-07-06 17:34:20,833 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-07-06 17:34:20,833 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-07-06 17:34:20,834 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-07-06 17:34:20,836 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:20,837 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-07-06 17:34:20,837 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:20,927 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:34:20,961 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-07-06 17:34:20,985 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-07-06 17:34:22,486 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-07-06 17:34:22,498 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-07-06 17:34:22,531 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-07-06 17:34:22,536 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:22,539 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:34:22,548 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:34:22,599 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: found a subdirectory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn2_1    | 2023-07-06 17:34:22,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-6C471268B20E:[] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@23e9a424[Not completed]
dn2_1    | 2023-07-06 17:34:22,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: found a subdirectory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn2_1    | 2023-07-06 17:34:22,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-D4A90FBE2E68:[] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@263efbbc[Not completed]
dn2_1    | 2023-07-06 17:34:22,719 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: found a subdirectory /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151
dn2_1    | 2023-07-06 17:34:22,721 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: addNew group-262CDF97B151:[] returns group-262CDF97B151:java.util.concurrent.CompletableFuture@2f6d73b8[Not completed]
dn2_1    | 2023-07-06 17:34:22,904 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-07-06 17:34:23,265 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-6C471268B20E:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:34:23,306 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:34:23,354 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:34:23,354 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:34:23,354 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:23,354 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:34:23,354 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-06 17:34:23,387 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn2_1    | 2023-07-06 17:34:23,458 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:34:23,462 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:34:23,545 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:34:23,556 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:34:23,651 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:23,713 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:23,747 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:34:23,785 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:34:24,095 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:34:24,388 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-07-06 17:34:24,518 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:24,521 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:34:24,551 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:34:09,944 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-07-06 17:34:11,998 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b/container.db to cache
dn3_1    | 2023-07-06 17:34:11,998 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b/container.db for volume DS-cd4f52be-98fc-4949-8d5d-61faa1d5435b
dn3_1    | 2023-07-06 17:34:12,045 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-07-06 17:34:13,420 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-07-06 17:34:13,422 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn3_1    | 2023-07-06 17:34:19,679 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-07-06 17:34:20,001 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-07-06 17:34:20,318 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-07-06 17:34:20,388 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-06 17:34:20,394 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-07-06 17:34:20,396 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-06 17:34:20,397 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-07-06 17:34:20,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-07-06 17:34:20,404 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-07-06 17:34:20,404 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-07-06 17:34:20,413 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:20,423 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-07-06 17:34:20,429 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:20,486 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:34:20,519 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-07-06 17:34:20,524 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-07-06 17:34:22,290 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-07-06 17:34:22,337 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-07-06 17:34:22,338 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-07-06 17:34:22,339 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:22,380 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:34:22,383 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:34:22,505 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: found a subdirectory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn3_1    | 2023-07-06 17:34:22,544 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-6C471268B20E:[] returns group-6C471268B20E:java.util.concurrent.CompletableFuture@510e0271[Not completed]
dn3_1    | 2023-07-06 17:34:22,544 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: found a subdirectory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn3_1    | 2023-07-06 17:34:22,544 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-D4A90FBE2E68:[] returns group-D4A90FBE2E68:java.util.concurrent.CompletableFuture@3006580a[Not completed]
dn3_1    | 2023-07-06 17:34:22,544 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: found a subdirectory /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2
dn3_1    | 2023-07-06 17:34:22,553 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: addNew group-87B51E7AE7A2:[] returns group-87B51E7AE7A2:java.util.concurrent.CompletableFuture@76480860[Not completed]
dn3_1    | 2023-07-06 17:34:22,870 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-07-06 17:34:22,964 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-6C471268B20E:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:34:22,966 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:34:23,074 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:34:23,086 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:34:23,092 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:23,092 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:34:23,092 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:34:23,176 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:34:23,176 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:34:23,245 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:34:23,266 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:34:23,383 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-07-06 17:34:23,412 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:23,431 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:23,467 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:34:23,508 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:34:23,771 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:34:24,228 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:24,288 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:34:24,289 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,305 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,325 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,328 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,346 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-D4A90FBE2E68:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:34:24,374 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:34:24,380 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:34:24,380 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:34:24,380 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:24,380 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:34:24,386 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:34:24,386 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:34:24,388 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:34:24,400 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:34:24,404 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:34:24,405 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:24,406 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:24,409 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:34:24,409 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:34:24,413 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:34:24,617 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:24,617 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:34:24,620 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,620 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,624 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,626 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,629 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9: new RaftServerImpl for group-87B51E7AE7A2:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-06 17:34:24,641 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-06 17:34:24,656 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-06 17:34:24,656 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-06 17:34:24,660 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:24,660 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-06 17:34:24,663 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-06 17:34:24,663 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-06 17:34:24,664 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-06 17:34:24,664 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-06 17:34:24,668 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-06 17:34:24,675 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:24,676 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:24,680 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-06 17:34:24,684 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-06 17:34:24,684 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-06 17:34:24,676 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-07-06 17:34:24,720 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:24,722 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:34:24,728 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,728 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,728 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-06 17:34:24,729 [95d60ec2-4839-4bc7-b249-86264ebe00e9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-06 17:34:24,774 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-07-06 17:34:24,955 [main] INFO util.log: Logging initialized @29622ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-07-06 17:34:25,886 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-07-06 17:34:25,913 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-07-06 17:34:25,952 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-07-06 17:34:50,081 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn1_1    | 2023-07-06 17:34:50,107 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/log_inprogress_0 to /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/log_0-0
dn1_1    | 2023-07-06 17:34:50,179 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7176bee7-3bd9-4464-8f69-8c1413a610ac/current/log_inprogress_1
dn1_1    | 2023-07-06 17:34:50,211 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC-LeaderElection2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-8C1413A610AC: set configuration 1: peers:[9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,287 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 1, (t:1, i:4))
dn1_1    | 2023-07-06 17:34:50,294 [grpc-default-executor-0] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-CANDIDATE: accept PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:34:50,369 [grpc-default-executor-0] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t1. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t1, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,378 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: changes role from CANDIDATE to FOLLOWER at term 2 for appendEntries
dn1_1    | 2023-07-06 17:34:50,378 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3
dn1_1    | 2023-07-06 17:34:50,379 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FollowerState
dn1_1    | 2023-07-06 17:34:50,371 [grpc-default-executor-2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-6C471268B20E, 1, (t:1, i:200))
dn1_1    | 2023-07-06 17:34:50,393 [grpc-default-executor-2] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-CANDIDATE: accept PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 0
dn1_1    | 2023-07-06 17:34:50,426 [grpc-default-executor-2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t1. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,438 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:34:50,511 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 2 for appendEntries, leader elected after 26653ms
dn1_1    | 2023-07-06 17:34:50,520 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(ELECTION, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 2, (t:1, i:4))
dn1_1    | 2023-07-06 17:34:50,473 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-07-06 17:34:50,604 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection:   Response 0: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:FAIL-t1
dn1_1    | 2023-07-06 17:34:50,604 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-07-06 17:34:50,604 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn1_1    | 2023-07-06 17:34:50,605 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1
dn1_1    | 2023-07-06 17:34:50,605 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-LeaderElection1] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:34:50,472 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3: PRE_VOTE DISCOVERED_A_NEW_TERM (term=2) received 1 response(s) and 0 exception(s):
dn1_1    | 2023-07-06 17:34:50,605 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO impl.LeaderElection:   Response 0: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t2
dn1_1    | 2023-07-06 17:34:50,605 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3] INFO impl.LeaderElection: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-LeaderElection3 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=2)
dn1_1    | 2023-07-06 17:34:50,607 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: set configuration 5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,607 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread3] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolling segment log-0_4 to index:4
dn1_1    | 2023-07-06 17:34:50,609 [grpc-default-executor-3] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FOLLOWER: reject ELECTION from 95d60ec2-4839-4bc7-b249-86264ebe00e9: this server is a follower and still has a valid leader 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:34:50,652 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0 to /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_0-4
dn1_1    | 2023-07-06 17:34:50,653 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to ELECTION vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:FAIL-t2. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t2, leader=95d60ec2-4839-4bc7-b249-86264ebe00e9, voted=null, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,672 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_5
dn1_1    | 2023-07-06 17:34:50,790 [grpc-default-executor-4] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 1, (t:1, i:200))
dn1_1    | 2023-07-06 17:34:50,794 [grpc-default-executor-4] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FOLLOWER: accept PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:34:50,794 [grpc-default-executor-4] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t1. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,793 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-D4A90FBE2E68, 1, (t:1, i:4))
dn1_1    | 2023-07-06 17:34:50,796 [grpc-default-executor-3] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-FOLLOWER: reject PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: this server is a follower and still has a valid leader 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn1_1    | 2023-07-06 17:34:50,796 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68 replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:FAIL-t2. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68:t2, leader=95d60ec2-4839-4bc7-b249-86264ebe00e9, voted=null, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c5, conf=5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:50,863 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: receive requestVote(ELECTION, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 2, (t:1, i:200))
dn1_1    | 2023-07-06 17:34:50,863 [grpc-default-executor-3] INFO impl.VoteContext: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FOLLOWER: accept ELECTION from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn1_1    | 2023-07-06 17:34:50,863 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn1_1    | 2023-07-06 17:34:50,863 [grpc-default-executor-3] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: shutdown 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:34:50,864 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState was interrupted
dn1_1    | 2023-07-06 17:34:50,867 [grpc-default-executor-3] INFO impl.RoleInfo: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: start 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-FollowerState
dn1_1    | 2023-07-06 17:34:50,886 [grpc-default-executor-3] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E replies to ELECTION vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:OK-t2. Peer's state: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E:t2, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:51,175 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn1_1    | 2023-07-06 17:34:51,176 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread1] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 2 for appendEntries, leader elected after 28267ms
dn1_1    | 2023-07-06 17:34:51,207 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread2] INFO server.RaftServer$Division: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E: set configuration 201: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-06 17:34:51,210 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8-server-thread2] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: Rolling segment log-0_200 to index:200
dn1_1    | 2023-07-06 17:34:51,211 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0 to /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_0-200
dn1_1    | 2023-07-06 17:34:51,214 [9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_201
dn1_1    | 2023-07-06 17:35:44,369 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-06 17:35:50,565 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_0, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,634 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_1, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,670 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_2, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,726 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_3, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,749 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_4, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,781 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_5, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,836 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_6, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,860 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_7, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,885 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_8, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,909 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_9, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,938 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_10, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,965 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_11, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:50,995 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_12, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,021 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_13, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,060 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_14, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,086 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_15, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,110 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_16, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,132 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_17, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,158 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_18, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,182 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_19, offset=0, len=1024}
dn1_1    | 2023-07-06 17:35:51,218 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_20, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,243 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_21, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,264 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_22, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:34:24,553 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,539 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-07-06 17:34:24,572 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:34:24,573 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,596 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-D4A90FBE2E68:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:34:24,632 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:34:24,644 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:34:24,656 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:34:24,656 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:24,656 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:34:24,656 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-06 17:34:24,656 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:34:24,657 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:34:24,657 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:34:24,657 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:34:24,657 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:24,657 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:24,670 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:34:24,670 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:34:24,670 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:34:24,671 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:24,675 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:34:24,675 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-06 17:34:24,675 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,688 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:34:24,690 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,712 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: new RaftServerImpl for group-262CDF97B151:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-06 17:34:24,728 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-06 17:34:24,728 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-06 17:34:24,728 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-06 17:34:24,729 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-06 17:34:24,748 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-06 17:34:24,748 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-06 17:34:24,748 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-06 17:34:24,748 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-06 17:34:24,749 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-06 17:34:24,749 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-06 17:34:24,749 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-06 17:34:25,974 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-07-06 17:34:25,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-07-06 17:34:25,982 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-07-06 17:34:26,290 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-07-06 17:34:26,321 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-07-06 17:34:26,330 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn3_1    | 2023-07-06 17:34:26,513 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-07-06 17:34:26,514 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-07-06 17:34:26,538 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-07-06 17:34:26,758 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a14e60d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-07-06 17:34:26,783 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ef2f7ad{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-07-06 17:34:27,468 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5fdd97c1{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12819511056818470263/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-07-06 17:34:27,501 [main] INFO server.AbstractConnector: Started ServerConnector@7f3c0399{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-07-06 17:34:27,506 [main] INFO server.Server: Started @32167ms
dn3_1    | 2023-07-06 17:34:27,513 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-07-06 17:34:27,513 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-07-06 17:34:27,528 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-07-06 17:34:27,762 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-07-06 17:34:27,927 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn3_1    | 2023-07-06 17:34:27,947 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn3_1    | 2023-07-06 17:34:29,183 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn3_1    | 2023-07-06 17:34:29,192 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-07-06 17:34:29,201 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-07-06 17:34:29,204 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn3_1    | 2023-07-06 17:34:29,245 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-07-06 17:34:29,494 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn3_1    | 2023-07-06 17:34:29,537 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-07-06 17:34:32,594 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:34:33,595 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:34:34,596 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-06 17:34:36,772 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From e99cd87b06ef/10.9.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:53892 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:53892 remote=recon/10.9.0.15:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn3_1    | 2023-07-06 17:34:39,607 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From e99cd87b06ef/10.9.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:38788 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
om_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1     | 2023-07-06 17:34:03,555 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1     | /************************************************************
om_1     | STARTUP_MSG: Starting OzoneManager
om_1     | STARTUP_MSG:   host = 4257e50d2700/10.9.0.14
om_1     | STARTUP_MSG:   args = []
om_1     | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-07-06 17:34:24,752 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:24,752 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-06 17:34:24,758 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-06 17:34:24,758 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-06 17:34:24,759 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:24,769 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:34:24,769 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-06 17:34:24,779 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,796 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-06 17:34:24,796 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-06 17:34:24,824 [main] INFO util.log: Logging initialized @29307ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-07-06 17:34:25,611 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-07-06 17:34:25,632 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-07-06 17:34:25,659 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-07-06 17:34:25,661 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-07-06 17:34:25,666 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-07-06 17:34:25,667 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-07-06 17:34:25,872 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-07-06 17:34:25,929 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-07-06 17:34:25,935 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn2_1    | 2023-07-06 17:34:26,159 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-07-06 17:34:26,159 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-07-06 17:34:26,189 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-07-06 17:34:26,279 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5aa76ad2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-07-06 17:34:26,313 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7abeabe9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-07-06 17:34:27,305 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7eb27768{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12491134150190350136/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-07-06 17:34:27,362 [main] INFO server.AbstractConnector: Started ServerConnector@7d66e544{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-07-06 17:34:27,375 [main] INFO server.Server: Started @31857ms
dn2_1    | 2023-07-06 17:34:27,379 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-07-06 17:34:27,384 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-07-06 17:34:27,388 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-07-06 17:34:27,632 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-07-06 17:34:27,829 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn2_1    | 2023-07-06 17:34:27,847 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn2_1    | 2023-07-06 17:34:29,162 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn2_1    | 2023-07-06 17:34:29,162 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn2_1    | 2023-07-06 17:34:29,177 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 2023-07-06 17:34:29,178 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn2_1    | 2023-07-06 17:34:29,241 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-07-06 17:34:29,545 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.15:9891
dn2_1    | 2023-07-06 17:34:29,581 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-07-06 17:34:32,526 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:34:33,527 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:34:34,528 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.17:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-06 17:34:36,587 [EndpointStateMachine task thread for recon/10.9.0.15:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 1ffe21382799/10.9.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:44630 remote=recon/10.9.0.15:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:44630 remote=recon/10.9.0.15:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-06 17:34:39,538 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 1ffe21382799/10.9.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:37328 remote=scm/10.9.0.17:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.12:37328 remote=scm/10.9.0.17:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-06 17:34:40,562 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-07-06 17:34:40,576 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-07-06 17:34:40,717 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-07-06 17:34:40,723 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:34:40,805 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 7@1ffe21382799
dn2_1    | 2023-07-06 17:34:40,840 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5} from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/raft-meta
dn2_1    | 2023-07-06 17:34:40,826 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/in_use.lock acquired by nodename 7@1ffe21382799
dn2_1    | 2023-07-06 17:34:40,871 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 7@1ffe21382799
dn2_1    | 2023-07-06 17:34:40,877 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=95d60ec2-4839-4bc7-b249-86264ebe00e9} from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/raft-meta
dn2_1    | 2023-07-06 17:34:40,861 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5} from /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/raft-meta
dn2_1    | 2023-07-06 17:34:41,008 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:41,014 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:40,996 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:41,104 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: Setting the last applied index to (t:1, i:4)
dn2_1    | 2023-07-06 17:34:41,120 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO ratis.ContainerStateMachine: group-262CDF97B151: Setting the last applied index to (t:1, i:0)
dn2_1    | 2023-07-06 17:34:41,108 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO ratis.ContainerStateMachine: group-6C471268B20E: Setting the last applied index to (t:1, i:200)
dn2_1    | 2023-07-06 17:34:41,966 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:34:41,985 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:34:42,007 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-06 17:34:42,021 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:34:42,034 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,035 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:34:42,034 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:34:42,035 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,036 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:34:42,037 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:34:42,042 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,034 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-06 17:34:42,060 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,060 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-06 17:34:42,061 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:34:42,043 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-06 17:34:42,069 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,064 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,094 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:34:42,095 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:34:42,095 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,095 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:34:42,103 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:34:42,103 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-06 17:35:51,288 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_23, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,314 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_24, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,333 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_25, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,364 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_26, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,395 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_27, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,423 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_28, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,444 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_29, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,465 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_30, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,489 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_31, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,521 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_32, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,565 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_33, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,605 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_34, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,635 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_35, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,657 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_36, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,681 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_37, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,707 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_38, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,732 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_39, offset=1024, len=1024}
dn1_1    | 2023-07-06 17:35:51,757 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_40, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,780 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_41, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,801 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_42, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,823 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_43, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,845 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_44, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,866 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_45, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,889 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_46, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,911 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_47, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,933 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_48, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,957 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_49, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:51,986 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_50, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,023 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_51, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,058 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_52, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,072 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_53, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,094 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_54, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,122 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_55, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,144 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_56, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,167 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_57, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,197 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_58, offset=2048, len=1024}
dn1_1    | 2023-07-06 17:35:52,221 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_59, offset=2048, len=1024}
om_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1     | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
om_1     | STARTUP_MSG:   java = 11.0.19
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-07-06 17:34:03,157 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 5e48db04312d/10.9.0.15
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
recon_1  | STARTUP_MSG:   java = 11.0.19
om_1     | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1     | ************************************************************/
om_1     | 2023-07-06 17:34:03,625 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1     | 2023-07-06 17:34:09,553 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1     | 2023-07-06 17:34:11,887 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1     | 2023-07-06 17:34:12,055 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/10.9.0.14:9862
om_1     | 2023-07-06 17:34:12,055 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1     | 2023-07-06 17:34:12,056 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1     | 2023-07-06 17:34:12,167 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:34:12,396 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1     | 2023-07-06 17:34:14,387 [main] INFO reflections.Reflections: Reflections took 1721 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1     | 2023-07-06 17:34:14,521 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1     | 2023-07-06 17:34:14,681 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:34:16,497 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863]
om_1     | 2023-07-06 17:34:17,126 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863]
om_1     | 2023-07-06 17:34:19,728 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1     | 2023-07-06 17:34:21,730 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1     | 2023-07-06 17:34:23,732 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1     | 2023-07-06 17:34:25,734 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1     | 2023-07-06 17:34:27,737 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1     | 2023-07-06 17:34:29,739 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1     | 2023-07-06 17:34:31,740 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1     | 2023-07-06 17:34:33,742 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4257e50d2700/10.9.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1     | 2023-07-06 17:34:36,710 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
om_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1     | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1     | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1     | , while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1     | 2023-07-06 17:34:38,715 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
om_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1     | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1     | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1     | , while invoking $Proxy34.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1     | 2023-07-06 17:34:43,369 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1     | 2023-07-06 17:34:43,627 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:34:44,180 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1     | 2023-07-06 17:34:45,115 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1     | 2023-07-06 17:34:45,159 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1     | 2023-07-06 17:34:45,175 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1     | 2023-07-06 17:34:45,278 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1     | 2023-07-06 17:34:45,281 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1     | 2023-07-06 17:34:45,504 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1     | 2023-07-06 17:34:45,506 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1     | 2023-07-06 17:34:45,570 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1     | 2023-07-06 17:34:45,582 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1     | 2023-07-06 17:34:45,643 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1     | 2023-07-06 17:34:45,657 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:1, i:14)
om_1     | 2023-07-06 17:34:45,696 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1     | 2023-07-06 17:34:45,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1     | 2023-07-06 17:34:45,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1     | 2023-07-06 17:34:45,718 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1     | 2023-07-06 17:34:45,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1     | 2023-07-06 17:34:45,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1     | 2023-07-06 17:34:45,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1     | 2023-07-06 17:34:45,720 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1     | 2023-07-06 17:34:45,724 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:34:45,726 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1     | 2023-07-06 17:34:45,727 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1     | 2023-07-06 17:34:45,755 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1     | 2023-07-06 17:34:45,762 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1     | 2023-07-06 17:34:45,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1     | 2023-07-06 17:34:46,173 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1     | 2023-07-06 17:34:46,182 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1     | 2023-07-06 17:34:46,183 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1     | 2023-07-06 17:34:46,184 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1     | 2023-07-06 17:34:46,185 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1     | 2023-07-06 17:34:46,192 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1     | 2023-07-06 17:34:46,200 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1     | 2023-07-06 17:34:46,216 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@7e5efcab[Not completed]
om_1     | 2023-07-06 17:34:46,216 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1     | 2023-07-06 17:34:46,219 [main] INFO om.OzoneManager: Creating RPC Server
om_1     | 2023-07-06 17:34:46,253 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1     | 2023-07-06 17:34:46,260 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1     | 2023-07-06 17:34:46,264 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1     | 2023-07-06 17:34:46,264 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1     | 2023-07-06 17:34:46,265 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1     | 2023-07-06 17:34:46,265 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1     | 2023-07-06 17:34:46,266 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1     | 2023-07-06 17:34:46,287 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1     | 2023-07-06 17:34:46,289 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1     | 2023-07-06 17:34:46,307 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1     | 2023-07-06 17:34:46,310 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1     | 2023-07-06 17:34:46,389 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1     | 2023-07-06 17:34:46,405 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1     | 2023-07-06 17:34:46,425 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1     | 2023-07-06 17:34:46,426 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1     | 2023-07-06 17:34:46,602 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1     | 2023-07-06 17:34:46,775 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1     | 2023-07-06 17:34:46,795 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1     | 2023-07-06 17:34:46,801 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1     | 2023-07-06 17:34:46,802 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1     | 2023-07-06 17:34:46,808 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1     | 2023-07-06 17:34:46,809 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1     | 2023-07-06 17:34:47,890 [main] INFO reflections.Reflections: Reflections took 1599 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1     | 2023-07-06 17:34:48,336 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1     | 2023-07-06 17:34:48,373 [main] INFO ipc.Server: Listener at om:9862
om_1     | 2023-07-06 17:34:48,376 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1     | 2023-07-06 17:34:50,972 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1     | 2023-07-06 17:34:51,046 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1     | 2023-07-06 17:34:51,046 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1     | 2023-07-06 17:34:51,308 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/10.9.0.14:9862
om_1     | 2023-07-06 17:34:51,308 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1     | 2023-07-06 17:34:51,326 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@4257e50d2700
om_1     | 2023-07-06 17:34:51,359 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=om1} from /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/raft-meta
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.13:38788 remote=scm/10.9.0.17:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn3_1    | 2023-07-06 17:34:40,454 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-07-06 17:34:40,467 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-07-06 17:34:40,599 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-07-06 17:34:40,600 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9
om_1     | 2023-07-06 17:34:51,445 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:51,450 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1     | 2023-07-06 17:34:51,467 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1     | 2023-07-06 17:34:51,467 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:34:51,470 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1     | 2023-07-06 17:34:51,471 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1     | 2023-07-06 17:34:51,478 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1     | 2023-07-06 17:34:51,489 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1     | 2023-07-06 17:34:51,490 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1     | 2023-07-06 17:34:51,491 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:34:51,513 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1     | 2023-07-06 17:34:51,514 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1     | 2023-07-06 17:34:51,515 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1     | 2023-07-06 17:34:51,516 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1     | 2023-07-06 17:34:51,518 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1     | 2023-07-06 17:34:51,519 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1     | 2023-07-06 17:34:51,521 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1     | 2023-07-06 17:34:51,521 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1     | 2023-07-06 17:34:51,528 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1     | 2023-07-06 17:34:51,549 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1     | 2023-07-06 17:34:51,550 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1     | 2023-07-06 17:34:51,575 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1     | 2023-07-06 17:34:51,576 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1     | 2023-07-06 17:34:51,577 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1     | 2023-07-06 17:34:51,634 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:51,650 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 15 entries from segment file /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1     | 2023-07-06 17:34:51,654 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 14
om_1     | 2023-07-06 17:34:51,654 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1     | 2023-07-06 17:34:51,775 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:51,775 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 1 for startAsFollower
om_1     | 2023-07-06 17:34:51,778 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1     | 2023-07-06 17:34:51,789 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1     | 2023-07-06 17:34:51,791 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1     | 2023-07-06 17:34:51,793 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1     | 2023-07-06 17:34:51,794 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1     | 2023-07-06 17:34:51,795 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1     | 2023-07-06 17:34:51,797 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1     | 2023-07-06 17:34:51,798 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1     | 2023-07-06 17:34:51,817 [main] INFO server.RaftServer: om1: start RPC server
om_1     | 2023-07-06 17:34:51,905 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1     | 2023-07-06 17:34:51,915 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1     | 2023-07-06 17:34:51,915 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1     | 2023-07-06 17:34:52,023 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1     | 2023-07-06 17:34:52,023 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1     | 2023-07-06 17:34:52,047 [main] INFO util.log: Logging initialized @56221ms to org.eclipse.jetty.util.log.Slf4jLog
om_1     | 2023-07-06 17:34:52,462 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1     | 2023-07-06 17:34:52,475 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1     | 2023-07-06 17:34:52,492 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1     | 2023-07-06 17:34:52,495 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1     | 2023-07-06 17:34:52,496 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-07-06 17:34:03,317 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-07-06 17:34:06,178 [main] INFO reflections.Reflections: Reflections took 291 ms to scan 1 urls, producing 20 keys and 75 values 
dn2_1    | 2023-07-06 17:34:42,104 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-06 17:34:42,105 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-06 17:34:42,105 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,116 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn2_1    | 2023-07-06 17:34:42,117 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn2_1    | 2023-07-06 17:34:42,125 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:34:42,132 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:42,137 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,140 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:34:42,142 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:34:42,145 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:34:42,149 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151
dn2_1    | 2023-07-06 17:34:42,149 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:34:42,152 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:42,153 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,153 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:34:42,155 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:34:42,155 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:34:42,155 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:42,156 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:34:42,153 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-06 17:34:42,210 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:42,216 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:42,220 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:34:42,220 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,222 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-06 17:34:42,224 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-06 17:34:42,224 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-06 17:34:42,225 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-06 17:34:42,231 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-06 17:34:42,291 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,297 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:42,310 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,336 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-06 17:34:42,337 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:43,355 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:34:43,355 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:34:43,355 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:43,360 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:43,405 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:34:43,406 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:34:43,406 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:43,459 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1     | 2023-07-06 17:34:52,496 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1     | 2023-07-06 17:34:52,617 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1     | 2023-07-06 17:34:52,621 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1     | 2023-07-06 17:34:52,623 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1     | 2023-07-06 17:34:52,739 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1     | 2023-07-06 17:34:52,739 [main] INFO server.session: No SessionScavenger set, using defaults
om_1     | 2023-07-06 17:34:52,746 [main] INFO server.session: node0 Scavenging every 600000ms
om_1     | 2023-07-06 17:34:52,792 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@341ccfd1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1     | 2023-07-06 17:34:52,793 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4345fd45{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1     | 2023-07-06 17:34:53,189 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8aa5ab4{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-18161774517771433851/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1     | 2023-07-06 17:34:53,244 [main] INFO server.AbstractConnector: Started ServerConnector@7a730479{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1     | 2023-07-06 17:34:53,251 [main] INFO server.Server: Started @57425ms
om_1     | 2023-07-06 17:34:53,263 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1     | 2023-07-06 17:34:53,263 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1     | 2023-07-06 17:34:53,266 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1     | 2023-07-06 17:34:53,273 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1     | 2023-07-06 17:34:53,277 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1     | 2023-07-06 17:34:53,331 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1     | 2023-07-06 17:34:53,695 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1     | 2023-07-06 17:34:56,821 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5043654913ns, electionTimeout:5019ms
om_1     | 2023-07-06 17:34:56,822 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1     | 2023-07-06 17:34:56,823 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om_1     | 2023-07-06 17:34:56,829 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1     | 2023-07-06 17:34:56,829 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1     | 2023-07-06 17:34:56,832 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:56,833 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
om_1     | 2023-07-06 17:34:56,835 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:56,836 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=2)
om_1     | 2023-07-06 17:34:56,836 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1     | 2023-07-06 17:34:56,836 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
om_1     | 2023-07-06 17:34:56,840 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 2 for becomeLeader, leader elected after 10447ms
om_1     | 2023-07-06 17:34:56,847 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1     | 2023-07-06 17:34:56,856 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1     | 2023-07-06 17:34:56,857 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1     | 2023-07-06 17:34:56,863 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1     | 2023-07-06 17:34:56,863 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1     | 2023-07-06 17:34:56,864 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1     | 2023-07-06 17:34:56,874 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1     | 2023-07-06 17:34:56,875 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1     | 2023-07-06 17:34:56,880 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1     | 2023-07-06 17:34:56,887 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Rolling segment log-0_14 to index:14
om_1     | 2023-07-06 17:34:56,899 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0 to /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_0-14
om_1     | 2023-07-06 17:34:56,915 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 15: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1     | 2023-07-06 17:34:56,931 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_15
om_1     | 2023-07-06 17:34:57,093 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1     | [id: "om1"
om_1     | address: "om:9872"
om_1     | startupRole: FOLLOWER
om_1     | ]
om_1     | 2023-07-06 17:35:32,834 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombgpost0 of layout LEGACY in volume: vol1
dn2_1    | 2023-07-06 17:34:43,459 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-06 17:34:43,459 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:43,561 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:43,564 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:43,572 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/log_inprogress_0
dn2_1    | 2023-07-06 17:34:43,579 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:43,591 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:34:43,638 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn2_1    | 2023-07-06 17:34:43,649 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-07-06 17:34:43,649 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:34:43,858 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO segmented.LogSegment: Successfully read 201 entries from segment file /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn2_1    | 2023-07-06 17:34:43,861 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 200
dn2_1    | 2023-07-06 17:34:43,861 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-06 17:34:43,968 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:43,968 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn2_1    | 2023-07-06 17:34:43,970 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:43,980 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:34:43,981 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn2_1    | 2023-07-06 17:34:43,981 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:44,002 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn2_1    | 2023-07-06 17:34:44,002 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState
dn2_1    | 2023-07-06 17:34:44,002 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState
dn2_1    | 2023-07-06 17:34:44,003 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:44,003 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:44,005 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
recon_1  | 2023-07-06 17:34:08,792 [main] INFO reflections.Reflections: Reflections took 433 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1  | 2023-07-06 17:34:09,058 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-07-06 17:34:09,152 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1688664710082
recon_1  | 2023-07-06 17:34:10,202 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-07-06 17:34:14,503 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-07-06 17:34:16,813 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-07-06 17:34:16,845 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-07-06 17:34:16,847 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-07-06 17:34:19,770 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:19,873 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:19,935 [main] INFO codegen.SqlDbUtils: CONTAINER_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:20,021 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:20,158 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:20,244 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
recon_1  | 2023-07-06 17:34:20,520 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-07-06 17:34:20,591 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-07-06 17:34:20,686 [main] INFO util.log: Logging initialized @25369ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-07-06 17:34:21,360 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-07-06 17:34:21,405 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1  | 2023-07-06 17:34:21,445 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-07-06 17:34:21,460 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-07-06 17:34:21,477 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-07-06 17:34:21,477 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-07-06 17:34:21,717 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-07-06 17:34:21,734 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-07-06 17:34:21,976 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-07-06 17:34:21,989 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1  | 2023-07-06 17:34:22,017 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-07-06 17:34:22,207 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-07-06 17:34:22,207 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1  | 2023-07-06 17:34:24,941 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:34:25,392 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:34:25,959 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1  | 2023-07-06 17:34:25,962 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-07-06 17:34:26,369 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:34:26,604 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1  | 2023-07-06 17:34:26,686 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-07-06 17:34:26,773 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-07-06 17:34:27,430 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95d60ec2-4839-4bc7-b249-86264ebe00e9
recon_1  | 2023-07-06 17:34:27,448 [main] INFO node.SCMNodeManager: Registered Data node : 95d60ec2-4839-4bc7-b249-86264ebe00e9{ip: 10.9.0.13, host: restart_dn3_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:34:27,480 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
recon_1  | 2023-07-06 17:34:27,480 [main] INFO node.SCMNodeManager: Registered Data node : 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8{ip: 10.9.0.11, host: restart_dn1_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:34:27,481 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
recon_1  | 2023-07-06 17:34:27,482 [main] INFO node.SCMNodeManager: Registered Data node : b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5{ip: 10.9.0.12, host: restart_dn2_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-06 17:34:27,482 [main] INFO scm.ReconNodeManager: Loaded 3 nodes from node DB.
dn1_1    | 2023-07-06 17:35:52,242 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_60, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,263 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_61, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,285 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_62, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,306 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_63, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,328 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_64, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,349 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_65, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,370 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_66, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,398 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_67, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,418 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_68, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,438 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_69, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,459 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_70, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,483 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_71, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,504 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_72, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,526 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_73, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,547 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_74, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,569 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_75, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,591 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_76, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,611 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_77, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,634 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_78, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,656 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_79, offset=3072, len=1024}
dn1_1    | 2023-07-06 17:35:52,678 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_80, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,701 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_81, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,723 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_82, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,745 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_83, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,770 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_84, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,792 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_85, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,814 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_86, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,835 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_87, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,856 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_88, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,885 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_89, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,911 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_90, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,931 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_91, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,954 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_92, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,979 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_93, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:52,998 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_94, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:53,020 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_95, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:53,044 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_96, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:53,071 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_97, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:53,094 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_98, offset=4096, len=1024}
dn1_1    | 2023-07-06 17:35:53,115 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_99, offset=4096, len=1024}
recon_1  | 2023-07-06 17:34:27,572 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 2023-07-06 17:34:29,489 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-07-06 17:34:29,587 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-07-06 17:34:29,703 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1  | 2023-07-06 17:34:29,716 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-07-06 17:34:30,068 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1  | 2023-07-06 17:34:30,072 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-07-06 17:34:30,072 [main] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-07-06 17:34:30,202 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-07-06 17:34:30,221 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-07-06 17:34:30,221 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-07-06 17:34:31,281 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-07-06 17:34:31,285 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1  | 2023-07-06 17:34:31,505 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-07-06 17:34:31,512 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-07-06 17:34:31,515 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 2023-07-06 17:34:31,615 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@62d40e31{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-07-06 17:34:31,619 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48cb2d73{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-07-06 17:34:36,199 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2318651f{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-3043251134723604405/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1  | 2023-07-06 17:34:36,209 [main] INFO server.AbstractConnector: Started ServerConnector@2df65a56{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-07-06 17:34:36,209 [main] INFO server.Server: Started @40892ms
recon_1  | 2023-07-06 17:34:36,211 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-07-06 17:34:36,211 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-07-06 17:34:36,213 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-07-06 17:34:36,213 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-07-06 17:34:36,217 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-07-06 17:34:36,218 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-06 17:34:36,218 [main] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1688664793893
recon_1  | 2023-07-06 17:34:36,233 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:34:36,336 [main] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688664793893.
recon_1  | 2023-07-06 17:34:36,368 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-07-06 17:34:36,369 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-07-06 17:34:38,876 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1  | 2023-07-06 17:34:41,951 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1  | 2023-07-06 17:34:41,952 [main] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1  | 2023-07-06 17:34:42,370 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-07-06 17:34:42,374 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-07-06 17:34:42,420 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-07-06 17:34:42,422 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-07-06 17:34:43,455 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:53892 / 10.9.0.13:53892: output error
recon_1  | 2023-07-06 17:34:43,458 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:44630 / 10.9.0.12:44630: output error
recon_1  | 2023-07-06 17:34:43,469 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:44644 / 10.9.0.12:44644: output error
dn2_1    | 2023-07-06 17:34:44,012 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:44,028 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:34:44,032 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:34:44,037 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:34:44,020 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:44,015 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:44,036 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-262CDF97B151,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:34:44,036 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:34:44,051 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:44,051 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:34:44,051 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:34:44,051 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:44,053 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:44,496 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start RPC server
dn2_1    | 2023-07-06 17:34:44,538 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9858
dn2_1    | 2023-07-06 17:34:44,540 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9856
dn2_1    | 2023-07-06 17:34:44,541 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: GrpcService started, listening on 9857
dn2_1    | 2023-07-06 17:34:44,544 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9858 for RATIS
dn2_1    | 2023-07-06 17:34:44,545 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-07-06 17:34:44,545 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-07-06 17:34:44,545 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: Started
dn2_1    | 2023-07-06 17:34:44,709 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-06 17:34:49,120 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5150378244ns, electionTimeout:5116ms
dn2_1    | 2023-07-06 17:34:49,124 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:34:49,124 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn2_1    | 2023-07-06 17:34:49,139 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:34:49,140 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1
dn2_1    | 2023-07-06 17:34:49,162 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,199 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5197425269ns, electionTimeout:5145ms
dn2_1    | 2023-07-06 17:34:49,219 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState
dn2_1    | 2023-07-06 17:34:49,219 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn3_1    | 2023-07-06 17:34:40,625 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/in_use.lock acquired by nodename 7@e99cd87b06ef
dn3_1    | 2023-07-06 17:34:40,642 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5} from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/raft-meta
dn3_1    | 2023-07-06 17:34:40,651 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/in_use.lock acquired by nodename 7@e99cd87b06ef
dn3_1    | 2023-07-06 17:34:40,660 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=95d60ec2-4839-4bc7-b249-86264ebe00e9} from /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/raft-meta
dn3_1    | 2023-07-06 17:34:40,657 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/in_use.lock acquired by nodename 7@e99cd87b06ef
dn3_1    | 2023-07-06 17:34:40,661 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=95d60ec2-4839-4bc7-b249-86264ebe00e9} from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/raft-meta
dn3_1    | 2023-07-06 17:34:40,752 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:40,752 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:40,791 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: set configuration 0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:40,812 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO ratis.ContainerStateMachine: group-D4A90FBE2E68: Setting the last applied index to (t:1, i:4)
dn3_1    | 2023-07-06 17:34:40,813 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO ratis.ContainerStateMachine: group-87B51E7AE7A2: Setting the last applied index to (t:1, i:0)
dn3_1    | 2023-07-06 17:34:40,822 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO ratis.ContainerStateMachine: group-6C471268B20E: Setting the last applied index to (t:1, i:200)
dn3_1    | 2023-07-06 17:34:41,275 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:34:41,286 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:34:41,307 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-06 17:34:41,319 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:34:41,319 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:34:41,320 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,327 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:34:41,332 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:34:41,319 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-06 17:34:41,333 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,337 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:34:41,340 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:34:41,340 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,339 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,340 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,357 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-06 17:34:41,358 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-06 17:34:41,358 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,382 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:34:41,388 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:34:41,389 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,382 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:34:41,407 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:34:41,407 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:49,219 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:34:49,219 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2
dn2_1    | 2023-07-06 17:34:49,243 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.FollowerState: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5241156401ns, electionTimeout:5189ms
dn2_1    | 2023-07-06 17:34:49,247 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState
dn2_1    | 2023-07-06 17:34:49,247 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn2_1    | 2023-07-06 17:34:49,248 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-06 17:34:49,248 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-FollowerState] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3
dn2_1    | 2023-07-06 17:34:49,277 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,287 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,293 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3 PRE_VOTE round 0: result PASSED (term=1)
dn2_1    | 2023-07-06 17:34:49,321 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 1, (t:1, i:4))
dn2_1    | 2023-07-06 17:34:49,323 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: receive requestVote(PRE_VOTE, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-6C471268B20E, 1, (t:1, i:200))
dn2_1    | 2023-07-06 17:34:49,344 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:49,344 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:49,345 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:49,345 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:49,349 [grpc-default-executor-0] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-CANDIDATE: reject PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 1 > candidate's priority 0
dn2_1    | 2023-07-06 17:34:49,358 [grpc-default-executor-1] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-CANDIDATE: accept PRE_VOTE from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn2_1    | 2023-07-06 17:34:49,391 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:34:49,397 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn2_1    | 2023-07-06 17:34:49,413 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t1. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68:t1, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,440 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E replies to PRE_VOTE vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:FAIL-t1. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,468 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-07-06 17:34:03,534 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-07-06 17:34:03,548 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-07-06 17:34:03,872 [main] INFO util.log: Logging initialized @8177ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-07-06 17:34:04,596 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-07-06 17:34:04,726 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-07-06 17:34:04,777 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-07-06 17:34:04,801 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-07-06 17:34:04,801 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-07-06 17:34:04,802 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-07-06 17:34:05,040 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir17194894409357417685
s3g_1    | 2023-07-06 17:34:05,816 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = 2d55879135e7/10.9.0.16
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | 2023-07-06 17:34:43,469 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:35770 / 10.9.0.11:35770: output error
recon_1  | 2023-07-06 17:34:43,470 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:35760 / 10.9.0.11:35760: output error
recon_1  | 2023-07-06 17:34:43,482 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:53896 / 10.9.0.13:53896: output error
recon_1  | 2023-07-06 17:34:43,485 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,486 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,486 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,486 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:46Z
s3g_1    | STARTUP_MSG:   java = 11.0.19
dn2_1    | 2023-07-06 17:34:49,487 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3 ELECTION round 0: result PASSED (term=2)
dn2_1    | 2023-07-06 17:34:49,490 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3
dn2_1    | 2023-07-06 17:34:49,506 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn2_1    | 2023-07-06 17:34:49,507 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-262CDF97B151 with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn2_1    | 2023-07-06 17:34:49,519 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 2 for becomeLeader, leader elected after 24757ms
dn2_1    | 2023-07-06 17:34:49,588 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: receive requestVote(ELECTION, 95d60ec2-4839-4bc7-b249-86264ebe00e9, group-D4A90FBE2E68, 2, (t:1, i:4))
dn2_1    | 2023-07-06 17:34:49,598 [grpc-default-executor-1] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-CANDIDATE: accept ELECTION from 95d60ec2-4839-4bc7-b249-86264ebe00e9: our priority 0 <= candidate's priority 1
dn2_1    | 2023-07-06 17:34:49,599 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: changes role from CANDIDATE to FOLLOWER at term 2 for candidate:95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:34:49,599 [grpc-default-executor-1] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1
dn2_1    | 2023-07-06 17:34:49,599 [grpc-default-executor-1] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FollowerState
dn2_1    | 2023-07-06 17:34:49,611 [grpc-default-executor-1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68 replies to ELECTION vote request: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t2. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68:t2, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:49,649 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-07-06 17:34:49,720 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:49,728 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-07-06 17:34:49,841 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-07-06 17:34:49,848 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-07-06 17:34:49,849 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-07-06 17:34:49,928 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:49,945 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-07-06 17:34:50,099 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderStateImpl
dn2_1    | 2023-07-06 17:34:50,152 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn2_1    | 2023-07-06 17:34:50,192 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/log_inprogress_0 to /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/log_0-0
dn2_1    | 2023-07-06 17:34:50,306 [grpc-default-executor-2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, group-D4A90FBE2E68, 1, (t:1, i:4))
dn2_1    | 2023-07-06 17:34:50,306 [grpc-default-executor-2] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-FOLLOWER: accept PRE_VOTE from 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: our priority 0 <= candidate's priority 0
dn2_1    | 2023-07-06 17:34:50,306 [grpc-default-executor-2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t2. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68:t2, leader=null, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:50,304 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: receive requestVote(PRE_VOTE, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, group-6C471268B20E, 1, (t:1, i:200))
dn2_1    | 2023-07-06 17:34:50,316 [grpc-default-executor-0] INFO impl.VoteContext: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-CANDIDATE: reject PRE_VOTE from 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: our priority 1 > candidate's priority 0
dn2_1    | 2023-07-06 17:34:50,316 [grpc-default-executor-0] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E replies to PRE_VOTE vote request: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:FAIL-t1. Peer's state: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:50,281 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-LeaderElection3] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151: set configuration 1: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:50,343 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn2_1    | 2023-07-06 17:34:50,423 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 2 for appendEntries, leader elected after 25685ms
dn2_1    | 2023-07-06 17:34:50,393 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-262CDF97B151-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/526c1993-adf9-4c08-bd3e-262cdf97b151/current/log_inprogress_1
dn2_1    | 2023-07-06 17:34:50,474 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68: set configuration 5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:50,484 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5-server-thread1] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolling segment log-0_4 to index:4
dn2_1    | 2023-07-06 17:34:50,522 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0 to /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_0-4
dn2_1    | 2023-07-06 17:34:50,532 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_5
dn2_1    | 2023-07-06 17:34:50,821 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:34:50,822 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t1
dn2_1    | 2023-07-06 17:34:50,822 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2 PRE_VOTE round 0: result PASSED
dn2_1    | 2023-07-06 17:34:50,829 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:34:50,832 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=2) received 1 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:34:50,832 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-9bd98c5f-653f-4ccb-a937-fb9bf18cffc8#0:FAIL-t2
dn2_1    | 2023-07-06 17:34:50,832 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-D4A90FBE2E68-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=2)
dn2_1    | 2023-07-06 17:34:50,840 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-06 17:34:50,843 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-06 17:34:50,867 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-07-06 17:34:50,867 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection:   Response 0: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t2
dn2_1    | 2023-07-06 17:34:50,867 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.LeaderElection: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2 ELECTION round 0: result PASSED
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: shutdown b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,486 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir17194894409357417685, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-07-06 17:34:05,847 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-07-06 17:34:05,955 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-07-06 17:34:06,548 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-07-06 17:34:07,448 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-07-06 17:34:07,448 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-07-06 17:34:07,675 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-07-06 17:34:07,672 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1    | 2023-07-06 17:34:07,704 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1    | 2023-07-06 17:34:08,035 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-07-06 17:34:08,068 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-07-06 17:34:08,093 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1    | 2023-07-06 17:34:08,195 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f5991f6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-07-06 17:34:08,211 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32c726ee{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | 2023-07-06 17:34:13,244 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.124s with 0.149s GC time.
s3g_1    | GC pool 'ParNew' had collection(s): count=1 time=149ms
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 2023-07-06 17:34:27,314 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@56ec6960{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir17194894409357417685/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-8941730941248265484/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-07-06 17:34:27,405 [main] INFO server.AbstractConnector: Started ServerConnector@561868a0{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-07-06 17:34:27,436 [main] INFO server.Server: Started @31758ms
s3g_1    | 2023-07-06 17:34:27,465 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-07-06 17:34:27,465 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-07-06 17:34:27,469 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1    | 2023-07-06 17:34:08,486 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1    | /************************************************************
scm_1    | STARTUP_MSG: Starting StorageContainerManager
scm_1    | STARTUP_MSG:   host = 05fcf274c3ca/10.9.0.17
scm_1    | STARTUP_MSG:   args = []
scm_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/4980c2087b6a32f9f9979c1b97017d191aa4f790 ; compiled by 'runner' on 2023-07-06T16:45Z
scm_1    | STARTUP_MSG:   java = 11.0.19
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 2 for becomeLeader, leader elected after 27217ms
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:50,868 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-07-06 17:34:50,869 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-07-06 17:34:50,869 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-07-06 17:34:50,869 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-07-06 17:34:50,869 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:34:50,873 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-07-06 17:34:50,947 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-07-06 17:34:50,947 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:50,947 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-07-06 17:34:50,950 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-07-06 17:34:50,951 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-07-06 17:34:50,958 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:50,959 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn2_1    | 2023-07-06 17:34:50,959 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn2_1    | 2023-07-06 17:34:50,960 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:34:50,960 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:34:50,983 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-07-06 17:34:50,983 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-06 17:34:50,983 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-07-06 17:34:50,983 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-07-06 17:34:50,988 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-07-06 17:34:50,988 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-06 17:34:50,989 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn2_1    | 2023-07-06 17:34:50,989 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn2_1    | 2023-07-06 17:34:50,989 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-06 17:34:50,989 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-06 17:34:50,991 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO impl.RoleInfo: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: start b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderStateImpl
dn2_1    | 2023-07-06 17:34:51,001 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: Rolling segment log-0_200 to index:200
dn2_1    | 2023-07-06 17:34:51,004 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0 to /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_0-200
dn2_1    | 2023-07-06 17:34:51,020 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_201
dn2_1    | 2023-07-06 17:34:51,032 [b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E-LeaderElection2] INFO server.RaftServer$Division: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5@group-6C471268B20E: set configuration 201: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-06 17:35:44,711 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-07-06 17:34:41,420 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-06 17:34:41,420 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-06 17:34:41,425 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,427 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e
dn3_1    | 2023-07-06 17:34:41,429 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:34:41,438 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:41,440 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,466 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2
dn3_1    | 2023-07-06 17:34:41,466 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:34:41,467 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68
dn3_1    | 2023-07-06 17:34:41,466 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:34:41,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:41,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,468 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:34:41,472 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:34:41,469 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-06 17:34:41,473 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:34:41,469 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:34:41,474 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:34:41,477 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:41,478 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:34:41,488 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:41,489 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,474 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:41,489 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-06 17:34:41,489 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:34:41,489 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-06 17:34:41,490 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-06 17:34:41,499 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-06 17:34:41,500 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-06 17:34:41,555 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,558 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,558 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,559 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-06 17:34:41,584 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:41,563 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:42,293 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,293 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,294 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:34:42,295 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,295 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,295 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:34:42,350 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,351 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-06 17:34:42,352 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-06 17:34:42,435 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: set configuration 0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:42,441 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/log_inprogress_0
dn3_1    | 2023-07-06 17:34:42,443 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:42,444 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: set configuration 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:42,446 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:34:42,464 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0
dn3_1    | 2023-07-06 17:34:42,473 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn3_1    | 2023-07-06 17:34:42,474 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:34:42,547 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO segmented.LogSegment: Successfully read 201 entries from segment file /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0
dn3_1    | 2023-07-06 17:34:42,547 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 200
dn3_1    | 2023-07-06 17:34:42,547 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-06 17:34:42,673 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:42,684 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn3_1    | 2023-07-06 17:34:42,686 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState
dn3_1    | 2023-07-06 17:34:42,708 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: start as a follower, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,486 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-06 17:34:43,889 [IPC Server handler 0 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for e99cd87b06ef
recon_1  | 2023-07-06 17:34:45,133 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for f487a00cfd86
recon_1  | 2023-07-06 17:34:45,417 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 1ffe21382799
recon_1  | 2023-07-06 17:35:12,746 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-07-06 17:35:12,756 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1  | 2023-07-06 17:35:12,769 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-07-06 17:35:12,830 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1  | 2023-07-06 17:35:12,847 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1  | 2023-07-06 17:35:12,865 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 109 milliseconds.
recon_1  | 2023-07-06 17:35:12,964 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 181 milliseconds to process 0 existing database records.
recon_1  | 2023-07-06 17:35:13,065 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 101 milliseconds for processing 1 containers.
recon_1  | 2023-07-06 17:35:23,209 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from restart_dn3_1.restart_net.
recon_1  | 2023-07-06 17:35:23,312 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1  | 2023-07-06 17:35:36,369 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-07-06 17:35:36,374 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:35:36,374 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:35:36,375 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:35:36,375 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:35:36,375 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-06 17:35:36,376 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-07-06 17:35:36,376 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 28 
recon_1  | 2023-07-06 17:35:36,619 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 21, SequenceNumber Lag from OM 0.
recon_1  | 2023-07-06 17:35:36,619 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 21 records
recon_1  | 2023-07-06 17:35:36,790 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,791 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,794 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol1/bucket1.
recon_1  | 2023-07-06 17:35:36,795 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,795 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol1/bucket1.
recon_1  | 2023-07-06 17:35:36,796 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,799 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol1/bucket1.
recon_1  | 2023-07-06 17:35:36,799 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,799 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol1/bucket1.
recon_1  | 2023-07-06 17:35:36,802 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-06 17:35:36,803 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol1.
recon_1  | 2023-07-06 17:35:37,075 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
recon_1  | 2023-07-06 17:35:37,103 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-07-06 17:35:37,104 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-07-06 17:35:37,140 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 2 OM DB update event(s).
recon_1  | 2023-07-06 17:35:37,215 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn3_1    | 2023-07-06 17:34:42,714 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn3_1    | 2023-07-06 17:34:42,714 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:34:42,719 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C471268B20E,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:34:42,746 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:42,765 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4A90FBE2E68,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:34:42,765 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-06 17:34:42,766 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-06 17:34:42,771 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:34:42,771 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:34:42,757 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1    | ************************************************************/
scm_1    | 2023-07-06 17:34:08,561 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1    | 2023-07-06 17:34:09,123 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:34:11,542 [main] INFO reflections.Reflections: Reflections took 1736 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1    | 2023-07-06 17:34:12,480 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1    | 2023-07-06 17:34:12,565 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1    | 2023-07-06 17:34:20,307 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-07-06 17:34:22,415 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 2023-07-06 17:35:50,561 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_0, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,630 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_1, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,667 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_2, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,722 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_3, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,744 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_4, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,774 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_5, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,807 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_6, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,843 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_7, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,878 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_8, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,905 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_9, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,926 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_10, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,952 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_11, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:50,985 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_12, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,017 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_13, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,052 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_14, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,076 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_15, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,099 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_16, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,125 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_17, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,153 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_18, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,181 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_19, offset=0, len=1024}
dn2_1    | 2023-07-06 17:35:51,210 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_20, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,232 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_21, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,247 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_22, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,275 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_23, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,297 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_24, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,324 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_25, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,362 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_26, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,385 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_27, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,419 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_28, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,441 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_29, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,462 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_30, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,486 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_31, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,501 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_32, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,537 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_33, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,601 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_34, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,635 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_35, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,649 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_36, offset=1024, len=1024}
scm_1    | 2023-07-06 17:34:24,426 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1    | 2023-07-06 17:34:24,436 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1    | 2023-07-06 17:34:25,219 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1    | 2023-07-06 17:34:27,046 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:34:27,897 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1    | 2023-07-06 17:34:28,004 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:34:28,157 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:34:28,158 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1    | 2023-07-06 17:34:28,166 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1    | 2023-07-06 17:34:28,188 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1    | 2023-07-06 17:34:28,212 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1    | 2023-07-06 17:34:28,247 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1    | 2023-07-06 17:34:28,304 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:34:28,312 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1    | 2023-07-06 17:34:28,315 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:34:28,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1    | 2023-07-06 17:34:28,562 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1    | 2023-07-06 17:34:28,571 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1    | 2023-07-06 17:34:30,102 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1    | 2023-07-06 17:34:30,110 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1    | 2023-07-06 17:34:30,111 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1    | 2023-07-06 17:34:30,111 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:34:30,112 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:34:30,123 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:34:30,141 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: found a subdirectory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c
scm_1    | 2023-07-06 17:34:30,186 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: addNew group-8CA20BA5223C:[] returns group-8CA20BA5223C:java.util.concurrent.CompletableFuture@36c0d0bd[Not completed]
scm_1    | 2023-07-06 17:34:30,281 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 2#32 with transactionInfo term andIndex
scm_1    | 2023-07-06 17:34:30,283 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: new RaftServerImpl for group-8CA20BA5223C:[] with SCMStateMachine:uninitialized
scm_1    | 2023-07-06 17:34:30,287 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1    | 2023-07-06 17:34:30,287 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1    | 2023-07-06 17:34:30,288 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1    | 2023-07-06 17:34:30,288 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1    | 2023-07-06 17:34:30,288 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-07-06 17:34:30,288 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1    | 2023-07-06 17:34:30,329 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1    | 2023-07-06 17:34:30,329 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1    | 2023-07-06 17:34:30,335 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1    | 2023-07-06 17:34:30,335 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1    | 2023-07-06 17:34:30,399 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1    | 2023-07-06 17:34:30,417 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1    | 2023-07-06 17:34:30,441 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1    | 2023-07-06 17:34:30,442 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1    | 2023-07-06 17:34:30,530 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1    | 2023-07-06 17:34:30,999 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1    | 2023-07-06 17:34:31,009 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1    | 2023-07-06 17:34:31,013 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1    | 2023-07-06 17:34:31,019 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1    | 2023-07-06 17:34:31,019 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1    | 2023-07-06 17:34:31,020 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1    | 2023-07-06 17:34:31,022 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1    | 2023-07-06 17:34:31,023 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1    | 2023-07-06 17:34:31,024 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1    | 2023-07-06 17:34:31,171 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1    | 2023-07-06 17:34:31,335 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1    | 2023-07-06 17:34:31,719 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1    | 2023-07-06 17:34:31,804 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1    | 2023-07-06 17:34:31,807 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-07-06 17:34:32,054 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1    | 2023-07-06 17:34:32,054 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-07-06 17:34:32,083 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1    | 2023-07-06 17:34:32,084 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:34:32,094 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1    | 2023-07-06 17:34:32,101 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1    | 2023-07-06 17:34:32,111 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1    | 2023-07-06 17:34:32,115 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1    | 2023-07-06 17:34:32,149 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-07-06 17:34:32,149 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-07-06 17:34:32,171 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1    | 2023-07-06 17:34:32,297 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1    | 2023-07-06 17:34:32,369 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1    | 2023-07-06 17:34:32,370 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1    | WARNING: An illegal reflective access operation has occurred
scm_1    | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm_1    | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1    | WARNING: All illegal access operations will be denied in a future release
scm_1    | 2023-07-06 17:34:32,479 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1    | 2023-07-06 17:34:32,483 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:34:32,492 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm_1    | 2023-07-06 17:34:32,634 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1    | 2023-07-06 17:34:33,823 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:34:33,867 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:34:33,911 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1    | 2023-07-06 17:34:34,008 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1    | 2023-07-06 17:34:34,121 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:34:34,125 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:34:34,126 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1    | 2023-07-06 17:34:34,126 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1    | 2023-07-06 17:34:34,200 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-07-06 17:34:34,213 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-07-06 17:34:34,214 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1    | 2023-07-06 17:34:34,214 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1    | 2023-07-06 17:34:34,375 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1    | 2023-07-06 17:34:34,379 [main] INFO server.StorageContainerManager: 
scm_1    | Container Balancer status:
scm_1    | Key                            Value
scm_1    | Running                        false
scm_1    | Container Balancer Configuration values:
scm_1    | Key                                                Value
scm_1    | Threshold                                          10
scm_1    | Max Datanodes to Involve per Iteration(percent)    20
scm_1    | Max Size to Move per Iteration                     500GB
scm_1    | Max Size Entering Target per Iteration             26GB
scm_1    | Max Size Leaving Source per Iteration              26GB
scm_1    | 
scm_1    | 2023-07-06 17:34:34,379 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1    | 2023-07-06 17:34:34,404 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1    | 2023-07-06 17:34:34,428 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1    | 2023-07-06 17:34:34,443 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/in_use.lock acquired by nodename 7@05fcf274c3ca
scm_1    | 2023-07-06 17:34:34,463 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5} from /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/raft-meta
dn3_1    | 2023-07-06 17:34:42,777 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-06 17:34:42,777 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:34:42,778 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:34:42,746 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:42,746 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: start as a follower, conf=0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:42,779 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn3_1    | 2023-07-06 17:34:42,779 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState
dn3_1    | 2023-07-06 17:34:42,779 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:42,780 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:42,781 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-87B51E7AE7A2,id=95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:34:42,781 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-06 17:34:42,781 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-06 17:34:42,781 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-06 17:34:42,782 [95d60ec2-4839-4bc7-b249-86264ebe00e9-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-06 17:34:42,783 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:42,784 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:42,791 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.RaftServer: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start RPC server
dn3_1    | 2023-07-06 17:34:43,516 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9858
dn3_1    | 2023-07-06 17:34:43,538 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9856
dn3_1    | 2023-07-06 17:34:43,557 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO server.GrpcService: 95d60ec2-4839-4bc7-b249-86264ebe00e9: GrpcService started, listening on 9857
dn3_1    | 2023-07-06 17:34:43,591 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9858 for RATIS
dn3_1    | 2023-07-06 17:34:43,591 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-07-06 17:34:43,592 [EndpointStateMachine task thread for scm/10.9.0.17:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95d60ec2-4839-4bc7-b249-86264ebe00e9 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-07-06 17:34:43,595 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-95d60ec2-4839-4bc7-b249-86264ebe00e9: Started
dn3_1    | 2023-07-06 17:34:43,740 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-07-06 17:34:47,782 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5096560802ns, electionTimeout:5001ms
dn3_1    | 2023-07-06 17:34:47,784 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState
dn3_1    | 2023-07-06 17:34:47,784 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn3_1    | 2023-07-06 17:34:47,786 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072304461ns, electionTimeout:5006ms
dn3_1    | 2023-07-06 17:34:47,787 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:34:47,787 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn3_1    | 2023-07-06 17:34:47,787 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-06 17:34:47,788 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1
dn3_1    | 2023-07-06 17:34:47,788 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-06 17:34:47,788 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2
scm_1    | 2023-07-06 17:34:34,580 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:34,590 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1    | 2023-07-06 17:34:34,603 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1    | 2023-07-06 17:34:34,603 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:34:34,604 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1    | 2023-07-06 17:34:34,605 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1    | 2023-07-06 17:34:34,609 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:34:34,626 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1    | 2023-07-06 17:34:34,627 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1    | 2023-07-06 17:34:34,628 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:34:34,640 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c
scm_1    | 2023-07-06 17:34:34,642 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:34:34,645 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:34:34,645 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1    | 2023-07-06 17:34:34,646 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1    | 2023-07-06 17:34:34,648 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1    | 2023-07-06 17:34:34,650 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1    | 2023-07-06 17:34:34,650 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1    | 2023-07-06 17:34:34,657 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1    | 2023-07-06 17:34:34,678 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1    | 2023-07-06 17:34:34,679 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-07-06 17:34:34,694 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1    | 2023-07-06 17:34:34,695 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1    | 2023-07-06 17:34:34,699 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1    | 2023-07-06 17:34:34,752 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 0: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:34,753 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_0-0
scm_1    | 2023-07-06 17:34:34,755 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:34,799 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.LogSegment: Successfully read 32 entries from segment file /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_1
scm_1    | 2023-07-06 17:34:34,812 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 32
scm_1    | 2023-07-06 17:34:34,894 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: start as a follower, conf=1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:34,895 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from      null to FOLLOWER at term 2 for startAsFollower
scm_1    | 2023-07-06 17:34:34,898 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:34:34,904 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8CA20BA5223C,id=72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5
scm_1    | 2023-07-06 17:34:34,919 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1    | 2023-07-06 17:34:34,919 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1    | 2023-07-06 17:34:34,923 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-07-06 17:34:34,924 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1    | 2023-07-06 17:34:34,923 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1    | 2023-07-06 17:34:34,925 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-06 17:35:51,665 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_37, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,700 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_38, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,721 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_39, offset=1024, len=1024}
dn2_1    | 2023-07-06 17:35:51,750 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_40, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,768 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_41, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,792 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_42, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,817 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_43, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,838 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_44, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,852 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_45, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,875 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_46, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,903 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_47, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,923 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_48, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,948 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_49, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:51,980 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_50, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,005 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_51, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,033 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_52, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,050 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_53, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,092 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_54, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,109 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_55, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,130 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_56, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,172 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_57, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,194 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_58, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,227 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_59, offset=2048, len=1024}
dn2_1    | 2023-07-06 17:35:52,235 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_60, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,261 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_61, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,278 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_62, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,299 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_63, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,320 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_64, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,341 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_65, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,359 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_66, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,380 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_67, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,411 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_68, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,434 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_69, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,446 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_70, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,466 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_71, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,496 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_72, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:34:47,793 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:47,793 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:47,854 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:47,856 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:47,855 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:34:47,883 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:47,885 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:47,899 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5119978829ns, electionTimeout:5109ms
dn3_1    | 2023-07-06 17:34:47,899 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState
dn3_1    | 2023-07-06 17:34:47,900 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn3_1    | 2023-07-06 17:34:47,900 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-06 17:34:47,900 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-FollowerState] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3
dn3_1    | 2023-07-06 17:34:47,892 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
dn3_1    | 2023-07-06 17:34:47,909 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:47,912 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3 PRE_VOTE round 0: result PASSED (term=1)
dn3_1    | 2023-07-06 17:34:47,947 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for 0: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:47,948 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3 ELECTION round 0: result PASSED (term=2)
dn3_1    | 2023-07-06 17:34:47,948 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3
dn3_1    | 2023-07-06 17:34:47,949 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn3_1    | 2023-07-06 17:34:47,950 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-87B51E7AE7A2 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:34:47,960 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 2 for becomeLeader, leader elected after 23275ms
dn3_1    | 2023-07-06 17:34:48,001 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-06 17:34:48,009 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:48,011 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-06 17:34:48,027 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-06 17:34:48,029 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-06 17:34:48,030 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-06 17:34:48,072 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:48,079 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-06 17:34:48,084 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderStateImpl
scm_1    | 2023-07-06 17:34:34,948 [main] INFO server.RaftServer: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start RPC server
scm_1    | 2023-07-06 17:34:35,049 [main] INFO server.GrpcService: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: GrpcService started, listening on 9894
scm_1    | 2023-07-06 17:34:35,067 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: Started
scm_1    | 2023-07-06 17:34:35,081 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1    | 2023-07-06 17:34:35,081 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1    | 2023-07-06 17:34:35,086 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1    | 2023-07-06 17:34:35,088 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1    | 2023-07-06 17:34:35,088 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1    | 2023-07-06 17:34:35,367 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1    | 2023-07-06 17:34:35,423 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1    | 2023-07-06 17:34:35,424 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1    | 2023-07-06 17:34:36,006 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1    | 2023-07-06 17:34:36,047 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:34:36,047 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1    | 2023-07-06 17:34:36,165 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1    | 2023-07-06 17:34:36,166 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1    | 2023-07-06 17:34:36,167 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:34:36,167 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1    | 2023-07-06 17:34:36,299 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1    | 2023-07-06 17:34:36,300 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1    | 2023-07-06 17:34:36,408 [main] INFO util.log: Logging initialized @41296ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1    | 2023-07-06 17:34:36,765 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1    | 2023-07-06 17:34:36,780 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1    | 2023-07-06 17:34:36,801 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1    | 2023-07-06 17:34:36,843 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1    | 2023-07-06 17:34:36,843 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1    | 2023-07-06 17:34:36,843 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1    | 2023-07-06 17:34:36,910 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1    | 2023-07-06 17:34:36,911 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1    | 2023-07-06 17:34:36,912 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1    | 2023-07-06 17:34:36,941 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1    | 2023-07-06 17:34:36,941 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1    | 2023-07-06 17:34:36,945 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1    | 2023-07-06 17:34:36,961 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@74b1838{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1    | 2023-07-06 17:34:36,962 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c8ff67{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1    | 2023-07-06 17:34:37,088 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1b475663{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-1762961782281700422/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1    | 2023-07-06 17:34:37,099 [main] INFO server.AbstractConnector: Started ServerConnector@388f1258{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1    | 2023-07-06 17:34:37,099 [main] INFO server.Server: Started @41987ms
scm_1    | 2023-07-06 17:34:37,102 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1    | 2023-07-06 17:34:37,102 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1    | 2023-07-06 17:34:37,104 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1    | 2023-07-06 17:34:39,956 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.FollowerState: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5060259248ns, electionTimeout:5031ms
scm_1    | 2023-07-06 17:34:39,958 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState
scm_1    | 2023-07-06 17:34:39,959 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
scm_1    | 2023-07-06 17:34:39,971 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1    | 2023-07-06 17:34:39,971 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-FollowerState] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
scm_1    | 2023-07-06 17:34:39,985 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 2 for 1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:39,986 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=2)
scm_1    | 2023-07-06 17:34:40,001 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: submit vote requests at term 3 for 1: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:48,115 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn3_1    | 2023-07-06 17:34:48,131 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/log_inprogress_0 to /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/log_0-0
dn3_1    | 2023-07-06 17:34:48,143 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/661977b4-3795-4a95-9edc-87b51e7ae7a2/current/log_inprogress_1
dn3_1    | 2023-07-06 17:34:48,168 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2-LeaderElection3] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-87B51E7AE7A2: set configuration 1: peers:[95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:49,513 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-06 17:34:49,515 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 0: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t1
dn3_1    | 2023-07-06 17:34:49,516 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-07-06 17:34:49,527 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for 0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:49,539 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-06 17:34:49,540 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection:   Response 0: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:FAIL-t1
dn3_1    | 2023-07-06 17:34:49,540 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-07-06 17:34:49,540 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn3_1    | 2023-07-06 17:34:49,540 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1
dn3_1    | 2023-07-06 17:34:49,541 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-LeaderElection1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:34:49,541 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-06 17:34:49,553 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-06 17:34:49,621 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-06 17:34:49,621 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection:   Response 0: 95d60ec2-4839-4bc7-b249-86264ebe00e9<-b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5#0:OK-t2
dn3_1    | 2023-07-06 17:34:49,621 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.LeaderElection: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2 ELECTION round 0: result PASSED
dn3_1    | 2023-07-06 17:34:49,621 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2
dn3_1    | 2023-07-06 17:34:49,622 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn3_1    | 2023-07-06 17:34:49,622 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4A90FBE2E68 with new leaderId: 95d60ec2-4839-4bc7-b249-86264ebe00e9
dn3_1    | 2023-07-06 17:34:49,622 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: change Leader from null to 95d60ec2-4839-4bc7-b249-86264ebe00e9 at term 2 for becomeLeader, leader elected after 25217ms
dn3_1    | 2023-07-06 17:34:49,623 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-06 17:34:49,625 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-06 17:34:49,626 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-06 17:34:49,626 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-06 17:34:49,626 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-06 17:34:49,627 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-06 17:34:49,627 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-06 17:35:52,512 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_73, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,533 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_74, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,556 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_75, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,578 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_76, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,601 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_77, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,619 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_78, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,644 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_79, offset=3072, len=1024}
dn2_1    | 2023-07-06 17:35:52,668 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_80, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,686 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_81, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,708 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_82, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,731 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_83, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,768 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_84, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,781 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_85, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,803 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_86, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,820 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_87, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,845 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_88, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,883 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_89, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,894 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_90, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,919 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_91, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,949 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_92, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,977 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_93, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:52,994 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_94, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:53,006 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_95, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:53,033 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_96, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:53,063 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_97, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:53,083 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_98, offset=4096, len=1024}
dn2_1    | 2023-07-06 17:35:53,102 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_99, offset=4096, len=1024}
scm_1    | 2023-07-06 17:34:40,001 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.LeaderElection: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1 ELECTION round 0: result PASSED (term=3)
scm_1    | 2023-07-06 17:34:40,001 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: shutdown 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1
scm_1    | 2023-07-06 17:34:40,002 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
scm_1    | 2023-07-06 17:34:40,002 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 3.
scm_1    | 2023-07-06 17:34:40,002 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,3>
scm_1    | 2023-07-06 17:34:40,005 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: change Leader from null to 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5 at term 3 for becomeLeader, leader elected after 9604ms
scm_1    | 2023-07-06 17:34:40,013 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1    | 2023-07-06 17:34:40,017 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:34:40,018 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1    | 2023-07-06 17:34:40,022 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1    | 2023-07-06 17:34:40,023 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1    | 2023-07-06 17:34:40,023 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1    | 2023-07-06 17:34:40,028 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1    | 2023-07-06 17:34:40,037 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1    | 2023-07-06 17:34:40,040 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO impl.RoleInfo: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5: start 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderStateImpl
scm_1    | 2023-07-06 17:34:40,048 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: Rolling segment log-1_32 to index:32
scm_1    | 2023-07-06 17:34:40,057 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_1 to /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_1-32
scm_1    | 2023-07-06 17:34:40,074 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-LeaderElection1] INFO server.RaftServer$Division: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C: set configuration 33: peers:[72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5|rpc:27de9db37231:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:40,088 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/3e9dbfd8-7c8d-47f4-b4ca-8ca20ba5223c/current/log_inprogress_33
scm_1    | 2023-07-06 17:34:40,110 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1    | 2023-07-06 17:34:40,111 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1    | 2023-07-06 17:34:40,120 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm_1    | 2023-07-06 17:34:40,123 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1    | 2023-07-06 17:34:40,124 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm_1    | 2023-07-06 17:34:40,126 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1    | 2023-07-06 17:34:40,132 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1    | 2023-07-06 17:34:40,171 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-07-06 17:34:40,385 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn1_1.restart_net:59494 / 10.9.0.11:59494: output error
scm_1    | 2023-07-06 17:34:40,399 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-07-06 17:34:49,627 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-06 17:34:49,675 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-06 17:34:49,677 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:49,678 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-06 17:34:49,682 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-06 17:34:49,683 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-06 17:34:49,685 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:49,689 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-06 17:34:49,689 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-06 17:34:49,690 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:34:49,690 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:34:49,694 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-06 17:34:49,697 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-06 17:34:49,701 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-06 17:34:49,705 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-06 17:34:49,705 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-06 17:34:49,708 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-06 17:34:49,711 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-06 17:34:49,711 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-06 17:34:49,713 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-06 17:34:49,715 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-06 17:34:49,722 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderStateImpl
dn3_1    | 2023-07-06 17:34:49,728 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolling segment log-0_4 to index:4
dn3_1    | 2023-07-06 17:34:49,732 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_0 to /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_0-4
dn3_1    | 2023-07-06 17:34:49,734 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c56ee59e-00e2-4ece-bcce-d4a90fbe2e68/current/log_inprogress_5
dn3_1    | 2023-07-06 17:34:49,738 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LeaderElection2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: set configuration 5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:50,556 [grpc-default-executor-0] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, group-D4A90FBE2E68, 1, (t:1, i:4))
dn3_1    | 2023-07-06 17:34:50,556 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: receive requestVote(PRE_VOTE, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8, group-6C471268B20E, 1, (t:1, i:200))
dn3_1    | 2023-07-06 17:34:50,558 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FOLLOWER: accept PRE_VOTE from 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: our priority 0 <= candidate's priority 0
dn3_1    | 2023-07-06 17:34:50,567 [grpc-default-executor-0] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LEADER: reject PRE_VOTE from 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8: this server is the leader and still has leadership
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1    | 2023-07-06 17:34:40,386 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn2_1.restart_net:37328 / 10.9.0.12:37328: output error
scm_1    | 2023-07-06 17:34:40,385 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from restart_dn3_1.restart_net:38788 / 10.9.0.13:38788: output error
scm_1    | 2023-07-06 17:34:40,414 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1    | 2023-07-06 17:34:40,416 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1    | java.nio.channels.ClosedChannelException
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1    | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1    | 2023-07-06 17:34:45,152 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9bd98c5f-653f-4ccb-a937-fb9bf18cffc8
scm_1    | 2023-07-06 17:34:45,159 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8{ip: 10.9.0.11, host: restart_dn1_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:34:45,174 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:34:45,189 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:34:45,199 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1    | 2023-07-06 17:34:45,213 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-07-06 17:34:45,215 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1    | 2023-07-06 17:34:45,217 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:34:45,440 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
scm_1    | 2023-07-06 17:34:45,441 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5{ip: 10.9.0.12, host: restart_dn2_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:34:45,442 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:34:45,458 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1    | 2023-07-06 17:34:45,458 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:34:45,556 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95d60ec2-4839-4bc7-b249-86264ebe00e9
scm_1    | 2023-07-06 17:34:45,560 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 95d60ec2-4839-4bc7-b249-86264ebe00e9{ip: 10.9.0.13, host: restart_dn3_1.restart_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-07-06 17:34:45,563 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:34:45,575 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
dn3_1    | 2023-07-06 17:34:50,572 [grpc-default-executor-0] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68 replies to PRE_VOTE vote request: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:FAIL-t2. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68:t2, leader=95d60ec2-4839-4bc7-b249-86264ebe00e9, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c4, conf=5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:50,573 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E replies to PRE_VOTE vote request: 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t1. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:50,749 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-D4A90FBE2E68, 1, (t:1, i:4))
dn3_1    | 2023-07-06 17:34:50,751 [grpc-default-executor-0] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: receive requestVote(PRE_VOTE, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 1, (t:1, i:200))
dn3_1    | 2023-07-06 17:34:50,754 [grpc-default-executor-0] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FOLLOWER: accept PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-06 17:34:50,754 [grpc-default-executor-0] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t1. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E:t1, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:50,749 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-LEADER: reject PRE_VOTE from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: this server is the leader and still has leadership
dn3_1    | 2023-07-06 17:34:50,761 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68 replies to PRE_VOTE vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:FAIL-t2. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68:t2, leader=95d60ec2-4839-4bc7-b249-86264ebe00e9, voted=95d60ec2-4839-4bc7-b249-86264ebe00e9, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-D4A90FBE2E68-SegmentedRaftLog:OPENED:c6, conf=5: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:0|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:50,846 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: receive requestVote(ELECTION, b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, group-6C471268B20E, 2, (t:1, i:200))
dn3_1    | 2023-07-06 17:34:50,846 [grpc-default-executor-1] INFO impl.VoteContext: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FOLLOWER: accept ELECTION from b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-06 17:34:50,846 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:34:50,846 [grpc-default-executor-1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: shutdown 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:34:50,846 [grpc-default-executor-1] INFO impl.RoleInfo: 95d60ec2-4839-4bc7-b249-86264ebe00e9: start 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState
dn3_1    | 2023-07-06 17:34:50,847 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState] INFO impl.FollowerState: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-FollowerState was interrupted
dn3_1    | 2023-07-06 17:34:50,860 [grpc-default-executor-1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E replies to ELECTION vote request: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5<-95d60ec2-4839-4bc7-b249-86264ebe00e9#0:OK-t2. Peer's state: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E:t2, leader=null, voted=b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5, raftlog=Memoized:95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLog:OPENED:c200, conf=0: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-06 17:34:51,166 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6C471268B20E with new leaderId: b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5
dn3_1    | 2023-07-06 17:34:51,171 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread1] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: change Leader from null to b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5 at term 2 for appendEntries, leader elected after 27794ms
dn3_1    | 2023-07-06 17:34:51,207 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread2] INFO server.RaftServer$Division: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E: set configuration 201: peers:[b1b0f5f4-d001-4c76-878b-25bd4ab9c2d5|rpc:10.9.0.12:9856|admin:10.9.0.12:9857|client:10.9.0.12:9858|dataStream:10.9.0.12:9858|priority:1|startupRole:FOLLOWER, 9bd98c5f-653f-4ccb-a937-fb9bf18cffc8|rpc:10.9.0.11:9856|admin:10.9.0.11:9857|client:10.9.0.11:9858|dataStream:10.9.0.11:9858|priority:0|startupRole:FOLLOWER, 95d60ec2-4839-4bc7-b249-86264ebe00e9|rpc:10.9.0.13:9856|admin:10.9.0.13:9857|client:10.9.0.13:9858|dataStream:10.9.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-07-06 17:34:45,575 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1    | 2023-07-06 17:34:45,575 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:34:45,575 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1    | 2023-07-06 17:34:45,576 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1    | 2023-07-06 17:34:45,576 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1    | 2023-07-06 17:34:45,576 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-07-06 17:34:45,577 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1    | 2023-07-06 17:34:45,578 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1    | 2023-07-06 17:34:45,578 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-07-06 17:34:45,594 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:34:45,595 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:34:45,578 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1    | 2023-07-06 17:34:45,600 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1    | 2023-07-06 17:34:45,602 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1    | 2023-07-06 17:34:45,615 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1    | 2023-07-06 17:34:45,615 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1    | 2023-07-06 17:34:45,616 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1    | 2023-07-06 17:34:45,616 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1    | 2023-07-06 17:34:45,616 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=false, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-07-06 17:34:45,619 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1    | 2023-07-06 17:35:00,645 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:00,736 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:02,581 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:15,597 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:35:20,895 [72ffd4dc-7ee9-43b8-adf1-05ffe9b89bb5@group-8CA20BA5223C-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019201000.
scm_1    | 2023-07-06 17:35:20,923 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019201000 to 111677748019202000.
scm_1    | 2023-07-06 17:35:39,956 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:40,010 [IPC Server handler 18 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:41,608 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.17
scm_1    | 2023-07-06 17:35:45,599 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1    | 2023-07-06 17:35:52,044 [IPC Server handler 18 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
dn3_1    | 2023-07-06 17:34:51,257 [95d60ec2-4839-4bc7-b249-86264ebe00e9-server-thread2] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: Rolling segment log-0_200 to index:200
dn3_1    | 2023-07-06 17:34:51,309 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_0 to /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_0-200
dn3_1    | 2023-07-06 17:34:51,326 [95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95d60ec2-4839-4bc7-b249-86264ebe00e9@group-6C471268B20E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab0c656-6867-4094-952d-6c471268b20e/current/log_inprogress_201
dn3_1    | 2023-07-06 17:35:43,750 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-07-06 17:35:50,569 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_0, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,639 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_1, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,672 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_2, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,737 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_3, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,753 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_4, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,789 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_5, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,812 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_6, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,850 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_7, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,884 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_8, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,911 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_9, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,935 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_10, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,960 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_11, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:50,993 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_12, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,021 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_13, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,055 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_14, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,080 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_15, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,103 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_16, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,131 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_17, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,157 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_18, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,186 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_19, offset=0, len=1024}
dn3_1    | 2023-07-06 17:35:51,212 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_20, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,235 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_21, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,261 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_22, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,283 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_23, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,308 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_24, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,346 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_25, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,370 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_26, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,409 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_27, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,438 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_28, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,503 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_29, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,532 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_30, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,565 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_31, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,584 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_32, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,606 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_33, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,629 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_34, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,652 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_35, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,679 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_36, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,703 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_37, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,728 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_38, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,750 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_39, offset=1024, len=1024}
dn3_1    | 2023-07-06 17:35:51,773 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_40, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,795 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_41, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,820 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_42, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,842 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_43, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,862 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_44, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,885 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_45, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,907 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_46, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,929 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_47, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,952 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_48, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,973 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_49, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:51,994 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_50, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,017 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_51, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,039 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_52, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,063 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_53, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,095 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_54, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,116 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_55, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,138 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_56, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,169 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_57, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,201 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_58, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,225 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_59, offset=2048, len=1024}
dn3_1    | 2023-07-06 17:35:52,247 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_60, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,270 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_61, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,290 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_62, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,312 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_63, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,334 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_64, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,357 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_65, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,387 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_66, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,411 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_67, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,427 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_68, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,452 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_69, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,477 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_70, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,498 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_71, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,519 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_72, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,540 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_73, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,562 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_74, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,586 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_75, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,606 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_76, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,627 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_77, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,650 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_78, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,671 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_79, offset=3072, len=1024}
dn3_1    | 2023-07-06 17:35:52,693 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_80, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,715 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_81, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,737 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_82, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,764 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_83, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,786 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_84, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,808 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_85, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,830 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_86, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,850 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_87, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,886 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_88, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,907 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_89, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,935 [ChunkWriter-0-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_90, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,957 [ChunkWriter-1-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_91, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:52,988 [ChunkWriter-2-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_92, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,012 [ChunkWriter-3-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_93, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,037 [ChunkWriter-4-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_94, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,065 [ChunkWriter-5-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_95, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,090 [ChunkWriter-6-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_96, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,111 [ChunkWriter-7-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_97, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,135 [ChunkWriter-8-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_98, offset=4096, len=1024}
dn3_1    | 2023-07-06 17:35:53,155 [ChunkWriter-9-0] WARN helpers.ChunkUtils: Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='dcgpost_testdata_chunk_99, offset=4096, len=1024}
