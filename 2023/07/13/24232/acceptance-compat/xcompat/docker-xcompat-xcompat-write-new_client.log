Attaching to xcompat_new_client_1, xcompat_scm_1, xcompat_old_client_1_1_0_1, xcompat_old_client_1_0_0_1, xcompat_old_client_1_3_0_1, xcompat_datanode_1, xcompat_old_client_1_2_1_1, xcompat_s3g_1, xcompat_datanode_3, xcompat_datanode_2, xcompat_om_1, xcompat_recon_1
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:17:36,232 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 78741db3611d/172.19.0.4
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:17:37,347 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = c479229b80bc/172.19.0.8
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:17:37,424 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:17:37,691 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:17:38,750 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:17:39,882 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:17:39,883 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:17:41,042 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c479229b80bc ip:172.19.0.8
datanode_1          | 2023-07-13 21:17:42,094 [main] INFO reflections.Reflections: Reflections took 728 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_1          | 2023-07-13 21:17:46,513 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-07-13 21:17:46,923 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-13 21:17:48,893 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:17:49,164 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-13 21:17:49,182 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:17:49,212 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:17:49,490 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:17:49,529 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:17:49,617 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-13 21:17:49,651 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-13 21:17:49,662 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-13 21:17:49,663 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-13 21:17:49,999 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:17:50,000 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:18:00,954 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-13 21:18:02,361 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:18:02,732 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:18:03,522 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:18:03,610 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-13 21:18:03,611 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:18:03,622 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-13 21:18:03,642 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-13 21:18:03,645 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-13 21:18:03,646 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:18:03,687 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:03,696 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:18:03,696 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:18:03,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:18:04,101 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-13 21:18:04,137 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-13 21:18:06,802 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-13 21:18:06,851 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-13 21:18:06,851 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-13 21:18:06,855 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:06,855 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:18:06,890 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:18:07,281 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-13 21:18:07,599 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-07-13 21:18:09,197 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:18:09,292 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:18:09,550 [main] INFO util.log: Logging initialized @45174ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:18:10,268 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-13 21:18:10,311 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:18:10,385 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:18:10,403 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:18:10,410 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:18:10,410 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:18:10,969 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-07-13 21:18:11,005 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-13 21:18:11,034 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-07-13 21:18:11,225 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:18:11,226 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:18:11,237 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-07-13 21:18:11,296 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3da6950f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:18:11,324 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5478597{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:18:12,084 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ce24203{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8431801383759167554/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:17:36,307 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:17:36,580 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:17:37,341 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:17:38,711 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:17:38,711 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:17:39,710 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:78741db3611d ip:172.19.0.4
datanode_3          | 2023-07-13 21:17:41,309 [main] INFO reflections.Reflections: Reflections took 1240 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_3          | 2023-07-13 21:17:45,539 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-07-13 21:17:46,063 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-13 21:17:48,083 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:17:48,150 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-13 21:17:48,169 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:17:48,187 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:17:48,448 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:17:48,552 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:17:48,599 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-13 21:17:48,601 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-13 21:17:48,616 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-13 21:17:48,621 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-13 21:17:48,853 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:17:48,887 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:17:59,493 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-13 21:18:00,330 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:18:00,619 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:18:01,857 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:18:01,919 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-13 21:18:01,920 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:18:01,933 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-13 21:18:01,938 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-07-13 21:18:01,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-13 21:18:01,965 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:18:02,019 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:02,055 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:18:02,069 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:18:02,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-13 21:18:02,375 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-13 21:18:02,389 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-07-13 21:18:04,831 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-13 21:18:04,841 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-13 21:18:04,879 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-07-13 21:18:04,879 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:18:04,879 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:18:04,926 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:18:05,499 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-13 21:18:06,357 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-07-13 21:18:07,609 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:18:07,692 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:18:08,038 [main] INFO util.log: Logging initialized @43770ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:18:09,087 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-13 21:18:09,137 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:18:09,206 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:18:09,228 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:18:09,228 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-13 21:18:09,241 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:18:09,579 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-07-13 21:18:09,606 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-13 21:18:09,607 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-07-13 21:18:09,792 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:18:09,792 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:18:09,808 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-07-13 21:18:09,907 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42684d86{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:18:09,911 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@39a1e1e6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:18:11,115 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4cc28ad0{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-10117648381175950004/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:18:11,160 [main] INFO server.AbstractConnector: Started ServerConnector@3c5cb013{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:18:11,166 [main] INFO server.Server: Started @46899ms
datanode_3          | 2023-07-13 21:18:11,179 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:18:12,133 [main] INFO server.AbstractConnector: Started ServerConnector@1a22325d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:18:12,148 [main] INFO server.Server: Started @47772ms
datanode_1          | 2023-07-13 21:18:12,166 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:18:12,175 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-13 21:18:12,187 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:18:12,336 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-07-13 21:18:12,554 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-07-13 21:18:12,580 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-07-13 21:18:14,080 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-07-13 21:18:14,080 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-07-13 21:18:14,089 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-07-13 21:18:14,138 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:17:36,812 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 21e4ffd2cfbb/172.19.0.5
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_3          | 2023-07-13 21:18:11,179 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-13 21:18:11,181 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:18:11,420 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-07-13 21:18:11,633 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-07-13 21:18:11,651 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-07-13 21:18:13,291 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-07-13 21:18:13,291 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-07-13 21:18:13,302 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-07-13 21:18:13,303 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-07-13 21:18:13,340 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-13 21:18:13,818 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.3:9891
datanode_3          | 2023-07-13 21:18:14,039 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:18:16,805 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:16,809 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:17,806 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:17,810 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:18,810 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:18,811 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:19,811 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:19,812 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:20,814 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:21,815 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:22,816 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:23,817 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:24,819 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:18:24,915 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 78741db3611d/172.19.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:39096 remote=recon/172.19.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:39096 remote=recon/172.19.0.3:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-13 21:18:29,838 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 78741db3611d/172.19.0.4 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:51390 remote=scm/172.19.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:51390 remote=scm/172.19.0.12:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-13 21:18:32,587 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-30700f11-8bce-413a-b3f0-7c5880fb35f4/container.db to cache
datanode_3          | 2023-07-13 21:18:32,587 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-30700f11-8bce-413a-b3f0-7c5880fb35f4/container.db for volume DS-30700f11-8bce-413a-b3f0-7c5880fb35f4
datanode_3          | 2023-07-13 21:18:32,659 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:18:32,684 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-07-13 21:18:32,930 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-07-13 21:18:32,930 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_3          | 2023-07-13 21:18:33,016 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.RaftServer: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start RPC server
datanode_3          | 2023-07-13 21:18:33,033 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 41fce95e-f628-4a1d-9d42-c4765136d0c7: GrpcService started, listening on 9858
datanode_3          | 2023-07-13 21:18:33,040 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 41fce95e-f628-4a1d-9d42-c4765136d0c7: GrpcService started, listening on 9856
datanode_3          | 2023-07-13 21:18:33,046 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 41fce95e-f628-4a1d-9d42-c4765136d0c7: GrpcService started, listening on 9857
datanode_3          | 2023-07-13 21:18:33,059 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 41fce95e-f628-4a1d-9d42-c4765136d0c7 is started using port 9858 for RATIS
datanode_3          | 2023-07-13 21:18:33,059 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 41fce95e-f628-4a1d-9d42-c4765136d0c7 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-13 21:18:33,060 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 41fce95e-f628-4a1d-9d42-c4765136d0c7 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-13 21:18:33,061 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-41fce95e-f628-4a1d-9d42-c4765136d0c7: Started
datanode_3          | 2023-07-13 21:18:33,145 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:18:42,834 [grpc-default-executor-1] INFO server.RaftServer: 41fce95e-f628-4a1d-9d42-c4765136d0c7: addNew group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-EA8F921E3322:java.util.concurrent.CompletableFuture@6e39754a[Not completed]
datanode_1          | 2023-07-13 21:18:14,150 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-13 21:18:14,701 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.3:9891
datanode_1          | 2023-07-13 21:18:15,155 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:18:17,654 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:17,654 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:18,660 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:18,661 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:19,661 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:19,666 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:20,667 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:21,668 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:22,669 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:23,670 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:24,671 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:24,706 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From c479229b80bc/172.19.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:42572 remote=recon/172.19.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:42572 remote=recon/172.19.0.3:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-13 21:18:25,672 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:18:26,417 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-13 21:18:30,680 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From c479229b80bc/172.19.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:39358 remote=scm/172.19.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:39358 remote=scm/172.19.0.12:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-13 21:18:32,421 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:17:36,864 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:17:37,096 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:17:37,831 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:17:39,410 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:17:39,410 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:17:40,183 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:21e4ffd2cfbb ip:172.19.0.5
datanode_2          | 2023-07-13 21:17:41,814 [main] INFO reflections.Reflections: Reflections took 955 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_2          | 2023-07-13 21:17:46,038 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-13 21:17:46,378 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-13 21:17:48,220 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:17:48,452 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-13 21:17:48,476 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:17:48,495 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:17:48,737 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:17:48,781 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:17:48,792 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-13 21:17:48,802 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-13 21:17:48,813 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-13 21:17:48,814 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-13 21:17:48,987 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:17:48,989 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:17:59,425 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-13 21:17:59,957 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:18:00,356 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:18:01,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:18:01,260 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-13 21:18:01,321 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:18:01,327 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-13 21:18:01,330 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-13 21:18:01,338 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-13 21:18:01,343 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:18:01,409 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:01,425 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:18:01,427 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:18:01,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:18:01,688 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-13 21:18:01,745 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-13 21:18:04,773 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:18:04,805 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-13 21:18:04,811 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-13 21:18:04,814 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:18:04,819 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:18:04,838 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:18:05,188 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-13 21:18:05,530 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-07-13 21:18:06,807 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:18:06,885 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:18:07,226 [main] INFO util.log: Logging initialized @43022ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:18:07,802 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-13 21:18:07,919 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:18:07,996 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:18:08,020 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:18:08,042 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:18:08,043 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:18:08,553 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-07-13 21:18:08,635 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-13 21:18:08,644 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-07-13 21:18:08,848 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:18:08,862 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:18:08,871 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-07-13 21:18:09,007 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@981d9d2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:18:09,008 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3314f179{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:18:42,974 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7: new RaftServerImpl for group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:18:42,984 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:18:42,988 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:18:42,988 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:18:42,988 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:18:42,989 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:18:42,989 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:18:43,062 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:18:43,066 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:18:43,104 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:18:43,125 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:18:43,203 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:18:43,219 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:18:43,239 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:18:43,247 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:18:43,359 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:18:43,401 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:18:43,410 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:18:43,417 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:18:43,418 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:18:43,427 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:18:43,434 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:18:43,436 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 does not exist. Creating ...
datanode_3          | 2023-07-13 21:18:43,448 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/in_use.lock acquired by nodename 8@78741db3611d
datanode_3          | 2023-07-13 21:18:43,463 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 has been successfully formatted.
datanode_3          | 2023-07-13 21:18:43,517 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO ratis.ContainerStateMachine: group-EA8F921E3322: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:18:43,530 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:18:43,601 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:18:43,602 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:43,604 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:18:43,610 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:18:43,617 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:43,630 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:18:43,633 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:18:43,634 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:43,649 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322
datanode_3          | 2023-07-13 21:18:43,651 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:18:43,653 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:18:43,656 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:43,658 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:18:43,658 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:18:43,660 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:18:43,663 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:18:43,663 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:18:43,686 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:43,689 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:43,757 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:18:43,759 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:18:43,760 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:18:43,799 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:18:43,799 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:18:43,806 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:43,806 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:18:43,812 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState
datanode_3          | 2023-07-13 21:18:43,837 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA8F921E3322,id=41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_3          | 2023-07-13 21:18:43,841 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:18:43,842 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:18:43,847 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:18:43,848 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:18:43,849 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:18:43,851 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:18:43,954 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: receive requestVote(PRE_VOTE, 2425729f-e655-496f-bd8d-4ebe87d658f2, group-EA8F921E3322, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:18:43,966 [grpc-default-executor-0] INFO impl.VoteContext: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FOLLOWER: accept PRE_VOTE from 2425729f-e655-496f-bd8d-4ebe87d658f2: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:18:44,003 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322 replies to PRE_VOTE vote request: 2425729f-e655-496f-bd8d-4ebe87d658f2<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t0. Peer's state: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322:t0, leader=null, voted=, raftlog=Memoized:41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:32,761 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-e01ef8d1-dc81-4af3-b911-c5b57a9ba922/container.db to cache
datanode_1          | 2023-07-13 21:18:32,762 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-e01ef8d1-dc81-4af3-b911-c5b57a9ba922/container.db for volume DS-e01ef8d1-dc81-4af3-b911-c5b57a9ba922
datanode_1          | 2023-07-13 21:18:32,781 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-13 21:18:32,791 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-07-13 21:18:32,988 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-07-13 21:18:32,988 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_1          | 2023-07-13 21:18:33,104 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.RaftServer: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start RPC server
datanode_1          | 2023-07-13 21:18:33,121 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: ee3a319d-844c-4f03-bf3b-9b2017254b7d: GrpcService started, listening on 9858
datanode_1          | 2023-07-13 21:18:33,128 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: ee3a319d-844c-4f03-bf3b-9b2017254b7d: GrpcService started, listening on 9856
datanode_1          | 2023-07-13 21:18:33,134 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: ee3a319d-844c-4f03-bf3b-9b2017254b7d: GrpcService started, listening on 9857
datanode_1          | 2023-07-13 21:18:33,146 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ee3a319d-844c-4f03-bf3b-9b2017254b7d is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:18:33,146 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ee3a319d-844c-4f03-bf3b-9b2017254b7d is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-13 21:18:33,147 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ee3a319d-844c-4f03-bf3b-9b2017254b7d is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:18:33,147 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-ee3a319d-844c-4f03-bf3b-9b2017254b7d: Started
datanode_1          | 2023-07-13 21:18:33,203 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:18:37,606 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ee3a319d-844c-4f03-bf3b-9b2017254b7d: addNew group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-EA8F921E3322:java.util.concurrent.CompletableFuture@12582027[Not completed]
datanode_1          | 2023-07-13 21:18:37,704 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d: new RaftServerImpl for group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:18:37,717 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:18:37,718 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:18:37,719 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:18:37,720 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:37,721 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:18:37,722 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:18:37,750 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:18:37,754 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:18:37,768 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:18:37,770 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:18:37,819 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:37,835 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:37,847 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:18:37,850 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:18:37,954 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-13 21:18:38,137 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:18:38,147 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:18:38,155 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:18:38,168 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:18:38,168 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:18:38,169 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:18:38,169 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 does not exist. Creating ...
datanode_1          | 2023-07-13 21:18:38,177 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/in_use.lock acquired by nodename 6@c479229b80bc
datanode_1          | 2023-07-13 21:18:38,199 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 has been successfully formatted.
datanode_1          | 2023-07-13 21:18:38,230 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO ratis.ContainerStateMachine: group-EA8F921E3322: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:18:38,253 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:18:38,274 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:18:38,331 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:38,332 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:18:38,333 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:18:38,365 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:38,382 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:18:38,407 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:18:38,408 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:38,425 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322
datanode_1          | 2023-07-13 21:18:38,432 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:18:38,433 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:18:38,442 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:38,452 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:18:38,453 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:18:38,458 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:18:38,458 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:38,458 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:18:38,501 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:38,507 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:38,610 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:38,628 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:38,631 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:38,678 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:38,678 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:38,696 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:38,696 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:18:38,702 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState
datanode_1          | 2023-07-13 21:18:38,742 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA8F921E3322,id=ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_3          | 2023-07-13 21:18:44,159 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: receive requestVote(ELECTION, 2425729f-e655-496f-bd8d-4ebe87d658f2, group-EA8F921E3322, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:18:44,160 [grpc-default-executor-0] INFO impl.VoteContext: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FOLLOWER: accept ELECTION from 2425729f-e655-496f-bd8d-4ebe87d658f2: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:18:44,163 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_3          | 2023-07-13 21:18:44,164 [grpc-default-executor-0] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: shutdown 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState
datanode_3          | 2023-07-13 21:18:44,164 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState] INFO impl.FollowerState: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState was interrupted
datanode_3          | 2023-07-13 21:18:44,167 [grpc-default-executor-0] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FollowerState
datanode_2          | 2023-07-13 21:18:09,846 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5b27d03d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-10499372556163683734/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-13 21:18:09,940 [main] INFO server.AbstractConnector: Started ServerConnector@126e2710{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:18:09,940 [main] INFO server.Server: Started @45737ms
datanode_2          | 2023-07-13 21:18:09,966 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:18:09,967 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:18:09,972 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:18:10,330 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-13 21:18:10,844 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-07-13 21:18:10,912 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-07-13 21:18:11,966 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-07-13 21:18:11,966 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-07-13 21:18:11,982 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-07-13 21:18:12,021 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-07-13 21:18:12,033 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-13 21:18:12,963 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.3:9891
datanode_2          | 2023-07-13 21:18:13,479 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:18:15,556 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:15,557 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:16,558 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:16,560 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:17,560 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:17,562 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:18,561 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:18,563 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:19,566 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:19,567 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.3:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:20,567 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:21,568 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:22,257 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-07-13 21:18:22,569 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:23,570 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:24,571 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:24,755 [EndpointStateMachine task thread for recon/172.19.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 21e4ffd2cfbb/172.19.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:48148 remote=recon/172.19.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:48148 remote=recon/172.19.0.3:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-13 21:18:25,573 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.12:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:18:30,582 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 21e4ffd2cfbb/172.19.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:52072 remote=scm/172.19.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 2023-07-13 21:18:44,170 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322 replies to ELECTION vote request: 2425729f-e655-496f-bd8d-4ebe87d658f2<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1. Peer's state: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322:t1, leader=null, voted=2425729f-e655-496f-bd8d-4ebe87d658f2, raftlog=Memoized:41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:44,305 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: receive requestVote(PRE_VOTE, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-EA8F921E3322, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:18:44,305 [grpc-default-executor-0] INFO impl.VoteContext: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-FOLLOWER: accept PRE_VOTE from ee3a319d-844c-4f03-bf3b-9b2017254b7d: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:18:44,305 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322 replies to PRE_VOTE vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1. Peer's state: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322:t1, leader=null, voted=2425729f-e655-496f-bd8d-4ebe87d658f2, raftlog=Memoized:41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:45,123 [grpc-default-executor-0] INFO server.RaftServer: 41fce95e-f628-4a1d-9d42-c4765136d0c7: addNew group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] returns group-59EBAFE36A17:java.util.concurrent.CompletableFuture@7eb32205[Not completed]
datanode_3          | 2023-07-13 21:18:45,125 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7: new RaftServerImpl for group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:18:45,126 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:18:45,126 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:18:45,127 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:18:45,127 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:18:45,127 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:18:45,127 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:18:45,127 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:18:45,128 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:18:45,128 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:18:45,128 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:18:45,129 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:18:45,129 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:18:45,129 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:18:45,129 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:18:45,130 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:18:45,133 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:18:45,134 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:18:45,134 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:18:45,135 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:18:38,743 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:38,748 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:18:38,764 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:38,768 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:18:38,769 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:18:38,769 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:18:38,835 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322
datanode_1          | 2023-07-13 21:18:43,121 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322.
datanode_1          | 2023-07-13 21:18:43,122 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ee3a319d-844c-4f03-bf3b-9b2017254b7d: addNew group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] returns group-59EBAFE36A17:java.util.concurrent.CompletableFuture@53da01dc[Not completed]
datanode_1          | 2023-07-13 21:18:43,135 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d: new RaftServerImpl for group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:18:43,142 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:18:43,142 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:18:43,143 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:18:43,143 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:43,143 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:18:43,143 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:18:43,143 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:18:43,147 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:18:43,148 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:18:43,148 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:18:43,149 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:43,149 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:43,153 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:18:43,153 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:18:43,154 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-13 21:18:43,157 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:18:43,157 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:18:43,170 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:18:43,170 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:18:43,170 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:18:43,173 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:18:43,174 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 does not exist. Creating ...
datanode_1          | 2023-07-13 21:18:43,194 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/in_use.lock acquired by nodename 6@c479229b80bc
datanode_1          | 2023-07-13 21:18:43,196 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 has been successfully formatted.
datanode_1          | 2023-07-13 21:18:43,205 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO ratis.ContainerStateMachine: group-59EBAFE36A17: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:18:43,205 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:18:43,214 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:18:43,214 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:43,214 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:18:43,214 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:18:43,214 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:43,219 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:18:43,219 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:18:43,223 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:43,223 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17
datanode_1          | 2023-07-13 21:18:43,227 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:18:43,230 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:18:43,231 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:43,231 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:18:43,231 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:18:43,238 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:18:43,242 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:43,242 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:18:43,248 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:43,250 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:43,921 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:43,921 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:43,921 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:43,927 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:43,927 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:43,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:43,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:52072 remote=scm/172.19.0.12:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-13 21:18:32,518 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-84efecc9-c432-4d9d-80ba-29d8ef354e28/container.db to cache
datanode_2          | 2023-07-13 21:18:32,518 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/DS-84efecc9-c432-4d9d-80ba-29d8ef354e28/container.db for volume DS-84efecc9-c432-4d9d-80ba-29d8ef354e28
datanode_2          | 2023-07-13 21:18:32,543 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:18:45,135 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:18:45,135 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:18:45,136 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 does not exist. Creating ...
datanode_3          | 2023-07-13 21:18:45,144 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/in_use.lock acquired by nodename 8@78741db3611d
datanode_3          | 2023-07-13 21:18:45,152 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 has been successfully formatted.
datanode_3          | 2023-07-13 21:18:45,173 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO ratis.ContainerStateMachine: group-59EBAFE36A17: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:18:45,173 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:18:45,174 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:18:45,174 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:45,175 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:18:45,175 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:18:45,176 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:45,178 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:18:45,183 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:18:45,183 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:45,183 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17
datanode_3          | 2023-07-13 21:18:45,183 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:18:45,183 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:18:45,184 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:45,184 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:18:45,184 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:18:45,185 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:18:45,185 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:18:45,185 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:18:45,186 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:18:45,192 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:18:45,850 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-41fce95e-f628-4a1d-9d42-c4765136d0c7: Detected pause in JVM or host machine approximately 0.196s with 0.646s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=55ms
datanode_3          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=591ms
datanode_3          | 2023-07-13 21:18:45,858 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:18:45,892 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:18:45,892 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:18:45,893 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:18:45,893 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:18:45,893 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:45,893 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:18:45,893 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState
datanode_3          | 2023-07-13 21:18:45,894 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-59EBAFE36A17,id=41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_3          | 2023-07-13 21:18:45,896 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:18:45,896 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:18:45,896 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:18:45,896 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:18:45,916 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:18:45,938 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:18:46,637 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EA8F921E3322 with new leaderId: 2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_3          | 2023-07-13 21:18:46,637 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: change Leader from null to 2425729f-e655-496f-bd8d-4ebe87d658f2 at term 1 for appendEntries, leader elected after 3444ms
datanode_3          | 2023-07-13 21:18:46,810 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:46,822 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:18:47,062 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-EA8F921E3322-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/current/log_inprogress_0
datanode_3          | 2023-07-13 21:18:49,167 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: receive requestVote(PRE_VOTE, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-59EBAFE36A17, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:18:49,168 [grpc-default-executor-0] INFO impl.VoteContext: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FOLLOWER: accept PRE_VOTE from ee3a319d-844c-4f03-bf3b-9b2017254b7d: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:18:49,168 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17 replies to PRE_VOTE vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t0. Peer's state: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17:t0, leader=null, voted=, raftlog=Memoized:41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:49,227 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: receive requestVote(ELECTION, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-59EBAFE36A17, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:18:49,227 [grpc-default-executor-0] INFO impl.VoteContext: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FOLLOWER: accept ELECTION from ee3a319d-844c-4f03-bf3b-9b2017254b7d: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:18:49,227 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_3          | 2023-07-13 21:18:49,227 [grpc-default-executor-0] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: shutdown 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState
datanode_3          | 2023-07-13 21:18:49,227 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState] INFO impl.FollowerState: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState was interrupted
datanode_3          | 2023-07-13 21:18:49,229 [grpc-default-executor-0] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-FollowerState
datanode_3          | 2023-07-13 21:18:49,233 [grpc-default-executor-0] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17 replies to ELECTION vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1. Peer's state: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17:t1, leader=null, voted=ee3a319d-844c-4f03-bf3b-9b2017254b7d, raftlog=Memoized:41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:18:49,546 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-59EBAFE36A17 with new leaderId: ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_3          | 2023-07-13 21:18:49,547 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: change Leader from null to ee3a319d-844c-4f03-bf3b-9b2017254b7d at term 1 for appendEntries, leader elected after 4417ms
datanode_3          | 2023-07-13 21:18:49,637 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread2] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:43,931 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState
datanode_1          | 2023-07-13 21:18:43,934 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-59EBAFE36A17,id=ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_1          | 2023-07-13 21:18:43,934 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:43,934 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:18:43,935 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO impl.FollowerState: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5234631964ns, electionTimeout:5108ms
datanode_1          | 2023-07-13 21:18:43,937 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ee3a319d-844c-4f03-bf3b-9b2017254b7d: Detected pause in JVM or host machine approximately 0.206s with 0.662s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=111ms
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=551ms
datanode_1          | 2023-07-13 21:18:43,935 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:43,977 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:18:43,935 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:18:43,977 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:18:44,000 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17
datanode_1          | 2023-07-13 21:18:43,976 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState
datanode_1          | 2023-07-13 21:18:44,003 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:18:44,041 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-13 21:18:44,042 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1
datanode_1          | 2023-07-13 21:18:44,048 [grpc-default-executor-0] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: receive requestVote(PRE_VOTE, 2425729f-e655-496f-bd8d-4ebe87d658f2, group-EA8F921E3322, 0, (t:0, i:0))
datanode_1          | 2023-07-13 21:18:44,068 [grpc-default-executor-0] INFO impl.VoteContext: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-CANDIDATE: accept PRE_VOTE from 2425729f-e655-496f-bd8d-4ebe87d658f2: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:18:44,138 [grpc-default-executor-0] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322 replies to PRE_VOTE vote request: 2425729f-e655-496f-bd8d-4ebe87d658f2<-ee3a319d-844c-4f03-bf3b-9b2017254b7d#0:OK-t0. Peer's state: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322:t0, leader=null, voted=, raftlog=Memoized:ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:44,140 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:44,160 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:44,161 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:18:44,170 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_1          | 2023-07-13 21:18:44,173 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_1          | 2023-07-13 21:18:44,248 [grpc-default-executor-0] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: receive requestVote(ELECTION, 2425729f-e655-496f-bd8d-4ebe87d658f2, group-EA8F921E3322, 1, (t:0, i:0))
datanode_1          | 2023-07-13 21:18:44,250 [grpc-default-executor-0] INFO impl.VoteContext: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-CANDIDATE: accept ELECTION from 2425729f-e655-496f-bd8d-4ebe87d658f2: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:18:44,264 [grpc-default-executor-0] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_1          | 2023-07-13 21:18:44,266 [grpc-default-executor-0] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1
datanode_1          | 2023-07-13 21:18:44,267 [grpc-default-executor-0] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-FollowerState
datanode_1          | 2023-07-13 21:18:44,282 [grpc-default-executor-0] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322 replies to ELECTION vote request: 2425729f-e655-496f-bd8d-4ebe87d658f2<-ee3a319d-844c-4f03-bf3b-9b2017254b7d#0:OK-t1. Peer's state: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322:t1, leader=null, voted=2425729f-e655-496f-bd8d-4ebe87d658f2, raftlog=Memoized:ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:44,347 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=1) received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:18:44,347 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection:   Response 0: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1
datanode_1          | 2023-07-13 21:18:44,347 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=1)
datanode_1          | 2023-07-13 21:18:45,998 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17.
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d: new RaftServerImpl for group-4316DE8AB2F9:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:18:49,637 [41fce95e-f628-4a1d-9d42-c4765136d0c7-server-thread2] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:18:49,643 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-59EBAFE36A17-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/current/log_inprogress_0
datanode_3          | 2023-07-13 21:19:06,728 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 41fce95e-f628-4a1d-9d42-c4765136d0c7: addNew group-E612A82B9DC6:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER] returns group-E612A82B9DC6:java.util.concurrent.CompletableFuture@37068807[Not completed]
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7: new RaftServerImpl for group-E612A82B9DC6:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:19:06,736 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:19:06,737 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:19:06,738 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:19:06,738 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:19:06,738 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:19:06,738 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:19:06,745 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:19:06,745 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:19:06,745 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6b80d23e-d59b-4573-b55c-e612a82b9dc6 does not exist. Creating ...
datanode_3          | 2023-07-13 21:19:06,751 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6b80d23e-d59b-4573-b55c-e612a82b9dc6/in_use.lock acquired by nodename 8@78741db3611d
datanode_3          | 2023-07-13 21:19:06,759 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6b80d23e-d59b-4573-b55c-e612a82b9dc6 has been successfully formatted.
datanode_3          | 2023-07-13 21:19:06,764 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO ratis.ContainerStateMachine: group-E612A82B9DC6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:19:06,766 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:19:06,767 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:19:06,767 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:19:06,768 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:19:06,768 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:19:06,768 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:19:06,769 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:19:06,769 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:19:06,769 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6b80d23e-d59b-4573-b55c-e612a82b9dc6
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:19:06,770 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:19:06,771 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:19:06,771 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:19:06,771 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:19:06,772 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:19:06,775 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:19:07,273 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-41fce95e-f628-4a1d-9d42-c4765136d0c7: Detected pause in JVM or host machine approximately 0.366s with 0.458s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=458ms
datanode_3          | 2023-07-13 21:19:07,310 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:19:07,313 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:19:07,314 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:19:07,314 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:19:07,324 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:19:07,337 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:19:07,347 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:19:07,347 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState
datanode_3          | 2023-07-13 21:19:07,348 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E612A82B9DC6,id=41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_3          | 2023-07-13 21:19:07,352 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:19:07,352 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:19:07,352 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:19:07,352 [41fce95e-f628-4a1d-9d42-c4765136d0c7-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:18:32,577 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:18:32,842 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-07-13 21:18:32,843 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2425729f-e655-496f-bd8d-4ebe87d658f2
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:17:37,052 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | 2023-07-13 21:19:07,353 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:19:07,353 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:18:32,943 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.RaftServer: 2425729f-e655-496f-bd8d-4ebe87d658f2: start RPC server
datanode_2          | 2023-07-13 21:18:32,957 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 2425729f-e655-496f-bd8d-4ebe87d658f2: GrpcService started, listening on 9858
datanode_2          | 2023-07-13 21:18:32,966 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 2425729f-e655-496f-bd8d-4ebe87d658f2: GrpcService started, listening on 9856
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = df0a8b52cca8/172.19.0.2
om_1                | STARTUP_MSG:   args = [--init]
datanode_2          | 2023-07-13 21:18:32,973 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO server.GrpcService: 2425729f-e655-496f-bd8d-4ebe87d658f2: GrpcService started, listening on 9857
datanode_2          | 2023-07-13 21:18:33,004 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2425729f-e655-496f-bd8d-4ebe87d658f2 is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:18:33,004 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2425729f-e655-496f-bd8d-4ebe87d658f2 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-13 21:18:33,004 [EndpointStateMachine task thread for scm/172.19.0.12:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2425729f-e655-496f-bd8d-4ebe87d658f2 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: ConfigurationManager, init=-1: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:19:07,355 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:17:34,377 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
datanode_2          | 2023-07-13 21:18:33,013 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2425729f-e655-496f-bd8d-4ebe87d658f2: Started
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
s3g_1               | 2023-07-13 21:17:37,242 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
datanode_3          | 2023-07-13 21:19:07,362 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6.
datanode_3          | 2023-07-13 21:19:11,944 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-41fce95e-f628-4a1d-9d42-c4765136d0c7: Detected pause in JVM or host machine approximately 0.151s with 0.393s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=393ms
datanode_3          | 2023-07-13 21:19:12,482 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO impl.FollowerState: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5135377436ns, electionTimeout:5127ms
datanode_2          | 2023-07-13 21:18:33,084 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
s3g_1               | 2023-07-13 21:17:37,249 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 32fd5414c769/172.19.0.3
recon_1             | STARTUP_MSG:   args = []
datanode_2          | 2023-07-13 21:18:37,410 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 2425729f-e655-496f-bd8d-4ebe87d658f2: addNew group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-EA8F921E3322:java.util.concurrent.CompletableFuture@133089c[Not completed]
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
s3g_1               | 2023-07-13 21:17:37,446 [main] INFO util.log: Logging initialized @12862ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:19:12,487 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: shutdown 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState
datanode_3          | 2023-07-13 21:19:12,488 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | 2023-07-13 21:18:37,504 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2: new RaftServerImpl for group-EA8F921E3322:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om_1                | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-13 21:18:46,005 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
s3g_1               | 2023-07-13 21:17:39,074 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-13 21:19:12,495 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-13 21:19:12,496 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-FollowerState] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:19:12,514 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.LeaderElection: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:37,513 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
s3g_1               | 2023-07-13 21:17:39,278 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-07-13 21:17:34,468 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:19:12,515 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.LeaderElection: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-13 21:18:37,515 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
s3g_1               | 2023-07-13 21:17:39,349 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:17:40,126 [main] INFO reflections.Reflections: Reflections took 535 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-07-13 21:17:46,242 [main] INFO reflections.Reflections: Reflections took 443 ms to scan 3 urls, producing 132 keys and 288 values 
recon_1             | 2023-07-13 21:17:46,810 [main] INFO recon.ReconServer: Initializing Recon server...
om_1                | ************************************************************/
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:19:12,529 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.LeaderElection: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:37,516 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
s3g_1               | 2023-07-13 21:17:39,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1             | 2023-07-13 21:17:50,108 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
om_1                | 2023-07-13 21:17:37,154 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:17:45,184 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
datanode_3          | 2023-07-13 21:19:12,529 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.LeaderElection: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-13 21:19:12,529 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: shutdown 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1
s3g_1               | 2023-07-13 21:17:39,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1             | 2023-07-13 21:17:58,568 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
om_1                | 2023-07-13 21:17:48,415 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:17:48,908 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.2:9862
datanode_3          | 2023-07-13 21:19:12,531 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-13 21:17:37,442 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
s3g_1               | 2023-07-13 21:17:39,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:18:46,006 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | WARNING: An illegal reflective access operation has occurred
datanode_2          | 2023-07-13 21:18:37,517 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
om_1                | 2023-07-13 21:17:48,908 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:17:48,908 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
scm_1               | /************************************************************
s3g_1               | 2023-07-13 21:17:39,641 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir17697334103420114471
datanode_1          | 2023-07-13 21:18:46,020 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:18:46,026 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
datanode_2          | 2023-07-13 21:18:37,519 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:17:49,361 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:17:51,265 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863]
scm_1               | STARTUP_MSG: Starting StorageContainerManager
s3g_1               | 2023-07-13 21:17:40,682 [main] INFO s3.Gateway: STARTUP_MSG: 
datanode_1          | 2023-07-13 21:18:46,026 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:18:46,026 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
datanode_2          | 2023-07-13 21:18:37,519 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-13 21:17:55,401 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-07-13 21:17:57,405 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
scm_1               | STARTUP_MSG:   host = 9d26f28165f7/172.19.0.12
s3g_1               | /************************************************************
datanode_1          | 2023-07-13 21:18:46,026 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:18:46,026 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_2          | 2023-07-13 21:18:37,547 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-13 21:17:59,407 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-07-13 21:18:01,414 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm_1               | STARTUP_MSG:   args = [--init]
datanode_3          | 2023-07-13 21:19:12,533 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E612A82B9DC6 with new leaderId: 41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_1          | 2023-07-13 21:18:46,027 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ee3a319d-844c-4f03-bf3b-9b2017254b7d: addNew group-4316DE8AB2F9:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] returns      null ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_1          | 2023-07-13 21:18:46,027 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e94175d1-58c0-4137-b171-4316de8ab2f9 does not exist. Creating ...
recon_1             | WARNING: All illegal access operations will be denied in a future release
datanode_2          | 2023-07-13 21:18:37,550 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:18:03,415 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = cc7a05a62559/172.19.0.6
datanode_3          | 2023-07-13 21:19:12,535 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: change Leader from null to 41fce95e-f628-4a1d-9d42-c4765136d0c7 at term 1 for becomeLeader, leader elected after 5796ms
datanode_1          | 2023-07-13 21:18:46,049 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e94175d1-58c0-4137-b171-4316de8ab2f9/in_use.lock acquired by nodename 6@c479229b80bc
datanode_1          | 2023-07-13 21:18:46,058 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e94175d1-58c0-4137-b171-4316de8ab2f9 has been successfully formatted.
recon_1             | 2023-07-13 21:18:00,984 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
datanode_2          | 2023-07-13 21:18:37,565 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:18:37,569 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:18:37,634 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
s3g_1               | STARTUP_MSG:   args = []
datanode_3          | 2023-07-13 21:19:12,556 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:18:46,059 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO ratis.ContainerStateMachine: group-4316DE8AB2F9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1             | 2023-07-13 21:18:01,000 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
datanode_2          | 2023-07-13 21:18:37,656 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-13 21:18:37,672 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:18:37,675 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:19:12,603 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | 2023-07-13 21:19:12,607 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1             | 2023-07-13 21:18:01,473 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:18:01,776 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
datanode_3          | 2023-07-13 21:19:12,637 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:18:37,818 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:18:37,920 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
recon_1             | 2023-07-13 21:18:01,780 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-13 21:18:05,979 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:18:37,936 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:37,937 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 2023-07-13 21:18:06,122 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:19:12,641 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:18:37,942 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:18:37,945 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:18:37,945 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-13 21:18:05,417 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
s3g_1               | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:18:06,355 [main] INFO util.log: Logging initialized @43941ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:18:06,954 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-07-13 21:18:07,006 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e94175d1-58c0-4137-b171-4316de8ab2f9
datanode_3          | 2023-07-13 21:19:12,643 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 2023-07-13 21:18:07,072 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:18:37,950 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 does not exist. Creating ...
datanode_2          | 2023-07-13 21:18:37,971 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/in_use.lock acquired by nodename 6@21e4ffd2cfbb
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:19:12,677 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:19:12,698 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 2023-07-13 21:18:07,087 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:18:07,091 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:18:07,095 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:19:12,705 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO impl.RoleInfo: 41fce95e-f628-4a1d-9d42-c4765136d0c7: start 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderStateImpl
recon_1             | 2023-07-13 21:18:07,457 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
om_1                | 2023-07-13 21:18:07,419 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-07-13 21:18:09,421 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-07-13 21:18:11,423 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir17697334103420114471, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | 2023-07-13 21:18:46,060 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:19:12,718 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 2023-07-13 21:18:07,497 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
datanode_2          | 2023-07-13 21:18:37,987 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322 has been successfully formatted.
datanode_2          | 2023-07-13 21:18:38,026 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO ratis.ContainerStateMachine: group-EA8F921E3322: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:18:38,031 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
s3g_1               | ************************************************************/
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:19:12,723 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b80d23e-d59b-4573-b55c-e612a82b9dc6/current/log_inprogress_0
recon_1             | 2023-07-13 21:18:09,234 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
datanode_3          | 2023-07-13 21:19:12,743 [41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6-LeaderElection1] INFO server.RaftServer$Division: 41fce95e-f628-4a1d-9d42-c4765136d0c7@group-E612A82B9DC6: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:18:13,425 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-07-13 21:18:15,428 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
s3g_1               | 2023-07-13 21:17:40,799 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 2023-07-13 21:18:09,251 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
datanode_3          | 2023-07-13 21:19:33,146 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-07-13 21:18:17,432 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-07-13 21:18:19,433 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
s3g_1               | 2023-07-13 21:17:41,000 [main] INFO s3.Gateway: Starting Ozone S3 gateway
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 2023-07-13 21:18:09,275 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
datanode_3          | 2023-07-13 21:20:33,147 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
s3g_1               | 2023-07-13 21:17:41,613 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:18:46,061 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:18:46,062 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:18:09,601 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_3          | 2023-07-13 21:21:33,148 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-07-13 21:18:21,435 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-07-13 21:18:23,441 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From df0a8b52cca8/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-07-13 21:18:28,664 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fa59c48c-1a24-41e9-970b-2dd56a1aecbd is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_2          | 2023-07-13 21:18:38,158 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1             | 2023-07-13 21:18:09,602 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
datanode_3          | 2023-07-13 21:22:33,148 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:23:33,149 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
s3g_1               | 2023-07-13 21:17:42,674 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-13 21:17:42,675 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
datanode_2          | 2023-07-13 21:18:38,161 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:18:14,965 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_3          | 2023-07-13 21:24:33,151 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
datanode_1          | 2023-07-13 21:18:46,808 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:46,808 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:18:46,808 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:46,809 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:46,809 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:46,809 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ee3a319d-844c-4f03-bf3b-9b2017254b7d: Detected pause in JVM or host machine approximately 0.287s with 0.701s GC time.
datanode_2          | 2023-07-13 21:18:38,162 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:18:38,182 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:18:38,218 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:18:38,286 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:18:38,286 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:18:38,287 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:38,313 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322
datanode_2          | 2023-07-13 21:18:38,320 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:18:38,325 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:18:38,335 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:18:38,336 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:18:38,342 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:18:38,346 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=701ms
datanode_1          | 2023-07-13 21:18:46,826 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: start as a follower, conf=-1: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:38,349 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:18:38,349 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:18:38,394 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:18:38,403 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:38,524 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:18:38,532 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:18:38,534 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:46,826 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState
datanode_2          | 2023-07-13 21:18:38,564 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:18:38,569 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:18:38,585 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:38,585 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:18:38,596 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4316DE8AB2F9,id=ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_2          | 2023-07-13 21:18:38,611 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:18:38,613 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:18:38,617 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA8F921E3322,id=2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_2          | 2023-07-13 21:18:38,620 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:18:38,621 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:18:38,622 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
s3g_1               | 2023-07-13 21:17:43,292 [main] INFO http.HttpServer2: Jetty bound to port 9878
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
datanode_2          | 2023-07-13 21:18:38,625 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:18:38,682 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
datanode_2          | 2023-07-13 21:18:43,735 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO impl.FollowerState: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139341551ns, electionTimeout:5120ms
datanode_2          | 2023-07-13 21:18:43,736 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: shutdown 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
datanode_2          | 2023-07-13 21:18:43,736 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:18:43,740 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-13 21:18:46,827 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:18:46,828 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:46,828 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:18:46,828 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9
datanode_1          | 2023-07-13 21:18:46,829 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9.
datanode_1          | 2023-07-13 21:18:46,976 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EA8F921E3322 with new leaderId: 2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_1          | 2023-07-13 21:18:46,988 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-server-thread1] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: change Leader from null to 2425729f-e655-496f-bd8d-4ebe87d658f2 at term 1 for appendEntries, leader elected after 9160ms
datanode_1          | 2023-07-13 21:18:47,111 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-server-thread3] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:47,125 [ee3a319d-844c-4f03-bf3b-9b2017254b7d-server-thread3] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:18:47,389 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-EA8F921E3322-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/current/log_inprogress_0
datanode_1          | 2023-07-13 21:18:49,146 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO impl.FollowerState: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5215577754ns, electionTimeout:5169ms
datanode_1          | 2023-07-13 21:18:49,147 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState
datanode_1          | 2023-07-13 21:18:49,148 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:18:49,148 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-13 21:18:49,148 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2
datanode_2          | 2023-07-13 21:18:43,740 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-FollowerState] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1
datanode_2          | 2023-07-13 21:18:43,769 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:43,819 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:49,157 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-13 21:18:16,067 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_1          | 2023-07-13 21:18:49,189 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:49,190 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 2023-07-13 21:18:16,476 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
datanode_1          | 2023-07-13 21:18:49,218 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:18:49,218 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection:   Response 0: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t0
datanode_1          | 2023-07-13 21:18:49,218 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2 PRE_VOTE round 0: result PASSED
s3g_1               | 2023-07-13 21:17:43,300 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-07-13 21:17:43,228 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
recon_1             | 2023-07-13 21:18:16,489 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
datanode_2          | 2023-07-13 21:18:43,819 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:18:43,832 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 41fce95e-f628-4a1d-9d42-c4765136d0c7
datanode_2          | 2023-07-13 21:18:43,832 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for ee3a319d-844c-4f03-bf3b-9b2017254b7d
s3g_1               | 2023-07-13 21:17:43,707 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:17:43,715 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-13 21:18:16,958 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-13 21:18:44,117 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:18:44,119 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2425729f-e655-496f-bd8d-4ebe87d658f2<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t0
datanode_2          | 2023-07-13 21:18:44,119 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1 PRE_VOTE round 0: result PASSED
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1               | 2023-07-13 21:17:43,717 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-07-13 21:18:17,703 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-13 21:18:44,131 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:44,137 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2: new RaftServerImpl for group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:18:49,222 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
s3g_1               | 2023-07-13 21:17:43,768 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e33c391{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:18:17,799 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | ************************************************************/
datanode_2          | 2023-07-13 21:18:44,139 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:18:49,246 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
s3g_1               | 2023-07-13 21:17:43,775 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@20312893{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:18:17,899 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:17:37,528 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:18:44,139 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
s3g_1               | 2023-07-13 21:17:50,167 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.230s with 0.123s GC time.
recon_1             | 2023-07-13 21:18:17,943 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
scm_1               | 2023-07-13 21:17:38,402 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-13 21:18:44,139 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:17:39,980 [main] INFO reflections.Reflections: Reflections took 945 ms to scan 3 urls, producing 132 keys and 288 values 
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=123ms
recon_1             | 2023-07-13 21:18:17,981 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-07-13 21:18:18,807 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
datanode_2          | 2023-07-13 21:18:44,134 [grpc-default-executor-0] INFO server.RaftServer: 2425729f-e655-496f-bd8d-4ebe87d658f2: addNew group-59EBAFE36A17:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER] returns group-59EBAFE36A17:java.util.concurrent.CompletableFuture@2fb9bc10[Not completed]
datanode_2          | 2023-07-13 21:18:44,156 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-13 21:17:41,511 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
s3g_1               | 2023-07-13 21:18:10,127 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6bfdaa7a{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir17697334103420114471/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-7156638040001512640/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-07-13 21:18:10,280 [main] INFO server.AbstractConnector: Started ServerConnector@106faf11{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:18:10,280 [main] INFO server.Server: Started @45696ms
om_1                | 2023-07-13 21:18:30,683 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fa59c48c-1a24-41e9-970b-2dd56a1aecbd is not the leader. Could not determine the leader node.
datanode_2          | 2023-07-13 21:18:44,173 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:18:44,140 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
s3g_1               | 2023-07-13 21:18:10,336 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-07-13 21:18:10,362 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-13 21:18:18,909 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_2          | 2023-07-13 21:18:44,179 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:18:44,180 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:18:44,180 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: ConfigurationManager, init=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
s3g_1               | 2023-07-13 21:18:10,387 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-07-13 21:18:19,057 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
datanode_2          | 2023-07-13 21:18:44,184 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:18:49,246 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:18:49,264 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
recon_1             | 2023-07-13 21:18:19,086 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
datanode_2          | 2023-07-13 21:18:44,185 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
datanode_1          | 2023-07-13 21:18:49,264 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection:   Response 0: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1
datanode_1          | 2023-07-13 21:18:49,264 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2 ELECTION round 0: result PASSED
datanode_1          | 2023-07-13 21:18:49,264 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2
datanode_1          | 2023-07-13 21:18:49,265 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:18:49,265 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-59EBAFE36A17 with new leaderId: ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_1          | 2023-07-13 21:18:49,265 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: change Leader from null to ee3a319d-844c-4f03-bf3b-9b2017254b7d at term 1 for becomeLeader, leader elected after 6115ms
datanode_1          | 2023-07-13 21:18:49,274 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
datanode_2          | 2023-07-13 21:18:44,185 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:18:44,185 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm_1               | 2023-07-13 21:17:41,624 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:17:42,844 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1             | 2023-07-13 21:18:19,223 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-13 21:18:19,802 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-07-13 21:18:19,822 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-13 21:18:19,822 [main] INFO recon.ReconServer: Starting Recon server
datanode_2          | 2023-07-13 21:18:44,186 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 2023-07-13 21:18:20,259 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:18:49,324 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:18:49,325 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:18:44,186 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 2023-07-13 21:18:20,328 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:18:44,186 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 2023-07-13 21:18:20,328 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
scm_1               | 2023-07-13 21:17:43,990 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:17:44,004 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
datanode_2          | 2023-07-13 21:18:44,186 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:18:44,188 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 2023-07-13 21:18:20,936 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-13 21:18:20,938 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-07-13 21:18:21,072 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:18:44,191 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 2023-07-13 21:18:21,074 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:18:49,334 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:18:49,335 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:18:44,193 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:18:44,194 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-13 21:17:44,019 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:18:49,335 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 2023-07-13 21:18:21,077 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-13 21:18:21,111 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@396ec737{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-13 21:17:44,023 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1             | 2023-07-13 21:18:21,114 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@490704a5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:18:49,342 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:18:49,349 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:17:44,023 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1             | 2023-07-13 21:18:26,875 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@78865ca7{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-12608710985159514410/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
datanode_2          | 2023-07-13 21:18:44,194 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-13 21:17:44,023 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1             | 2023-07-13 21:18:26,910 [main] INFO server.AbstractConnector: Started ServerConnector@43de88f3{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
datanode_2          | 2023-07-13 21:18:44,194 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:17:44,031 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1             | 2023-07-13 21:18:26,910 [main] INFO server.Server: Started @64496ms
datanode_2          | 2023-07-13 21:18:44,194 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 does not exist. Creating ...
scm_1               | 2023-07-13 21:17:44,048 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:18:26,932 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:18:44,195 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm_1               | 2023-07-13 21:17:44,052 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_1          | 2023-07-13 21:18:49,381 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:18:44,202 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2425729f-e655-496f-bd8d-4ebe87d658f2<-41fce95e-f628-4a1d-9d42-c4765136d0c7#0:OK-t1
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:17:44,063 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1             | 2023-07-13 21:18:26,932 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:18:44,202 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1 ELECTION round 0: result PASSED
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
scm_1               | 2023-07-13 21:17:44,128 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 2023-07-13 21:18:26,939 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
datanode_1          | 2023-07-13 21:18:49,385 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:49,387 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:18:49,396 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1             | 2023-07-13 21:18:26,939 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0;layoutVersion=6
datanode_2          | 2023-07-13 21:18:44,202 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: shutdown 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1
scm_1               | 2023-07-13 21:17:44,158 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-13 21:18:49,398 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1             | 2023-07-13 21:18:27,065 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
om_1                | 2023-07-13 21:18:33,498 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
datanode_2          | 2023-07-13 21:18:44,203 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-13 21:17:44,171 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-13 21:18:49,398 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 2023-07-13 21:18:27,093 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:18:27,093 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
datanode_2          | 2023-07-13 21:18:44,204 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EA8F921E3322 with new leaderId: 2425729f-e655-496f-bd8d-4ebe87d658f2
scm_1               | 2023-07-13 21:17:46,481 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1             | 2023-07-13 21:18:27,093 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | /************************************************************
datanode_2          | 2023-07-13 21:18:44,204 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: change Leader from null to 2425729f-e655-496f-bd8d-4ebe87d658f2 at term 1 for becomeLeader, leader elected after 6575ms
scm_1               | 2023-07-13 21:17:46,510 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1             | 2023-07-13 21:18:27,095 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at df0a8b52cca8/172.19.0.2
datanode_2          | 2023-07-13 21:18:44,199 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/in_use.lock acquired by nodename 6@21e4ffd2cfbb
datanode_1          | 2023-07-13 21:18:49,398 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm_1               | 2023-07-13 21:17:46,531 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1             | 2023-07-13 21:18:27,108 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
om_1                | ************************************************************/
datanode_2          | 2023-07-13 21:18:44,213 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17 has been successfully formatted.
scm_1               | 2023-07-13 21:17:46,535 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1             | 2023-07-13 21:18:30,997 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fa59c48c-1a24-41e9-970b-2dd56a1aecbd is not the leader. Could not determine the leader node.
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | 2023-07-13 21:18:44,214 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO ratis.ContainerStateMachine: group-59EBAFE36A17: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:18:49,399 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm_1               | 2023-07-13 21:17:46,535 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_2          | 2023-07-13 21:18:44,215 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:18:49,403 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-13 21:17:46,566 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
datanode_2          | 2023-07-13 21:18:44,215 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:18:49,403 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-13 21:17:46,623 [main] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: addNew group-6B16FD24A1A0:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER] returns group-6B16FD24A1A0:java.util.concurrent.CompletableFuture@68ab0936[Not completed]
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
datanode_2          | 2023-07-13 21:18:44,253 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:40,490 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
scm_1               | 2023-07-13 21:17:46,781 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: new RaftServerImpl for group-6B16FD24A1A0:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
datanode_1          | 2023-07-13 21:18:49,421 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:18:44,258 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | /************************************************************
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
datanode_2          | 2023-07-13 21:18:44,258 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | STARTUP_MSG: Starting OzoneManager
scm_1               | 2023-07-13 21:17:46,803 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | STARTUP_MSG:   host = df0a8b52cca8/172.19.0.2
scm_1               | 2023-07-13 21:17:46,813 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
datanode_2          | 2023-07-13 21:18:44,259 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | 2023-07-13 21:17:46,813 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:18:49,421 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:44,261 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-13 21:17:46,817 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:17:46,822 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
datanode_2          | 2023-07-13 21:18:44,261 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:17:46,822 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:18:49,421 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
datanode_2          | 2023-07-13 21:18:44,262 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:17:46,883 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: ConfigurationManager, init=-1: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | STARTUP_MSG:   java = 11.0.19
datanode_2          | 2023-07-13 21:18:44,262 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_1          | 2023-07-13 21:18:49,422 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-13 21:18:44,262 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:18:44,262 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | 2023-07-13 21:18:49,422 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:18:49,426 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:18:44,264 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:18:44,264 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:18:44,264 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:18:49,426 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-13 21:18:49,426 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-07-13 21:18:49,426 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:44,264 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_2          | 2023-07-13 21:18:44,264 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:18:44,265 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:18:44,267 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
datanode_1          | 2023-07-13 21:18:49,426 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:18:44,269 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:17:46,895 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:17:46,941 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:18:49,433 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderStateImpl
datanode_2          | 2023-07-13 21:18:44,790 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:17:46,947 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:17:47,172 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1          | 2023-07-13 21:18:49,435 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 2023-07-13 21:18:34,820 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 1 pipelines from SCM.
scm_1               | 2023-07-13 21:17:47,240 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-13 21:17:47,281 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
datanode_2          | 2023-07-13 21:18:44,794 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322.
datanode_1          | 2023-07-13 21:18:49,443 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/current/log_inprogress_0
scm_1               | 2023-07-13 21:17:47,327 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:17:47,649 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:18:44,798 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2425729f-e655-496f-bd8d-4ebe87d658f2: Detected pause in JVM or host machine approximately 0.215s with 0.494s GC time.
datanode_1          | 2023-07-13 21:18:49,478 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17-LeaderElection2] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-59EBAFE36A17: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | ************************************************************/
datanode_1          | 2023-07-13 21:18:51,928 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO impl.FollowerState: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5101469236ns, electionTimeout:5100ms
datanode_1          | 2023-07-13 21:18:51,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState
recon_1             | 2023-07-13 21:18:34,820 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:18:34,822 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6 from SCM.
om_1                | 2023-07-13 21:18:40,547 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:18:51,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:18:51,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1             | 2023-07-13 21:18:35,590 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=81ms
om_1                | 2023-07-13 21:18:44,409 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
scm_1               | 2023-07-13 21:17:47,745 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:18:51,930 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-FollowerState] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3
recon_1             | 2023-07-13 21:18:35,602 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:18:35,611 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-13 21:18:45,799 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:18:46,276 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.2:9862
datanode_1          | 2023-07-13 21:18:51,942 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:18:51,943 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=413ms
om_1                | 2023-07-13 21:18:46,280 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:18:46,282 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
datanode_1          | 2023-07-13 21:18:51,951 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-13 21:18:35,612 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
scm_1               | 2023-07-13 21:17:49,168 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:17:49,185 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:44,805 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 2425729f-e655-496f-bd8d-4ebe87d658f2: addNew group-F8B3FB7A52C2:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-F8B3FB7A52C2:java.util.concurrent.CompletableFuture@a2fb46c[Not completed]
datanode_2          | 2023-07-13 21:18:44,813 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 2023-07-13 21:17:49,190 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-13 21:17:49,190 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:17:49,200 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:18:44,815 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:18:44,817 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-13 21:18:46,573 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:18:46,993 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
datanode_1          | 2023-07-13 21:18:51,951 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.LeaderElection: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-13 21:18:51,954 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: shutdown ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3
datanode_1          | 2023-07-13 21:18:51,955 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1             | 2023-07-13 21:18:36,927 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:41914 / 172.19.0.8:41914: output error
recon_1             | 2023-07-13 21:18:36,944 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:55560 / 172.19.0.5:55560: output error
recon_1             | 2023-07-13 21:18:36,951 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
datanode_2          | 2023-07-13 21:18:44,826 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: receive requestVote(PRE_VOTE, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-EA8F921E3322, 0, (t:0, i:0))
datanode_2          | 2023-07-13 21:18:44,818 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:18:44,827 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1               | 2023-07-13 21:17:49,211 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:17:49,237 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0 does not exist. Creating ...
scm_1               | 2023-07-13 21:17:49,275 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/in_use.lock acquired by nodename 12@9d26f28165f7
datanode_2          | 2023-07-13 21:18:44,827 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:18:51,956 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4316DE8AB2F9 with new leaderId: ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_2          | 2023-07-13 21:18:44,827 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:18:44,835 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: start as a follower, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:44,835 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_1          | 2023-07-13 21:18:51,957 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: change Leader from null to ee3a319d-844c-4f03-bf3b-9b2017254b7d at term 1 for becomeLeader, leader elected after 5949ms
datanode_2          | 2023-07-13 21:18:44,836 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_2          | 2023-07-13 21:18:44,836 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-59EBAFE36A17,id=2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_1          | 2023-07-13 21:18:51,958 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-13 21:18:47,694 [main] INFO reflections.Reflections: Reflections took 519 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-07-13 21:18:47,717 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-07-13 21:18:47,812 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-13 21:18:44,836 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:18:51,973 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:18:44,836 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1               | 2023-07-13 21:17:49,331 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0 has been successfully formatted.
scm_1               | 2023-07-13 21:17:49,385 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-13 21:17:49,405 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:18:44,836 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:18:51,973 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:18:51,975 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:18:44,837 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:18:44,844 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2: new RaftServerImpl for group-F8B3FB7A52C2:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
om_1                | 2023-07-13 21:18:48,736 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863]
datanode_2          | 2023-07-13 21:18:44,844 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1               | 2023-07-13 21:17:49,406 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
om_1                | 2023-07-13 21:18:48,938 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.12:9863]
om_1                | 2023-07-13 21:18:50,785 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-07-13 21:18:50,844 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:17:49,417 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:17:49,422 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:18:51,975 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-13 21:18:51,275 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
datanode_2          | 2023-07-13 21:18:44,845 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-13 21:17:49,471 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
om_1                | 2023-07-13 21:18:52,359 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
datanode_2          | 2023-07-13 21:18:44,846 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:17:49,519 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_1          | 2023-07-13 21:18:51,975 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-13 21:18:52,516 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
datanode_2          | 2023-07-13 21:18:44,846 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm_1               | 2023-07-13 21:17:49,532 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:17:49,535 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:18:51,975 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om_1                | 2023-07-13 21:18:52,541 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-13 21:18:44,847 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-07-13 21:17:49,557 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0
datanode_1          | 2023-07-13 21:18:51,976 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-13 21:18:52,650 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-07-13 21:18:52,652 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-07-13 21:18:53,121 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:18:53,336 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:18:53,339 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_1          | 2023-07-13 21:18:51,978 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO impl.RoleInfo: ee3a319d-844c-4f03-bf3b-9b2017254b7d: start ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderStateImpl
om_1                | 2023-07-13 21:18:53,442 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-13 21:18:53,464 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-13 21:17:49,573 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | 2023-07-13 21:18:44,848 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:17:49,575 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:18:53,542 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1          | 2023-07-13 21:18:51,983 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:18:51,987 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e94175d1-58c0-4137-b171-4316de8ab2f9/current/log_inprogress_0
datanode_2          | 2023-07-13 21:18:44,848 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: ConfigurationManager, init=-1: peers:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:18:44,849 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_2          | 2023-07-13 21:18:44,845 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:18:51,992 [ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9-LeaderElection3] INFO server.RaftServer$Division: ee3a319d-844c-4f03-bf3b-9b2017254b7d@group-4316DE8AB2F9: set configuration 0: peers:[ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:17:49,591 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:18:53,574 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_1          | 2023-07-13 21:19:11,636 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ee3a319d-844c-4f03-bf3b-9b2017254b7d: Detected pause in JVM or host machine approximately 0.224s with 0.479s GC time.
recon_1             | 2023-07-13 21:18:36,951 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:48148 / 172.19.0.5:48148: output error
scm_1               | 2023-07-13 21:17:49,592 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=479ms
recon_1             | 2023-07-13 21:18:36,960 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:42572 / 172.19.0.8:42572: output error
recon_1             | 2023-07-13 21:18:36,960 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
om_1                | 2023-07-13 21:18:53,740 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:18:44,852 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:19:33,204 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_1          | 2023-07-13 21:20:33,204 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:21:33,205 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:17:49,598 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:22:33,206 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:17:49,604 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_1          | 2023-07-13 21:23:33,206 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:17:49,614 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_1          | 2023-07-13 21:24:33,207 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:17:49,615 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-13 21:18:53,759 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:18:53,762 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
datanode_2          | 2023-07-13 21:18:44,853 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm_1               | 2023-07-13 21:17:49,785 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-13 21:17:49,819 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:53,762 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:18:53,764 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-13 21:18:53,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:17:50,238 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:18:44,859 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-13 21:17:50,247 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-13 21:18:53,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
datanode_2          | 2023-07-13 21:18:44,859 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-07-13 21:17:50,251 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:18:53,767 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:44,866 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-07-13 21:17:50,379 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:18:53,771 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:44,866 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-13 21:17:50,379 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:18:53,773 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_2          | 2023-07-13 21:18:44,867 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-07-13 21:17:50,393 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: start as a follower, conf=-1: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:17:50,467 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:18:44,867 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-07-13 21:17:50,468 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState
om_1                | 2023-07-13 21:18:53,775 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_2          | 2023-07-13 21:18:44,867 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:18:44,867 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:17:50,584 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:18:53,802 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:18:44,867 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:18:44,876 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm_1               | 2023-07-13 21:17:50,618 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-13 21:18:53,810 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-13 21:18:44,876 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-07-13 21:17:50,785 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6B16FD24A1A0,id=fa59c48c-1a24-41e9-970b-2dd56a1aecbd
om_1                | 2023-07-13 21:18:53,810 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-13 21:18:44,876 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-07-13 21:17:50,821 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:18:54,580 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:18:44,876 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-07-13 21:17:50,839 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om_1                | 2023-07-13 21:18:54,594 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-13 21:18:44,877 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-13 21:17:50,839 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:18:54,597 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-13 21:18:44,877 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:17:50,840 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om_1                | 2023-07-13 21:18:54,599 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
datanode_2          | 2023-07-13 21:18:44,878 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2 does not exist. Creating ...
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-13 21:17:50,882 [main] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start RPC server
om_1                | 2023-07-13 21:18:54,601 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:18:44,884 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2/in_use.lock acquired by nodename 6@21e4ffd2cfbb
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:17:51,532 [main] INFO server.GrpcService: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: GrpcService started, listening on 9894
datanode_2          | 2023-07-13 21:18:44,887 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2 has been successfully formatted.
recon_1             | 2023-07-13 21:18:36,960 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:39096 / 172.19.0.4:39096: output error
scm_1               | 2023-07-13 21:17:51,608 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fa59c48c-1a24-41e9-970b-2dd56a1aecbd: Started
om_1                | 2023-07-13 21:18:54,608 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:18:54,661 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@614da024[Not completed]
recon_1             | 2023-07-13 21:18:36,960 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
scm_1               | 2023-07-13 21:17:55,762 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.FollowerState: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5293465048ns, electionTimeout:5134ms
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-07-13 21:17:55,763 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState
om_1                | 2023-07-13 21:18:54,661 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
datanode_2          | 2023-07-13 21:18:44,890 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-13 21:17:55,769 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:18:54,667 [main] INFO om.OzoneManager: Creating RPC Server
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO ratis.ContainerStateMachine: group-F8B3FB7A52C2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-07-13 21:17:55,787 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-07-13 21:18:54,726 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-13 21:18:54,755 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-13 21:18:54,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
om_1                | 2023-07-13 21:18:54,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-13 21:18:54,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:18:54,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:17:55,787 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-13 21:18:54,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:17:55,796 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:17:55,798 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:54,799 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:17:55,814 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-13 21:18:54,800 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1               | 2023-07-13 21:17:55,814 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_2          | 2023-07-13 21:18:44,904 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-13 21:18:54,848 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:17:55,816 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_2          | 2023-07-13 21:18:44,905 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om_1                | 2023-07-13 21:18:54,849 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:17:55,817 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_2          | 2023-07-13 21:18:44,920 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-13 21:18:54,926 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm_1               | 2023-07-13 21:17:55,817 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: change Leader from null to fa59c48c-1a24-41e9-970b-2dd56a1aecbd at term 1 for becomeLeader, leader elected after 8645ms
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-13 21:18:54,961 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-13 21:17:55,835 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:17:55,853 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 2023-07-13 21:18:54,992 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2
scm_1               | 2023-07-13 21:17:55,854 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 2023-07-13 21:18:55,005 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm_1               | 2023-07-13 21:17:55,910 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 2023-07-13 21:18:55,192 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-13 21:17:55,936 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-07-13 21:18:55,354 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm_1               | 2023-07-13 21:17:55,937 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | 2023-07-13 21:18:55,371 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:44,933 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm_1               | 2023-07-13 21:17:55,977 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 2023-07-13 21:18:36,948 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:41910 / 172.19.0.8:41910: output error
om_1                | 2023-07-13 21:18:55,372 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:18:44,935 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm_1               | 2023-07-13 21:17:55,983 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 2023-07-13 21:18:36,961 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
datanode_2          | 2023-07-13 21:18:44,935 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:18:44,935 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-13 21:18:55,372 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:17:56,185 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderStateImpl
datanode_2          | 2023-07-13 21:18:44,935 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om_1                | 2023-07-13 21:18:55,373 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-13 21:18:55,376 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:18:44,935 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_2          | 2023-07-13 21:18:44,936 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1               | 2023-07-13 21:17:56,606 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-13 21:17:57,232 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: set configuration 0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_2          | 2023-07-13 21:18:44,942 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:17:57,902 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/log_inprogress_0
scm_1               | 2023-07-13 21:17:59,668 [main] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: close
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-07-13 21:17:59,669 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: shutdown
scm_1               | 2023-07-13 21:17:59,669 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-6B16FD24A1A0,id=fa59c48c-1a24-41e9-970b-2dd56a1aecbd
datanode_2          | 2023-07-13 21:18:44,956 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-13 21:17:59,669 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderStateImpl
scm_1               | 2023-07-13 21:17:59,684 [main] INFO server.GrpcService: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown server GrpcServerProtocolService now
datanode_2          | 2023-07-13 21:18:45,052 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-07-13 21:17:59,717 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO impl.PendingRequests: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-13 21:17:59,873 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO impl.StateMachineUpdater: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater: set stopIndex = 0
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-07-13 21:17:59,884 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO impl.StateMachineUpdater: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-07-13 21:17:59,919 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO impl.StateMachineUpdater: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
datanode_2          | 2023-07-13 21:18:45,052 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
om_1                | 2023-07-13 21:18:56,358 [main] INFO reflections.Reflections: Reflections took 1394 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1                | 2023-07-13 21:18:56,927 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-13 21:18:45,052 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om_1                | 2023-07-13 21:18:56,944 [main] INFO ipc.Server: Listener at om:9862
scm_1               | 2023-07-13 21:17:59,930 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: closes. applyIndex: 0
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om_1                | 2023-07-13 21:18:56,953 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
datanode_2          | 2023-07-13 21:18:45,055 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:17:59,954 [main] INFO server.GrpcService: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown server GrpcServerProtocolService successfully
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 2023-07-13 21:18:58,100 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:18:45,055 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:17:59,975 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker close()
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-07-13 21:17:59,978 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fa59c48c-1a24-41e9-970b-2dd56a1aecbd: Stopped
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2          | 2023-07-13 21:18:45,058 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om_1                | 2023-07-13 21:18:58,118 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:18:58,119 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-13 21:18:58,191 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.19.0.2:9862
scm_1               | 2023-07-13 21:17:59,979 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:18:58,192 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
scm_1               | 2023-07-13 21:17:59,997 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0; layoutVersion=7; scmId=fa59c48c-1a24-41e9-970b-2dd56a1aecbd
datanode_2          | 2023-07-13 21:18:45,060 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:18:45,061 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om_1                | 2023-07-13 21:18:58,195 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
scm_1               | 2023-07-13 21:18:00,072 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
datanode_2          | 2023-07-13 21:18:45,064 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: start as a follower, conf=-1: peers:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-07-13 21:18:58,200 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@df0a8b52cca8
scm_1               | /************************************************************
datanode_2          | 2023-07-13 21:18:45,066 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | 2023-07-13 21:18:58,207 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 9d26f28165f7/172.19.0.12
datanode_2          | 2023-07-13 21:18:45,068 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState
datanode_2          | 2023-07-13 21:18:45,081 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F8B3FB7A52C2,id=2425729f-e655-496f-bd8d-4ebe87d658f2
datanode_2          | 2023-07-13 21:18:45,081 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | 2023-07-13 21:18:36,950 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
om_1                | 2023-07-13 21:18:58,212 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-13 21:18:58,222 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:18:45,081 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1               | 2023-07-13 21:18:12,788 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om_1                | 2023-07-13 21:18:58,223 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | /************************************************************
om_1                | 2023-07-13 21:18:58,224 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | STARTUP_MSG: Starting StorageContainerManager
datanode_2          | 2023-07-13 21:18:45,081 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om_1                | 2023-07-13 21:18:58,225 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-13 21:18:58,248 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:18:58,255 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | STARTUP_MSG:   host = 9d26f28165f7/172.19.0.12
datanode_2          | 2023-07-13 21:18:45,081 [2425729f-e655-496f-bd8d-4ebe87d658f2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om_1                | 2023-07-13 21:18:58,255 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | STARTUP_MSG:   args = []
datanode_2          | 2023-07-13 21:18:45,082 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:18:58,256 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:58,263 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-13 21:18:58,263 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | 2023-07-13 21:18:45,082 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:18:45,092 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_2          | 2023-07-13 21:18:45,092 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2.
om_1                | 2023-07-13 21:18:58,264 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:18:58,266 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:18:58,266 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-13 21:18:58,268 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-13 21:18:58,269 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
om_1                | 2023-07-13 21:18:58,272 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:18:45,094 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm_1               | STARTUP_MSG:   java = 11.0.19
om_1                | 2023-07-13 21:18:58,273 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | 2023-07-13 21:18:45,096 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 2023-07-13 21:18:58,283 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
datanode_2          | 2023-07-13 21:18:45,097 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-07-13 21:18:58,283 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:58,307 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-13 21:18:58,308 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:18:36,967 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:38894 / 172.19.0.4:38894: output error
om_1                | 2023-07-13 21:18:58,308 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:18:58,316 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:18:45,100 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-07-13 21:18:45,100 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:18:12,937 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:18:45,100 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:45,100 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 2023-07-13 21:18:36,967 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:18:13,533 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:18:14,276 [main] INFO reflections.Reflections: Reflections took 469 ms to scan 3 urls, producing 132 keys and 288 values 
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-13 21:18:58,316 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:18:14,790 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-13 21:18:14,881 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-07-13 21:18:17,778 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:18:45,115 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-07-13 21:18:45,126 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm_1               | 2023-07-13 21:18:18,306 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om_1                | 2023-07-13 21:18:58,319 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:18:58,323 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-13 21:18:58,327 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
scm_1               | 2023-07-13 21:18:19,054 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-13 21:18:19,073 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-13 21:18:19,378 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:18:45,126 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:45,126 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:18:45,130 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderStateImpl
scm_1               | 2023-07-13 21:18:20,154 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:fa59c48c-1a24-41e9-970b-2dd56a1aecbd
om_1                | 2023-07-13 21:18:58,328 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:18:58,328 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-13 21:18:58,329 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
scm_1               | 2023-07-13 21:18:20,435 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-13 21:18:58,332 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:18:58,333 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-13 21:18:58,333 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:18:58,334 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
om_1                | 2023-07-13 21:18:58,343 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-13 21:18:58,399 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-13 21:18:58,403 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-13 21:18:58,408 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-13 21:18:58,468 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:18:58,469 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:18:45,873 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2425729f-e655-496f-bd8d-4ebe87d658f2: Detected pause in JVM or host machine approximately 0.574s with 0.723s GC time.
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-07-13 21:18:20,454 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=723ms
datanode_2          | 2023-07-13 21:18:45,967 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:18:46,247 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LeaderElection1] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:20,458 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
datanode_2          | 2023-07-13 21:18:46,266 [grpc-default-executor-0] INFO impl.VoteContext: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-LEADER: reject PRE_VOTE from ee3a319d-844c-4f03-bf3b-9b2017254b7d: this server is the leader and still has leadership
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-07-13 21:18:20,460 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:18:20,460 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
datanode_2          | 2023-07-13 21:18:46,267 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322 replies to PRE_VOTE vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-2425729f-e655-496f-bd8d-4ebe87d658f2#0:FAIL-t1. Peer's state: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322:t1, leader=2425729f-e655-496f-bd8d-4ebe87d658f2, voted=2425729f-e655-496f-bd8d-4ebe87d658f2, raftlog=Memoized:2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLog:OPENED:c-1, conf=0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:47,040 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-EA8F921E3322-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5c61ba5b-cb65-4360-91cd-ea8f921e3322/current/log_inprogress_0
datanode_2          | 2023-07-13 21:18:49,198 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: receive requestVote(PRE_VOTE, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-59EBAFE36A17, 0, (t:0, i:0))
scm_1               | 2023-07-13 21:18:20,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:18:20,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-13 21:18:20,463 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:18:36,964 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:55572 / 172.19.0.5:55572: output error
recon_1             | 2023-07-13 21:18:36,964 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:38896 / 172.19.0.4:38896: output error
recon_1             | 2023-07-13 21:18:36,985 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-07-13 21:18:20,466 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:18:58,497 [main] INFO util.log: Logging initialized @24224ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:18:58,631 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-13 21:18:49,198 [grpc-default-executor-0] INFO impl.VoteContext: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FOLLOWER: accept PRE_VOTE from ee3a319d-844c-4f03-bf3b-9b2017254b7d: our priority 0 <= candidate's priority 1
om_1                | 2023-07-13 21:18:58,640 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:18:58,650 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:18:58,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm_1               | 2023-07-13 21:18:20,468 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-13 21:18:20,470 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om_1                | 2023-07-13 21:18:58,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2          | 2023-07-13 21:18:49,201 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17 replies to PRE_VOTE vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-2425729f-e655-496f-bd8d-4ebe87d658f2#0:OK-t0. Peer's state: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17:t0, leader=null, voted=, raftlog=Memoized:2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:49,256 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: receive requestVote(ELECTION, ee3a319d-844c-4f03-bf3b-9b2017254b7d, group-59EBAFE36A17, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:18:49,258 [grpc-default-executor-0] INFO impl.VoteContext: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FOLLOWER: accept ELECTION from ee3a319d-844c-4f03-bf3b-9b2017254b7d: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:18:49,261 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_2          | 2023-07-13 21:18:49,261 [grpc-default-executor-0] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: shutdown 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState
datanode_2          | 2023-07-13 21:18:49,261 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState] INFO impl.FollowerState: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState was interrupted
scm_1               | 2023-07-13 21:18:20,500 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-13 21:18:58,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:18:58,788 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
datanode_2          | 2023-07-13 21:18:49,262 [grpc-default-executor-0] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-FollowerState
scm_1               | 2023-07-13 21:18:20,509 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-13 21:18:58,802 [main] INFO http.HttpServer2: Jetty bound to port 9874
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_2          | 2023-07-13 21:18:49,268 [grpc-default-executor-0] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17 replies to ELECTION vote request: ee3a319d-844c-4f03-bf3b-9b2017254b7d<-2425729f-e655-496f-bd8d-4ebe87d658f2#0:OK-t1. Peer's state: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17:t1, leader=null, voted=ee3a319d-844c-4f03-bf3b-9b2017254b7d, raftlog=Memoized:2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:49,700 [2425729f-e655-496f-bd8d-4ebe87d658f2-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-59EBAFE36A17 with new leaderId: ee3a319d-844c-4f03-bf3b-9b2017254b7d
datanode_2          | 2023-07-13 21:18:49,700 [2425729f-e655-496f-bd8d-4ebe87d658f2-server-thread1] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: change Leader from null to ee3a319d-844c-4f03-bf3b-9b2017254b7d at term 1 for appendEntries, leader elected after 5514ms
scm_1               | 2023-07-13 21:18:20,510 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-13 21:18:58,806 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-07-13 21:18:58,915 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:18:58,915 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:18:49,721 [2425729f-e655-496f-bd8d-4ebe87d658f2-server-thread2] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17: set configuration 0: peers:[41fce95e-f628-4a1d-9d42-c4765136d0c7|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, ee3a319d-844c-4f03-bf3b-9b2017254b7d|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER, 2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:20,904 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-13 21:18:58,920 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-13 21:18:58,939 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4626eef0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:18:58,941 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a140446{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:18:49,736 [2425729f-e655-496f-bd8d-4ebe87d658f2-server-thread2] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:18:49,746 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-59EBAFE36A17-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1f99a7c-9744-4db1-b65b-59ebafe36a17/current/log_inprogress_0
om_1                | 2023-07-13 21:18:59,131 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6c35001b{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-7660822155845960366/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:18:59,149 [main] INFO server.AbstractConnector: Started ServerConnector@67303bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-13 21:18:59,153 [main] INFO server.Server: Started @24876ms
datanode_2          | 2023-07-13 21:18:50,207 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO impl.FollowerState: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5140844663ns, electionTimeout:5124ms
datanode_2          | 2023-07-13 21:18:50,208 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: shutdown 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState
om_1                | 2023-07-13 21:18:59,165 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-13 21:18:20,907 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-13 21:18:20,908 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-07-13 21:18:59,165 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-13 21:18:59,167 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:18:59,172 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:18:20,908 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:18:20,909 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:18:59,199 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-13 21:18:20,912 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:18:20,916 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: found a subdirectory /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0
om_1                | 2023-07-13 21:18:59,383 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-07-13 21:18:59,739 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-07-13 21:19:03,423 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5097453681ns, electionTimeout:5093ms
scm_1               | 2023-07-13 21:18:20,931 [main] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: addNew group-6B16FD24A1A0:[] returns group-6B16FD24A1A0:java.util.concurrent.CompletableFuture@655621fd[Not completed]
scm_1               | 2023-07-13 21:18:21,024 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: new RaftServerImpl for group-6B16FD24A1A0:[] with SCMStateMachine:uninitialized
om_1                | 2023-07-13 21:19:03,424 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
scm_1               | 2023-07-13 21:18:21,026 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-13 21:18:21,027 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-13 21:19:03,425 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:19:03,429 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-13 21:18:21,028 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:18:21,028 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:18:21,028 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:19:03,429 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_2          | 2023-07-13 21:18:50,208 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:18:50,208 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-07-13 21:18:50,209 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-FollowerState] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_2          | 2023-07-13 21:18:50,212 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:50,212 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-13 21:18:50,214 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:18:50,214 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.LeaderElection: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-13 21:18:50,214 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: shutdown 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2
om_1                | 2023-07-13 21:19:03,432 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:19:03,434 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-07-13 21:18:21,029 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:18:21,049 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:18:21,050 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om_1                | 2023-07-13 21:19:03,443 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-07-13 21:18:21,061 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:18:21,066 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-07-13 21:18:21,093 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om_1                | 2023-07-13 21:19:03,443 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-13 21:19:03,443 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-07-13 21:18:21,103 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-13 21:18:21,110 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
datanode_2          | 2023-07-13 21:18:50,214 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:18:50,214 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F8B3FB7A52C2 with new leaderId: 2425729f-e655-496f-bd8d-4ebe87d658f2
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | 2023-07-13 21:18:50,215 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: change Leader from null to 2425729f-e655-496f-bd8d-4ebe87d658f2 at term 1 for becomeLeader, leader elected after 5348ms
datanode_2          | 2023-07-13 21:18:50,215 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:18:50,215 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 2023-07-13 21:18:21,112 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:18:21,163 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-13 21:18:21,449 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:18:21,452 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:18:50,215 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:18:50,218 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:18:36,961 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
datanode_2          | 2023-07-13 21:18:50,219 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:18:50,219 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:18:50,219 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 2023-07-13 21:18:21,452 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2          | 2023-07-13 21:18:50,219 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:18:21,453 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_2          | 2023-07-13 21:18:50,219 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO impl.RoleInfo: 2425729f-e655-496f-bd8d-4ebe87d658f2: start 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderStateImpl
datanode_2          | 2023-07-13 21:18:50,220 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-13 21:18:21,459 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-07-13 21:18:21,459 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:18:21,462 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_2          | 2023-07-13 21:18:50,226 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2/current/log_inprogress_0
datanode_2          | 2023-07-13 21:18:50,254 [2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2-LeaderElection2] INFO server.RaftServer$Division: 2425729f-e655-496f-bd8d-4ebe87d658f2@group-F8B3FB7A52C2: set configuration 0: peers:[2425729f-e655-496f-bd8d-4ebe87d658f2|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:21,462 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-13 21:18:21,463 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
datanode_2          | 2023-07-13 21:19:29,051 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2425729f-e655-496f-bd8d-4ebe87d658f2: Detected pause in JVM or host machine approximately 0.109s without any GCs.
scm_1               | 2023-07-13 21:18:21,552 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
om_1                | 2023-07-13 21:19:03,444 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-13 21:19:03,453 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 8521ms
scm_1               | 2023-07-13 21:18:21,706 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-13 21:18:21,714 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
datanode_2          | 2023-07-13 21:19:33,084 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:18:21,746 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_2          | 2023-07-13 21:20:33,085 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:21:33,086 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:22:33,086 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:23:33,087 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | 2023-07-13 21:24:33,088 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-13 21:18:21,749 [main] INFO ha.SequenceIdGenerator: upgrade rootCertificateId to 1
om_1                | 2023-07-13 21:19:03,465 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-13 21:18:21,766 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-13 21:18:22,122 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:18:22,153 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | 2023-07-13 21:19:03,471 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:18:22,159 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 2023-07-13 21:18:37,010 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
om_1                | 2023-07-13 21:19:03,471 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:18:22,202 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-13 21:19:03,477 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:18:22,300 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om_1                | 2023-07-13 21:19:03,478 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:18:22,300 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-07-13 21:19:03,479 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:18:22,308 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
om_1                | 2023-07-13 21:19:03,493 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:18:22,309 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om_1                | 2023-07-13 21:19:03,496 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:18:22,312 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
om_1                | 2023-07-13 21:19:03,500 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
scm_1               | 2023-07-13 21:18:22,313 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
om_1                | 2023-07-13 21:19:03,518 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-13 21:18:22,321 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
om_1                | 2023-07-13 21:19:03,578 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:22,322 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
om_1                | 2023-07-13 21:19:03,629 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
scm_1               | 2023-07-13 21:18:22,388 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-13 21:18:22,388 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om_1                | 2023-07-13 21:19:03,749 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm_1               | 2023-07-13 21:18:22,438 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om_1                | [id: "om1"
scm_1               | 2023-07-13 21:18:22,674 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om_1                | address: "om:9872"
scm_1               | 2023-07-13 21:18:22,762 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | startupRole: FOLLOWER
scm_1               | 2023-07-13 21:18:22,762 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | ]
om_1                | 2023-07-13 21:19:06,854 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 2023-07-13 21:19:07,162 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
scm_1               | 2023-07-13 21:18:22,799 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-07-13 21:19:28,293 [qtp1731942690-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
scm_1               | 2023-07-13 21:18:22,816 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | 2023-07-13 21:19:28,518 [qtp1731942690-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1689283168475 in 43 milliseconds
scm_1               | 2023-07-13 21:18:22,840 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
recon_1             | 2023-07-13 21:18:38,076 [IPC Server handler 86 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2425729f-e655-496f-bd8d-4ebe87d658f2
om_1                | 2023-07-13 21:19:28,551 [qtp1731942690-54] INFO db.RDBCheckpointUtils: Waited for 30 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1689283168475 availability.
scm_1               | 2023-07-13 21:18:22,944 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-07-13 21:18:24,658 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-13 21:18:38,100 [IPC Server handler 86 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 2425729f-e655-496f-bd8d-4ebe87d658f2{ip: 172.19.0.5, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om_1                | 2023-07-13 21:19:28,605 [qtp1731942690-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 52 milliseconds
scm_1               | 2023-07-13 21:18:24,716 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:18:38,220 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322. Trying to get from SCM.
om_1                | 2023-07-13 21:19:28,605 [qtp1731942690-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
scm_1               | 2023-07-13 21:18:24,779 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
recon_1             | 2023-07-13 21:18:38,207 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 2425729f-e655-496f-bd8d-4ebe87d658f2 to Node DB.
om_1                | 2023-07-13 21:19:28,605 [qtp1731942690-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689283168475
scm_1               | 2023-07-13 21:18:24,792 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
recon_1             | 2023-07-13 21:18:38,322 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5c61ba5b-cb65-4360-91cd-ea8f921e3322, Nodes: 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.895Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-07-13 21:18:24,962 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-13 21:18:38,347 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ee3a319d-844c-4f03-bf3b-9b2017254b7d
recon_1             | 2023-07-13 21:18:38,349 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : ee3a319d-844c-4f03-bf3b-9b2017254b7d{ip: 172.19.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:18:24,990 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:18:24,991 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-07-13 21:18:24,998 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
recon_1             | 2023-07-13 21:18:38,359 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node ee3a319d-844c-4f03-bf3b-9b2017254b7d to Node DB.
scm_1               | 2023-07-13 21:18:25,102 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-13 21:18:38,408 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 reported by 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)
scm_1               | 2023-07-13 21:18:25,157 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:18:38,408 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | 2023-07-13 21:18:25,157 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-07-13 21:18:25,166 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:18:25,363 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-07-13 21:18:25,365 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
recon_1             | 2023-07-13 21:18:43,233 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | Key                            Value
recon_1             | 2023-07-13 21:18:43,233 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17. Trying to get from SCM.
scm_1               | Running                        false
recon_1             | 2023-07-13 21:18:43,248 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d1f99a7c-9744-4db1-b65b-59ebafe36a17, Nodes: ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.983Z[UTC]] to Recon pipeline metadata.
scm_1               | Container Balancer Configuration values:
recon_1             | 2023-07-13 21:18:43,249 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | Key                                                Value
recon_1             | 2023-07-13 21:18:43,528 [IPC Server handler 13 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | Threshold                                          10
recon_1             | 2023-07-13 21:18:43,531 [IPC Server handler 13 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 41fce95e-f628-4a1d-9d42-c4765136d0c7{ip: 172.19.0.4, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
recon_1             | 2023-07-13 21:18:43,535 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 41fce95e-f628-4a1d-9d42-c4765136d0c7 to Node DB.
scm_1               | Max Size to Move per Iteration                     500GB
recon_1             | 2023-07-13 21:18:43,537 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 reported by 41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4)
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
recon_1             | 2023-07-13 21:18:44,227 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 reported by 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)
scm_1               | 2023-07-13 21:18:25,365 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
recon_1             | 2023-07-13 21:18:44,230 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)
scm_1               | 2023-07-13 21:18:25,422 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
recon_1             | 2023-07-13 21:18:44,896 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)
scm_1               | 2023-07-13 21:18:25,440 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
recon_1             | 2023-07-13 21:18:44,897 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2. Trying to get from SCM.
recon_1             | 2023-07-13 21:18:44,901 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2, Nodes: 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.941Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-07-13 21:18:25,469 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/in_use.lock acquired by nodename 6@9d26f28165f7
recon_1             | 2023-07-13 21:18:44,905 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2 reported by 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)
scm_1               | 2023-07-13 21:18:25,480 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=fa59c48c-1a24-41e9-970b-2dd56a1aecbd} from /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/raft-meta
scm_1               | 2023-07-13 21:18:25,545 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: set configuration 0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:25,552 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-13 21:18:25,571 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-13 21:18:25,572 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:18:25,579 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:18:25,580 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1             | 2023-07-13 21:18:45,175 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by 41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4)
recon_1             | 2023-07-13 21:18:46,767 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9. Trying to get from SCM.
recon_1             | 2023-07-13 21:18:46,771 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e94175d1-58c0-4137-b171-4316de8ab2f9, Nodes: ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:35.009Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:18:46,773 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | 2023-07-13 21:18:25,586 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 2023-07-13 21:18:46,777 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | 2023-07-13 21:18:25,616 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1             | 2023-07-13 21:18:49,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 reported by ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)
scm_1               | 2023-07-13 21:18:25,616 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1             | 2023-07-13 21:19:07,368 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6 reported by 41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4)
scm_1               | 2023-07-13 21:18:25,616 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:19:11,061 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
scm_1               | 2023-07-13 21:18:25,645 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0
recon_1             | 2023-07-13 21:19:11,236 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm_1               | 2023-07-13 21:18:25,646 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:18:25,646 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
recon_1             | 2023-07-13 21:19:27,101 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm_1               | 2023-07-13 21:18:25,647 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 2023-07-13 21:19:27,101 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
scm_1               | 2023-07-13 21:18:25,650 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1             | 2023-07-13 21:19:28,755 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689283167101
scm_1               | 2023-07-13 21:18:25,655 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1             | 2023-07-13 21:19:28,768 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm_1               | 2023-07-13 21:18:25,656 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 2023-07-13 21:19:29,840 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689283167101.
scm_1               | 2023-07-13 21:18:25,656 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1             | 2023-07-13 21:19:30,060 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
scm_1               | 2023-07-13 21:18:25,660 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 2023-07-13 21:19:30,750 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
scm_1               | 2023-07-13 21:18:25,717 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
recon_1             | 2023-07-13 21:19:30,775 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
scm_1               | 2023-07-13 21:18:25,718 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-13 21:19:30,781 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
scm_1               | 2023-07-13 21:18:25,801 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1             | 2023-07-13 21:19:30,782 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
scm_1               | 2023-07-13 21:18:25,805 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 2023-07-13 21:19:30,783 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
scm_1               | 2023-07-13 21:18:25,806 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1             | 2023-07-13 21:19:30,785 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
scm_1               | 2023-07-13 21:18:25,946 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: set configuration 0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:25,957 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/log_inprogress_0
recon_1             | 2023-07-13 21:19:30,959 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm_1               | 2023-07-13 21:18:25,969 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1             | 2023-07-13 21:19:30,962 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.179 seconds to process 2 keys.
scm_1               | 2023-07-13 21:18:26,346 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: start as a follower, conf=0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-13 21:19:31,000 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm_1               | 2023-07-13 21:18:26,346 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from      null to FOLLOWER at term 1 for startAsFollower
recon_1             | 2023-07-13 21:19:31,028 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | 2023-07-13 21:18:26,351 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState
recon_1             | 2023-07-13 21:19:36,081 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
scm_1               | 2023-07-13 21:18:26,364 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6B16FD24A1A0,id=fa59c48c-1a24-41e9-970b-2dd56a1aecbd
recon_1             | 2023-07-13 21:19:36,081 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm_1               | 2023-07-13 21:18:26,367 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1             | 2023-07-13 21:19:36,088 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
scm_1               | 2023-07-13 21:18:26,367 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-13 21:18:26,370 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 2023-07-13 21:19:36,088 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm_1               | 2023-07-13 21:18:26,370 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1             | 2023-07-13 21:19:36,107 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-07-13 21:19:36,108 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-07-13 21:19:36,140 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-07-13 21:19:36,174 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 70 milliseconds.
recon_1             | 2023-07-13 21:19:36,182 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-07-13 21:19:36,296 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 206 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:19:36,336 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 40 milliseconds for processing 1 containers.
scm_1               | 2023-07-13 21:18:26,371 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
recon_1             | 2023-07-13 21:19:38,442 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
scm_1               | 2023-07-13 21:18:26,373 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1             | 2023-07-13 21:19:38,473 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
scm_1               | 2023-07-13 21:18:26,421 [main] INFO server.RaftServer: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start RPC server
recon_1             | 2023-07-13 21:19:38,473 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
scm_1               | 2023-07-13 21:18:26,625 [main] INFO server.GrpcService: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: GrpcService started, listening on 9894
recon_1             | 2023-07-13 21:19:38,531 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
scm_1               | 2023-07-13 21:18:26,644 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fa59c48c-1a24-41e9-970b-2dd56a1aecbd: Started
recon_1             | 2023-07-13 21:19:38,533 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
scm_1               | 2023-07-13 21:18:26,690 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
recon_1             | 2023-07-13 21:19:38,533 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
scm_1               | 2023-07-13 21:18:26,690 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
recon_1             | 2023-07-13 21:20:36,187 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
scm_1               | 2023-07-13 21:18:26,696 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
recon_1             | 2023-07-13 21:20:36,246 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-13 21:18:26,710 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-07-13 21:18:26,710 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-07-13 21:18:27,107 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:20:36,246 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 59
scm_1               | 2023-07-13 21:18:27,202 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:21:36,246 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-13 21:18:27,203 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
recon_1             | 2023-07-13 21:21:36,247 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-13 21:18:27,899 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
recon_1             | 2023-07-13 21:22:36,248 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:22:36,248 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-13 21:18:27,900 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:23:36,248 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-13 21:18:27,914 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
recon_1             | 2023-07-13 21:23:36,248 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-13 21:18:28,029 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
recon_1             | 2023-07-13 21:24:36,222 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
scm_1               | 2023-07-13 21:18:28,030 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
recon_1             | 2023-07-13 21:24:36,228 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 18 milliseconds.
scm_1               | 2023-07-13 21:18:28,031 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:24:36,249 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-13 21:18:28,035 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
recon_1             | 2023-07-13 21:24:36,249 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-13 21:18:28,200 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
recon_1             | 2023-07-13 21:24:36,337 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
scm_1               | 2023-07-13 21:18:28,201 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:24:36,341 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 3 milliseconds for processing 2 containers.
scm_1               | 2023-07-13 21:18:28,285 [main] INFO util.log: Logging initialized @26621ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:18:28,516 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-13 21:18:28,540 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:18:28,581 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:18:28,588 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:18:28,592 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:18:28,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-13 21:18:28,742 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-07-13 21:18:28,744 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-13 21:18:28,749 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-07-13 21:18:28,888 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:18:28,888 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:18:28,890 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-13 21:18:28,906 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@395ac2e5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:18:28,907 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4ca6e265{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:18:29,191 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@25000d3d{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-713884752193564955/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-07-13 21:18:29,204 [main] INFO server.AbstractConnector: Started ServerConnector@109db924{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:18:29,204 [main] INFO server.Server: Started @27541ms
scm_1               | 2023-07-13 21:18:29,206 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-13 21:18:29,206 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-13 21:18:29,208 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:18:31,396 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.FollowerState: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5044625581ns, electionTimeout:5027ms
scm_1               | 2023-07-13 21:18:31,397 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState
scm_1               | 2023-07-13 21:18:31,400 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-13 21:18:31,410 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-13 21:18:31,410 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-FollowerState] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1
scm_1               | 2023-07-13 21:18:31,419 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:31,421 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-07-13 21:18:31,460 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:31,462 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.LeaderElection: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-13 21:18:31,462 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: shutdown fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1
scm_1               | 2023-07-13 21:18:31,463 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-13 21:18:31,463 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-13 21:18:31,463 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-07-13 21:18:31,489 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: change Leader from null to fa59c48c-1a24-41e9-970b-2dd56a1aecbd at term 2 for becomeLeader, leader elected after 10370ms
scm_1               | 2023-07-13 21:18:31,515 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:18:31,531 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:18:31,532 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:18:31,548 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:18:31,551 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:18:31,556 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:18:31,573 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:18:31,582 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:18:31,586 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO impl.RoleInfo: fa59c48c-1a24-41e9-970b-2dd56a1aecbd: start fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderStateImpl
scm_1               | 2023-07-13 21:18:31,598 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-13 21:18:31,607 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/log_inprogress_0 to /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/log_0-0
scm_1               | 2023-07-13 21:18:31,619 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-LeaderElection1] INFO server.RaftServer$Division: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0: set configuration 1: peers:[fa59c48c-1a24-41e9-970b-2dd56a1aecbd|rpc:9d26f28165f7:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:18:31,641 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/fa9d5e74-ea5c-449c-9d2e-6b16fd24a1a0/current/log_inprogress_1
scm_1               | 2023-07-13 21:18:31,651 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-07-13 21:18:31,653 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-13 21:18:31,667 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:31,669 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-13 21:18:31,670 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:18:31,670 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:18:31,677 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:18:31,683 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:18:31,701 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:51390 / 172.19.0.4:51390
scm_1               | 2023-07-13 21:18:31,708 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:39358 / 172.19.0.8:39358
scm_1               | 2023-07-13 21:18:31,726 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:52072 / 172.19.0.5:52072
scm_1               | 2023-07-13 21:18:34,038 [IPC Server handler 52 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | 2023-07-13 21:18:34,095 [IPC Server handler 52 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 41fce95e-f628-4a1d-9d42-c4765136d0c7{ip: 172.19.0.4, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:18:34,138 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:18:34,162 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:18:34,191 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:18:34,233 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6 to datanode:41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | 2023-07-13 21:18:34,287 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:18:34,562 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2425729f-e655-496f-bd8d-4ebe87d658f2
scm_1               | 2023-07-13 21:18:34,586 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2425729f-e655-496f-bd8d-4ebe87d658f2{ip: 172.19.0.5, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:18:34,591 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:18:34,593 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:18:34,707 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ee3a319d-844c-4f03-bf3b-9b2017254b7d
scm_1               | 2023-07-13 21:18:34,707 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ee3a319d-844c-4f03-bf3b-9b2017254b7d{ip: 172.19.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:18:34,717 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:18:34,719 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:18:34,719 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:18:34,719 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-13 21:18:34,720 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:18:34,749 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:18:34,759 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:34,864 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 6b80d23e-d59b-4573-b55c-e612a82b9dc6, Nodes: 41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.228723Z[UTC]]
scm_1               | 2023-07-13 21:18:34,895 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 to datanode:2425729f-e655-496f-bd8d-4ebe87d658f2
scm_1               | 2023-07-13 21:18:34,898 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 to datanode:ee3a319d-844c-4f03-bf3b-9b2017254b7d
scm_1               | 2023-07-13 21:18:34,904 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 to datanode:41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | 2023-07-13 21:18:34,925 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:34,932 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5c61ba5b-cb65-4360-91cd-ea8f921e3322, Nodes: 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.895678Z[UTC]]
scm_1               | 2023-07-13 21:18:34,950 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2 to datanode:2425729f-e655-496f-bd8d-4ebe87d658f2
scm_1               | 2023-07-13 21:18:34,961 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:34,971 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2, Nodes: 2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.941657Z[UTC]]
scm_1               | 2023-07-13 21:18:34,983 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 to datanode:ee3a319d-844c-4f03-bf3b-9b2017254b7d
scm_1               | 2023-07-13 21:18:34,985 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 to datanode:2425729f-e655-496f-bd8d-4ebe87d658f2
scm_1               | 2023-07-13 21:18:34,990 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 to datanode:41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | 2023-07-13 21:18:34,998 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:35,006 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17 contains same datanodes as previous pipelines: PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322 nodeIds: ee3a319d-844c-4f03-bf3b-9b2017254b7d, 2425729f-e655-496f-bd8d-4ebe87d658f2, 41fce95e-f628-4a1d-9d42-c4765136d0c7
scm_1               | 2023-07-13 21:18:35,007 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d1f99a7c-9744-4db1-b65b-59ebafe36a17, Nodes: ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8)2425729f-e655-496f-bd8d-4ebe87d658f2(xcompat_datanode_2.xcompat_default/172.19.0.5)41fce95e-f628-4a1d-9d42-c4765136d0c7(xcompat_datanode_3.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:34.983605Z[UTC]]
scm_1               | 2023-07-13 21:18:35,009 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9 to datanode:ee3a319d-844c-4f03-bf3b-9b2017254b7d
scm_1               | 2023-07-13 21:18:35,017 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:35,026 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: e94175d1-58c0-4137-b171-4316de8ab2f9, Nodes: ee3a319d-844c-4f03-bf3b-9b2017254b7d(xcompat_datanode_1.xcompat_default/172.19.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:18:35.009749Z[UTC]]
scm_1               | 2023-07-13 21:18:35,027 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-13 21:18:35,041 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-13 21:18:44,263 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:18:44,265 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=5c61ba5b-cb65-4360-91cd-ea8f921e3322
scm_1               | 2023-07-13 21:18:44,281 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:18:44,285 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:18:44,285 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-13 21:18:44,286 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-13 21:18:44,286 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-13 21:18:44,286 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:18:44,290 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:18:44,293 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-13 21:18:44,331 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-13 21:18:44,332 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-07-13 21:18:44,929 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=010fcf82-fa88-4eeb-99b1-f8b3fb7a52c2
scm_1               | 2023-07-13 21:18:46,787 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=e94175d1-58c0-4137-b171-4316de8ab2f9
scm_1               | 2023-07-13 21:18:49,301 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d1f99a7c-9744-4db1-b65b-59ebafe36a17
scm_1               | 2023-07-13 21:19:07,347 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=6b80d23e-d59b-4573-b55c-e612a82b9dc6
scm_1               | 2023-07-13 21:19:07,633 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-13 21:19:07,680 [fa59c48c-1a24-41e9-970b-2dd56a1aecbd@group-6B16FD24A1A0-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-13 21:19:07,689 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-13 21:19:47,944 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:20:00,538 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:20:35,048 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-13 21:20:56,178 [IPC Server handler 88 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:21:08,101 [IPC Server handler 44 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:22:01,403 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:22:13,317 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:22:35,051 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-13 21:23:12,879 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:23:22,763 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-13 21:23:24,705 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:24:25,013 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-07-13 21:24:35,052 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-13 21:24:36,728 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
Attaching to xcompat_scm_1, xcompat_datanode_3, xcompat_recon_1, xcompat_old_client_1_3_0_1, xcompat_datanode_1, xcompat_s3g_1, xcompat_datanode_2, xcompat_old_client_1_2_1_1, xcompat_new_client_1, xcompat_old_client_1_1_0_1, xcompat_old_client_1_0_0_1, xcompat_om_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:25:08 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 442a004edd8b/172.20.0.9
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.0.0
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:25:07 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = f7f25ec73a93/172.20.0.7
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.0.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_2          | STARTUP_MSG:   java = 11.0.3
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:25:07 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:25:10 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:25:11 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:25:12 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:25:12 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:25:13 INFO  HddsDatanodeService:209 - HddsDatanodeService host:f7f25ec73a93 ip:172.20.0.7
datanode_2          | 2023-07-13 21:25:14 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:25:14 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-07-13 21:25:14 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:25:14 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:25:14 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:25:15 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:25:15 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:25:15 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:25:21 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:25:21 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:25:22 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_2          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:25:24 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:25:24 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:25:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_2          | 2023-07-13 21:25:24 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:25:24 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:25:25 INFO  log:169 - Logging initialized @23693ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:25:25 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-13 21:25:25 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:25:25 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:25:25 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:25:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:25:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:25:26 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_2          | 2023-07-13 21:25:26 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2          | 2023-07-13 21:25:26 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_1          | STARTUP_MSG:   java = 11.0.3
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:25:08 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:25:10 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:25:11 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:25:13 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:25:13 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:25:13 INFO  HddsDatanodeService:209 - HddsDatanodeService host:442a004edd8b ip:172.20.0.9
datanode_1          | 2023-07-13 21:25:14 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:25:14 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-07-13 21:25:14 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:25:14 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:25:14 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:25:15 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:25:15 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:25:15 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:25:21 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:25:22 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:25:22 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_1          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:25:24 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:25:24 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:25:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1          | 2023-07-13 21:25:24 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:25:25 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:25:25 INFO  log:169 - Logging initialized @23286ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:25:26 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-13 21:25:26 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:25:26 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:25:26 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:25:26 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:25:26 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:25:26 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_1          | 2023-07-13 21:25:26 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1          | 2023-07-13 21:25:26 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:25:26 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:25:26 INFO  session:140 - node0 Scavenging every 660000ms
datanode_1          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a451491{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a92be4f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:25:28 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@5981f4a6{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-9282687572239834080.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:25:28 INFO  AbstractConnector:330 - Started ServerConnector@56dfab87{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:25:28 INFO  Server:399 - Started @26274ms
datanode_1          | 2023-07-13 21:25:28 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1          | 2023-07-13 21:25:28 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1          | 2023-07-13 21:25:28 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:25:28 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1          | 2023-07-13 21:25:29 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_1          | 2023-07-13 21:25:29 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:25:31 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 442a004edd8b/172.20.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.9:42550 remote=scm/172.20.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_2          | 2023-07-13 21:25:26 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:25:26 INFO  session:140 - node0 Scavenging every 600000ms
datanode_2          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a451491{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a92be4f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:25:28 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@5981f4a6{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-17427292019558054090.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-13 21:25:28 INFO  AbstractConnector:330 - Started ServerConnector@56dfab87{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:25:28 INFO  Server:399 - Started @26897ms
datanode_2          | 2023-07-13 21:25:28 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2          | 2023-07-13 21:25:28 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2          | 2023-07-13 21:25:28 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:25:28 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2          | 2023-07-13 21:25:29 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_2          | 2023-07-13 21:25:29 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:25:31 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From f7f25ec73a93/172.20.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.7:51452 remote=scm/172.20.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.9:42550 remote=scm/172.20.0.12:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_1          | 2023-07-13 21:25:34 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-07-13 21:25:54 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_1          | 2023-07-13 21:26:14 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_1          | 2023-07-13 21:26:30 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_1          | 2023-07-13 21:26:30 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_1          | 2023-07-13 21:26:30 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_1          | 2023-07-13 21:26:30 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 at port 9858
datanode_1          | 2023-07-13 21:26:30 INFO  RaftServerProxy:304 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start RPC server
datanode_1          | 2023-07-13 21:26:30 INFO  GrpcService:160 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerProxy:89 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: addNew group-4FF914EAA674:[86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] returns group-4FF914EAA674:java.util.concurrent.CompletableFuture@3274db30[Not completed]
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:107 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: new RaftServerImpl for group-4FF914EAA674:[86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:103 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: ConfigurationManager, init=-1: [86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/de69f1fb-497e-4296-815b-4ff914eaa674 does not exist. Creating ...
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/de69f1fb-497e-4296-815b-4ff914eaa674/in_use.lock acquired by nodename 7@442a004edd8b
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/de69f1fb-497e-4296-815b-4ff914eaa674 has been successfully formatted.
datanode_1          | 2023-07-13 21:26:35 INFO  ContainerStateMachine:225 - group-4FF914EAA674: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:180 - new 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/de69f1fb-497e-4296-815b-4ff914eaa674
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.7:51452 remote=scm/172.20.0.12:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_2          | 2023-07-13 21:25:34 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_2          | 2023-07-13 21:25:54 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_2          | 2023-07-13 21:26:14 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_2          | 2023-07-13 21:26:30 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_2          | 2023-07-13 21:26:30 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_2          | 2023-07-13 21:26:30 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_2          | 2023-07-13 21:26:30 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 6c97c744-03a7-4611-93eb-0d198f07fac2 at port 9858
datanode_2          | 2023-07-13 21:26:30 INFO  RaftServerProxy:304 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start RPC server
datanode_2          | 2023-07-13 21:26:31 INFO  GrpcService:160 - 6c97c744-03a7-4611-93eb-0d198f07fac2: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerProxy:89 - 6c97c744-03a7-4611-93eb-0d198f07fac2: addNew group-2016C8830ACC:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858] returns group-2016C8830ACC:java.util.concurrent.CompletableFuture@1cf67846[Not completed]
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:107 - 6c97c744-03a7-4611-93eb-0d198f07fac2: new RaftServerImpl for group-2016C8830ACC:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:103 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: ConfigurationManager, init=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/3d164048-22b8-420c-a159-2016c8830acc does not exist. Creating ...
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/3d164048-22b8-420c-a159-2016c8830acc/in_use.lock acquired by nodename 7@f7f25ec73a93
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/3d164048-22b8-420c-a159-2016c8830acc has been successfully formatted.
datanode_2          | 2023-07-13 21:26:35 INFO  ContainerStateMachine:225 - group-2016C8830ACC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:180 - new 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3d164048-22b8-420c-a159-2016c8830acc
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:196 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: start as a follower, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858], old=null
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:26:35 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start FollowerState
datanode_2          | 2023-07-13 21:26:35 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2016C8830ACC,id=6c97c744-03a7-4611-93eb-0d198f07fac2
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC
datanode_2          | 2023-07-13 21:26:35 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "3d164048-22b8-420c-a159-2016c8830acc"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 4401776364365562380
datanode_2          |   leastSigBits: -6820384878450505012
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerProxy:89 - 6c97c744-03a7-4611-93eb-0d198f07fac2: addNew group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] returns group-FB3F76093525:java.util.concurrent.CompletableFuture@299df9a1[Not completed]
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:107 - 6c97c744-03a7-4611-93eb-0d198f07fac2: new RaftServerImpl for group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:103 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: ConfigurationManager, init=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 does not exist. Creating ...
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/in_use.lock acquired by nodename 7@f7f25ec73a93
datanode_2          | 2023-07-13 21:26:35 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 has been successfully formatted.
datanode_2          | 2023-07-13 21:26:35 INFO  ContainerStateMachine:225 - group-FB3F76093525: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:180 - new 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:196 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: start as a follower, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_2          | 2023-07-13 21:26:35 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:26:35 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start FollowerState
datanode_2          | 2023-07-13 21:26:35 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FB3F76093525,id=6c97c744-03a7-4611-93eb-0d198f07fac2
datanode_2          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:196 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: start as a follower, conf=-1: [86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:26:35 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start FollowerState
datanode_1          | 2023-07-13 21:26:35 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4FF914EAA674,id=86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674
datanode_1          | 2023-07-13 21:26:35 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "de69f1fb-497e-4296-815b-4ff914eaa674"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: -2420137263162834282
datanode_1          |   leastSigBits: -9125612288719804812
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerProxy:89 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: addNew group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] returns group-FB3F76093525:java.util.concurrent.CompletableFuture@2206ba66[Not completed]
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:107 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: new RaftServerImpl for group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:103 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: ConfigurationManager, init=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 does not exist. Creating ...
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/in_use.lock acquired by nodename 7@442a004edd8b
datanode_1          | 2023-07-13 21:26:35 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 has been successfully formatted.
datanode_1          | 2023-07-13 21:26:35 INFO  ContainerStateMachine:225 - group-FB3F76093525: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:180 - new 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:26:37 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "8e3da1eb-08a4-4e48-a237-fb3f76093525"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: -8197217715910062520
datanode_2          |   leastSigBits: -6757656465864313563
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-07-13 21:26:40 INFO  FollowerState:108 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-FollowerState: change to CANDIDATE, lastRpcTime:5080ms, electionTimeout:5067ms
datanode_2          | 2023-07-13 21:26:40 INFO  RoleInfo:121 - 6c97c744-03a7-4611-93eb-0d198f07fac2: shutdown FollowerState
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:26:40 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start LeaderElection
datanode_2          | 2023-07-13 21:26:40 INFO  LeaderElection:209 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-LeaderElection1: begin an election at term 1 for -1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858], old=null
datanode_2          | 2023-07-13 21:26:40 INFO  RoleInfo:134 - 6c97c744-03a7-4611-93eb-0d198f07fac2: shutdown LeaderElection
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:26:40 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-2016C8830ACC with new leaderId: 6c97c744-03a7-4611-93eb-0d198f07fac2
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerImpl:255 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: change Leader from null to 6c97c744-03a7-4611-93eb-0d198f07fac2 at term 1 for becomeLeader, leader elected after 5324ms
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:26:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:26:40 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start LeaderState
datanode_2          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:26:41 INFO  FollowerState:108 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-FollowerState: change to CANDIDATE, lastRpcTime:5166ms, electionTimeout:5162ms
datanode_2          | 2023-07-13 21:26:41 INFO  RoleInfo:121 - 6c97c744-03a7-4611-93eb-0d198f07fac2: shutdown FollowerState
datanode_2          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start LeaderElection
datanode_2          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC: set configuration 0: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858], old=null at 0
datanode_2          | 2023-07-13 21:26:41 INFO  LeaderElection:209 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-LeaderElection2: begin an election at term 1 for -1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_2          | 2023-07-13 21:26:41 INFO  LeaderElection:61 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-LeaderElection2: Election REJECTED; received 2 response(s) [6c97c744-03a7-4611-93eb-0d198f07fac2<-375f870c-6317-48e9-a102-4c22ab94c2d2#0:FAIL-t1, 6c97c744-03a7-4611-93eb-0d198f07fac2<-86d878c2-26cc-4775-9efa-2acc3bfcd9c5#0:FAIL-t1] and 0 exception(s); 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525:t1, leader=null, voted=6c97c744-03a7-4611-93eb-0d198f07fac2, raftlog=6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_2          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
datanode_2          | 2023-07-13 21:26:41 INFO  RoleInfo:134 - 6c97c744-03a7-4611-93eb-0d198f07fac2: shutdown LeaderElection
datanode_2          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 6c97c744-03a7-4611-93eb-0d198f07fac2: start FollowerState
datanode_2          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-2016C8830ACC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3d164048-22b8-420c-a159-2016c8830acc/current/log_inprogress_0
datanode_2          | 2023-07-13 21:26:41 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-FB3F76093525 with new leaderId: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_2          | 2023-07-13 21:26:41 INFO  RaftServerImpl:255 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: change Leader from null to 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 at term 1 for appendEntries, leader elected after 5580ms
datanode_2          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525: set configuration 0: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null at 0
datanode_2          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 6c97c744-03a7-4611-93eb-0d198f07fac2@group-FB3F76093525-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/current/log_inprogress_0
datanode_1          | 2023-07-13 21:26:35 INFO  SegmentedRaftLogWorker:129 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:196 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: start as a follower, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_1          | 2023-07-13 21:26:35 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:26:35 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start FollowerState
datanode_1          | 2023-07-13 21:26:35 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FB3F76093525,id=86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_1          | 2023-07-13 21:26:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:37 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "8e3da1eb-08a4-4e48-a237-fb3f76093525"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: -8197217715910062520
datanode_1          |   leastSigBits: -6757656465864313563
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-07-13 21:26:40 INFO  FollowerState:108 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-FollowerState: change to CANDIDATE, lastRpcTime:5015ms, electionTimeout:5008ms
datanode_1          | 2023-07-13 21:26:40 INFO  RoleInfo:121 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: shutdown FollowerState
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:26:40 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start LeaderElection
datanode_1          | 2023-07-13 21:26:40 INFO  LeaderElection:209 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-LeaderElection1: begin an election at term 1 for -1: [86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_1          | 2023-07-13 21:26:40 INFO  RoleInfo:134 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: shutdown LeaderElection
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:26:40 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-4FF914EAA674 with new leaderId: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerImpl:255 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: change Leader from null to 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 at term 1 for becomeLeader, leader elected after 5202ms
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:26:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:26:40 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:26:40 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start LeaderState
datanode_1          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:26:41 INFO  FollowerState:108 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-FollowerState: change to CANDIDATE, lastRpcTime:5122ms, electionTimeout:5114ms
datanode_1          | 2023-07-13 21:26:41 INFO  RoleInfo:121 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: shutdown FollowerState
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start LeaderElection
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674: set configuration 0: [86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null at 0
datanode_1          | 2023-07-13 21:26:41 INFO  LeaderElection:209 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-LeaderElection2: begin an election at term 1 for -1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_1          | 2023-07-13 21:26:41 INFO  LeaderElection:61 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-LeaderElection2: Election PASSED; received 1 response(s) [86d878c2-26cc-4775-9efa-2acc3bfcd9c5<-375f870c-6317-48e9-a102-4c22ab94c2d2#0:OK-t1] and 0 exception(s); 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525:t1, leader=null, voted=86d878c2-26cc-4775-9efa-2acc3bfcd9c5, raftlog=86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_1          | 2023-07-13 21:26:41 INFO  RoleInfo:134 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: shutdown LeaderElection
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:26:41 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-FB3F76093525 with new leaderId: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerImpl:255 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: change Leader from null to 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 at term 1 for becomeLeader, leader elected after 5317ms
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:25:08 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 30809cd03a6a/172.20.0.11
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.0.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_3          | STARTUP_MSG:   java = 11.0.3
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:25:08 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:25:10 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:25:11 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:25:12 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:25:12 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:25:13 INFO  HddsDatanodeService:209 - HddsDatanodeService host:30809cd03a6a ip:172.20.0.11
datanode_3          | 2023-07-13 21:25:13 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:25:13 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-07-13 21:25:14 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:25:14 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:25:14 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:25:15 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:25:15 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:25:15 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:25:20 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:25:21 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:25:22 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_3          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:25:22 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:25:22 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:25:24 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:25:24 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:25:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_3          | 2023-07-13 21:25:24 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:25:25 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:25:25 INFO  log:169 - Logging initialized @23172ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:25:26 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-13 21:25:26 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:25:26 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:25:26 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:25:26 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:25:26 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-13 21:25:26 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_3          | 2023-07-13 21:25:26 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_3          | 2023-07-13 21:25:26 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:25:26 INFO  session:338 - No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:25:26 INFO  session:140 - node0 Scavenging every 600000ms
datanode_3          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a451491{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:25:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a92be4f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:25:28 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@5981f4a6{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-1302230757154959159.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:25:28 INFO  AbstractConnector:330 - Started ServerConnector@56dfab87{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:25:28 INFO  Server:399 - Started @26668ms
datanode_3          | 2023-07-13 21:25:28 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3          | 2023-07-13 21:25:28 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3          | 2023-07-13 21:25:28 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:25:28 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3          | 2023-07-13 21:25:29 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_3          | 2023-07-13 21:25:30 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:25:32 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 30809cd03a6a/172.20.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:54810 remote=scm/172.20.0.12:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:54810 remote=scm/172.20.0.12:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_3          | 2023-07-13 21:25:34 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-07-13 21:25:54 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_3          | 2023-07-13 21:26:14 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_3          | 2023-07-13 21:26:31 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_3          | 2023-07-13 21:26:31 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_3          | 2023-07-13 21:26:31 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_3          | 2023-07-13 21:26:31 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 375f870c-6317-48e9-a102-4c22ab94c2d2 at port 9858
datanode_3          | 2023-07-13 21:26:31 INFO  RaftServerProxy:304 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start RPC server
datanode_3          | 2023-07-13 21:26:31 INFO  GrpcService:160 - 375f870c-6317-48e9-a102-4c22ab94c2d2: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerProxy:89 - 375f870c-6317-48e9-a102-4c22ab94c2d2: addNew group-8B3B2337A1DD:[375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858] returns group-8B3B2337A1DD:java.util.concurrent.CompletableFuture@28a182ec[Not completed]
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerImpl:107 - 375f870c-6317-48e9-a102-4c22ab94c2d2: new RaftServerImpl for group-8B3B2337A1DD:[375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-07-13 21:26:35 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:103 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: ConfigurationManager, init=-1: [375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd does not exist. Creating ...
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd/in_use.lock acquired by nodename 6@30809cd03a6a
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd has been successfully formatted.
datanode_3          | 2023-07-13 21:26:36 INFO  ContainerStateMachine:225 - group-8B3B2337A1DD: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:180 - new 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:129 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:129 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:196 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: start as a follower, conf=-1: [375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:185 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:26:36 INFO  RoleInfo:143 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start FollowerState
datanode_3          | 2023-07-13 21:26:36 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8B3B2337A1DD,id=375f870c-6317-48e9-a102-4c22ab94c2d2
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD
datanode_3          | 2023-07-13 21:26:36 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: -1248575235340351763
datanode_3          |   leastSigBits: -6291094118349692451
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerProxy:89 - 375f870c-6317-48e9-a102-4c22ab94c2d2: addNew group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] returns group-FB3F76093525:java.util.concurrent.CompletableFuture@7ae98966[Not completed]
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:107 - 375f870c-6317-48e9-a102-4c22ab94c2d2: new RaftServerImpl for group-FB3F76093525:[6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:103 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: ConfigurationManager, init=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 does not exist. Creating ...
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/in_use.lock acquired by nodename 6@30809cd03a6a
datanode_3          | 2023-07-13 21:26:36 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525 has been successfully formatted.
datanode_3          | 2023-07-13 21:26:36 INFO  ContainerStateMachine:225 - group-FB3F76093525: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:180 - new 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:129 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:26:36 INFO  SegmentedRaftLogWorker:129 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:196 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: start as a follower, conf=-1: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null
datanode_3          | 2023-07-13 21:26:36 INFO  RaftServerImpl:185 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:26:36 INFO  RoleInfo:143 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start FollowerState
datanode_3          | 2023-07-13 21:26:36 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FB3F76093525,id=375f870c-6317-48e9-a102-4c22ab94c2d2
datanode_3          | 2023-07-13 21:26:36 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525
datanode_3          | 2023-07-13 21:26:37 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "8e3da1eb-08a4-4e48-a237-fb3f76093525"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: -8197217715910062520
datanode_3          |   leastSigBits: -6757656465864313563
datanode_3          | }
datanode_3          | .
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis_grpc.log_appender.86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5: start LeaderState
datanode_1          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525: set configuration 0: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null at 0
datanode_1          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-FB3F76093525-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/current/log_inprogress_0
datanode_1          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 86d878c2-26cc-4775-9efa-2acc3bfcd9c5@group-4FF914EAA674-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/de69f1fb-497e-4296-815b-4ff914eaa674/current/log_inprogress_0
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:121 - 375f870c-6317-48e9-a102-4c22ab94c2d2: shutdown FollowerState
datanode_3          | 2023-07-13 21:26:41 INFO  FollowerState:117 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start FollowerState
datanode_3          | 2023-07-13 21:26:41 INFO  FollowerState:108 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-FollowerState: change to CANDIDATE, lastRpcTime:5074ms, electionTimeout:5066ms
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:121 - 375f870c-6317-48e9-a102-4c22ab94c2d2: shutdown FollowerState
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start LeaderElection
datanode_3          | 2023-07-13 21:26:41 INFO  LeaderElection:209 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-LeaderElection1: begin an election at term 1 for -1: [375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:134 - 375f870c-6317-48e9-a102-4c22ab94c2d2: shutdown LeaderElection
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:185 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-13 21:26:41 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-8B3B2337A1DD with new leaderId: 375f870c-6317-48e9-a102-4c22ab94c2d2
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:255 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: change Leader from null to 375f870c-6317-48e9-a102-4c22ab94c2d2 at term 1 for becomeLeader, leader elected after 5354ms
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:26:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-13 21:26:41 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-FB3F76093525 with new leaderId: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:255 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: change Leader from null to 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 at term 1 for appendEntries, leader elected after 5021ms
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525: set configuration 0: [6c97c744-03a7-4611-93eb-0d198f07fac2:172.20.0.7:9858, 375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858, 86d878c2-26cc-4775-9efa-2acc3bfcd9c5:172.20.0.9:9858], old=null at 0
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:26:41 INFO  RoleInfo:143 - 375f870c-6317-48e9-a102-4c22ab94c2d2: start LeaderState
datanode_3          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:397 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:26:41 INFO  RaftServerImpl:356 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD: set configuration 0: [375f870c-6317-48e9-a102-4c22ab94c2d2:172.20.0.11:9858], old=null at 0
datanode_3          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-FB3F76093525-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8e3da1eb-08a4-4e48-a237-fb3f76093525/current/log_inprogress_0
datanode_3          | 2023-07-13 21:26:41 INFO  SegmentedRaftLogWorker:596 - 375f870c-6317-48e9-a102-4c22ab94c2d2@group-8B3B2337A1DD-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd/current/log_inprogress_0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:25:08 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = ccce3bd47084/172.20.0.4
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-07-13 21:25:08 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:25:15 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:25:15 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.4:9862
om_1                | 2023-07-13 21:25:15 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:25:15 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:25:15 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-07-13 21:25:18 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:19 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:20 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:21 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:22 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:23 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:24 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:25 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:26 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:25:27 INFO  Client:958 - Retrying connect to server: scm/172.20.0.12:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f7810262-a5c6-4839-8603-b10e1b9e35e4;layoutVersion=0
om_1                | 2023-07-13 21:25:33 INFO  OzoneManagerStarter:124 - SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at ccce3bd47084/172.20.0.4
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:25:35 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = ccce3bd47084/172.20.0.4
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-07-13 21:25:35 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:25:37 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:25:37 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.4:9862
om_1                | 2023-07-13 21:25:37 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:25:37 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:25:38 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:25:38 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-07-13 21:25:39 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:25:39 INFO  OzoneManager:3574 - Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:25:39 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-13 21:25:39 INFO  Server:1219 - Starting Socket Reader #1 for port 9862
om_1                | 2023-07-13 21:25:39 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-13 21:25:39 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:25:39 INFO  MetricsSystemImpl:191 - OzoneManager metrics system started
om_1                | 2023-07-13 21:25:39 INFO  OzoneManager:1114 - OzoneManager RPC server is listening at om/172.20.0.4:9862
om_1                | 2023-07-13 21:25:39 INFO  BaseHttpServer:207 - Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:25:39 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-13 21:25:39 INFO  log:169 - Logging initialized @6358ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:25:39 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-13 21:25:39 INFO  HttpRequestLog:86 - Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:25:39 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:25:39 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-13 21:25:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-13 21:25:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:25:40 INFO  HttpServer2:1237 - Jetty bound to port 9874
om_1                | 2023-07-13 21:25:40 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
om_1                | 2023-07-13 21:25:40 INFO  session:333 - DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:25:40 INFO  session:338 - No SessionScavenger set, using defaults
om_1                | 2023-07-13 21:25:40 INFO  session:140 - node0 Scavenging every 660000ms
om_1                | 2023-07-13 21:25:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@4af45442{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:25:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@2bc426f0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-13 21:25:40 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@35ee466f{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_0_0_jar-_-any-3635479155348637853.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:25:40 INFO  AbstractConnector:330 - Started ServerConnector@8d8f754{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1                | 2023-07-13 21:25:40 INFO  Server:399 - Started @6808ms
om_1                | 2023-07-13 21:25:40 INFO  MetricsSinkAdapter:204 - Sink prometheus started
om_1                | 2023-07-13 21:25:40 INFO  MetricsSystemImpl:301 - Registered sink prometheus
om_1                | 2023-07-13 21:25:40 INFO  BaseHttpServer:327 - HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:25:40 INFO  Server:1460 - IPC Server Responder: starting
om_1                | 2023-07-13 21:25:40 INFO  Server:1298 - IPC Server listener on 9862: starting
om_1                | 2023-07-13 21:25:40 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
om_1                | 2023-07-13 21:26:38 INFO  OMDBCheckpointServlet:101 - Received request to obtain OM DB checkpoint snapshot
om_1                | 2023-07-13 21:26:38 INFO  RDBCheckpointManager:86 - Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1689283598527 in 8 milliseconds
om_1                | 2023-07-13 21:26:38 INFO  OMDBCheckpointServlet:144 - Time taken to write the checkpoint to response output stream: 29 milliseconds
om_1                | 2023-07-13 21:26:38 INFO  RocksDBCheckpoint:78 - Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1689283598527
om_1                | 2023-07-13 21:26:45 INFO  OMVolumeCreateRequest:195 - created volume:vol1 for user:hadoop
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:25:06 INFO  ReconServer:112 - STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 50fcd332ecd6/172.20.0.13
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.0.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
recon_1             | STARTUP_MSG:   java = 11.0.3
recon_1             | ************************************************************/
recon_1             | 2023-07-13 21:25:06 INFO  ReconServer:90 - registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-13 21:25:11 INFO  ReconRestServletModule:75 - rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-07-13 21:25:14 INFO  ReconServer:93 - Initializing Recon server...
recon_1             | 2023-07-13 21:25:16 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:25:23 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:25:25 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:25:25 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:25:25 INFO  ReconServer:101 - Creating Recon Schema.
recon_1             | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1             | 2023-07-13 21:25:30 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
recon_1             | 2023-07-13 21:25:30 INFO  BaseHttpServer:207 - Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-13 21:25:31 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:25:31 INFO  log:169 - Logging initialized @28862ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:25:31 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-13 21:25:31 WARN  HttpRequestLog:103 - Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-13 21:25:31 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:25:31 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:25:31 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-13 21:25:31 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:25:31 INFO  ReconTaskControllerImpl:79 - Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-13 21:25:32 INFO  ReconTaskControllerImpl:79 - Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-13 21:25:32 INFO  OmUtils:550 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-13 21:25:32 INFO  OmUtils:569 - No OzoneManager ServiceID configured.
recon_1             | 2023-07-13 21:25:32 INFO  deprecation:1395 - No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-07-13 21:25:32 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:25:33 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:25:33 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@66451058
recon_1             | 2023-07-13 21:25:33 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
recon_1             | 2023-07-13 21:25:33 WARN  DBStoreBuilder:277 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:25:33 INFO  SCMNodeManager:116 - Entering startup safe mode.
recon_1             | 2023-07-13 21:25:33 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:25:33 INFO  ReconNodeManager:100 - Loaded 0 nodes from node DB.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-13 21:25:08 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
s3g_1               | 2023-07-13 21:25:08 INFO  BaseHttpServer:207 - Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:25:08 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-13 21:25:09 INFO  log:169 - Logging initialized @8585ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-13 21:25:10 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-13 21:25:10 INFO  HttpRequestLog:86 - Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-13 21:25:10 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-13 21:25:10 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-13 21:25:10 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-13 21:25:10 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-13 21:25:10 INFO  Gateway:112 - STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 38a7c3cf7c3d/172.20.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.0.0
recon_1             | 2023-07-13 21:25:33 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-13 21:25:33 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:25:33 INFO  Server:1219 - Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-13 21:25:33 INFO  SCMPipelineManager:161 - No pipeline exists in current db
recon_1             | 2023-07-13 21:25:33 INFO  ReconServer:109 - Recon server initialized successfully!
recon_1             | 2023-07-13 21:25:33 INFO  ReconServer:134 - Starting Recon server
recon_1             | 2023-07-13 21:25:33 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:25:34 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:25:34 INFO  MetricsSystemImpl:191 - Recon metrics system started
recon_1             | 2023-07-13 21:25:34 INFO  HttpServer2:1237 - Jetty bound to port 9888
recon_1             | 2023-07-13 21:25:34 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
recon_1             | 2023-07-13 21:25:34 INFO  session:333 - DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-13 21:25:34 INFO  session:338 - No SessionScavenger set, using defaults
recon_1             | 2023-07-13 21:25:34 INFO  session:140 - node0 Scavenging every 600000ms
recon_1             | 2023-07-13 21:25:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@723877dd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:25:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@6d229b1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:25:38 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@aa633e6{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_0_0_jar-_-any-5235152758017450097.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/recon}
recon_1             | 2023-07-13 21:25:38 INFO  AbstractConnector:330 - Started ServerConnector@62765aec{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1             | 2023-07-13 21:25:38 INFO  Server:399 - Started @36138ms
recon_1             | 2023-07-13 21:25:38 INFO  MetricsSinkAdapter:204 - Sink prometheus started
recon_1             | 2023-07-13 21:25:38 INFO  MetricsSystemImpl:301 - Registered sink prometheus
recon_1             | 2023-07-13 21:25:38 INFO  BaseHttpServer:327 - HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-13 21:25:38 INFO  OzoneManagerServiceProviderImpl:198 - Starting Ozone Manager Service Provider.
recon_1             | 2023-07-13 21:25:38 INFO  OzoneManagerServiceProviderImpl:176 - Registered OmDeltaRequest task 
recon_1             | 2023-07-13 21:25:38 INFO  OzoneManagerServiceProviderImpl:186 - Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:25:38 INFO  ReconOmMetadataManagerImpl:65 - Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-13 21:25:38 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:25:38 INFO  ReconTaskControllerImpl:221 - Starting Recon Task Controller.
recon_1             | 2023-07-13 21:25:38 INFO  ReconStorageContainerManagerFacade:206 - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:25:38 INFO  ReconStorageContainerManagerFacade:256 - Obtained 0 pipelines from SCM.
recon_1             | 2023-07-13 21:25:38 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:25:38 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:25:38 INFO  Server:1460 - IPC Server Responder: starting
recon_1             | 2023-07-13 21:25:38 INFO  Server:1298 - IPC Server listener on 9891: starting
recon_1             | 2023-07-13 21:25:38 INFO  ReconScmTask:46 - Registered PipelineSyncTask task 
recon_1             | 2023-07-13 21:25:38 INFO  ReconScmTask:56 - Starting PipelineSyncTask Thread.
recon_1             | 2023-07-13 21:25:38 INFO  ReconScmTask:46 - Registered ContainerHealthTask task 
recon_1             | 2023-07-13 21:25:38 INFO  ReconScmTask:56 - Starting ContainerHealthTask Thread.
recon_1             | 2023-07-13 21:25:38 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:25:38 INFO  PipelineSyncTask:61 - Pipeline sync Thread took 17 milliseconds.
recon_1             | 2023-07-13 21:25:38 INFO  ContainerHealthTask:77 - Container Health task thread took 128 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:25:38 INFO  ContainerHealthTask:86 - Container Health task thread took 15 milliseconds for processing 0 containers.
recon_1             | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/6c97c744-03a7-4611-93eb-0d198f07fac2
recon_1             | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:32 INFO  ReconNodeManager:116 - Adding new node 6c97c744-03a7-4611-93eb-0d198f07fac2 to Node DB.
recon_1             | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/86d878c2-26cc-4775-9efa-2acc3bfcd9c5
recon_1             | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:32 INFO  ReconNodeManager:116 - Adding new node 86d878c2-26cc-4775-9efa-2acc3bfcd9c5 to Node DB.
recon_1             | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/375f870c-6317-48e9-a102-4c22ab94c2d2
recon_1             | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:32 INFO  ReconNodeManager:116 - Adding new node 375f870c-6317-48e9-a102-4c22ab94c2d2 to Node DB.
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=3d164048-22b8-420c-a159-2016c8830acc. Trying to get from SCM.
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 3d164048-22b8-420c-a159-2016c8830acc, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:6c97c744-03a7-4611-93eb-0d198f07fac2, CreationTimestamp2023-07-13T21:26:32.581Z] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:26:35 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 3d164048-22b8-420c-a159-2016c8830acc, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:6c97c744-03a7-4611-93eb-0d198f07fac2, CreationTimestamp2023-07-13T21:26:32.581Z]
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=de69f1fb-497e-4296-815b-4ff914eaa674. Trying to get from SCM.
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: de69f1fb-497e-4296-815b-4ff914eaa674, Nodes: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.634Z] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:26:35 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: de69f1fb-497e-4296-815b-4ff914eaa674, Nodes: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.634Z]
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:83 - Pipeline ONE PipelineID=de69f1fb-497e-4296-815b-4ff914eaa674 reported by 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:35 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: de69f1fb-497e-4296-815b-4ff914eaa674, Nodes: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:86d878c2-26cc-4775-9efa-2acc3bfcd9c5, CreationTimestamp2023-07-13T21:26:32.634Z] moved to OPEN state
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525. Trying to get from SCM.
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 8e3da1eb-08a4-4e48-a237-fb3f76093525, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.941Z] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:26:35 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 8e3da1eb-08a4-4e48-a237-fb3f76093525, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.941Z]
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:35 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:36 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd. Trying to get from SCM.
recon_1             | 2023-07-13 21:26:36 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd, Nodes: 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:375f870c-6317-48e9-a102-4c22ab94c2d2, CreationTimestamp2023-07-13T21:26:32.935Z] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:26:36 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd, Nodes: 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:375f870c-6317-48e9-a102-4c22ab94c2d2, CreationTimestamp2023-07-13T21:26:32.935Z]
recon_1             | 2023-07-13 21:26:36 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:38 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-07-13 21:26:38 INFO  OzoneManagerServiceProviderImpl:409 - Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-13 21:26:38 INFO  OzoneManagerServiceProviderImpl:316 - Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689283598402
recon_1             | 2023-07-13 21:26:38 INFO  ReconOmMetadataManagerImpl:91 - Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689283598402.
recon_1             | 2023-07-13 21:26:38 INFO  OzoneManagerServiceProviderImpl:421 - Calling reprocess on Recon tasks.
recon_1             | 2023-07-13 21:26:38 INFO  ContainerKeyMapperTask:73 - Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:26:38 INFO  ContainerDBServiceProviderImpl:117 - Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1689283598713
recon_1             | 2023-07-13 21:26:38 INFO  ContainerDBServiceProviderImpl:122 - Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1689283515166.
recon_1             | 2023-07-13 21:26:38 INFO  ContainerKeyMapperTask:89 - Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:26:38 INFO  ContainerKeyMapperTask:92 - It took me 0.074 seconds to process 0 keys.
recon_1             | 2023-07-13 21:26:38 INFO  FileSizeCountTask:102 - Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-13 21:26:40 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:40 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:41 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 reported by 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-13 21:26:41 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 8e3da1eb-08a4-4e48-a237-fb3f76093525, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:86d878c2-26cc-4775-9efa-2acc3bfcd9c5, CreationTimestamp2023-07-13T21:26:32.941Z] moved to OPEN state
recon_1             | 2023-07-13 21:26:47 INFO  ReconContainerManager:89 - New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:26:47 INFO  ReconContainerManager:157 - Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:26:57 INFO  ReconContainerManager:89 - New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-07-13 21:26:57 INFO  ReconContainerManager:157 - Successfully added container #2 to Recon.
recon_1             | 2023-07-13 21:27:38 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:25:09 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = b9352ad94224/172.20.0.12
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.0.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.10.3.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
s3g_1               | STARTUP_MSG:   java = 11.0.3
s3g_1               | ************************************************************/
s3g_1               | 2023-07-13 21:25:10 INFO  Gateway:90 - registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-13 21:25:11 INFO  Gateway:68 - Starting Ozone S3 gateway
s3g_1               | 2023-07-13 21:25:11 INFO  HttpServer2:1237 - Jetty bound to port 9878
s3g_1               | 2023-07-13 21:25:11 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
s3g_1               | 2023-07-13 21:25:11 INFO  session:333 - DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:25:11 INFO  session:338 - No SessionScavenger set, using defaults
s3g_1               | 2023-07-13 21:25:11 INFO  session:140 - node0 Scavenging every 600000ms
s3g_1               | 2023-07-13 21:25:11 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7dc19a70{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-13 21:25:11 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@23941fb4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/static,AVAILABLE}
s3g_1               | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 13, 2023 9:25:31 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-13 21:25:31 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@531ec978{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_0_0_jar-_-any-13962245803651212041.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/s3gateway}
s3g_1               | 2023-07-13 21:25:31 INFO  AbstractConnector:330 - Started ServerConnector@81d9a72{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:25:31 INFO  Server:399 - Started @31190ms
s3g_1               | 2023-07-13 21:25:31 INFO  BaseHttpServer:327 - HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:25:09 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:25:09 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:25:10 INFO  StorageContainerManager:644 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-f7810262-a5c6-4839-8603-b10e1b9e35e4;layoutVersion=0
scm_1               | 2023-07-13 21:25:10 INFO  StorageContainerManagerStarter:124 - SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at b9352ad94224/172.20.0.12
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:25:18 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = b9352ad94224/172.20.0.12
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:25:18 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:25:18 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:25:20 WARN  DBStoreBuilder:277 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:25:20 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@7b4c50bc
scm_1               | 2023-07-13 21:25:20 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1               | 2023-07-13 21:25:21 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1               | 2023-07-13 21:25:22 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-13 21:25:22 INFO  SCMPipelineManager:161 - No pipeline exists in current db
scm_1               | 2023-07-13 21:25:22 INFO  HealthyPipelineSafeModeRule:89 - Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:25:22 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:25:23 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
scm_1               | 2023-07-13 21:25:26 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:25:26 INFO  Server:1219 - Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-13 21:25:26 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:25:26 INFO  Server:1219 - Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-13 21:25:27 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:25:27 INFO  Server:1219 - Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:25:27 INFO  BaseHttpServer:207 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-13 21:25:27 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-13 21:25:27 INFO  log:169 - Logging initialized @16227ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:25:27 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-13 21:25:28 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:25:28 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:25:28 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:25:28 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:25:28 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-13 21:25:29 INFO  StorageContainerManager:784 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:25:29 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-13 21:25:30 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-13 21:25:30 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1               | 2023-07-13 21:25:31 INFO  SCMClientProtocolServer:156 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:25:31 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-13 21:25:31 INFO  Server:1298 - IPC Server listener on 9860: starting
scm_1               | 2023-07-13 21:25:31 INFO  StorageContainerManager:796 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:25:32 INFO  SCMBlockProtocolServer:149 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:25:32 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-13 21:25:32 INFO  Server:1298 - IPC Server listener on 9863: starting
scm_1               | 2023-07-13 21:25:32 INFO  StorageContainerManager:802 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:25:32 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:25:32 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-13 21:25:32 INFO  Server:1298 - IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:25:32 INFO  HttpServer2:1237 - Jetty bound to port 9876
scm_1               | 2023-07-13 21:25:32 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1               | 2023-07-13 21:25:32 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:25:32 INFO  session:338 - No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:25:32 INFO  session:140 - node0 Scavenging every 600000ms
scm_1               | 2023-07-13 21:25:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@66ba7e45{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:25:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7573e12f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:25:33 WARN  Server:1670 - IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.7:51452: output error
scm_1               | 2023-07-13 21:25:33 WARN  Server:1670 - IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.11:54810: output error
scm_1               | 2023-07-13 21:25:33 INFO  Server:2928 - IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-07-13 21:25:33 WARN  Server:1670 - IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.9:42550: output error
scm_1               | 2023-07-13 21:25:33 INFO  Server:2928 - IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-07-13 21:25:33 INFO  Server:2928 - IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-07-13 21:25:34 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@e9ef5b6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_0_0_jar-_-any-4436498540833822694.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/scm}
scm_1               | 2023-07-13 21:25:34 INFO  AbstractConnector:330 - Started ServerConnector@5f80fa43{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:25:34 INFO  Server:399 - Started @23392ms
scm_1               | 2023-07-13 21:25:34 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1               | 2023-07-13 21:25:34 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1               | 2023-07-13 21:25:34 INFO  BaseHttpServer:327 - HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:25:34 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1               | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/6c97c744-03a7-4611-93eb-0d198f07fac2
scm_1               | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=3d164048-22b8-420c-a159-2016c8830acc to datanode:6c97c744-03a7-4611-93eb-0d198f07fac2
scm_1               | 2023-07-13 21:26:32 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 3d164048-22b8-420c-a159-2016c8830acc, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.581721Z]
scm_1               | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/86d878c2-26cc-4775-9efa-2acc3bfcd9c5
scm_1               | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:71 - SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=de69f1fb-497e-4296-815b-4ff914eaa674 to datanode:86d878c2-26cc-4775-9efa-2acc3bfcd9c5
scm_1               | 2023-07-13 21:26:32 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: de69f1fb-497e-4296-815b-4ff914eaa674, Nodes: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.634295Z]
recon_1             | 2023-07-13 21:27:38 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-07-13 21:27:38 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 14
recon_1             | 2023-07-13 21:27:39 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 8 OM DB update event(s).
recon_1             | 2023-07-13 21:27:39 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
scm_1               | 2023-07-13 21:26:32 INFO  NetworkTopology:111 - Added a new node: /default-rack/375f870c-6317-48e9-a102-4c22ab94c2d2
scm_1               | 2023-07-13 21:26:32 INFO  SCMNodeManager:273 - Registered Data node : 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:71 - SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:214 - DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:242 - All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:26:32 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd to datanode:375f870c-6317-48e9-a102-4c22ab94c2d2
scm_1               | 2023-07-13 21:26:32 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd, Nodes: 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.935133Z]
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 to datanode:6c97c744-03a7-4611-93eb-0d198f07fac2
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 to datanode:375f870c-6317-48e9-a102-4c22ab94c2d2
scm_1               | 2023-07-13 21:26:32 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=8e3da1eb-08a4-4e48-a237-fb3f76093525 to datanode:86d878c2-26cc-4775-9efa-2acc3bfcd9c5
scm_1               | 2023-07-13 21:26:32 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 8e3da1eb-08a4-4e48-a237-fb3f76093525, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:26:32.941039Z]
scm_1               | 2023-07-13 21:26:35 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 3d164048-22b8-420c-a159-2016c8830acc, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6c97c744-03a7-4611-93eb-0d198f07fac2, CreationTimestamp2023-07-13T21:26:32.581721Z] moved to OPEN state
scm_1               | 2023-07-13 21:26:35 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:26:35 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:26:35 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: de69f1fb-497e-4296-815b-4ff914eaa674, Nodes: 86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:86d878c2-26cc-4775-9efa-2acc3bfcd9c5, CreationTimestamp2023-07-13T21:26:32.634295Z] moved to OPEN state
scm_1               | 2023-07-13 21:26:35 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:26:35 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:26:36 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: eeac2b70-4e30-46ed-a8b1-8b3b2337a1dd, Nodes: 375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:375f870c-6317-48e9-a102-4c22ab94c2d2, CreationTimestamp2023-07-13T21:26:32.935133Z] moved to OPEN state
scm_1               | 2023-07-13 21:26:36 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:26:36 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:26:41 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 8e3da1eb-08a4-4e48-a237-fb3f76093525, Nodes: 6c97c744-03a7-4611-93eb-0d198f07fac2{ip: 172.20.0.7, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}375f870c-6317-48e9-a102-4c22ab94c2d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}86d878c2-26cc-4775-9efa-2acc3bfcd9c5{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:86d878c2-26cc-4775-9efa-2acc3bfcd9c5, CreationTimestamp2023-07-13T21:26:32.941039Z] moved to OPEN state
scm_1               | 2023-07-13 21:26:41 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:26:41 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:26:41 INFO  SCMSafeModeManager:214 - HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:26:41 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-13 21:26:41 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-07-13 21:27:15 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.3
scm_1               | 2023-07-13 21:27:25 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.3
scm_1               | 2023-07-13 21:28:12 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.3
scm_1               | 2023-07-13 21:28:22 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.3
Attaching to xcompat_om_1, xcompat_recon_1, xcompat_old_client_1_2_1_1, xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_scm_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_0_0_1, xcompat_old_client_1_1_0_1, xcompat_s3g_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:28:41,738 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 3f68be370e5f/172.21.0.3
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.1.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_1          | STARTUP_MSG:   java = 11.0.10
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:28:41,811 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:28:44,049 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:28:44,470 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:28:45,607 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:28:45,607 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:28:46,160 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3f68be370e5f ip:172.21.0.3
datanode_1          | 2023-07-13 21:28:47,626 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:28:47,723 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-07-13 21:28:47,727 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:28:47,857 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:28:48,091 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:28:48,251 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:28:48,255 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:28:48,255 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:28:58,618 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:28:59,100 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:29:00,263 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-13 21:29:00,387 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-13 21:29:00,387 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-13 21:29:00,388 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:29:00,395 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:00,395 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:29:00,397 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:29:02,692 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-13 21:29:02,712 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:02,761 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:29:02,930 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:29:02,933 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:29:03,597 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:29:03,728 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:29:03,894 [main] INFO util.log: Logging initialized @28106ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:29:05,124 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-13 21:29:05,132 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:29:05,165 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:29:05,226 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:29:05,234 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:29:05,237 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:29:05,521 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-13 21:29:05,537 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_1          | 2023-07-13 21:29:05,934 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:29:05,934 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:29:05,945 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-07-13 21:29:06,242 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:29:06,255 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:29:08,747 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-830041571789462647/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:29:08,807 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:29:08,807 [main] INFO server.Server: Started @33019ms
datanode_1          | 2023-07-13 21:29:08,838 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:29:08,838 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-13 21:29:08,849 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:29:09,184 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@107145cc] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-13 21:29:09,892 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_1          | 2023-07-13 21:29:10,418 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:29:12,514 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:29:13,515 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:29:14,484 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-13 21:29:14,512 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-13 21:29:14,518 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:29:15,097 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:15,258 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.RaftServer: 6f624696-19ad-4034-8341-e1488a622757: start RPC server
datanode_1          | 2023-07-13 21:29:15,289 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 6f624696-19ad-4034-8341-e1488a622757: GrpcService started, listening on 9856
datanode_1          | 2023-07-13 21:29:15,294 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 6f624696-19ad-4034-8341-e1488a622757: GrpcService started, listening on 9857
datanode_1          | 2023-07-13 21:29:15,295 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 6f624696-19ad-4034-8341-e1488a622757: GrpcService started, listening on 9858
datanode_1          | 2023-07-13 21:29:15,307 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@1161c014] INFO util.JvmPauseMonitor: JvmPauseMonitor-6f624696-19ad-4034-8341-e1488a622757: Started
datanode_1          | 2023-07-13 21:29:15,315 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6f624696-19ad-4034-8341-e1488a622757 is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:29:15,315 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6f624696-19ad-4034-8341-e1488a622757 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-13 21:29:15,315 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6f624696-19ad-4034-8341-e1488a622757 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:29:19,621 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 3f68be370e5f/172.21.0.3 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.3:48858 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.3:48858 remote=recon/172.21.0.12:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_1          | 2023-07-13 21:29:20,230 [Command processor thread] INFO server.RaftServer: 6f624696-19ad-4034-8341-e1488a622757: addNew group-4D7A358BA103:[6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1] returns group-4D7A358BA103:java.util.concurrent.CompletableFuture@2a7a03ff[Not completed]
datanode_1          | 2023-07-13 21:29:20,341 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757: new RaftServerImpl for group-4D7A358BA103:[6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:29:20,362 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:29:20,363 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:29:20,373 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:29:20,374 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:28:44,665 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 6acc1c06cb52/172.21.0.4
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.1.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_2          | STARTUP_MSG:   java = 11.0.10
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:28:44,681 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:28:46,666 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:28:47,295 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:28:48,770 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:28:48,770 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:28:49,379 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6acc1c06cb52 ip:172.21.0.4
datanode_2          | 2023-07-13 21:28:51,373 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:28:51,382 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-07-13 21:28:51,400 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:28:51,447 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:28:51,541 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:28:51,802 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:28:51,802 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:28:51,812 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:29:02,067 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:29:02,560 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:29:03,027 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-13 21:29:03,045 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-13 21:29:03,046 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-13 21:29:03,047 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:29:03,048 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:29:03,049 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:29:03,049 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:29:05,366 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:29:05,368 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:05,379 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:29:05,507 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:29:05,513 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:29:06,413 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:29:06,632 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:29:06,914 [main] INFO util.log: Logging initialized @28874ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:29:08,061 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-13 21:29:08,100 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:29:08,144 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:29:08,147 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:29:08,165 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:29:08,182 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:29:08,456 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-13 21:29:08,458 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_2          | 2023-07-13 21:29:09,062 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:29:09,096 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:29:09,098 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-13 21:29:09,319 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:29:09,319 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:29:11,008 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-3539469320156248060/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-13 21:29:11,046 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:29:11,049 [main] INFO server.Server: Started @33057ms
datanode_2          | 2023-07-13 21:29:11,075 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:29:11,076 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:29:11,084 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:29:11,169 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70d6eda4] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-13 21:29:11,594 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_2          | 2023-07-13 21:29:11,918 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:29:14,438 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:29:14,512 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-13 21:29:14,517 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-13 21:29:14,955 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:15,032 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.RaftServer: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start RPC server
datanode_2          | 2023-07-13 21:29:15,047 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 5233c3d5-13a9-4934-81e6-18d02ce266a2: GrpcService started, listening on 9856
datanode_2          | 2023-07-13 21:29:15,048 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 5233c3d5-13a9-4934-81e6-18d02ce266a2: GrpcService started, listening on 9857
datanode_2          | 2023-07-13 21:29:15,048 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: 5233c3d5-13a9-4934-81e6-18d02ce266a2: GrpcService started, listening on 9858
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:28:41,803 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = b393538687e1/172.21.0.2
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.1.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_3          | STARTUP_MSG:   java = 11.0.10
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:28:41,853 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:28:44,080 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:28:44,633 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:28:46,020 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:28:46,020 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:28:46,592 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b393538687e1 ip:172.21.0.2
datanode_3          | 2023-07-13 21:28:48,236 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:28:48,248 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-07-13 21:28:48,249 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:28:48,301 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:28:48,446 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:28:48,699 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:28:48,701 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:28:48,701 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:29:00,010 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:29:00,567 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:29:01,026 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-13 21:29:01,027 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-13 21:29:01,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-13 21:29:01,047 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:29:01,048 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:01,065 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:29:01,066 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:29:03,331 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-13 21:29:03,336 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:03,347 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:29:03,421 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:29:03,434 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:29:04,266 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:29:04,733 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:29:05,115 [main] INFO util.log: Logging initialized @29310ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:29:05,998 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-13 21:29:06,006 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:29:06,027 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:29:06,036 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:29:06,036 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-13 21:29:06,036 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:29:06,300 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-13 21:29:06,315 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_3          | 2023-07-13 21:29:06,555 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:29:06,555 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:29:06,577 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-13 21:29:06,696 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:29:06,699 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:29:09,577 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-15326272309741069922/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:29:09,659 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:29:09,660 [main] INFO server.Server: Started @33854ms
datanode_3          | 2023-07-13 21:29:09,713 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-13 21:29:09,723 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-13 21:29:09,727 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:29:09,971 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1bae2aa1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-13 21:29:10,463 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_3          | 2023-07-13 21:29:10,748 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:29:13,215 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:29:14,219 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:29:14,591 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:29:14,595 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-13 21:29:14,883 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:14,922 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.RaftServer: f8c819d0-1ba4-46a8-98ef-94a593675c85: start RPC server
datanode_3          | 2023-07-13 21:29:14,941 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: f8c819d0-1ba4-46a8-98ef-94a593675c85: GrpcService started, listening on 9856
datanode_3          | 2023-07-13 21:29:14,944 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: f8c819d0-1ba4-46a8-98ef-94a593675c85: GrpcService started, listening on 9857
datanode_3          | 2023-07-13 21:29:14,946 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO server.GrpcService: f8c819d0-1ba4-46a8-98ef-94a593675c85: GrpcService started, listening on 9858
datanode_3          | 2023-07-13 21:29:14,997 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f8c819d0-1ba4-46a8-98ef-94a593675c85 is started using port 9858 for RATIS
datanode_3          | 2023-07-13 21:29:14,998 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f8c819d0-1ba4-46a8-98ef-94a593675c85 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-13 21:29:14,999 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f8c819d0-1ba4-46a8-98ef-94a593675c85 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-13 21:29:15,006 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@176afc13] INFO util.JvmPauseMonitor: JvmPauseMonitor-f8c819d0-1ba4-46a8-98ef-94a593675c85: Started
datanode_3          | 2023-07-13 21:29:15,220 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:29:18,008 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-13 21:29:19,074 [Command processor thread] INFO server.RaftServer: f8c819d0-1ba4-46a8-98ef-94a593675c85: addNew group-828A1E8E8935:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1] returns group-828A1E8E8935:java.util.concurrent.CompletableFuture@60d30e46[Not completed]
datanode_3          | 2023-07-13 21:29:19,131 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85: new RaftServerImpl for group-828A1E8E8935:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:29:19,137 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:29:19,138 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:29:19,144 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:29:19,144 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:19,144 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:29:19,144 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:29:19,145 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:19,155 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:29:19,163 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:29:19,169 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:29:19,174 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e2cb8a73-a3d5-4676-b1f2-828a1e8e8935 does not exist. Creating ...
datanode_3          | 2023-07-13 21:29:19,186 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e2cb8a73-a3d5-4676-b1f2-828a1e8e8935/in_use.lock acquired by nodename 7@b393538687e1
datanode_3          | 2023-07-13 21:29:19,226 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e2cb8a73-a3d5-4676-b1f2-828a1e8e8935 has been successfully formatted.
datanode_3          | 2023-07-13 21:29:19,244 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-828A1E8E8935: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:29:19,249 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:19,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:29:20,374 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:29:20,375 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:29:20,375 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:29:20,390 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: ConfigurationManager, init=-1: [6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:29:20,391 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:29:20,417 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:29:20,420 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/16fcb917-c37a-4fb6-9616-4d7a358ba103 does not exist. Creating ...
datanode_1          | 2023-07-13 21:29:20,438 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/16fcb917-c37a-4fb6-9616-4d7a358ba103/in_use.lock acquired by nodename 7@3f68be370e5f
datanode_1          | 2023-07-13 21:29:20,465 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/16fcb917-c37a-4fb6-9616-4d7a358ba103 has been successfully formatted.
datanode_1          | 2023-07-13 21:29:20,474 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-4D7A358BA103: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:29:20,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:20,498 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:29:20,516 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:29:20,516 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:20,526 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103
datanode_1          | 2023-07-13 21:29:20,557 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:20,605 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:29:20,605 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:29:20,646 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/16fcb917-c37a-4fb6-9616-4d7a358ba103
datanode_1          | 2023-07-13 21:29:20,646 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:29:20,647 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:29:20,648 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:20,670 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:29:20,671 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:29:20,672 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:29:20,674 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:29:20,675 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:29:20,735 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:20,736 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:20,753 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:20,753 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:20,797 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:20,798 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:29:20,798 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:29:20,817 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:29:20,818 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:29:20,818 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:29:20,958 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103
datanode_1          | 2023-07-13 21:29:20,970 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103
datanode_1          | 2023-07-13 21:29:21,053 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: start as a follower, conf=-1: [6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:29:21,080 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:29:21,094 [pool-19-thread-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState
datanode_1          | 2023-07-13 21:29:21,126 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D7A358BA103,id=6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:21,127 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103
datanode_1          | 2023-07-13 21:29:21,224 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=16fcb917-c37a-4fb6-9616-4d7a358ba103.
datanode_2          | 2023-07-13 21:29:15,095 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5233c3d5-13a9-4934-81e6-18d02ce266a2 is started using port 9858 for RATIS
datanode_2          | 2023-07-13 21:29:15,095 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5233c3d5-13a9-4934-81e6-18d02ce266a2 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-13 21:29:15,107 [EndpointStateMachine task thread for scm/172.21.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5233c3d5-13a9-4934-81e6-18d02ce266a2 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-13 21:29:15,095 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@19771985] INFO util.JvmPauseMonitor: JvmPauseMonitor-5233c3d5-13a9-4934-81e6-18d02ce266a2: Started
datanode_2          | 2023-07-13 21:29:18,218 [Command processor thread] INFO server.RaftServer: 5233c3d5-13a9-4934-81e6-18d02ce266a2: addNew group-B154E0209F6E:[5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] returns group-B154E0209F6E:java.util.concurrent.CompletableFuture@4468be08[Not completed]
datanode_2          | 2023-07-13 21:29:18,255 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2: new RaftServerImpl for group-B154E0209F6E:[5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:29:18,257 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:29:18,259 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:29:18,259 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:29:18,259 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:18,259 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:29:18,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:29:18,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:29:18,268 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: ConfigurationManager, init=-1: [5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:29:18,274 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:29:18,278 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:29:18,286 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e does not exist. Creating ...
datanode_2          | 2023-07-13 21:29:18,303 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e/in_use.lock acquired by nodename 6@6acc1c06cb52
datanode_2          | 2023-07-13 21:29:18,320 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e has been successfully formatted.
datanode_2          | 2023-07-13 21:29:18,336 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B154E0209F6E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:29:18,343 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:18,349 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:29:18,367 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:29:18,370 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:29:18,374 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E
datanode_2          | 2023-07-13 21:29:18,414 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:18,442 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:29:18,453 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:29:18,479 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e
datanode_2          | 2023-07-13 21:29:18,480 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:29:18,483 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:29:18,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:18,494 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:29:18,494 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:29:18,495 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:29:18,496 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:29:18,496 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:29:18,528 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:18,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:18,569 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:18,570 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:18,590 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:18,591 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:29:18,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:29:18,602 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:29:18,603 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:29:18,604 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:29:18,711 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E
datanode_2          | 2023-07-13 21:29:18,722 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E
datanode_2          | 2023-07-13 21:29:18,748 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: start as a follower, conf=-1: [5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:29:18,759 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:29:18,763 [pool-19-thread-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState
datanode_2          | 2023-07-13 21:29:18,786 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B154E0209F6E,id=5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:18,789 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E
datanode_2          | 2023-07-13 21:29:18,868 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e.
datanode_2          | 2023-07-13 21:29:19,485 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 6acc1c06cb52/172.21.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.4:35442 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.4:35442 remote=recon/172.21.0.12:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_2          | 2023-07-13 21:29:23,634 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:23,634 [grpc-default-executor-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:23,723 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2: new RaftServerImpl for group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:29:23,726 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:29:23,727 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:29:23,727 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:29:23,727 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:23,730 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:29:23,730 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:29:23,730 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:29:23,731 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:29:23,733 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:29:23,733 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:29:23,737 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c does not exist. Creating ...
datanode_2          | 2023-07-13 21:29:23,738 [grpc-default-executor-0] INFO server.RaftServer: 5233c3d5-13a9-4934-81e6-18d02ce266a2: addNew group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0] returns group-A8A350D9574C:java.util.concurrent.CompletableFuture@657d517c[Not completed]
datanode_2          | 2023-07-13 21:29:23,742 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/in_use.lock acquired by nodename 6@6acc1c06cb52
datanode_2          | 2023-07-13 21:29:23,746 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c has been successfully formatted.
datanode_2          | 2023-07-13 21:29:23,746 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A8A350D9574C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:29:23,755 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:23,756 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:29:23,756 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:29:23,756 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:29:23,760 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C
datanode_2          | 2023-07-13 21:29:23,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:23,764 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:29:23,769 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:29:23,770 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c
datanode_2          | 2023-07-13 21:29:23,770 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:29:23,771 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:29:23,771 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:23,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:29:23,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:29:23,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:29:23,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:29:23,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:29:23,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:23,784 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:23,800 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:23,803 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:23,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:23,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:29:23,828 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:29:23,828 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:29:23,829 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:29:23,829 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:29:23,830 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C
datanode_2          | 2023-07-13 21:29:23,830 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:21,224 [Command processor thread] INFO server.RaftServer: 6f624696-19ad-4034-8341-e1488a622757: addNew group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] returns group-A8A350D9574C:java.util.concurrent.CompletableFuture@1e7d767f[Not completed]
datanode_1          | 2023-07-13 21:29:21,266 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757: new RaftServerImpl for group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:29:21,268 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:29:21,274 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:29:21,275 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:29:21,275 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:21,276 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:29:21,277 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:29:21,290 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:29:21,296 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:29:21,300 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:29:21,377 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:29:21,391 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c does not exist. Creating ...
datanode_1          | 2023-07-13 21:29:21,401 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/in_use.lock acquired by nodename 7@3f68be370e5f
datanode_1          | 2023-07-13 21:29:21,428 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c has been successfully formatted.
datanode_1          | 2023-07-13 21:29:21,429 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A8A350D9574C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:29:21,432 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:29:21,432 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:21,432 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:29:21,438 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:29:21,438 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:21,438 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:21,439 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:21,441 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:29:21,446 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:29:21,446 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c
datanode_1          | 2023-07-13 21:29:21,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:29:21,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:29:21,449 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:21,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:29:21,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:29:21,457 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:29:21,457 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:29:21,457 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:29:21,459 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:21,467 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:21,472 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:21,472 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:21,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:21,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:29:21,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:29:21,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:29:21,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:29:21,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:29:21,474 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:21,478 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_2          | 2023-07-13 21:29:23,831 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:23,834 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:29:23,835 [pool-19-thread-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState
datanode_2          | 2023-07-13 21:29:23,839 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8A350D9574C,id=5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:23,839 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C
datanode_2          | 2023-07-13 21:29:23,969 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5206117801ns, electionTimeout:5191ms
datanode_2          | 2023-07-13 21:29:23,975 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState
datanode_2          | 2023-07-13 21:29:23,976 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:29:23,995 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:29:23,995 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-FollowerState] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1
datanode_2          | 2023-07-13 21:29:24,026 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO impl.LeaderElection: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:29:24,029 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO impl.LeaderElection: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-13 21:29:24,034 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1
datanode_2          | 2023-07-13 21:29:24,037 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:29:24,039 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B154E0209F6E with new leaderId: 5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:24,040 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: change Leader from null to 5233c3d5-13a9-4934-81e6-18d02ce266a2 at term 1 for becomeLeader, leader elected after 5696ms
datanode_2          | 2023-07-13 21:29:24,073 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:29:24,075 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E
datanode_2          | 2023-07-13 21:29:24,086 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:29:24,088 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-07-13 21:29:24,105 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:29:24,120 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:29:24,127 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:29:24,164 [grpc-default-executor-1] INFO server.RaftServer: 5233c3d5-13a9-4934-81e6-18d02ce266a2: addNew group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0] returns group-63455D05C85C:java.util.concurrent.CompletableFuture@14849002[Not completed]
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2: new RaftServerImpl for group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:19,304 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:29:19,305 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:19,318 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935
datanode_3          | 2023-07-13 21:29:19,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,368 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:29:19,369 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:29:19,400 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e2cb8a73-a3d5-4676-b1f2-828a1e8e8935
datanode_3          | 2023-07-13 21:29:19,406 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:29:19,416 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:29:19,421 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:29:19,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:29:19,427 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:29:19,433 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:29:19,434 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:29:19,472 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:19,510 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:29:19,514 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:29:19,533 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:19,543 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:29:19,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:29:19,547 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:29:19,553 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:29:19,553 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:29:19,594 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935
datanode_3          | 2023-07-13 21:29:19,604 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935
datanode_3          | 2023-07-13 21:29:19,629 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:29:19,639 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:29:19,644 [pool-19-thread-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState
datanode_3          | 2023-07-13 21:29:19,662 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-828A1E8E8935,id=f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:19,667 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935
datanode_3          | 2023-07-13 21:29:19,715 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e2cb8a73-a3d5-4676-b1f2-828a1e8e8935.
datanode_3          | 2023-07-13 21:29:19,716 [Command processor thread] INFO server.RaftServer: f8c819d0-1ba4-46a8-98ef-94a593675c85: addNew group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] returns group-A8A350D9574C:java.util.concurrent.CompletableFuture@20d8bdd4[Not completed]
datanode_3          | 2023-07-13 21:29:19,718 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85: new RaftServerImpl for group-A8A350D9574C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:29:19,725 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:19,728 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:29:19,729 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:29:19,729 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:29:19,729 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c does not exist. Creating ...
datanode_3          | 2023-07-13 21:29:19,738 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/in_use.lock acquired by nodename 7@b393538687e1
datanode_3          | 2023-07-13 21:29:19,740 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c has been successfully formatted.
datanode_3          | 2023-07-13 21:29:19,740 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A8A350D9574C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C
datanode_3          | 2023-07-13 21:29:19,741 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,742 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:29:19,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:29:19,760 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:29:19,761 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c
datanode_3          | 2023-07-13 21:29:19,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:29:19,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:29:19,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:29:19,765 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:29:19,765 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:29:19,765 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:29:19,765 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:29:19,767 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:19,775 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:19,779 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:21,491 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:21,498 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:29:21,499 [pool-19-thread-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState
datanode_1          | 2023-07-13 21:29:21,499 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8A350D9574C,id=6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:21,499 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:21,678 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7342A5BF0958->f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_1          | 2023-07-13 21:29:22,369 [grpc-default-executor-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:23,131 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1D3AE6287113->5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_1          | 2023-07-13 21:29:23,901 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c.
datanode_1          | 2023-07-13 21:29:23,902 [Command processor thread] INFO server.RaftServer: 6f624696-19ad-4034-8341-e1488a622757: addNew group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] returns group-63455D05C85C:java.util.concurrent.CompletableFuture@4f499611[Not completed]
datanode_1          | 2023-07-13 21:29:23,903 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757: new RaftServerImpl for group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:29:23,903 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:29:23,903 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:29:23,903 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:29:23,904 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:29:23,905 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:29:23,905 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c does not exist. Creating ...
datanode_1          | 2023-07-13 21:29:23,907 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/in_use.lock acquired by nodename 7@3f68be370e5f
datanode_1          | 2023-07-13 21:29:23,909 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c has been successfully formatted.
datanode_1          | 2023-07-13 21:29:23,911 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-63455D05C85C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:29:23,911 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:29:23,911 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:29:23,911 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:29:23,912 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:23,912 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C
datanode_1          | 2023-07-13 21:29:23,912 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:23,912 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:29:23,912 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:29:23,923 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c
datanode_1          | 2023-07-13 21:29:23,923 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:29:23,923 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:29:23,923 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:23,924 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:29:23,924 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:29:23,924 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:29:23,924 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:29:23,924 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:29:23,925 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:29:23,927 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:23,927 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:23,930 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:29:23,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:29:23,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:29:23,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:29:23,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:29:23,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:29:23,936 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:29:23,936 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C
datanode_1          | 2023-07-13 21:29:23,936 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C
datanode_1          | 2023-07-13 21:29:23,943 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:23,943 [pool-19-thread-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:29:23,943 [pool-19-thread-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:23,950 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-63455D05C85C,id=6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:23,950 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C
datanode_1          | 2023-07-13 21:29:23,956 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-533C40DB5D14->f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_1          | 2023-07-13 21:29:24,128 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-12D8725665C3->5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_1          | 2023-07-13 21:29:24,358 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c.
datanode_1          | 2023-07-13 21:29:25,151 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: receive requestVote(ELECTION, f8c819d0-1ba4-46a8-98ef-94a593675c85, group-A8A350D9574C, 1, (t:0, i:0))
datanode_1          | 2023-07-13 21:29:25,156 [grpc-default-executor-1] INFO impl.VoteContext: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FOLLOWER: reject ELECTION from f8c819d0-1ba4-46a8-98ef-94a593675c85: our priority 1 > candidate's priority 0
datanode_1          | 2023-07-13 21:29:25,163 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_1          | 2023-07-13 21:29:25,163 [grpc-default-executor-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState
datanode_1          | 2023-07-13 21:29:25,164 [grpc-default-executor-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState
datanode_1          | 2023-07-13 21:29:25,164 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:29:25,181 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C replies to ELECTION vote request: f8c819d0-1ba4-46a8-98ef-94a593675c85<-6f624696-19ad-4034-8341-e1488a622757#0:FAIL-t1. Peer's state: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C:t1, leader=null, voted=null, raftlog=6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:26,158 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5077153653ns, electionTimeout:5050ms
datanode_1          | 2023-07-13 21:29:26,158 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState
datanode_1          | 2023-07-13 21:29:26,159 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:29:26,162 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:29:19,779 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:29:19,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:29:19,780 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C
datanode_3          | 2023-07-13 21:29:19,793 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C
datanode_3          | 2023-07-13 21:29:19,798 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:19,798 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:29:19,803 [pool-19-thread-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState
datanode_3          | 2023-07-13 21:29:19,820 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8A350D9574C,id=f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:19,820 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C
datanode_3          | 2023-07-13 21:29:19,988 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B528E7E0B1EA->6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:20,252 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | 2023-07-13 21:29:26,162 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1
datanode_1          | 2023-07-13 21:29:26,171 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:29:26,172 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-13 21:29:26,172 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1
datanode_1          | 2023-07-13 21:29:26,173 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:29:26,173 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4D7A358BA103 with new leaderId: 6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:26,178 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: change Leader from null to 6f624696-19ad-4034-8341-e1488a622757 at term 1 for becomeLeader, leader elected after 5686ms
datanode_1          | 2023-07-13 21:29:26,193 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:29:26,201 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103
datanode_1          | 2023-07-13 21:29:26,207 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:29:26,207 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-13 21:29:26,212 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:29:26,212 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:29:26,219 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:29:26,229 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderStateImpl
datanode_1          | 2023-07-13 21:29:26,263 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:29:26,330 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-LeaderElection1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103: set configuration 0: [6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-13 21:29:26,483 [6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-4D7A358BA103-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/16fcb917-c37a-4fb6-9616-4d7a358ba103/current/log_inprogress_0
datanode_1          | 2023-07-13 21:29:29,041 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5097373303ns, electionTimeout:5090ms
datanode_1          | 2023-07-13 21:29:29,041 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:29,041 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:29:29,041 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:29:29,042 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2
datanode_1          | 2023-07-13 21:29:29,047 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:29,140 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:29:29,140 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t1
datanode_1          | 2023-07-13 21:29:29,140 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.LeaderElection:   Response 1: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t1
datanode_1          | 2023-07-13 21:29:29,141 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:29:29,142 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-07-13 21:29:29,142 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2
datanode_1          | 2023-07-13 21:29:29,142 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection2] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_3          | java.net.SocketTimeoutException: Call From b393538687e1/172.21.0.2 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.2:54048 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.2:54048 remote=recon/172.21.0.12:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_3          | 2023-07-13 21:29:22,715 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:22,804 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-350B9A9FD79A->5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_3          | 2023-07-13 21:29:24,018 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c.
datanode_3          | 2023-07-13 21:29:24,025 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85: new RaftServerImpl for group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:29:24,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:29:24,028 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:29:24,028 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:29:24,028 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:24,028 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:29:24,032 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:29:24,033 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:24,033 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:29:24,033 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:29:24,036 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:29:24,038 [Command processor thread] INFO server.RaftServer: f8c819d0-1ba4-46a8-98ef-94a593675c85: addNew group-63455D05C85C:[f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0] returns group-63455D05C85C:java.util.concurrent.CompletableFuture@1c44dbf4[Not completed]
datanode_3          | 2023-07-13 21:29:24,051 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c does not exist. Creating ...
datanode_3          | 2023-07-13 21:29:24,066 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/in_use.lock acquired by nodename 7@b393538687e1
datanode_1          | 2023-07-13 21:29:30,319 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5155160348ns, electionTimeout:5148ms
datanode_1          | 2023-07-13 21:29:30,320 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState
datanode_1          | 2023-07-13 21:29:30,320 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-07-13 21:29:30,320 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:29:30,320 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3
datanode_1          | 2023-07-13 21:29:30,325 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:30,349 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:29:30,349 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.LeaderElection:   Response 0: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:OK-t2
datanode_1          | 2023-07-13 21:29:30,349 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-07-13 21:29:30,349 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3
datanode_1          | 2023-07-13 21:29:30,350 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_1          | 2023-07-13 21:29:30,350 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A8A350D9574C with new leaderId: 6f624696-19ad-4034-8341-e1488a622757
datanode_1          | 2023-07-13 21:29:30,350 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: change Leader from null to 6f624696-19ad-4034-8341-e1488a622757 at term 2 for becomeLeader, leader elected after 8918ms
datanode_1          | 2023-07-13 21:29:30,351 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:29:30,351 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:30,352 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:29:30,352 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-13 21:29:30,352 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:29:30,353 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:29:30,353 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:29:30,366 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:29:30,366 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:30,367 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:29:30,369 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:29:30,371 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:29:30,371 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:29:30,371 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C
datanode_1          | 2023-07-13 21:29:30,376 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:29:30,376 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:29:30,377 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:29:30,378 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:29:30,379 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:29:30,379 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:29:30,380 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderStateImpl
datanode_3          | 2023-07-13 21:29:24,071 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c has been successfully formatted.
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-63455D05C85C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:24,108 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:29:24,109 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:29:24,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:29:24,133 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:29:24,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:24,150 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:29:24,150 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:29:24,152 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:29:24,153 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:24,153 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:24,154 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:24,154 [pool-19-thread-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:29:24,154 [pool-19-thread-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:24,162 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-63455D05C85C,id=f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:24,162 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:24,177 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3EA53D2C7FB8->6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:24,264 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-AC764E5984E6->5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_3          | 2023-07-13 21:29:24,426 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c.
datanode_3          | 2023-07-13 21:29:24,719 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5077557614ns, electionTimeout:5044ms
datanode_3          | 2023-07-13 21:29:24,720 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState
datanode_3          | 2023-07-13 21:29:24,721 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:29:24,724 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:29:24,724 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1
datanode_3          | 2023-07-13 21:29:24,736 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:29:30,381 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:29:30,383 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/current/log_inprogress_0
datanode_1          | 2023-07-13 21:29:30,388 [6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C-LeaderElection3] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-A8A350D9574C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-13 21:29:34,167 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5025010835ns, electionTimeout:5015ms
datanode_1          | 2023-07-13 21:29:34,168 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:34,168 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-07-13 21:29:34,168 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:29:34,169 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4
datanode_1          | 2023-07-13 21:29:34,171 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:34,225 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:29:34,225 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection:   Response 0: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t2
datanode_1          | 2023-07-13 21:29:34,225 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection:   Response 1: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t2
datanode_1          | 2023-07-13 21:29:34,226 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:29:34,226 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-07-13 21:29:34,226 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4
datanode_1          | 2023-07-13 21:29:34,226 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection4] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:39,232 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5005843231ns, electionTimeout:5005ms
datanode_1          | 2023-07-13 21:29:39,233 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:39,233 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-07-13 21:29:39,233 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:29:39,233 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5
datanode_1          | 2023-07-13 21:29:39,243 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5 ELECTION round 0: submit vote requests at term 3 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:39,264 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:29:39,264 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.LeaderElection:   Response 0: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t3
datanode_1          | 2023-07-13 21:29:39,264 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.LeaderElection: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:29:39,265 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_1          | 2023-07-13 21:29:39,265 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5
datanode_1          | 2023-07-13 21:29:39,265 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-LeaderElection5] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:24,171 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: ConfigurationManager, init=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:29:24,172 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:29:24,172 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:29:24,172 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c does not exist. Creating ...
datanode_2          | 2023-07-13 21:29:24,175 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderStateImpl
datanode_2          | 2023-07-13 21:29:24,178 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/in_use.lock acquired by nodename 6@6acc1c06cb52
datanode_2          | 2023-07-13 21:29:24,196 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c has been successfully formatted.
datanode_2          | 2023-07-13 21:29:24,220 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-63455D05C85C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:29:24,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:29:24,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:29:24,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:29:24,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c
datanode_2          | 2023-07-13 21:29:24,221 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:29:24,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:29:24,242 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:29:24,244 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:24,247 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:24,258 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:29:24,259 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:29:24,265 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:29:24,265 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:29:24,266 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:29:24,267 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:29:24,267 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:29:24,271 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C
datanode_2          | 2023-07-13 21:29:24,272 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C
datanode_2          | 2023-07-13 21:29:24,273 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: start as a follower, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:24,274 [pool-19-thread-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:29:24,274 [pool-19-thread-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:24,311 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-63455D05C85C,id=5233c3d5-13a9-4934-81e6-18d02ce266a2
datanode_2          | 2023-07-13 21:29:24,311 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C
datanode_2          | 2023-07-13 21:29:24,282 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:29:24,458 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-LeaderElection1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E: set configuration 0: [5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-13 21:29:44,275 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: receive requestVote(ELECTION, f8c819d0-1ba4-46a8-98ef-94a593675c85, group-63455D05C85C, 4, (t:0, i:0))
datanode_1          | 2023-07-13 21:29:44,278 [grpc-default-executor-1] INFO impl.VoteContext: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FOLLOWER: accept ELECTION from f8c819d0-1ba4-46a8-98ef-94a593675c85: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:29:44,278 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_1          | 2023-07-13 21:29:44,279 [grpc-default-executor-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: shutdown 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:44,279 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:29:44,280 [grpc-default-executor-1] INFO impl.RoleInfo: 6f624696-19ad-4034-8341-e1488a622757: start 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-FollowerState
datanode_1          | 2023-07-13 21:29:44,286 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C replies to ELECTION vote request: f8c819d0-1ba4-46a8-98ef-94a593675c85<-6f624696-19ad-4034-8341-e1488a622757#0:OK-t4. Peer's state: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C:t4, leader=null, voted=f8c819d0-1ba4-46a8-98ef-94a593675c85, raftlog=6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:29:44,415 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-63455D05C85C with new leaderId: f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_1          | 2023-07-13 21:29:44,415 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: change Leader from null to f8c819d0-1ba4-46a8-98ef-94a593675c85 at term 4 for appendEntries, leader elected after 20503ms
datanode_1          | 2023-07-13 21:29:44,487 [grpc-default-executor-1] INFO server.RaftServer$Division: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-13 21:29:44,491 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:29:44,496 [6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6f624696-19ad-4034-8341-e1488a622757@group-63455D05C85C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/current/log_inprogress_0
datanode_2          | 2023-07-13 21:29:24,538 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-B154E0209F6E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e/current/log_inprogress_0
datanode_2          | 2023-07-13 21:29:25,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: receive requestVote(ELECTION, f8c819d0-1ba4-46a8-98ef-94a593675c85, group-A8A350D9574C, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:25,086 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FOLLOWER: accept ELECTION from f8c819d0-1ba4-46a8-98ef-94a593675c85: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:29:25,087 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_2          | 2023-07-13 21:29:25,087 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState
datanode_2          | 2023-07-13 21:29:25,087 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:29:25,088 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState
datanode_2          | 2023-07-13 21:29:25,107 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C replies to ELECTION vote request: f8c819d0-1ba4-46a8-98ef-94a593675c85<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t1. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C:t1, leader=null, voted=f8c819d0-1ba4-46a8-98ef-94a593675c85, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:29,094 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:29,095 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FOLLOWER: accept ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:29:29,095 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:29,095 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:29,095 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:29,095 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:29:29,098 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t1. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C:t1, leader=null, voted=6f624696-19ad-4034-8341-e1488a622757, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:30,332 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-A8A350D9574C, 2, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:30,333 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FOLLOWER: accept ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:29:30,334 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:30,334 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState
datanode_2          | 2023-07-13 21:29:30,336 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState
datanode_2          | 2023-07-13 21:29:30,336 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:29:24,737 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-13 21:29:24,737 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1
datanode_3          | 2023-07-13 21:29:24,738 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-13 21:29:24,738 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-828A1E8E8935 with new leaderId: f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:24,749 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: change Leader from null to f8c819d0-1ba4-46a8-98ef-94a593675c85 at term 1 for becomeLeader, leader elected after 5489ms
datanode_3          | 2023-07-13 21:29:24,764 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:29:24,766 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935
datanode_3          | 2023-07-13 21:29:24,772 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:29:24,773 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-13 21:29:24,780 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:29:24,780 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:29:24,781 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:29:24,806 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderStateImpl
datanode_3          | 2023-07-13 21:29:24,836 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:29:24,894 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-LeaderElection1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-13 21:29:24,949 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5146690671ns, electionTimeout:5128ms
datanode_3          | 2023-07-13 21:29:24,959 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState
datanode_3          | 2023-07-13 21:29:24,959 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:29:24,959 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:29:24,960 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2
datanode_3          | 2023-07-13 21:29:24,987 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:25,148 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-828A1E8E8935-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e2cb8a73-a3d5-4676-b1f2-828a1e8e8935/current/log_inprogress_0
datanode_3          | 2023-07-13 21:29:25,215 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:29:25,217 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.LeaderElection:   Response 0: f8c819d0-1ba4-46a8-98ef-94a593675c85<-6f624696-19ad-4034-8341-e1488a622757#0:FAIL-t1
datanode_3          | 2023-07-13 21:29:25,217 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.LeaderElection:   Response 1: f8c819d0-1ba4-46a8-98ef-94a593675c85<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t1
datanode_3          | 2023-07-13 21:29:25,222 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-13 21:29:25,224 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-07-13 21:29:25,224 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2
datanode_3          | 2023-07-13 21:29:25,227 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-LeaderElection2] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState
datanode_3          | 2023-07-13 21:29:29,119 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:29:29,121 [grpc-default-executor-1] INFO impl.VoteContext: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FOLLOWER: reject ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-13 21:29:29,122 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:30,340 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t2. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C:t2, leader=null, voted=6f624696-19ad-4034-8341-e1488a622757, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:30,450 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A8A350D9574C with new leaderId: 6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:30,450 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: change Leader from null to 6f624696-19ad-4034-8341-e1488a622757 at term 2 for appendEntries, leader elected after 6694ms
datanode_2          | 2023-07-13 21:29:30,479 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:30,480 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:29:30,482 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-A8A350D9574C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/current/log_inprogress_0
datanode_2          | 2023-07-13 21:29:34,175 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 2, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:34,176 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FOLLOWER: accept ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:29:34,176 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:34,176 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:34,176 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:29:34,177 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:34,182 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t2. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C:t2, leader=null, voted=6f624696-19ad-4034-8341-e1488a622757, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:39,255 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 3, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:39,256 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FOLLOWER: accept ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:29:39,256 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_2          | 2023-07-13 21:29:39,257 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:39,257 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:29:39,257 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:39,267 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t3. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C:t3, leader=null, voted=6f624696-19ad-4034-8341-e1488a622757, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:44,288 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: receive requestVote(ELECTION, f8c819d0-1ba4-46a8-98ef-94a593675c85, group-63455D05C85C, 4, (t:0, i:0))
datanode_2          | 2023-07-13 21:29:44,288 [grpc-default-executor-1] INFO impl.VoteContext: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FOLLOWER: accept ELECTION from f8c819d0-1ba4-46a8-98ef-94a593675c85: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:29:44,288 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_2          | 2023-07-13 21:29:44,288 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: shutdown 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:44,289 [grpc-default-executor-1] INFO impl.RoleInfo: 5233c3d5-13a9-4934-81e6-18d02ce266a2: start 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState
datanode_2          | 2023-07-13 21:29:44,289 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState] INFO impl.FollowerState: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:29:44,295 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C replies to ELECTION vote request: f8c819d0-1ba4-46a8-98ef-94a593675c85<-5233c3d5-13a9-4934-81e6-18d02ce266a2#0:OK-t4. Peer's state: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C:t4, leader=null, voted=f8c819d0-1ba4-46a8-98ef-94a593675c85, raftlog=5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:44,402 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-63455D05C85C with new leaderId: f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_2          | 2023-07-13 21:29:44,402 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: change Leader from null to f8c819d0-1ba4-46a8-98ef-94a593675c85 at term 4 for appendEntries, leader elected after 20181ms
datanode_2          | 2023-07-13 21:29:44,508 [grpc-default-executor-1] INFO server.RaftServer$Division: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:29:44,511 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:29:44,515 [5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5233c3d5-13a9-4934-81e6-18d02ce266a2@group-63455D05C85C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/current/log_inprogress_0
datanode_3          | 2023-07-13 21:29:29,122 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:29,122 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:29,122 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:29:29,130 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t1. Peer's state: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C:t1, leader=null, voted=null, raftlog=f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:30,332 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-A8A350D9574C, 2, (t:0, i:0))
datanode_3          | 2023-07-13 21:29:30,333 [grpc-default-executor-1] INFO impl.VoteContext: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FOLLOWER: accept ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:29:30,333 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:30,334 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState
datanode_3          | 2023-07-13 21:29:30,334 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState
datanode_3          | 2023-07-13 21:29:30,334 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:29:30,340 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:OK-t2. Peer's state: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C:t2, leader=null, voted=6f624696-19ad-4034-8341-e1488a622757, raftlog=f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:30,411 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A8A350D9574C with new leaderId: 6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:30,416 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: change Leader from null to 6f624696-19ad-4034-8341-e1488a622757 at term 2 for appendEntries, leader elected after 10670ms
datanode_3          | 2023-07-13 21:29:30,463 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:0, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:1, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-07-13 21:29:30,466 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:29:30,469 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-A8A350D9574C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1badc5b2-d9ca-4f72-a436-a8a350d9574c/current/log_inprogress_0
datanode_3          | 2023-07-13 21:29:34,192 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5069691754ns, electionTimeout:5066ms
datanode_3          | 2023-07-13 21:29:34,193 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:34,193 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-13 21:29:34,193 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:29:34,193 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection3
datanode_3          | 2023-07-13 21:29:34,204 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 2, (t:0, i:0))
datanode_3          | 2023-07-13 21:29:34,205 [grpc-default-executor-1] INFO impl.VoteContext: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-CANDIDATE: reject ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-13 21:29:34,205 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from CANDIDATE to FOLLOWER at term 2 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:34,205 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection3
datanode_3          | 2023-07-13 21:29:34,205 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:34,218 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection3] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection3: skip running since this is already CLOSING
datanode_3          | 2023-07-13 21:29:34,221 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t2. Peer's state: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C:t2, leader=null, voted=null, raftlog=f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:39,249 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: receive requestVote(ELECTION, 6f624696-19ad-4034-8341-e1488a622757, group-63455D05C85C, 3, (t:0, i:0))
datanode_3          | 2023-07-13 21:29:39,249 [grpc-default-executor-1] INFO impl.VoteContext: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FOLLOWER: reject ELECTION from 6f624696-19ad-4034-8341-e1488a622757: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-13 21:29:39,249 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:6f624696-19ad-4034-8341-e1488a622757
datanode_3          | 2023-07-13 21:29:39,249 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:39,249 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:29:39,250 [grpc-default-executor-1] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:39,259 [grpc-default-executor-1] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C replies to ELECTION vote request: 6f624696-19ad-4034-8341-e1488a622757<-f8c819d0-1ba4-46a8-98ef-94a593675c85#0:FAIL-t3. Peer's state: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C:t3, leader=null, voted=null, raftlog=f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:44,267 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.FollowerState: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5016238054ns, electionTimeout:5012ms
datanode_3          | 2023-07-13 21:29:44,267 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState
datanode_3          | 2023-07-13 21:29:44,267 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_3          | 2023-07-13 21:29:44,268 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:29:44,268 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-FollowerState] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4
datanode_3          | 2023-07-13 21:29:44,270 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4 ELECTION round 0: submit vote requests at term 4 for -1: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:29:44,294 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:29:44,298 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection:   Response 0: f8c819d0-1ba4-46a8-98ef-94a593675c85<-6f624696-19ad-4034-8341-e1488a622757#0:OK-t4
datanode_3          | 2023-07-13 21:29:44,302 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.LeaderElection: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4 ELECTION round 0: result PASSED
datanode_3          | 2023-07-13 21:29:44,306 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: shutdown f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4
datanode_3          | 2023-07-13 21:29:44,306 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
datanode_3          | 2023-07-13 21:29:44,306 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-63455D05C85C with new leaderId: f8c819d0-1ba4-46a8-98ef-94a593675c85
datanode_3          | 2023-07-13 21:29:44,311 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: change Leader from null to f8c819d0-1ba4-46a8-98ef-94a593675c85 at term 4 for becomeLeader, leader elected after 20198ms
datanode_3          | 2023-07-13 21:29:44,312 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:29:44,312 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:44,313 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:29:44,313 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-13 21:29:44,313 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:29:44,313 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:29:44,313 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:29:44,327 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-13 21:29:44,328 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:44,328 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-13 21:29:44,333 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-13 21:29:44,337 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:29:44,338 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:44,338 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C
datanode_3          | 2023-07-13 21:29:44,343 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-13 21:29:44,343 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:29:44,343 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-13 21:29:44,343 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-13 21:29:44,343 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:29:44,344 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:29:44,345 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO impl.RoleInfo: f8c819d0-1ba4-46a8-98ef-94a593675c85: start f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderStateImpl
datanode_3          | 2023-07-13 21:29:44,348 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:29:44,349 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/35d5fc08-3d2f-4fa6-813e-63455d05c85c/current/log_inprogress_0
datanode_3          | 2023-07-13 21:29:44,369 [f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C-LeaderElection4] INFO server.RaftServer$Division: f8c819d0-1ba4-46a8-98ef-94a593675c85@group-63455D05C85C: set configuration 0: [f8c819d0-1ba4-46a8-98ef-94a593675c85|rpc:172.21.0.2:9856|admin:172.21.0.2:9857|client:172.21.0.2:9858|dataStream:|priority:1, 6f624696-19ad-4034-8341-e1488a622757|rpc:172.21.0.3:9856|admin:172.21.0.3:9857|client:172.21.0.3:9858|dataStream:|priority:0, 5233c3d5-13a9-4934-81e6-18d02ce266a2|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0], old=null
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:28:45,223 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = e31280a8ebe8/172.21.0.13
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-07-13 21:28:45,317 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:28:55,487 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:28:55,917 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.13:9862
om_1                | 2023-07-13 21:28:55,917 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:28:55,918 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:28:55,976 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:28:59,498 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:00,499 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:01,500 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:02,501 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:03,502 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:04,529 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:05,530 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:06,532 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:07,564 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-13 21:29:08,572 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.9:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-9ed3531f-20c9-43ae-8879-8d9c665b69a6;layoutVersion=0
om_1                | 2023-07-13 21:29:14,622 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at e31280a8ebe8/172.21.0.13
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:29:18,142 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = e31280a8ebe8/172.21.0.13
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-07-13 21:29:18,162 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:29:22,684 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:29:22,904 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.13:9862
om_1                | 2023-07-13 21:29:22,904 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:29:22,905 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:29:22,932 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:29:23,009 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:29:25,423 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:29:25,580 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-13 21:29:25,581 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-13 21:29:25,734 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:29:25,738 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:29:25,738 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-13 21:29:25,744 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-13 21:29:25,771 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:29:25,798 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872
om_1                | 2023-07-13 21:29:25,806 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-13 21:29:25,828 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-13 21:29:25,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-07-13 21:29:25,890 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:29:25,890 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-07-13 21:29:25,891 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:29:25,891 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:29:25,892 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-13 21:29:25,893 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:29:25,894 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-13 21:29:25,895 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-13 21:29:26,122 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-13 21:29:26,124 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:29:26,124 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:29:26,137 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:29:26,144 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@7102ac3e[Not completed]
om_1                | 2023-07-13 21:29:26,144 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-13 21:29:26,234 [pool-17-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-13 21:29:26,244 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-13 21:29:26,251 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-13 21:29:26,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-13 21:29:26,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-13 21:29:26,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:29:26,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:29:26,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-13 21:29:26,262 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-13 21:29:26,276 [pool-17-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-13 21:29:26,278 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:29:26,286 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-13 21:29:26,290 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-13 21:29:26,291 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-13 21:29:26,340 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@e31280a8ebe8
om_1                | 2023-07-13 21:29:26,387 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-13 21:29:26,395 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-13 21:29:26,397 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:29:26,419 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-13 21:29:26,425 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:29:26,442 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-C5BA1605619E
om_1                | 2023-07-13 21:29:26,487 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:29:26,554 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-13 21:29:26,555 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-13 21:29:26,564 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-13 21:29:26,564 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:29:26,565 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:29:26,568 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:29:26,569 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-13 21:29:26,569 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-13 21:29:26,576 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-13 21:29:26,577 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-13 21:29:26,578 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-13 21:29:26,599 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-13 21:29:26,599 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:29:26,613 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:29:26,613 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:29:26,623 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:29:26,624 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-13 21:29:26,624 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:29:26,625 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-13 21:29:26,628 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-13 21:29:26,628 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-13 21:29:26,654 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-13 21:29:26,669 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-C5BA1605619E
om_1                | 2023-07-13 21:29:26,677 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-C5BA1605619E
om_1                | 2023-07-13 21:29:26,686 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:29:26,686 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-13 21:29:26,711 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.21.0.13:9862
om_1                | 2023-07-13 21:29:26,712 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-13 21:29:26,713 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-13 21:29:26,715 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-13 21:29:26,716 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:28:43,265 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 021569d64207/172.21.0.12
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.1.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.1.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | STARTUP_MSG:   java = 11.0.10
recon_1             | ************************************************************/
recon_1             | 2023-07-13 21:28:43,359 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-13 21:28:48,670 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-07-13 21:28:50,628 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-13 21:28:52,251 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:29:01,105 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:29:03,706 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:29:03,860 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:29:03,867 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-13 21:29:11,044 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-13 21:29:11,168 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:29:11,228 [main] INFO util.log: Logging initialized @32821ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:29:11,593 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-13 21:29:11,623 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-13 21:29:11,639 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:29:11,649 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:29:11,650 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:29:11,650 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-13 21:29:12,212 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-13 21:29:12,682 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-13 21:29:12,696 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-13 21:29:12,738 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-13 21:29:12,738 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-13 21:29:13,025 [main] INFO Configuration.deprecation: No unit for ozone.recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-07-13 21:29:13,548 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:29:13,781 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:29:13,824 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
recon_1             | 2023-07-13 21:29:13,831 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-13 21:29:13,994 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:29:14,121 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-13 21:29:14,146 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-13 21:29:14,150 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-13 21:29:14,208 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:29:14,252 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-13 21:29:14,380 [Listener at 0.0.0.0/9891] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1             | 2023-07-13 21:29:14,572 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-13 21:29:14,586 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-13 21:29:15,032 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:29:15,119 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:29:15,119 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-13 21:29:16,214 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-13 21:29:16,218 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1             | 2023-07-13 21:29:16,322 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-13 21:29:16,326 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-13 21:29:16,331 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-13 21:29:16,363 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@468e8565{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:29:16,367 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7edb6fca{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:29:21,585 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@652a1a17{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_1_0_jar-_-any-10097576997014317193/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/recon}
recon_1             | 2023-07-13 21:29:21,609 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1095d23a{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-13 21:29:21,612 [Listener at 0.0.0.0/9891] INFO server.Server: Started @43205ms
recon_1             | 2023-07-13 21:29:21,615 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-13 21:29:21,618 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-13 21:29:21,622 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-13 21:29:21,630 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-13 21:29:21,688 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-13 21:29:21,732 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:29:21,733 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-13 21:29:21,738 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:29:21,739 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-13 21:29:21,742 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:29:22,279 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-07-13 21:29:22,280 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:29:22,280 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=16fcb917-c37a-4fb6-9616-4d7a358ba103 from SCM.
recon_1             | 2023-07-13 21:29:22,311 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 16fcb917-c37a-4fb6-9616-4d7a358ba103, Nodes: 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.290Z]
recon_1             | 2023-07-13 21:29:22,321 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c from SCM.
recon_1             | 2023-07-13 21:29:22,327 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 35d5fc08-3d2f-4fa6-813e-63455d05c85c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.411Z]
recon_1             | 2023-07-13 21:29:22,337 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c from SCM.
recon_1             | 2023-07-13 21:29:22,338 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1badc5b2-d9ca-4f72-a436-a8a350d9574c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.364Z]
recon_1             | 2023-07-13 21:29:22,339 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e from SCM.
om_1                | 2023-07-13 21:29:26,717 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-13 21:29:26,719 [Listener at om/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-C5BA1605619E
om_1                | 2023-07-13 21:29:26,724 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-13 21:29:26,764 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-13 21:29:26,772 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$354/0x0000000840484440@34451ed8] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-13 21:29:26,812 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:29:26,813 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-13 21:29:26,838 [Listener at om/9862] INFO util.log: Logging initialized @11528ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:29:26,930 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-13 21:29:26,934 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:29:26,939 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:29:26,941 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-13 21:29:26,941 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-13 21:29:26,941 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:29:26,978 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-13 21:29:26,980 [Listener at om/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om_1                | 2023-07-13 21:29:27,007 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:29:27,012 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-13 21:29:27,013 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-13 21:29:27,061 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@195580ba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:29:27,063 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@70716259{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-13 21:29:27,309 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@474749b8{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_1_0_jar-_-any-1728374316099821102/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:29:27,318 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@70b2fa10{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-13 21:29:27,319 [Listener at om/9862] INFO server.Server: Started @12010ms
om_1                | 2023-07-13 21:29:27,324 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-13 21:29:27,324 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-13 21:29:27,326 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:29:27,327 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-13 21:29:27,331 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-13 21:29:27,346 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-07-13 21:29:27,354 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5b29ab61] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-13 21:29:31,833 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5117649002ns, electionTimeout:5105ms
om_1                | 2023-07-13 21:29:31,835 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:29:31,836 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:29:31,845 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-13 21:29:31,845 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:29:31,863 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-13 21:29:31,864 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-13 21:29:31,864 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:29:31,871 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-13 21:29:31,871 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5476ms
om_1                | 2023-07-13 21:29:31,883 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-13 21:29:31,891 [om1@group-C5BA1605619E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-C5BA1605619E
om_1                | 2023-07-13 21:29:31,904 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:29:31,915 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:29:31,924 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-13 21:29:31,924 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-13 21:29:31,929 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-13 21:29:31,953 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
recon_1             | 2023-07-13 21:29:22,347 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e, Nodes: 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:15.560Z]
recon_1             | 2023-07-13 21:29:22,348 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=e2cb8a73-a3d5-4676-b1f2-828a1e8e8935 from SCM.
recon_1             | 2023-07-13 21:29:22,348 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: e2cb8a73-a3d5-4676-b1f2-828a1e8e8935, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:16.154Z]
recon_1             | 2023-07-13 21:29:22,348 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:29:22,373 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-13 21:29:22,362 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:29:22,632 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-13 21:29:22,633 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-13 21:29:22,660 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.3:48858
recon_1             | 2023-07-13 21:29:22,720 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-07-13 21:29:22,743 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 80 milliseconds.
recon_1             | 2023-07-13 21:29:22,768 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-13 21:29:22,768 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-13 21:29:23,035 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 210 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:29:23,075 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 40 milliseconds for processing 0 containers.
recon_1             | 2023-07-13 21:29:23,396 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.4:35442: output error
recon_1             | 2023-07-13 21:29:23,400 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-13 21:29:23,406 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.2:54048: output error
recon_1             | 2023-07-13 21:29:23,407 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-13 21:29:23,778 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5233c3d5-13a9-4934-81e6-18d02ce266a2
recon_1             | 2023-07-13 21:29:23,795 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:23,831 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5233c3d5-13a9-4934-81e6-18d02ce266a2 to Node DB.
recon_1             | 2023-07-13 21:29:23,849 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om_1                | 2023-07-13 21:29:32,028 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-13 21:29:32,133 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-07-13 21:29:32,230 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-13 21:29:35,301 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-13 21:30:22,232 [qtp101676975-39] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-13 21:30:22,253 [qtp101676975-39] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689283822235 in 17 milliseconds
om_1                | 2023-07-13 21:30:22,300 [qtp101676975-39] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 46 milliseconds
om_1                | 2023-07-13 21:30:22,301 [qtp101676975-39] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689283822235
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:28:44,930 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 7037fefa8234/172.21.0.9
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:28:45,045 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:28:45,650 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:28:45,901 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-9ed3531f-20c9-43ae-8879-8d9c665b69a6;layoutVersion=0
scm_1               | 2023-07-13 21:28:45,953 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 7037fefa8234/172.21.0.9
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:28:54,889 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 7037fefa8234/172.21.0.9
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.1.0
recon_1             | 2023-07-13 21:29:23,868 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:23,870 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e, Nodes: 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:5233c3d5-13a9-4934-81e6-18d02ce266a2, CreationTimestamp2023-07-13T21:29:15.560Z] moved to OPEN state
recon_1             | 2023-07-13 21:29:23,915 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/6f624696-19ad-4034-8341-e1488a622757
recon_1             | 2023-07-13 21:29:23,915 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:23,916 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 6f624696-19ad-4034-8341-e1488a622757 to Node DB.
recon_1             | 2023-07-13 21:29:23,916 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=16fcb917-c37a-4fb6-9616-4d7a358ba103 reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:23,918 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 16fcb917-c37a-4fb6-9616-4d7a358ba103, Nodes: 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6f624696-19ad-4034-8341-e1488a622757, CreationTimestamp2023-07-13T21:29:17.290Z] moved to OPEN state
recon_1             | 2023-07-13 21:29:23,919 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:23,919 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,053 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,109 [IPC Server handler 10 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f8c819d0-1ba4-46a8-98ef-94a593675c85
recon_1             | 2023-07-13 21:29:24,110 [IPC Server handler 10 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,111 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,112 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,111 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f8c819d0-1ba4-46a8-98ef-94a593675c85 to Node DB.
recon_1             | 2023-07-13 21:29:24,113 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=e2cb8a73-a3d5-4676-b1f2-828a1e8e8935 reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,114 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e2cb8a73-a3d5-4676-b1f2-828a1e8e8935, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:f8c819d0-1ba4-46a8-98ef-94a593675c85, CreationTimestamp2023-07-13T21:29:16.154Z] moved to OPEN state
recon_1             | 2023-07-13 21:29:24,239 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,239 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:24,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:26,177 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:26,177 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:30,364 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:30,364 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:30,365 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1badc5b2-d9ca-4f72-a436-a8a350d9574c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6f624696-19ad-4034-8341-e1488a622757, CreationTimestamp2023-07-13T21:29:17.364Z] moved to OPEN state
recon_1             | 2023-07-13 21:29:37,748 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:37,751 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:29:37,813 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:29:38,000 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:38,136 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:44,311 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c reported by f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:29:44,314 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 35d5fc08-3d2f-4fa6-813e-63455d05c85c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:f8c819d0-1ba4-46a8-98ef-94a593675c85, CreationTimestamp2023-07-13T21:29:17.411Z] moved to OPEN state
recon_1             | 2023-07-13 21:29:48,583 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:29:48,593 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-13 21:29:58,969 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:29:58,976 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-07-13 21:30:21,740 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-13 21:30:21,741 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-13 21:30:22,363 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689283821741
recon_1             | 2023-07-13 21:30:22,399 [pool-14-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:30:22,400 [pool-14-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:30:22,574 [pool-14-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689283821741.
recon_1             | 2023-07-13 21:30:22,650 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-13 21:30:22,949 [pool-15-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-13 21:30:22,955 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:30:22,993 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1689283822963
recon_1             | 2023-07-13 21:30:22,994 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container key DB at /data/metadata/recon/recon-container-key.db_1689283731069.
recon_1             | 2023-07-13 21:30:23,114 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:30:23,115 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.159 seconds to process 4 keys.
recon_1             | 2023-07-13 21:30:23,177 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-13 21:30:23,224 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:28:54,981 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:28:55,901 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:28:56,855 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:28:57,621 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
scm_1               | 2023-07-13 21:28:57,651 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-13 21:28:58,488 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:28:59,756 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-13 21:28:59,993 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-13 21:29:00,018 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1               | 2023-07-13 21:29:00,123 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-13 21:29:00,507 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:29:00,511 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:29:06,689 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:29:07,103 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-13 21:29:07,644 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:29:07,670 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-13 21:29:07,807 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:29:07,834 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:29:08,063 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:29:09,547 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-13 21:29:09,812 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-13 21:29:09,812 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-13 21:29:12,031 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:29:12,070 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:29:12,083 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-13 21:29:12,545 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:29:12,626 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:29:12,634 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:29:12,638 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-13 21:29:13,088 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:29:13,098 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:29:13,103 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:29:13,103 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:29:13,679 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6014a9ba] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-13 21:29:13,742 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-13 21:29:13,746 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-13 21:29:13,935 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @27135ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:29:15,249 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-13 21:29:15,410 [IPC Server handler 44 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5233c3d5-13a9-4934-81e6-18d02ce266a2
scm_1               | 2023-07-13 21:29:15,421 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:29:15,447 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:29:15,457 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:29:15,460 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:29:15,461 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-13 21:28:46,249 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:28:46,250 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-13 21:28:46,362 [main] INFO util.log: Logging initialized @8091ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-13 21:28:46,972 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-13 21:28:47,120 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-13 21:28:47,137 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-13 21:28:47,175 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-13 21:28:47,191 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-13 21:28:47,191 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-13 21:28:48,061 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = df818e80bd4e/172.21.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | 2023-07-13 21:29:15,462 [IPC Server handler 44 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:29:15,501 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:29:15,511 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:29:15,541 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:15,562 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e to datanode:5233c3d5-13a9-4934-81e6-18d02ce266a2
scm_1               | 2023-07-13 21:29:15,631 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e, Nodes: 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:15.560681Z]
scm_1               | 2023-07-13 21:29:15,759 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-13 21:29:15,771 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm_1               | 2023-07-13 21:29:15,977 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:29:15,980 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:29:15,986 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-13 21:29:16,117 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f8c819d0-1ba4-46a8-98ef-94a593675c85
scm_1               | 2023-07-13 21:29:16,146 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:29:16,146 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:29:16,147 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:16,147 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:29:16,155 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e2cb8a73-a3d5-4676-b1f2-828a1e8e8935 to datanode:f8c819d0-1ba4-46a8-98ef-94a593675c85
scm_1               | 2023-07-13 21:29:16,155 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: e2cb8a73-a3d5-4676-b1f2-828a1e8e8935, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:16.154960Z]
scm_1               | 2023-07-13 21:29:16,164 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f49d153{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:29:16,167 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6724cdec{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:29:17,280 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/6f624696-19ad-4034-8341-e1488a622757
scm_1               | 2023-07-13 21:29:17,286 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:29:17,309 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:29:17,309 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:29:17,309 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:17,310 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:29:17,314 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:29:17,290 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=16fcb917-c37a-4fb6-9616-4d7a358ba103 to datanode:6f624696-19ad-4034-8341-e1488a622757
scm_1               | 2023-07-13 21:29:17,315 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 16fcb917-c37a-4fb6-9616-4d7a358ba103, Nodes: 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.290823Z]
scm_1               | 2023-07-13 21:29:17,364 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c to datanode:f8c819d0-1ba4-46a8-98ef-94a593675c85
scm_1               | 2023-07-13 21:29:17,364 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c to datanode:6f624696-19ad-4034-8341-e1488a622757
scm_1               | 2023-07-13 21:29:17,369 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c to datanode:5233c3d5-13a9-4934-81e6-18d02ce266a2
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
s3g_1               | STARTUP_MSG:   java = 11.0.10
s3g_1               | ************************************************************/
s3g_1               | 2023-07-13 21:28:48,188 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-13 21:28:48,492 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-13 21:28:48,546 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-13 21:28:48,564 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1               | 2023-07-13 21:28:48,769 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:28:48,769 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-13 21:28:48,801 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-07-13 21:28:48,917 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@19b843ba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-13 21:28:48,943 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b0d80ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 13, 2023 9:29:11 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-13 21:29:11,374 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7fe8c7db{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_1_0_jar-_-any-14817831329169781353/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/s3gateway}
s3g_1               | 2023-07-13 21:29:11,442 [main] INFO server.AbstractConnector: Started ServerConnector@51c693d{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:29:11,442 [main] INFO server.Server: Started @33171ms
s3g_1               | 2023-07-13 21:29:11,444 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | 2023-07-13 21:29:17,373 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1badc5b2-d9ca-4f72-a436-a8a350d9574c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.364352Z]
scm_1               | 2023-07-13 21:29:17,411 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c to datanode:f8c819d0-1ba4-46a8-98ef-94a593675c85
scm_1               | 2023-07-13 21:29:17,411 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c to datanode:6f624696-19ad-4034-8341-e1488a622757
scm_1               | 2023-07-13 21:29:17,412 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c to datanode:5233c3d5-13a9-4934-81e6-18d02ce266a2
scm_1               | 2023-07-13 21:29:17,412 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 35d5fc08-3d2f-4fa6-813e-63455d05c85c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:29:17.411355Z]
scm_1               | 2023-07-13 21:29:17,413 [RatisPipelineUtilsThread] INFO pipeline.SCMPipelineManager: Pipeline: PipelineID=35d5fc08-3d2f-4fa6-813e-63455d05c85c contains same datanodes as previous pipelines: PipelineID=1badc5b2-d9ca-4f72-a436-a8a350d9574c nodeIds: f8c819d0-1ba4-46a8-98ef-94a593675c85, 6f624696-19ad-4034-8341-e1488a622757, 5233c3d5-13a9-4934-81e6-18d02ce266a2
scm_1               | 2023-07-13 21:29:17,703 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@663bb8ef{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_1_0_jar-_-any-999661163788494386/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/scm}
scm_1               | 2023-07-13 21:29:17,762 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@623dcf2a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:29:17,765 [Listener at 0.0.0.0/9860] INFO server.Server: Started @30978ms
scm_1               | 2023-07-13 21:29:17,794 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-13 21:29:17,795 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-13 21:29:17,797 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:29:23,430 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:23,431 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 16fcb917-c37a-4fb6-9616-4d7a358ba103, Nodes: 6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6f624696-19ad-4034-8341-e1488a622757, CreationTimestamp2023-07-13T21:29:17.290823Z] moved to OPEN state
scm_1               | 2023-07-13 21:29:23,454 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:23,464 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6ae7a9c9-5c2b-4f9f-9a08-b154e0209f6e, Nodes: 5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:5233c3d5-13a9-4934-81e6-18d02ce266a2, CreationTimestamp2023-07-13T21:29:15.560681Z] moved to OPEN state
scm_1               | 2023-07-13 21:29:23,470 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:23,476 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:23,496 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:23,496 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e2cb8a73-a3d5-4676-b1f2-828a1e8e8935, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:f8c819d0-1ba4-46a8-98ef-94a593675c85, CreationTimestamp2023-07-13T21:29:16.154960Z] moved to OPEN state
scm_1               | 2023-07-13 21:29:23,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:23,799 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:23,799 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:23,923 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:23,923 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:24,069 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:24,070 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:24,112 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:24,112 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:24,229 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:24,230 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:24,757 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:24,757 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:26,179 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:26,180 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:30,355 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:29:30,355 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1badc5b2-d9ca-4f72-a436-a8a350d9574c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6f624696-19ad-4034-8341-e1488a622757, CreationTimestamp2023-07-13T21:29:17.364352Z] moved to OPEN state
scm_1               | 2023-07-13 21:29:30,356 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:30,356 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:29:30,356 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:29:30,356 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-13 21:29:30,356 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-13 21:29:44,313 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 35d5fc08-3d2f-4fa6-813e-63455d05c85c, Nodes: f8c819d0-1ba4-46a8-98ef-94a593675c85{ip: 172.21.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}6f624696-19ad-4034-8341-e1488a622757{ip: 172.21.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5233c3d5-13a9-4934-81e6-18d02ce266a2{ip: 172.21.0.4, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:f8c819d0-1ba4-46a8-98ef-94a593675c85, CreationTimestamp2023-07-13T21:29:17.411355Z] moved to OPEN state
scm_1               | 2023-07-13 21:30:07,460 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-13 21:30:17,964 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-13 21:31:07,274 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-13 21:31:17,407 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
Attaching to xcompat_recon_1, xcompat_scm_1, xcompat_old_client_1_2_1_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_s3g_1, xcompat_datanode_3, xcompat_datanode_1, xcompat_datanode_2, xcompat_old_client_1_0_0_1, xcompat_old_client_1_1_0_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:31:39,583 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = fc89a7ef9913/172.22.0.3
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.2.1
datanode_1          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_1          | STARTUP_MSG:   java = 11.0.13
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:31:39,622 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:31:41,829 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:31:42,381 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:31:43,150 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:31:43,151 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:31:44,540 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:fc89a7ef9913 ip:172.22.0.3
datanode_1          | 2023-07-13 21:31:46,393 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_1          | 2023-07-13 21:31:47,232 [main] INFO reflections.Reflections: Reflections took 656 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_1          | 2023-07-13 21:31:49,138 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:31:49,164 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-13 21:31:49,251 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:31:49,252 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:31:49,538 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:31:49,646 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:31:49,652 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-13 21:31:49,656 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-13 21:31:49,657 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-13 21:31:49,677 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-13 21:31:49,791 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:31:49,791 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:32:00,446 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:32:01,257 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:32:02,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-13 21:32:02,171 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-13 21:32:02,171 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-13 21:32:02,172 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:32:02,172 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:02,208 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:32:02,221 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:32:04,437 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-13 21:32:04,446 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:04,455 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:32:04,543 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:32:05,941 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:32:06,260 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:32:06,447 [main] INFO util.log: Logging initialized @34345ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:32:07,274 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-13 21:32:07,339 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:32:07,402 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:32:07,440 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:32:07,487 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:32:07,494 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:32:08,115 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-13 21:32:08,117 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_1          | 2023-07-13 21:32:08,396 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:32:08,434 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:32:08,436 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-13 21:32:08,622 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@59fc6d05{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:32:08,624 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@b791a81{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:32:10,961 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8fd91d1{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-4179760048186924222/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:32:11,080 [main] INFO server.AbstractConnector: Started ServerConnector@7126e26{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:32:11,090 [main] INFO server.Server: Started @38988ms
datanode_1          | 2023-07-13 21:32:11,132 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:32:11,133 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-13 21:32:11,146 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:32:11,174 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-13 21:32:11,365 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@42bba2f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-13 21:32:12,002 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.13:9891
datanode_1          | 2023-07-13 21:32:12,516 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:32:15,002 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:32:16,003 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:32:17,004 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:32:18,770 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-13 21:32:18,819 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-13 21:32:19,322 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:19,513 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.RaftServer: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start RPC server
datanode_1          | 2023-07-13 21:32:19,539 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: GrpcService started, listening on 9856
datanode_1          | 2023-07-13 21:32:19,550 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: GrpcService started, listening on 9857
datanode_1          | 2023-07-13 21:32:19,564 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: GrpcService started, listening on 9858
datanode_1          | 2023-07-13 21:32:19,590 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:32:19,590 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-13 21:32:19,590 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@1aad8114] INFO util.JvmPauseMonitor: JvmPauseMonitor-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: Started
datanode_1          | 2023-07-13 21:32:19,590 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:32:21,394 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-13 21:32:22,016 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From fc89a7ef9913/172.22.0.3 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.3:57140 remote=recon/172.22.0.13:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:31:40,988 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = a6c0fdf26715/172.22.0.6
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.2.1
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.3:57140 remote=recon/172.22.0.13:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-07-13 21:32:23,396 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-13 21:32:24,479 [Command processor thread] INFO server.RaftServer: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: addNew group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] returns group-96AE05DF0362:java.util.concurrent.CompletableFuture@b28c7bd[Not completed]
datanode_1          | 2023-07-13 21:32:24,627 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: new RaftServerImpl for group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:32:24,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:32:24,632 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:32:24,636 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:32:24,636 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:24,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:32:24,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:32:24,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:32:24,651 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:32:24,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:32:24,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:32:24,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:32:24,683 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 does not exist. Creating ...
datanode_1          | 2023-07-13 21:32:24,729 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/in_use.lock acquired by nodename 7@fc89a7ef9913
datanode_1          | 2023-07-13 21:32:24,752 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 has been successfully formatted.
datanode_2          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_2          | STARTUP_MSG:   java = 11.0.13
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:31:41,120 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:31:42,911 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:31:43,637 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:31:44,485 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:31:44,485 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:31:45,345 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:a6c0fdf26715 ip:172.22.0.6
datanode_2          | 2023-07-13 21:31:47,241 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_2          | 2023-07-13 21:31:48,381 [main] INFO reflections.Reflections: Reflections took 978 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_2          | 2023-07-13 21:31:50,293 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:31:50,337 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-13 21:31:50,348 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:31:50,396 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:31:50,770 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:31:51,057 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:31:51,067 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-13 21:31:51,095 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-13 21:31:51,124 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-13 21:31:51,126 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-13 21:31:51,321 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:31:51,322 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:32:01,024 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:32:01,828 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:32:03,330 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-13 21:32:03,347 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-13 21:32:03,387 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-13 21:32:03,395 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:32:03,396 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:32:03,396 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:32:03,431 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:32:05,925 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:32:05,943 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:05,949 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:32:06,014 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:32:07,119 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:32:07,395 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:32:07,802 [main] INFO util.log: Logging initialized @33864ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:32:08,925 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-13 21:32:08,957 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:32:09,034 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:32:09,043 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:32:09,053 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:32:09,058 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:32:09,500 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-13 21:32:09,529 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_2          | 2023-07-13 21:32:09,922 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:32:09,922 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:32:09,985 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-07-13 21:32:10,233 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e3df614{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:32:10,279 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b357eb6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:32:12,985 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@71560f51{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-14282060235396146329/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:32:24,783 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-96AE05DF0362: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:32:24,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:24,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:32:24,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:32:24,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:24,918 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:24,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:32:24,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:32:25,068 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_1          | 2023-07-13 21:32:25,068 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:32:25,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:25,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:25,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:32:25,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:32:25,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:32:25,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:32:25,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:32:25,167 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:25,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:25,200 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:25,202 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:25,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:25,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:32:25,229 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:32:25,231 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:32:25,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:32:25,244 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:32:25,398 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:25,412 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:32:25,421 [pool-22-thread-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:25,441 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96AE05DF0362,id=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:25,552 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_1          | 2023-07-13 21:32:28,178 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362.
datanode_1          | 2023-07-13 21:32:28,179 [Command processor thread] INFO server.RaftServer: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: addNew group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] returns group-626633BA49DF:java.util.concurrent.CompletableFuture@a62c490[Not completed]
datanode_1          | 2023-07-13 21:32:28,181 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: new RaftServerImpl for group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:32:28,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:32:28,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:32:28,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:32:28,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:28,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:32:28,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:32:28,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:32:13,058 [main] INFO server.AbstractConnector: Started ServerConnector@1c240cf2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:32:13,062 [main] INFO server.Server: Started @39120ms
datanode_2          | 2023-07-13 21:32:13,070 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:32:13,070 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:32:13,074 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:32:13,090 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-13 21:32:13,242 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5718e035] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-13 21:32:13,881 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.13:9891
datanode_2          | 2023-07-13 21:32:14,326 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:32:16,656 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:32:17,657 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:32:18,787 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-13 21:32:18,791 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-13 21:32:19,102 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:32:19,230 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.RaftServer: c89e3854-e92b-4097-a8a6-7827012e1c78: start RPC server
datanode_2          | 2023-07-13 21:32:19,240 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: c89e3854-e92b-4097-a8a6-7827012e1c78: GrpcService started, listening on 9856
datanode_2          | 2023-07-13 21:32:19,256 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: c89e3854-e92b-4097-a8a6-7827012e1c78: GrpcService started, listening on 9857
datanode_2          | 2023-07-13 21:32:19,258 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: c89e3854-e92b-4097-a8a6-7827012e1c78: GrpcService started, listening on 9858
datanode_2          | 2023-07-13 21:32:19,290 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c89e3854-e92b-4097-a8a6-7827012e1c78 is started using port 9858 for RATIS
datanode_2          | 2023-07-13 21:32:19,291 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c89e3854-e92b-4097-a8a6-7827012e1c78 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-13 21:32:19,291 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c89e3854-e92b-4097-a8a6-7827012e1c78 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-13 21:32:19,301 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@70037d08] INFO util.JvmPauseMonitor: JvmPauseMonitor-c89e3854-e92b-4097-a8a6-7827012e1c78: Started
datanode_2          | 2023-07-13 21:32:22,666 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From a6c0fdf26715/172.22.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.6:59670 remote=recon/172.22.0.13:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.6:59670 remote=recon/172.22.0.13:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-07-13 21:32:28,190 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:32:28,192 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:32:28,193 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:32:28,193 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:32:28,193 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df does not exist. Creating ...
datanode_1          | 2023-07-13 21:32:28,195 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/in_use.lock acquired by nodename 7@fc89a7ef9913
datanode_1          | 2023-07-13 21:32:28,214 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df has been successfully formatted.
datanode_1          | 2023-07-13 21:32:28,215 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:28,231 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-626633BA49DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:32:28,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:28,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:32:28,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:32:28,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:28,239 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,239 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:32:28,239 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:32:28,240 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_1          | 2023-07-13 21:32:28,240 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:32:28,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:28,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:32:28,244 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:32:28,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:32:28,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:32:28,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:32:28,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:28,272 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:28,274 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:28,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:28,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:32:28,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:32:28,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:32:28,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:32:28,296 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:32:28,297 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:32:28,301 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:32:28,301 [pool-22-thread-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState
datanode_1          | 2023-07-13 21:32:28,318 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-626633BA49DF,id=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:28,324 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_1          | 2023-07-13 21:32:28,605 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df.
datanode_1          | 2023-07-13 21:32:28,622 [Command processor thread] INFO server.RaftServer: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: addNew group-8794A2F79000:[7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1] returns group-8794A2F79000:java.util.concurrent.CompletableFuture@7546913e[Not completed]
datanode_1          | 2023-07-13 21:32:28,624 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: new RaftServerImpl for group-8794A2F79000:[7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:32:28,634 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:32:28,635 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:32:24,315 [Command processor thread] INFO server.RaftServer: c89e3854-e92b-4097-a8a6-7827012e1c78: addNew group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] returns group-96AE05DF0362:java.util.concurrent.CompletableFuture@7b642f84[Not completed]
datanode_2          | 2023-07-13 21:32:24,376 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78: new RaftServerImpl for group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:32:24,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:32:24,390 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:32:24,390 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:32:24,392 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:24,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:32:24,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:32:24,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:32:24,425 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:32:24,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:32:24,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:32:24,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:32:24,455 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 does not exist. Creating ...
datanode_2          | 2023-07-13 21:32:24,528 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/in_use.lock acquired by nodename 7@a6c0fdf26715
datanode_2          | 2023-07-13 21:32:24,587 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 has been successfully formatted.
datanode_2          | 2023-07-13 21:32:24,635 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-96AE05DF0362: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:32:24,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:24,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:32:24,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:32:24,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:32:24,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:24,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:32:24,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:32:24,911 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_2          | 2023-07-13 21:32:24,911 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:32:24,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:32:24,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:24,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:32:24,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:32:24,919 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:32:24,919 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:32:24,920 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:32:24,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:24,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:25,034 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:25,034 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:25,050 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:25,059 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:32:25,060 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:32:25,060 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:32:25,062 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:32:25,062 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:32:28,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: ConfigurationManager, init=-1: [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:32:28,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:32:28,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:32:28,646 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:32:28,647 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c5c065b2-d64a-490f-952f-8794a2f79000 does not exist. Creating ...
datanode_1          | 2023-07-13 21:32:28,655 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c5c065b2-d64a-490f-952f-8794a2f79000/in_use.lock acquired by nodename 7@fc89a7ef9913
datanode_1          | 2023-07-13 21:32:28,658 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c5c065b2-d64a-490f-952f-8794a2f79000 has been successfully formatted.
datanode_1          | 2023-07-13 21:32:28,659 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:28,661 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8794A2F79000: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:32:28,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:32:28,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:32:28,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:32:28,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:28,672 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,674 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:32:28,674 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:32:28,675 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c5c065b2-d64a-490f-952f-8794a2f79000
datanode_1          | 2023-07-13 21:32:28,675 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-13 21:32:28,686 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:32:28,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:32:28,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:32:28,692 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:28,695 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:28,695 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:32:28,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:32:28,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:32:28,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:32:28,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:32:28,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:32:28,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:32:28,712 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: start as a follower, conf=-1: [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:28,714 [pool-22-thread-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:32:28,715 [pool-22-thread-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState
datanode_1          | 2023-07-13 21:32:28,715 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8794A2F79000,id=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:28,726 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c5c065b2-d64a-490f-952f-8794a2f79000
datanode_1          | 2023-07-13 21:32:28,727 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c5c065b2-d64a-490f-952f-8794a2f79000.
datanode_1          | 2023-07-13 21:32:29,860 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:25,251 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:25,256 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:32:25,258 [pool-22-thread-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:25,285 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96AE05DF0362,id=c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:32:25,348 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_2          | 2023-07-13 21:32:28,037 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362.
datanode_2          | 2023-07-13 21:32:28,037 [Command processor thread] INFO server.RaftServer: c89e3854-e92b-4097-a8a6-7827012e1c78: addNew group-2CE2CEC903C0:[c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] returns group-2CE2CEC903C0:java.util.concurrent.CompletableFuture@1ce8dfff[Not completed]
datanode_2          | 2023-07-13 21:32:28,039 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78: new RaftServerImpl for group-2CE2CEC903C0:[c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:32:28,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:32:28,048 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: ConfigurationManager, init=-1: [c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:32:28,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:32:28,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:32:28,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:32:28,050 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/709eec37-995b-41fa-927a-2ce2cec903c0 does not exist. Creating ...
datanode_2          | 2023-07-13 21:32:28,077 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/709eec37-995b-41fa-927a-2ce2cec903c0/in_use.lock acquired by nodename 7@a6c0fdf26715
datanode_2          | 2023-07-13 21:32:28,081 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/709eec37-995b-41fa-927a-2ce2cec903c0 has been successfully formatted.
datanode_2          | 2023-07-13 21:32:28,086 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:32:28,088 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2CE2CEC903C0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:32:28,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:28,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:32:28,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:32:28,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:32:28,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:32:28,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:32:28,096 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/709eec37-995b-41fa-927a-2ce2cec903c0
datanode_2          | 2023-07-13 21:32:28,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:32:28,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:32:28,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:32:28,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:32:28,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:32:28,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:32:28,105 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:32:28,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:28,159 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:28,166 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:28,219 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:28,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:32:28,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:32:28,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:32:28,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:32:28,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:32:28,229 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: start as a follower, conf=-1: [c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:28,230 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:32:28,233 [pool-22-thread-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState
datanode_2          | 2023-07-13 21:32:28,240 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2CE2CEC903C0,id=c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:32:28,250 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=709eec37-995b-41fa-927a-2ce2cec903c0
datanode_2          | 2023-07-13 21:32:28,250 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=709eec37-995b-41fa-927a-2ce2cec903c0.
datanode_2          | 2023-07-13 21:32:28,251 [Command processor thread] INFO server.RaftServer: c89e3854-e92b-4097-a8a6-7827012e1c78: addNew group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] returns group-626633BA49DF:java.util.concurrent.CompletableFuture@3316af08[Not completed]
datanode_2          | 2023-07-13 21:32:28,255 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78: new RaftServerImpl for group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:32:28,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:32:28,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:32:28,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:32:28,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:28,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:32:28,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:32:28,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:32:28,264 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:32:28,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:32:28,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:32:28,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:32:28,269 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df does not exist. Creating ...
datanode_2          | 2023-07-13 21:32:28,274 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/in_use.lock acquired by nodename 7@a6c0fdf26715
datanode_2          | 2023-07-13 21:32:28,276 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df has been successfully formatted.
datanode_2          | 2023-07-13 21:32:28,277 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-626633BA49DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:32:28,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:32:28,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:32:28,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:32:28,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:32:28,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,284 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:32:28,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:32:28,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:32:28,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:32:29,863 [grpc-default-executor-0] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FOLLOWER: accept ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 0 <= candidate's priority 0
datanode_1          | 2023-07-13 21:32:29,863 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_1          | 2023-07-13 21:32:29,863 [grpc-default-executor-0] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:29,864 [grpc-default-executor-0] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:29,864 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:32:29,892 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t1. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362:t1, leader=null, voted=674ff7fc-a520-4090-839e-1ff0a8b3a1f6, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:33,343 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-626633BA49DF, 1, (t:0, i:0))
datanode_1          | 2023-07-13 21:32:33,343 [grpc-default-executor-0] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FOLLOWER: reject ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 1 > candidate's priority 0
datanode_1          | 2023-07-13 21:32:33,344 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_1          | 2023-07-13 21:32:33,344 [grpc-default-executor-0] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState
datanode_1          | 2023-07-13 21:32:33,344 [grpc-default-executor-0] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState
datanode_1          | 2023-07-13 21:32:33,344 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:32:33,352 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:FAIL-t1. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF:t1, leader=null, voted=null, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_1          | 2023-07-13 21:32:33,857 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5142401554ns, electionTimeout:5137ms
datanode_1          | 2023-07-13 21:32:33,857 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState
datanode_1          | 2023-07-13 21:32:33,858 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:32:33,860 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:33,860 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1
datanode_1          | 2023-07-13 21:32:33,866 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:33,867 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-13 21:32:33,868 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1
datanode_1          | 2023-07-13 21:32:33,868 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:32:33,869 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8794A2F79000 with new leaderId: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:33,869 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:33,871 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: change Leader from null to 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at term 1 for becomeLeader, leader elected after 5197ms
datanode_1          | 2023-07-13 21:32:33,889 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:32:33,893 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:33,894 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:32:33,899 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:32:33,899 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:32:33,900 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:32:33,908 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:33,912 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:32:33,918 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderStateImpl
datanode_1          | 2023-07-13 21:32:33,937 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:32:33,951 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-LeaderElection1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000: set configuration 0: [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-13 21:32:34,002 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-8794A2F79000-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c5c065b2-d64a-490f-952f-8794a2f79000/current/log_inprogress_0
datanode_1          | 2023-07-13 21:32:34,889 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5013109237ns, electionTimeout:5012ms
datanode_1          | 2023-07-13 21:32:34,889 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:34,889 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-07-13 21:32:34,890 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:34,890 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2
datanode_1          | 2023-07-13 21:32:34,892 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:34,937 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:32:34,937 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t2
datanode_1          | 2023-07-13 21:32:34,937 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:32:34,937 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-07-13 21:32:34,938 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2
datanode_1          | 2023-07-13 21:32:34,938 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:38,427 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5082692350ns, electionTimeout:5074ms
datanode_1          | 2023-07-13 21:32:38,427 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState
datanode_1          | 2023-07-13 21:32:38,428 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-07-13 21:32:38,428 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:38,428 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3
datanode_1          | 2023-07-13 21:32:38,433 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:31:41,436 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 90b15e73a5cc/172.22.0.7
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.2.1
datanode_2          | 2023-07-13 21:32:28,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:32:28,310 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:32:28,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:32:28,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:28,346 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:28,346 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:32:28,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:32:28,360 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:32:28,360 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:32:28,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:32:28,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:32:28,374 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:32:28,378 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_2          | 2023-07-13 21:32:28,378 [pool-22-thread-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:32:28,378 [pool-22-thread-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState
datanode_2          | 2023-07-13 21:32:28,395 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-626633BA49DF,id=c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:32:28,437 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_2          | 2023-07-13 21:32:28,629 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df.
datanode_2          | 2023-07-13 21:32:29,884 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:29,886 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:29,887 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_2          | 2023-07-13 21:32:29,890 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:29,890 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:29,891 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:29,906 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t1. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t1, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:33,349 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-626633BA49DF, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:33,349 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FOLLOWER: accept ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:32:33,350 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_2          | 2023-07-13 21:32:33,350 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState
datanode_2          | 2023-07-13 21:32:33,350 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState
datanode_2          | 2023-07-13 21:32:33,350 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection:   Response 0: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t2
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_1          | 2023-07-13 21:32:38,448 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-626633BA49DF with new leaderId: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_1          | 2023-07-13 21:32:38,449 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: change Leader from null to 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at term 2 for becomeLeader, leader elected after 10213ms
datanode_1          | 2023-07-13 21:32:38,450 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:32:38,449 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:38,451 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:38,458 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:32:38,458 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:32:38,458 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:32:38,458 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:32:38,458 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:32:38,459 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:32:38,478 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:32:38,479 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:38,479 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:32:38,482 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:32:38,482 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:32:38,482 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:32:38,486 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:32:38,487 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:32:38,487 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:32:38,487 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:32:38,487 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:32:38,487 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:32:38,489 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderStateImpl
datanode_1          | 2023-07-13 21:32:38,489 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:32:38,497 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-LeaderElection3] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-13 21:32:38,498 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-626633BA49DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/current/log_inprogress_0
datanode_1          | 2023-07-13 21:32:40,100 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5161908759ns, electionTimeout:5150ms
datanode_1          | 2023-07-13 21:32:40,100 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:40,100 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-07-13 21:32:40,100 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:40,100 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4
datanode_1          | 2023-07-13 21:32:40,107 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:40,144 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:32:40,145 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection:   Response 0: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t3
datanode_1          | 2023-07-13 21:32:40,146 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection:   Response 1: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t3
datanode_1          | 2023-07-13 21:32:40,146 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:32:40,147 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_1          | 2023-07-13 21:32:40,147 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4
datanode_1          | 2023-07-13 21:32:40,148 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection4] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:40,174 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: receive requestVote(ELECTION, c89e3854-e92b-4097-a8a6-7827012e1c78, group-96AE05DF0362, 3, (t:0, i:0))
datanode_1          | 2023-07-13 21:32:40,174 [grpc-default-executor-1] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FOLLOWER: reject ELECTION from c89e3854-e92b-4097-a8a6-7827012e1c78: already has voted for 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at current term 3
datanode_1          | 2023-07-13 21:32:40,175 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362 replies to ELECTION vote request: c89e3854-e92b-4097-a8a6-7827012e1c78<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:FAIL-t3. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362:t3, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:44,542 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:45,261 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5113707565ns, electionTimeout:5088ms
datanode_1          | 2023-07-13 21:32:45,262 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:45,262 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_1          | 2023-07-13 21:32:45,262 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:45,262 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5
datanode_1          | 2023-07-13 21:32:45,265 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5 ELECTION round 0: submit vote requests at term 4 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:45,279 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:32:45,280 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection:   Response 0: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t4
datanode_1          | 2023-07-13 21:32:45,280 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:32:45,280 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_1          | 2023-07-13 21:32:45,280 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5
datanode_1          | 2023-07-13 21:32:45,280 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection5] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:50,302 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5021721485ns, electionTimeout:5015ms
datanode_1          | 2023-07-13 21:32:50,303 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:50,303 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
datanode_1          | 2023-07-13 21:32:50,303 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:32:50,303 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6
datanode_1          | 2023-07-13 21:32:50,310 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6 ELECTION round 0: submit vote requests at term 5 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:50,343 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 6, (t:0, i:0))
datanode_1          | 2023-07-13 21:32:50,343 [grpc-default-executor-1] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-CANDIDATE: accept ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 0 <= candidate's priority 0
datanode_1          | 2023-07-13 21:32:50,343 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 6 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_1          | 2023-07-13 21:32:50,343 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6
datanode_1          | 2023-07-13 21:32:50,343 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:50,353 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t6. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362:t6, leader=null, voted=674ff7fc-a520-4090-839e-1ff0a8b3a1f6, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:32:50,363 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:32:50,363 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6] INFO impl.LeaderElection:   Response 0: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t5
datanode_1          | 2023-07-13 21:32:50,366 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6] INFO impl.LeaderElection:   Response 1: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t5
datanode_1          | 2023-07-13 21:32:50,366 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6] INFO impl.LeaderElection: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-LeaderElection6 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:32:54,452 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 7, (t:0, i:0))
datanode_1          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FOLLOWER: accept ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 0 <= candidate's priority 0
datanode_1          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_1          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:55,393 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:32:55,395 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:32:55,401 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t7. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362:t7, leader=null, voted=674ff7fc-a520-4090-839e-1ff0a8b3a1f6, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:33:00,595 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: receive requestVote(ELECTION, c89e3854-e92b-4097-a8a6-7827012e1c78, group-96AE05DF0362, 8, (t:0, i:0))
datanode_1          | 2023-07-13 21:33:00,595 [grpc-default-executor-1] INFO impl.VoteContext: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FOLLOWER: accept ELECTION from c89e3854-e92b-4097-a8a6-7827012e1c78: our priority 0 <= candidate's priority 1
datanode_3          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_3          | STARTUP_MSG:   java = 11.0.13
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:31:41,514 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:31:43,394 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:31:44,120 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:31:45,034 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:31:45,035 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:31:46,476 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:90b15e73a5cc ip:172.22.0.7
datanode_3          | 2023-07-13 21:31:48,043 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_3          | 2023-07-13 21:31:49,413 [main] INFO reflections.Reflections: Reflections took 1040 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_3          | 2023-07-13 21:31:51,473 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:31:51,544 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-13 21:31:51,546 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:31:51,562 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:31:51,762 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:31:52,000 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:31:52,009 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-13 21:31:52,016 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-13 21:31:52,022 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-13 21:31:52,037 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-13 21:31:52,185 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:31:52,210 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:32:02,260 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:32:03,239 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:32:04,387 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-13 21:32:04,388 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-13 21:32:04,397 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-13 21:32:04,398 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:32:04,398 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:32:04,418 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:32:04,435 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:32:06,299 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-13 21:32:06,301 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:06,310 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:32:06,366 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:32:07,940 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:32:08,213 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:32:08,455 [main] INFO util.log: Logging initialized @34523ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:32:09,238 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-13 21:32:09,268 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:32:09,294 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:32:09,304 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:32:09,317 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-13 21:32:09,317 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:32:09,754 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-13 21:32:09,812 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_3          | 2023-07-13 21:32:10,456 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:32:10,462 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:32:10,464 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-07-13 21:32:10,515 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a1b8a46{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:32:10,516 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@40d52be7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:32:12,373 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@45a1d057{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-7129798169973712710/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:32:12,463 [main] INFO server.AbstractConnector: Started ServerConnector@322e49ee{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:32:12,474 [main] INFO server.Server: Started @38543ms
datanode_3          | 2023-07-13 21:32:12,514 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-13 21:32:12,515 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-13 21:32:12,529 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:32:12,558 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-13 21:32:12,792 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@587e18ba] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-13 21:32:13,271 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.13:9891
datanode_3          | 2023-07-13 21:32:13,537 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:32:16,136 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:32:17,137 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.13:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:32:18,727 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:32:18,735 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-13 21:32:19,126 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_3          | 2023-07-13 21:32:19,312 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.RaftServer: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start RPC server
datanode_3          | 2023-07-13 21:32:19,320 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: GrpcService started, listening on 9856
datanode_3          | 2023-07-13 21:32:19,340 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: GrpcService started, listening on 9857
datanode_3          | 2023-07-13 21:32:19,342 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO server.GrpcService: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: GrpcService started, listening on 9858
datanode_3          | 2023-07-13 21:32:19,368 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 674ff7fc-a520-4090-839e-1ff0a8b3a1f6 is started using port 9858 for RATIS
datanode_3          | 2023-07-13 21:32:19,368 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 674ff7fc-a520-4090-839e-1ff0a8b3a1f6 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-13 21:32:19,368 [EndpointStateMachine task thread for scm/172.22.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 674ff7fc-a520-4090-839e-1ff0a8b3a1f6 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-13 21:32:19,375 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@6416f3d2] INFO util.JvmPauseMonitor: JvmPauseMonitor-674ff7fc-a520-4090-839e-1ff0a8b3a1f6: Started
datanode_3          | 2023-07-13 21:32:22,150 [EndpointStateMachine task thread for recon/172.22.0.13:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 90b15e73a5cc/172.22.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.7:37852 remote=recon/172.22.0.13:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.7:37852 remote=recon/172.22.0.13:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_2          | 2023-07-13 21:32:33,361 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:OK-t1. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF:t1, leader=null, voted=674ff7fc-a520-4090-839e-1ff0a8b3a1f6, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_2          | 2023-07-13 21:32:33,405 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172048833ns, electionTimeout:5163ms
datanode_2          | 2023-07-13 21:32:33,406 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState
datanode_2          | 2023-07-13 21:32:33,406 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:32:33,409 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:32:33,409 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1
datanode_2          | 2023-07-13 21:32:33,412 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:33,413 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-13 21:32:33,414 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1
datanode_2          | 2023-07-13 21:32:33,414 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:32:33,415 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2CE2CEC903C0 with new leaderId: c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:32:33,415 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: change Leader from null to c89e3854-e92b-4097-a8a6-7827012e1c78 at term 1 for becomeLeader, leader elected after 5326ms
datanode_2          | 2023-07-13 21:32:33,416 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:32:33,429 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:32:33,434 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:32:33,434 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:32:33,439 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:32:33,440 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:32:33,440 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:32:33,447 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:32:33,449 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-13 21:32:33,451 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderStateImpl
datanode_2          | 2023-07-13 21:32:33,464 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:32:33,494 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-LeaderElection1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0: set configuration 0: [c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-07-13 21:32:33,535 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-2CE2CEC903C0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/709eec37-995b-41fa-927a-2ce2cec903c0/current/log_inprogress_0
datanode_2          | 2023-07-13 21:32:34,917 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 2, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:34,918 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:34,918 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_2          | 2023-07-13 21:32:34,919 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:34,919 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:34,920 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:34,925 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t2. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t2, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:38,445 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-626633BA49DF, 2, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:38,446 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:32:38,446 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_2          | 2023-07-13 21:32:38,446 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState
datanode_2          | 2023-07-13 21:32:38,446 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState
datanode_2          | 2023-07-13 21:32:38,446 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:38,453 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:OK-t2. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF:t2, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_2          | 2023-07-13 21:32:38,528 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-626633BA49DF with new leaderId: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_2          | 2023-07-13 21:32:38,528 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: change Leader from null to 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at term 2 for appendEntries, leader elected after 10247ms
datanode_2          | 2023-07-13 21:32:38,556 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-13 21:32:38,556 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:32:38,560 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-626633BA49DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/current/log_inprogress_0
datanode_2          | 2023-07-13 21:32:40,072 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5151711266ns, electionTimeout:5148ms
datanode_2          | 2023-07-13 21:32:40,072 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:40,073 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-07-13 21:32:40,073 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:32:40,073 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2
datanode_2          | 2023-07-13 21:32:40,086 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:40,131 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 3, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:40,131 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-CANDIDATE: reject ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: already has voted for c89e3854-e92b-4097-a8a6-7827012e1c78 at current term 3
datanode_3          | 2023-07-13 21:32:23,820 [Command processor thread] INFO server.RaftServer: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: addNew group-03E8870E083E:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] returns group-03E8870E083E:java.util.concurrent.CompletableFuture@29b13048[Not completed]
datanode_3          | 2023-07-13 21:32:23,851 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: new RaftServerImpl for group-03E8870E083E:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:32:23,865 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:32:23,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:32:23,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:32:23,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:23,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:32:23,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:32:23,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:32:23,878 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:32:23,878 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:32:23,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:32:23,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:32:23,895 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8d17a42b-2c9e-433c-b999-03e8870e083e does not exist. Creating ...
datanode_3          | 2023-07-13 21:32:23,908 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8d17a42b-2c9e-433c-b999-03e8870e083e/in_use.lock acquired by nodename 7@90b15e73a5cc
datanode_3          | 2023-07-13 21:32:23,932 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8d17a42b-2c9e-433c-b999-03e8870e083e has been successfully formatted.
datanode_3          | 2023-07-13 21:32:23,951 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-03E8870E083E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:32:23,956 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:23,957 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:32:23,984 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:32:23,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:32:24,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,065 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:32:24,078 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:32:24,098 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8d17a42b-2c9e-433c-b999-03e8870e083e
datanode_3          | 2023-07-13 21:32:24,098 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:32:24,106 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:32:24,111 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,111 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:32:24,112 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:32:24,113 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:32:24,119 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:32:24,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:32:24,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:24,187 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:24,187 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:24,211 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:24,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:32:24,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:32:24,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:32:24,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:32:24,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:32:24,387 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:24,419 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:32:24,420 [pool-22-thread-1] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState
datanode_3          | 2023-07-13 21:32:24,458 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-03E8870E083E,id=674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_1          | 2023-07-13 21:33:00,595 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_1          | 2023-07-13 21:33:00,595 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: shutdown 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:33:00,596 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-13 21:33:00,597 [grpc-default-executor-1] INFO impl.RoleInfo: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: start 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-FollowerState
datanode_1          | 2023-07-13 21:33:00,600 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362 replies to ELECTION vote request: c89e3854-e92b-4097-a8a6-7827012e1c78<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t8. Peer's state: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362:t8, leader=null, voted=c89e3854-e92b-4097-a8a6-7827012e1c78, raftlog=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_1          | 2023-07-13 21:33:00,740 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-96AE05DF0362 with new leaderId: c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_1          | 2023-07-13 21:33:00,740 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: change Leader from null to c89e3854-e92b-4097-a8a6-7827012e1c78 at term 8 for appendEntries, leader elected after 35954ms
datanode_1          | 2023-07-13 21:33:00,798 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-13 21:33:00,799 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:33:00,802 [7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c@group-96AE05DF0362-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/current/log_inprogress_0
datanode_1          | 2023-07-13 21:33:04,648 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:32:40,132 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t3. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t3, leader=null, voted=c89e3854-e92b-4097-a8a6-7827012e1c78, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:40,177 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:32:40,178 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection:   Response 0: c89e3854-e92b-4097-a8a6-7827012e1c78<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:FAIL-t3
datanode_2          | 2023-07-13 21:32:40,178 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection:   Response 1: c89e3854-e92b-4097-a8a6-7827012e1c78<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:FAIL-t3
datanode_2          | 2023-07-13 21:32:40,179 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-07-13 21:32:40,179 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_2          | 2023-07-13 21:32:40,179 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2
datanode_2          | 2023-07-13 21:32:40,182 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:44,603 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:32:45,269 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 4, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:45,269 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:45,269 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_2          | 2023-07-13 21:32:45,269 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:45,270 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:45,270 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:45,273 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t4. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t4, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:50,326 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 5, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:50,326 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:50,326 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_2          | 2023-07-13 21:32:50,326 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:50,326 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:50,326 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:50,340 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t5. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t5, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:50,349 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 6, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:50,350 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:50,351 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_2          | 2023-07-13 21:32:50,351 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:50,352 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:50,354 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t6. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t6, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:32:50,369 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:55,392 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: receive requestVote(ELECTION, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6, group-96AE05DF0362, 7, (t:0, i:0))
datanode_2          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO impl.VoteContext: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FOLLOWER: reject ELECTION from 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_2          | 2023-07-13 21:32:55,393 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:55,394 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-13 21:32:55,395 [grpc-default-executor-1] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:32:55,404 [grpc-default-executor-1] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362 replies to ELECTION vote request: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t7. Peer's state: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362:t7, leader=null, voted=null, raftlog=c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:33:00,567 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172735111ns, electionTimeout:5165ms
datanode_2          | 2023-07-13 21:33:00,568 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState
datanode_2          | 2023-07-13 21:33:00,568 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
datanode_2          | 2023-07-13 21:33:00,568 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:33:00,568 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3
datanode_2          | 2023-07-13 21:33:00,580 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3 ELECTION round 0: submit vote requests at term 8 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-13 21:33:00,606 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:33:00,609 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.LeaderElection:   Response 0: c89e3854-e92b-4097-a8a6-7827012e1c78<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t8
datanode_2          | 2023-07-13 21:33:00,609 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.LeaderElection: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-07-13 21:33:00,610 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: shutdown c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3
datanode_2          | 2023-07-13 21:33:00,610 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
datanode_2          | 2023-07-13 21:33:00,610 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-96AE05DF0362 with new leaderId: c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_2          | 2023-07-13 21:33:00,611 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: change Leader from null to c89e3854-e92b-4097-a8a6-7827012e1c78 at term 8 for becomeLeader, leader elected after 35974ms
datanode_2          | 2023-07-13 21:33:00,611 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:33:00,611 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-13 21:33:00,611 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:33:00,635 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:33:00,636 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:33:00,636 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:33:00,637 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:33:00,638 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:33:00,638 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-13 21:33:00,665 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:33:00,665 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:33:00,669 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-13 21:33:00,672 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-13 21:33:00,674 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:33:00,675 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:33:00,695 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:33:00,695 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:33:00,695 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-13 21:33:00,696 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-13 21:33:00,696 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:33:00,696 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:33:00,697 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO impl.RoleInfo: c89e3854-e92b-4097-a8a6-7827012e1c78: start c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderStateImpl
datanode_2          | 2023-07-13 21:33:00,698 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:33:00,711 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-LeaderElection3] INFO server.RaftServer$Division: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-07-13 21:33:00,713 [c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c89e3854-e92b-4097-a8a6-7827012e1c78@group-96AE05DF0362-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/current/log_inprogress_0
datanode_2          | 2023-07-13 21:33:04,644 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:32:24,568 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8d17a42b-2c9e-433c-b999-03e8870e083e
datanode_3          | 2023-07-13 21:32:24,569 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8d17a42b-2c9e-433c-b999-03e8870e083e.
datanode_3          | 2023-07-13 21:32:24,574 [Command processor thread] INFO server.RaftServer: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: addNew group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] returns group-96AE05DF0362:java.util.concurrent.CompletableFuture@3c179f92[Not completed]
datanode_3          | 2023-07-13 21:32:24,610 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: new RaftServerImpl for group-96AE05DF0362:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:32:24,616 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:32:24,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:32:24,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:32:24,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:24,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:32:24,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:32:24,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:32:24,626 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:32:24,627 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:32:24,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:32:24,631 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:32:24,632 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 does not exist. Creating ...
datanode_3          | 2023-07-13 21:32:24,654 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/in_use.lock acquired by nodename 7@90b15e73a5cc
datanode_3          | 2023-07-13 21:32:24,657 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362 has been successfully formatted.
datanode_3          | 2023-07-13 21:32:24,662 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-96AE05DF0362: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:32:24,663 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:24,663 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:32:24,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:32:24,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:32:24,669 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:32:24,669 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:32:24,672 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:32:24,672 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_3          | 2023-07-13 21:32:24,672 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:32:24,677 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:32:24,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:32:24,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:32:24,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:32:24,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:32:24,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:32:24,684 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:24,685 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:24,699 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:24,699 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:24,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:32:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:32:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:32:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:32:24,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:32:24,705 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:24,710 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:32:24,710 [pool-22-thread-1] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:24,720 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96AE05DF0362,id=674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_3          | 2023-07-13 21:32:24,728 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362
datanode_3          | 2023-07-13 21:32:28,151 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362.
datanode_3          | 2023-07-13 21:32:28,152 [Command processor thread] INFO server.RaftServer: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: addNew group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] returns group-626633BA49DF:java.util.concurrent.CompletableFuture@1b526812[Not completed]
datanode_3          | 2023-07-13 21:32:28,153 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: new RaftServerImpl for group-626633BA49DF:[674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:32:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:32:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:32:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:32:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:32:28,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:32:28,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:32:28,155 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: ConfigurationManager, init=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:32:28,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:32:28,156 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:32:28,156 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:32:28,156 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df does not exist. Creating ...
datanode_3          | 2023-07-13 21:32:28,162 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/in_use.lock acquired by nodename 7@90b15e73a5cc
datanode_3          | 2023-07-13 21:32:28,164 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df has been successfully formatted.
datanode_3          | 2023-07-13 21:32:28,183 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-626633BA49DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:32:28,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:32:28,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:32:28,184 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:32:28,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:32:28,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:32:28,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:28,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:32:28,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:32:28,208 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_3          | 2023-07-13 21:32:28,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-13 21:32:28,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:32:28,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:28,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:32:28,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:32:28,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:31:41,694 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f8322087c031/172.22.0.8
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-07-13 21:31:41,756 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:31:52,056 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:31:52,470 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.8:9862
om_1                | 2023-07-13 21:31:52,470 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:31:52,482 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:31:52,543 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:31:57,953 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:31:59,959 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:01,961 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:03,963 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:05,964 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:07,966 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:09,979 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:32:11,981 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8322087c031/172.22.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.10:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-1e085d94-bca5-4fb4-a5b6-05615e090b61;layoutVersion=0
om_1                | 2023-07-13 21:32:19,010 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at f8322087c031/172.22.0.8
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:32:22,658 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f8322087c031/172.22.0.8
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-07-13 21:32:22,685 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:32:26,672 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:32:26,949 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.8:9862
om_1                | 2023-07-13 21:32:26,953 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:32:26,954 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:32:27,017 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:32:27,168 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om_1                | 2023-07-13 21:32:28,629 [main] INFO reflections.Reflections: Reflections took 1264 ms to scan 1 urls, producing 95 keys and 258 values [using 2 cores]
om_1                | 2023-07-13 21:32:28,731 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:32:29,793 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:32:30,046 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-13 21:32:30,047 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
datanode_3          | 2023-07-13 21:32:28,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:32:28,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:32:28,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:32:28,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:28,240 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:28,254 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:32:28,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:32:28,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:32:28,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:32:28,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:32:28,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:32:28,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:32:28,273 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: start as a follower, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:32:28,274 [pool-22-thread-1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:32:28,282 [pool-22-thread-1] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState
datanode_3          | 2023-07-13 21:32:28,287 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-626633BA49DF,id=674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_3          | 2023-07-13 21:32:28,311 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df
datanode_3          | 2023-07-13 21:32:28,506 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df.
datanode_3          | 2023-07-13 21:32:29,621 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200627488ns, electionTimeout:5176ms
datanode_3          | 2023-07-13 21:32:29,621 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState
datanode_3          | 2023-07-13 21:32:29,621 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:32:29,625 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:32:29,625 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1
datanode_3          | 2023-07-13 21:32:29,635 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:29,636 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-13 21:32:29,637 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1
datanode_3          | 2023-07-13 21:32:29,638 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-13 21:32:29,638 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-03E8870E083E with new leaderId: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6
datanode_3          | 2023-07-13 21:32:29,639 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:32:29,643 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: change Leader from null to 674ff7fc-a520-4090-839e-1ff0a8b3a1f6 at term 1 for becomeLeader, leader elected after 5683ms
datanode_3          | 2023-07-13 21:32:29,662 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:32:29,672 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:32:29,675 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-13 21:32:29,687 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:32:29,687 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:32:29,688 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:32:29,698 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:32:29,701 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-13 21:32:30,297 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:32:30,336 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:32:30,336 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-13 21:32:30,363 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-13 21:32:30,371 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:32:30,398 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-13 21:32:30,406 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-13 21:32:30,429 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-13 21:32:30,517 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-07-13 21:32:30,517 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:32:30,518 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-07-13 21:32:30,518 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:32:30,518 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:32:30,519 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-13 21:32:30,521 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:32:30,522 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-13 21:32:30,522 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-13 21:32:30,721 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-13 21:32:30,723 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:32:30,723 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:32:30,734 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:32:30,740 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@628b819d[Not completed]
om_1                | 2023-07-13 21:32:30,741 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-13 21:32:30,766 [pool-23-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-13 21:32:30,768 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-13 21:32:30,768 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-13 21:32:30,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-13 21:32:30,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:32:30,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:32:30,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-13 21:32:30,770 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-13 21:32:30,772 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-13 21:32:30,783 [pool-23-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-13 21:32:30,784 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:32:30,788 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-13 21:32:30,788 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-13 21:32:30,792 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-13 21:32:30,801 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-13 21:32:30,818 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@f8322087c031
om_1                | 2023-07-13 21:32:30,865 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-13 21:32:30,868 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-13 21:32:30,884 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:32:30,915 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-13 21:32:30,915 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:32:30,933 [Listener at om/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om_1                | 2023-07-13 21:32:30,946 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:32:30,970 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-13 21:32:30,970 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-13 21:32:30,975 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-13 21:32:30,986 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:32:30,988 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:32:30,989 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:32:30,991 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-13 21:32:30,992 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:32:29,703 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderStateImpl
datanode_3          | 2023-07-13 21:32:29,717 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:32:29,738 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-LeaderElection1] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-13 21:32:29,784 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5074189499ns, electionTimeout:5058ms
datanode_3          | 2023-07-13 21:32:29,785 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:29,785 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:32:29,785 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:32:29,785 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2
datanode_3          | 2023-07-13 21:32:29,790 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:29,942 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:32:29,943 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection:   Response 0: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t1
datanode_3          | 2023-07-13 21:32:29,947 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection:   Response 1: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t1
datanode_3          | 2023-07-13 21:32:29,948 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-13 21:32:29,949 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-07-13 21:32:29,950 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2
datanode_3          | 2023-07-13 21:32:29,950 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection2] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:29,984 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-03E8870E083E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8d17a42b-2c9e-433c-b999-03e8870e083e/current/log_inprogress_0
datanode_3          | 2023-07-13 21:32:33,328 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5045593442ns, electionTimeout:5041ms
datanode_3          | 2023-07-13 21:32:33,328 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState
datanode_3          | 2023-07-13 21:32:33,328 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:32:33,328 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:32:33,328 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3
datanode_3          | 2023-07-13 21:32:33,334 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:32:33,358 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:32:33,358 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection:   Response 0: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:FAIL-t1
datanode_3          | 2023-07-13 21:32:33,358 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-13 21:32:33,359 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-07-13 21:32:33,360 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3
datanode_3          | 2023-07-13 21:32:33,360 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-LeaderElection3] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState
datanode_3          | 2023-07-13 21:32:34,936 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 2, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:34,939 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:32:34,940 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:34,940 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:34,940 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:34,940 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:32:34,949 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t2. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t2, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:38,437 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-626633BA49DF, 2, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:38,438 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:32:38,438 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:38,438 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState
datanode_3          | 2023-07-13 21:32:38,438 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:32:38,439 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-FollowerState
datanode_3          | 2023-07-13 21:32:38,442 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t2. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF:t2, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0], old=null
datanode_3          | 2023-07-13 21:32:38,519 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-626633BA49DF with new leaderId: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:38,520 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: change Leader from null to 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at term 2 for appendEntries, leader elected after 10334ms
datanode_3          | 2023-07-13 21:32:38,556 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:1, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-07-13 21:32:38,556 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:32:38,560 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-626633BA49DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ae8f976e-f10d-4aa2-b9f8-626633ba49df/current/log_inprogress_0
datanode_3          | 2023-07-13 21:32:40,114 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 3, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:40,114 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:32:40,114 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:40,114 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:31:39,453 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 63b7b1b550f4/172.22.0.13
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.2.1
datanode_3          | 2023-07-13 21:32:40,115 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:40,115 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:32:40,118 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t3. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t3, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:40,153 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, c89e3854-e92b-4097-a8a6-7827012e1c78, group-96AE05DF0362, 3, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:40,154 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FOLLOWER: reject ELECTION from c89e3854-e92b-4097-a8a6-7827012e1c78: already has voted for 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c at current term 3
datanode_3          | 2023-07-13 21:32:40,154 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: c89e3854-e92b-4097-a8a6-7827012e1c78<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:FAIL-t3. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t3, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:44,580 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-13 21:32:45,276 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 4, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:45,277 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:32:45,277 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:45,277 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:45,277 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:32:45,277 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:45,280 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t4. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t4, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:50,315 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, group-96AE05DF0362, 5, (t:0, i:0))
datanode_3          | 2023-07-13 21:32:50,315 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FOLLOWER: accept ELECTION from 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:32:50,316 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
datanode_3          | 2023-07-13 21:32:50,317 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:50,317 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:50,321 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t5. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t5, leader=null, voted=7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
recon_1             | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
recon_1             | STARTUP_MSG:   java = 11.0.13
recon_1             | ************************************************************/
recon_1             | 2023-07-13 21:31:39,521 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-13 21:31:44,589 [main] INFO reflections.Reflections: Reflections took 193 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1             | 2023-07-13 21:31:47,454 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-13 21:31:49,030 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:31:57,650 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:31:59,951 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:32:00,273 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:32:00,321 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-13 21:32:08,776 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-13 21:32:08,958 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:32:09,057 [main] INFO util.log: Logging initialized @35129ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:32:09,815 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-13 21:32:09,897 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-13 21:32:10,030 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:32:10,052 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:32:10,062 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-13 21:32:10,062 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:32:11,167 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-13 21:32:13,097 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-13 21:32:13,138 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-13 21:32:13,168 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-13 21:32:13,307 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-13 21:32:13,308 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-13 21:32:15,086 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:32:15,765 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:32:15,888 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
recon_1             | 2023-07-13 21:32:15,901 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-13 21:32:16,104 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:32:16,265 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1             | 2023-07-13 21:32:16,411 [main] INFO reflections.Reflections: Reflections took 127 ms to scan 3 urls, producing 103 keys and 211 values 
recon_1             | 2023-07-13 21:32:16,477 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-13 21:32:16,527 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-13 21:32:16,535 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-13 21:32:16,542 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-13 21:32:16,623 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:32:16,722 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-13 21:32:16,788 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-13 21:32:16,872 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-13 21:32:16,872 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-13 21:32:17,023 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:32:17,055 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:32:17,055 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-13 21:32:17,452 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-13 21:32:17,453 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
recon_1             | 2023-07-13 21:32:17,507 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-13 21:32:17,510 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-13 21:32:17,512 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-13 21:32:17,534 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c604965{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:32:17,535 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e1237ab{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:32:22,568 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@42457891{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_2_1_jar-_-any-3696828163481269959/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/recon}
recon_1             | 2023-07-13 21:32:22,587 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@794eeaf8{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-13 21:32:22,588 [Listener at 0.0.0.0/9891] INFO server.Server: Started @48661ms
recon_1             | 2023-07-13 21:32:22,591 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-13 21:32:22,591 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-13 21:32:22,593 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-13 21:32:22,593 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-13 21:32:22,614 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-13 21:32:22,627 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:32:22,627 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-13 21:32:22,627 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:32:22,628 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-13 21:32:22,643 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:32:24,016 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-07-13 21:32:24,017 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:32:24,019 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df from SCM.
recon_1             | 2023-07-13 21:32:24,077 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ae8f976e-f10d-4aa2-b9f8-626633ba49df, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.664Z[UTC]].
recon_1             | 2023-07-13 21:32:24,099 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 54894.317us
recon_1             | 2023-07-13 21:32:24,115 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=709eec37-995b-41fa-927a-2ce2cec903c0 from SCM.
recon_1             | 2023-07-13 21:32:24,117 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 709eec37-995b-41fa-927a-2ce2cec903c0, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.662Z[UTC]].
recon_1             | 2023-07-13 21:32:24,120 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 3442.233us
recon_1             | 2023-07-13 21:32:24,120 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=c5c065b2-d64a-490f-952f-8794a2f79000 from SCM.
recon_1             | 2023-07-13 21:32:24,122 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c5c065b2-d64a-490f-952f-8794a2f79000, Nodes: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.665Z[UTC]].
recon_1             | 2023-07-13 21:32:24,122 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 1444.113us
recon_1             | 2023-07-13 21:32:24,124 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 from SCM.
recon_1             | 2023-07-13 21:32:24,126 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0cb8e1c0-f153-40ad-97c9-96ae05df0362, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.624Z[UTC]].
recon_1             | 2023-07-13 21:32:24,130 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 6030.057us
recon_1             | 2023-07-13 21:32:24,131 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=8d17a42b-2c9e-433c-b999-03e8870e083e from SCM.
recon_1             | 2023-07-13 21:32:24,132 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8d17a42b-2c9e-433c-b999-03e8870e083e, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.292Z[UTC]].
recon_1             | 2023-07-13 21:32:24,133 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 2113.82us
recon_1             | 2023-07-13 21:32:24,134 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:32:24,147 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:32:24,381 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-13 21:32:24,437 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-13 21:32:24,439 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-13 21:32:24,562 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-13 21:32:24,566 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-13 21:32:24,570 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.6:59670
recon_1             | 2023-07-13 21:32:24,574 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.7:37852
recon_1             | 2023-07-13 21:32:24,743 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-07-13 21:32:24,782 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 20650.095us
recon_1             | 2023-07-13 21:32:24,783 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 423.104us
recon_1             | 2023-07-13 21:32:24,785 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 352.004us
recon_1             | 2023-07-13 21:32:24,794 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 502.605us
om_1                | 2023-07-13 21:32:30,998 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-13 21:32:30,999 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-13 21:32:30,999 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-13 21:32:31,017 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-13 21:32:31,017 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:32:31,025 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:32:31,025 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:32:31,039 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:32:31,041 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-13 21:32:31,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:32:31,043 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-13 21:32:31,044 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-13 21:32:31,044 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-13 21:32:31,104 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-13 21:32:31,122 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:32:31,122 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-13 21:32:31,196 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.22.0.8:9862
om_1                | 2023-07-13 21:32:31,196 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-13 21:32:31,201 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-13 21:32:31,203 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-13 21:32:31,204 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:32:31,206 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-13 21:32:31,211 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-13 21:32:31,265 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-13 21:32:31,270 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-13 21:32:31,272 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$370/0x00000008404df040@110b7837] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-13 21:32:31,318 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:32:31,319 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-13 21:32:31,339 [Listener at om/9862] INFO util.log: Logging initialized @11467ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:32:31,525 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-13 21:32:31,535 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:32:31,544 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:32:31,550 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-13 21:32:31,552 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-13 21:32:31,552 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:32:31,609 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-13 21:32:31,611 [Listener at om/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
om_1                | 2023-07-13 21:32:31,658 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:32:31,658 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-13 21:32:31,660 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-13 21:32:31,673 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c951ada{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:32:31,674 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11f23038{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-13 21:32:32,008 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@ad3f70a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_1_jar-_-any-11975966619124146656/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:32:32,018 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@2eb0cefe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-13 21:32:32,018 [Listener at om/9862] INFO server.Server: Started @12146ms
om_1                | 2023-07-13 21:32:32,020 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-13 21:32:32,021 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-13 21:32:32,022 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:32:32,023 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-13 21:32:32,028 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-13 21:32:32,077 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-07-13 21:32:32,098 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@45bbc52f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-13 21:32:36,368 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5164186611ns, electionTimeout:5159ms
om_1                | 2023-07-13 21:32:36,370 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:32:36,370 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:32:36,374 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-13 21:32:36,374 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:32:36,381 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-13 21:32:36,381 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-13 21:32:36,382 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:32:36,383 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-13 21:32:36,383 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5514ms
om_1                | 2023-07-13 21:32:36,388 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-13 21:32:36,393 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:32:36,394 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:32:36,399 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-13 21:32:36,400 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-13 21:32:36,400 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-13 21:32:36,405 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:32:36,407 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-13 21:32:36,409 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-13 21:32:36,431 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-13 21:32:36,483 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-07-13 21:32:36,576 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-13 21:32:36,662 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | ]
om_1                | 2023-07-13 21:32:42,620 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-13 21:33:22,958 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-13 21:33:23,000 [qtp2081751971-43] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689284002961 in 38 milliseconds
om_1                | 2023-07-13 21:33:23,054 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 52 milliseconds
datanode_3          | 2023-07-13 21:32:50,321 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5040785963ns, electionTimeout:5037ms
datanode_3          | 2023-07-13 21:32:50,322 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:50,322 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
datanode_3          | 2023-07-13 21:32:50,322 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:32:50,322 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4
datanode_3          | 2023-07-13 21:32:50,333 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-13 21:32:50,337 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4 ELECTION round 0: submit vote requests at term 6 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:50,357 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:32:50,357 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection:   Response 0: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t6
datanode_3          | 2023-07-13 21:32:50,357 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-13 21:32:50,358 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
datanode_3          | 2023-07-13 21:32:50,359 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4
datanode_3          | 2023-07-13 21:32:50,359 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection4] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:55,381 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5021610305ns, electionTimeout:5015ms
datanode_3          | 2023-07-13 21:32:55,381 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:32:55,382 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
datanode_3          | 2023-07-13 21:32:55,382 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:32:55,382 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5
datanode_3          | 2023-07-13 21:32:55,388 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5 ELECTION round 0: submit vote requests at term 7 for -1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection:   Response 0: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c#0:OK-t7
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection:   Response 1: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6<-c89e3854-e92b-4097-a8a6-7827012e1c78#0:FAIL-t7
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.LeaderElection: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
datanode_3          | 2023-07-13 21:32:55,406 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5
datanode_3          | 2023-07-13 21:32:55,407 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection5] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:33:00,585 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.FollowerState: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5177869258ns, electionTimeout:5164ms
datanode_3          | 2023-07-13 21:33:00,585 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
recon_1             | 2023-07-13 21:32:24,796 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 370.703us
recon_1             | 2023-07-13 21:32:24,804 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 148 milliseconds.
recon_1             | 2023-07-13 21:32:25,023 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 527 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:32:25,378 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 354 milliseconds for processing 0 containers.
recon_1             | 2023-07-13 21:32:25,674 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.3:57140: output error
recon_1             | 2023-07-13 21:32:25,679 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-13 21:32:28,152 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c89e3854-e92b-4097-a8a6-7827012e1c78
recon_1             | 2023-07-13 21:32:28,160 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,172 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c89e3854-e92b-4097-a8a6-7827012e1c78 to Node DB.
recon_1             | 2023-07-13 21:32:28,202 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/674ff7fc-a520-4090-839e-1ff0a8b3a1f6
recon_1             | 2023-07-13 21:32:28,203 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,205 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 674ff7fc-a520-4090-839e-1ff0a8b3a1f6 to Node DB.
recon_1             | 2023-07-13 21:32:28,207 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=709eec37-995b-41fa-927a-2ce2cec903c0 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,216 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 709eec37-995b-41fa-927a-2ce2cec903c0, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c89e3854-e92b-4097-a8a6-7827012e1c78, CreationTimestamp2023-07-13T21:32:21.662Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:32:28,219 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 2499.724us
recon_1             | 2023-07-13 21:32:28,224 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,226 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,227 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,227 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=8d17a42b-2c9e-433c-b999-03e8870e083e reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om_1                | 2023-07-13 21:33:23,055 [qtp2081751971-43] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689284002961
recon_1             | 2023-07-13 21:32:28,227 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8d17a42b-2c9e-433c-b999-03e8870e083e, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:674ff7fc-a520-4090-839e-1ff0a8b3a1f6, CreationTimestamp2023-07-13T21:32:21.292Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:32:28,235 [IPC Server handler 13 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
recon_1             | 2023-07-13 21:32:28,250 [IPC Server handler 13 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,251 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c to Node DB.
recon_1             | 2023-07-13 21:32:28,254 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 430.204us
recon_1             | 2023-07-13 21:32:28,256 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,257 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,327 [IPC Server handler 78 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-13 21:32:28,328 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,329 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,675 [IPC Server handler 1 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-13 21:32:28,679 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,679 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=c5c065b2-d64a-490f-952f-8794a2f79000 reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:28,679 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c5c065b2-d64a-490f-952f-8794a2f79000, Nodes: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, CreationTimestamp2023-07-13T21:32:21.665Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:32:28,680 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 532.205us
recon_1             | 2023-07-13 21:32:28,682 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:29,647 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-13 21:32:29,651 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:29,651 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:33,422 [IPC Server handler 99 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
datanode_3          | 2023-07-13 21:33:00,585 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
datanode_3          | 2023-07-13 21:33:00,586 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:33:00,586 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection6
datanode_3          | 2023-07-13 21:33:00,586 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: receive requestVote(ELECTION, c89e3854-e92b-4097-a8a6-7827012e1c78, group-96AE05DF0362, 8, (t:0, i:0))
datanode_3          | 2023-07-13 21:33:00,587 [grpc-default-executor-0] INFO impl.VoteContext: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-CANDIDATE: accept ELECTION from c89e3854-e92b-4097-a8a6-7827012e1c78: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:33:00,587 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: changes role from CANDIDATE to FOLLOWER at term 8 for candidate:c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_3          | 2023-07-13 21:33:00,587 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: shutdown 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-LeaderElection6
datanode_3          | 2023-07-13 21:33:00,587 [grpc-default-executor-0] INFO impl.RoleInfo: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6: start 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-FollowerState
datanode_3          | 2023-07-13 21:33:00,590 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362 replies to ELECTION vote request: c89e3854-e92b-4097-a8a6-7827012e1c78<-674ff7fc-a520-4090-839e-1ff0a8b3a1f6#0:OK-t8. Peer's state: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362:t8, leader=null, voted=c89e3854-e92b-4097-a8a6-7827012e1c78, raftlog=674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLog:OPENED:c-1, conf=-1: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_3          | 2023-07-13 21:33:00,744 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-96AE05DF0362 with new leaderId: c89e3854-e92b-4097-a8a6-7827012e1c78
datanode_3          | 2023-07-13 21:33:00,744 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: change Leader from null to c89e3854-e92b-4097-a8a6-7827012e1c78 at term 8 for appendEntries, leader elected after 36080ms
datanode_3          | 2023-07-13 21:33:00,785 [grpc-default-executor-0] INFO server.RaftServer$Division: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362: set configuration 0: [674ff7fc-a520-4090-839e-1ff0a8b3a1f6|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c|rpc:172.22.0.3:9856|admin:172.22.0.3:9857|client:172.22.0.3:9858|dataStream:|priority:0, c89e3854-e92b-4097-a8a6-7827012e1c78|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-13 21:33:00,786 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:33:00,788 [674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6@group-96AE05DF0362-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0cb8e1c0-f153-40ad-97c9-96ae05df0362/current/log_inprogress_0
datanode_3          | 2023-07-13 21:33:04,584 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-13 21:31:39,693 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:31:39,719 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-13 21:31:39,905 [main] INFO util.log: Logging initialized @8365ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-13 21:31:41,319 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-13 21:31:41,528 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-13 21:31:41,558 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-13 21:31:41,575 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-13 21:31:41,584 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-13 21:31:41,598 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-13 21:31:42,005 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = e2aa9e3c42db/172.22.0.4
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:31:41,386 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 84a6feef6765/172.22.0.10
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.2.1
s3g_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
s3g_1               | STARTUP_MSG:   java = 11.0.13
s3g_1               | ************************************************************/
s3g_1               | 2023-07-13 21:31:42,110 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-13 21:31:42,396 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-13 21:31:42,450 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-13 21:31:42,478 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
s3g_1               | 2023-07-13 21:31:42,787 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:31:42,787 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-13 21:31:42,792 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-07-13 21:31:42,912 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f284218{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-13 21:31:42,927 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@723e88f9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 13, 2023 9:32:10 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-13 21:32:10,732 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5fb3111a{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_1_jar-_-any-17284425186421343497/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/s3gateway}
s3g_1               | 2023-07-13 21:32:10,802 [main] INFO server.AbstractConnector: Started ServerConnector@5495333e{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:32:10,803 [main] INFO server.Server: Started @39264ms
s3g_1               | 2023-07-13 21:32:10,815 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-07-13 21:32:33,424 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:33,424 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:33,872 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-13 21:32:33,873 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:33,874 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:38,460 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:38,460 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ae8f976e-f10d-4aa2-b9f8-626633ba49df, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, CreationTimestamp2023-07-13T21:32:21.664Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:32:38,461 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 628.606us
recon_1             | 2023-07-13 21:32:38,461 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:44,562 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:32:44,616 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:44,643 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-13 21:32:44,644 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:32:44,662 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 40257.675us
recon_1             | 2023-07-13 21:32:44,663 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:32:54,455 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:32:54,459 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 421.404us
recon_1             | 2023-07-13 21:32:54,460 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-13 21:33:00,628 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 reported by c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:31:41,484 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:31:42,159 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:31:42,324 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:31:42,327 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:31:42,746 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-1e085d94-bca5-4fb4-a5b6-05615e090b61; layoutVersion=2; scmId=0867e848-ecde-47cd-ab85-83235545ca2e
scm_1               | 2023-07-13 21:31:42,831 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 84a6feef6765/172.22.0.10
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:31:52,238 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 84a6feef6765/172.22.0.10
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:31:52,288 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:31:52,685 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:31:52,686 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:31:53,329 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:31:53,695 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm_1               | 2023-07-13 21:31:56,406 [main] INFO reflections.Reflections: Reflections took 1338 ms to scan 3 urls, producing 103 keys and 211 values 
scm_1               | 2023-07-13 21:32:00,758 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:32:01,817 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:32:02,927 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
scm_1               | 2023-07-13 21:32:02,940 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-13 21:32:03,538 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-13 21:32:03,719 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-13 21:32:03,719 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-13 21:32:03,722 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-13 21:32:03,788 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-13 21:32:04,059 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:32:04,120 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-13 21:32:04,149 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-13 21:32:04,311 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-13 21:32:04,376 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-13 21:32:04,376 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:32:04,521 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-13 21:32:04,636 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-13 21:32:04,753 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-13 21:32:04,799 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-13 21:32:04,890 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 61 milliseconds for processing 0 containers.
scm_1               | 2023-07-13 21:32:04,946 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-13 21:32:04,986 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:32:04,996 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:32:11,916 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:32:12,398 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-13 21:32:12,765 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:32:12,786 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-13 21:32:12,987 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:32:12,993 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:32:13,569 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-13 21:32:13,654 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          0.1
scm_1               | Max Datanodes to Involve per Iteration(ratio)      0.2
scm_1               | Max Size to Move per Iteration                     32212254720B
scm_1               | 
scm_1               | 2023-07-13 21:32:13,690 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:32:13,690 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-13 21:32:13,741 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:32:14,457 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-13 21:32:14,575 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-13 21:32:14,576 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-13 21:32:16,028 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:32:16,040 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:32:16,051 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-13 21:32:16,854 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:32:16,855 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:32:16,866 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:32:16,870 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-13 21:32:17,380 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:32:17,381 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:32:17,409 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:32:17,410 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:32:17,948 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3596b249] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-13 21:32:18,031 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-13 21:32:18,031 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-13 21:32:18,142 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @33760ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:32:19,253 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-13 21:32:19,285 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:32:19,390 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:32:19,465 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:32:19,465 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:32:19,465 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-13 21:32:20,187 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-13 21:32:20,190 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
scm_1               | 2023-07-13 21:32:20,451 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:32:20,451 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:32:20,466 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-13 21:32:20,557 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@25ffd826{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:32:20,581 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@39dec536{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:32:21,015 [IPC Server handler 80 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/674ff7fc-a520-4090-839e-1ff0a8b3a1f6
scm_1               | 2023-07-13 21:32:21,017 [IPC Server handler 80 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:32:21,108 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:32:21,128 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:32:21,143 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:32:21,216 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:32:21,310 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8d17a42b-2c9e-433c-b999-03e8870e083e to datanode:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
scm_1               | 2023-07-13 21:32:21,321 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c89e3854-e92b-4097-a8a6-7827012e1c78
scm_1               | 2023-07-13 21:32:21,348 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:32:21,349 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:32:21,357 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:32:21,450 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
scm_1               | 2023-07-13 21:32:21,451 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:32:21,452 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:32:21,424 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8d17a42b-2c9e-433c-b999-03e8870e083e, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.292Z[UTC]].
scm_1               | 2023-07-13 21:32:21,586 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:32:21,586 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:32:21,586 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:32:21,587 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:32:21,602 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-13 21:32:21,610 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:32:21,624 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 to datanode:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
scm_1               | 2023-07-13 21:32:21,631 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 to datanode:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
scm_1               | 2023-07-13 21:32:21,631 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 to datanode:c89e3854-e92b-4097-a8a6-7827012e1c78
scm_1               | 2023-07-13 21:32:21,632 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0cb8e1c0-f153-40ad-97c9-96ae05df0362, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.624Z[UTC]].
scm_1               | 2023-07-13 21:32:21,662 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=709eec37-995b-41fa-927a-2ce2cec903c0 to datanode:c89e3854-e92b-4097-a8a6-7827012e1c78
scm_1               | 2023-07-13 21:32:21,663 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 709eec37-995b-41fa-927a-2ce2cec903c0, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.662Z[UTC]].
scm_1               | 2023-07-13 21:32:21,664 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df to datanode:c89e3854-e92b-4097-a8a6-7827012e1c78
scm_1               | 2023-07-13 21:32:21,664 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df to datanode:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
scm_1               | 2023-07-13 21:32:21,664 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df to datanode:674ff7fc-a520-4090-839e-1ff0a8b3a1f6
scm_1               | 2023-07-13 21:32:21,665 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ae8f976e-f10d-4aa2-b9f8-626633ba49df, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.664Z[UTC]].
scm_1               | 2023-07-13 21:32:21,665 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=ae8f976e-f10d-4aa2-b9f8-626633ba49df contains same datanodes as previous pipelines: PipelineID=0cb8e1c0-f153-40ad-97c9-96ae05df0362 nodeIds: c89e3854-e92b-4097-a8a6-7827012e1c78, 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, 674ff7fc-a520-4090-839e-1ff0a8b3a1f6
scm_1               | 2023-07-13 21:32:21,665 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c5c065b2-d64a-490f-952f-8794a2f79000 to datanode:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c
scm_1               | 2023-07-13 21:32:21,666 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c5c065b2-d64a-490f-952f-8794a2f79000, Nodes: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:32:21.665Z[UTC]].
scm_1               | 2023-07-13 21:32:22,963 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@594131f2{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_1_jar-_-any-12444375504604106312/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/scm}
scm_1               | 2023-07-13 21:32:23,003 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@1983b48a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:32:23,011 [Listener at 0.0.0.0/9860] INFO server.Server: Started @38628ms
scm_1               | 2023-07-13 21:32:23,031 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-13 21:32:23,032 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-13 21:32:23,034 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:32:25,800 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8d17a42b-2c9e-433c-b999-03e8870e083e, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:674ff7fc-a520-4090-839e-1ff0a8b3a1f6, CreationTimestamp2023-07-13T21:32:21.292Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:32:25,861 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:28,118 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 709eec37-995b-41fa-927a-2ce2cec903c0, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c89e3854-e92b-4097-a8a6-7827012e1c78, CreationTimestamp2023-07-13T21:32:21.662Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:32:28,125 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:28,188 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:28,360 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:28,673 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c5c065b2-d64a-490f-952f-8794a2f79000, Nodes: 7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, CreationTimestamp2023-07-13T21:32:21.665Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:32:28,676 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:29,642 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:33,423 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:33,877 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:38,471 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ae8f976e-f10d-4aa2-b9f8-626633ba49df, Nodes: c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c, CreationTimestamp2023-07-13T21:32:21.664Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:32:38,472 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:32:38,472 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:32:38,472 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1             | 2023-07-13 21:33:00,629 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0cb8e1c0-f153-40ad-97c9-96ae05df0362, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c89e3854-e92b-4097-a8a6-7827012e1c78, CreationTimestamp2023-07-13T21:32:21.624Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:33:00,630 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 641.706us
recon_1             | 2023-07-13 21:33:04,617 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-13 21:33:04,630 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@70cdbd18, cost 443.804us
recon_1             | 2023-07-13 21:33:04,631 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-07-13 21:33:22,637 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-13 21:33:22,637 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-13 21:33:23,181 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689284002638
recon_1             | 2023-07-13 21:33:23,206 [pool-16-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:33:23,207 [pool-16-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:33:23,315 [pool-16-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689284002638.
recon_1             | 2023-07-13 21:33:23,352 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-13 21:33:23,353 [pool-17-thread-1] INFO tasks.NSSummaryTask: Completed a reprocess run of NSSummaryTask
recon_1             | 2023-07-13 21:33:23,668 [pool-17-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-13 21:33:23,668 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:33:23,895 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:33:23,895 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.226 seconds to process 4 keys.
recon_1             | 2023-07-13 21:33:23,934 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-13 21:33:23,996 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | 2023-07-13 21:32:38,472 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-13 21:32:38,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:32:38,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-13 21:32:38,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-13 21:32:38,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-13 21:32:42,755 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-13 21:32:42,770 [IPC Server handler 0 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-13 21:32:42,770 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-13 21:33:00,638 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0cb8e1c0-f153-40ad-97c9-96ae05df0362, Nodes: 674ff7fc-a520-4090-839e-1ff0a8b3a1f6{ip: 172.22.0.7, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7c8eeed4-1425-4b6f-b367-50d5a0ac3f3c{ip: 172.22.0.3, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c89e3854-e92b-4097-a8a6-7827012e1c78{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c89e3854-e92b-4097-a8a6-7827012e1c78, CreationTimestamp2023-07-13T21:32:21.624Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:33:12,881 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.9
scm_1               | 2023-07-13 21:33:22,584 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.9
scm_1               | 2023-07-13 21:34:15,119 [IPC Server handler 25 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.9
scm_1               | 2023-07-13 21:34:25,366 [IPC Server handler 93 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.9
Attaching to xcompat_s3g_1, xcompat_old_client_1_0_0_1, xcompat_old_client_1_2_1_1, xcompat_new_client_1, xcompat_recon_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_1_0_1, xcompat_old_client_1_3_0_1, xcompat_om_1, xcompat_scm_1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:34:55,540 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 5cc2defedb2a/172.23.0.7
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.3.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_2          | STARTUP_MSG:   java = 11.0.14.1
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:34:55,675 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:34:56,058 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:34:56,625 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:34:57,339 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:34:57,339 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:34:58,512 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5cc2defedb2a ip:172.23.0.7
datanode_2          | 2023-07-13 21:35:00,259 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2          | 2023-07-13 21:35:01,444 [main] INFO reflections.Reflections: Reflections took 874 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_2          | 2023-07-13 21:35:02,103 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-13 21:35:03,362 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:35:03,460 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-13 21:35:03,461 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:35:03,468 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:35:03,523 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:35:03,581 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:35:03,582 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-13 21:35:03,593 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-13 21:35:03,593 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-13 21:35:03,605 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-13 21:35:03,693 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:35:03,717 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:35:13,208 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-13 21:35:13,844 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:35:14,461 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:35:15,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:35:15,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-13 21:35:15,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:35:15,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-13 21:35:15,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-13 21:35:15,052 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-13 21:35:15,052 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:35:15,053 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:35:15,053 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:35:15,055 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:35:15,113 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:35:15,117 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-13 21:35:15,139 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-13 21:35:17,674 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:35:17,677 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-13 21:35:17,687 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-13 21:35:17,691 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:17,694 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:35:17,720 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:35:17,838 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-13 21:35:18,911 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:35:18,952 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:35:19,348 [main] INFO util.log: Logging initialized @33408ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:35:20,305 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-13 21:35:20,345 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:35:20,463 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:35:20,479 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:35:20,496 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:35:20,509 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:35:21,108 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:34:54,817 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = d80200d16468/172.23.0.8
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.3.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_1          | STARTUP_MSG:   java = 11.0.14.1
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:34:54,855 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:34:55,119 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:34:55,986 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:34:56,967 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:34:56,967 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:34:57,850 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d80200d16468 ip:172.23.0.8
datanode_1          | 2023-07-13 21:34:59,768 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1          | 2023-07-13 21:35:01,243 [main] INFO reflections.Reflections: Reflections took 1097 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_1          | 2023-07-13 21:35:01,922 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-13 21:35:02,925 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:35:03,096 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-13 21:35:03,105 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:35:03,121 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:35:03,246 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:35:03,332 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:35:03,340 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-13 21:35:03,355 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-13 21:35:03,356 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-13 21:35:03,356 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-13 21:35:03,547 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:35:03,549 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:35:12,924 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-13 21:35:13,354 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:35:13,714 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:35:14,590 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:35:14,592 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-13 21:35:14,592 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:35:14,592 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-13 21:35:14,593 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-13 21:35:14,593 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-13 21:35:14,605 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:35:14,606 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:14,607 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:35:14,608 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:35:14,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:35:14,621 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-13 21:35:14,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-13 21:35:17,022 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-13 21:35:17,059 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-13 21:35:17,062 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-13 21:35:17,063 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:17,066 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:35:17,084 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:35:17,300 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-13 21:35:18,328 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:35:18,542 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:35:18,854 [main] INFO util.log: Logging initialized @33196ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:35:19,956 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-13 21:35:20,037 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:35:20,163 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:35:20,172 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:35:20,191 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:35:20,191 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:35:20,650 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-13 21:35:20,804 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1          | 2023-07-13 21:35:21,175 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:35:21,175 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:35:21,195 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-07-13 21:35:21,313 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e002356{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:35:21,323 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f3fc42f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:35:23,423 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34045582{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-15340997537428016671/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:35:23,461 [main] INFO server.AbstractConnector: Started ServerConnector@4017fe2c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:35:23,467 [main] INFO server.Server: Started @37808ms
datanode_1          | 2023-07-13 21:35:23,494 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:35:23,499 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-13 21:35:23,505 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:35:23,545 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-13 21:35:23,890 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7d9179ea] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-13 21:35:24,323 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.11:9891
datanode_1          | 2023-07-13 21:35:24,548 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:35:27,207 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:27,208 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:28,208 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:28,209 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:29,208 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:29,209 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:30,209 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:30,210 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:31,211 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:35:35,245 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From d80200d16468/172.23.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.8:46410 remote=recon/172.23.0.11:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.8:46410 remote=recon/172.23.0.11:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-07-13 21:35:36,220 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From d80200d16468/172.23.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.8:41510 remote=scm/172.23.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.8:41510 remote=scm/172.23.0.5:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-07-13 21:35:38,620 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-c3e54c1a-d96f-4ec2-adf7-53f50faf3515/container.db to cache
datanode_1          | 2023-07-13 21:35:38,622 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-c3e54c1a-d96f-4ec2-adf7-53f50faf3515/container.db for volume DS-c3e54c1a-d96f-4ec2-adf7-53f50faf3515
datanode_1          | 2023-07-13 21:35:38,623 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-13 21:35:38,631 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-13 21:35:38,835 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:38,898 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.RaftServer: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start RPC server
datanode_1          | 2023-07-13 21:35:38,914 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: GrpcService started, listening on 9858
datanode_1          | 2023-07-13 21:35:38,915 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: GrpcService started, listening on 9856
datanode_1          | 2023-07-13 21:35:38,923 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: GrpcService started, listening on 9857
datanode_1          | 2023-07-13 21:35:38,937 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:35:38,937 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-13 21:35:38,937 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:35:38,942 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: Started
datanode_1          | 2023-07-13 21:35:42,939 [Command processor thread] INFO server.RaftServer: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: addNew group-D8FB65656228:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-D8FB65656228:java.util.concurrent.CompletableFuture@78f48bad[Not completed]
datanode_1          | 2023-07-13 21:35:42,988 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: new RaftServerImpl for group-D8FB65656228:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:35:42,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:35:42,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:35:42,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:35:42,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:42,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:35:42,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:35:43,012 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: ConfigurationManager, init=-1: peers:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:35:43,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:35:43,042 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:35:43,042 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:35:43,063 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:43,066 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:35:43,066 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:35:43,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:35:43,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:35:43,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:35:43,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:35:43,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:35:43,228 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a49cb362-f1d9-45d8-a3ee-d8fb65656228 does not exist. Creating ...
datanode_1          | 2023-07-13 21:35:43,238 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a49cb362-f1d9-45d8-a3ee-d8fb65656228/in_use.lock acquired by nodename 7@d80200d16468
datanode_1          | 2023-07-13 21:35:43,257 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a49cb362-f1d9-45d8-a3ee-d8fb65656228 has been successfully formatted.
datanode_1          | 2023-07-13 21:35:43,270 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D8FB65656228: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:35:43,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:35:43,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:35:43,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:43,314 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:35:43,325 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:35:43,327 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:43,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:35:43,338 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:35:43,354 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a49cb362-f1d9-45d8-a3ee-d8fb65656228
datanode_1          | 2023-07-13 21:35:43,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:35:43,357 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:43,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:43,367 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:35:43,367 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:35:43,369 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:35:43,369 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:35:43,369 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:35:43,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:43,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:43,404 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:43,405 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:43,496 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:43,497 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:43,499 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: start as a follower, conf=-1: peers:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:43,499 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:35:43,500 [pool-22-thread-1] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState
datanode_1          | 2023-07-13 21:35:43,516 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:43,516 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:43,542 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D8FB65656228,id=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:43,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:43,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:35:43,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:35:43,546 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:35:43,660 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a49cb362-f1d9-45d8-a3ee-d8fb65656228
datanode_1          | 2023-07-13 21:35:43,667 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=a49cb362-f1d9-45d8-a3ee-d8fb65656228.
datanode_1          | 2023-07-13 21:35:43,671 [Command processor thread] INFO server.RaftServer: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: addNew group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-C1A24067B8E3:java.util.concurrent.CompletableFuture@28ecee4d[Not completed]
datanode_1          | 2023-07-13 21:35:43,676 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: new RaftServerImpl for group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:35:43,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:35:43,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:43,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:35:43,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:35:43,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:35:43,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:35:43,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:35:43,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:35:43,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:35:43,711 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 does not exist. Creating ...
datanode_1          | 2023-07-13 21:35:43,715 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/in_use.lock acquired by nodename 7@d80200d16468
datanode_1          | 2023-07-13 21:35:43,724 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 has been successfully formatted.
datanode_1          | 2023-07-13 21:35:43,724 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C1A24067B8E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:21,134 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2          | 2023-07-13 21:35:21,622 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:35:21,636 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:35:21,643 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-13 21:35:21,820 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3e1f1046{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:35:21,827 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@230a73f2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:35:24,116 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@26e8ff8c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-18419017345578518056/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-13 21:35:24,170 [main] INFO server.AbstractConnector: Started ServerConnector@30a7653e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:35:24,170 [main] INFO server.Server: Started @38231ms
datanode_2          | 2023-07-13 21:35:24,192 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:35:24,192 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:35:24,194 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:35:24,257 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-13 21:35:24,582 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5bbcf5b5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-13 21:35:24,928 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.11:9891
datanode_2          | 2023-07-13 21:35:25,624 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:35:27,710 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:27,726 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:28,711 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:28,727 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:29,714 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:29,728 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:30,715 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:31,730 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:35:34,765 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 5cc2defedb2a/172.23.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.7:55906 remote=recon/172.23.0.11:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.7:55906 remote=recon/172.23.0.11:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-07-13 21:35:36,777 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 5cc2defedb2a/172.23.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.7:33240 remote=scm/172.23.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:34:55,106 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 5dc7158196b9/172.23.0.6
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.3.0
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.7:33240 remote=scm/172.23.0.5:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-07-13 21:35:38,486 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-15863b52-d7ce-48ab-b7ba-855e17783e66/container.db to cache
datanode_2          | 2023-07-13 21:35:38,487 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-15863b52-d7ce-48ab-b7ba-855e17783e66/container.db for volume DS-15863b52-d7ce-48ab-b7ba-855e17783e66
datanode_2          | 2023-07-13 21:35:38,492 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-13 21:35:38,499 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-13 21:35:38,691 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5017a172-51c5-4d66-9290-f85658670909
datanode_2          | 2023-07-13 21:35:38,758 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.RaftServer: 5017a172-51c5-4d66-9290-f85658670909: start RPC server
datanode_2          | 2023-07-13 21:35:38,771 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: 5017a172-51c5-4d66-9290-f85658670909: GrpcService started, listening on 9858
datanode_2          | 2023-07-13 21:35:38,775 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: 5017a172-51c5-4d66-9290-f85658670909: GrpcService started, listening on 9856
datanode_2          | 2023-07-13 21:35:38,777 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: 5017a172-51c5-4d66-9290-f85658670909: GrpcService started, listening on 9857
datanode_2          | 2023-07-13 21:35:38,798 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5017a172-51c5-4d66-9290-f85658670909 is started using port 9858 for RATIS
datanode_2          | 2023-07-13 21:35:38,798 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5017a172-51c5-4d66-9290-f85658670909 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-13 21:35:38,798 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5017a172-51c5-4d66-9290-f85658670909 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-13 21:35:38,798 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5017a172-51c5-4d66-9290-f85658670909: Started
datanode_2          | 2023-07-13 21:35:43,737 [Command processor thread] INFO server.RaftServer: 5017a172-51c5-4d66-9290-f85658670909: addNew group-963CDD6D806A:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-963CDD6D806A:java.util.concurrent.CompletableFuture@185ba1d8[Not completed]
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_3          | STARTUP_MSG:   java = 11.0.14.1
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:34:55,157 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:34:55,809 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:34:56,515 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:34:57,210 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:34:57,211 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:34:58,331 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5dc7158196b9 ip:172.23.0.6
datanode_3          | 2023-07-13 21:34:59,747 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3          | 2023-07-13 21:35:01,024 [main] INFO reflections.Reflections: Reflections took 948 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_3          | 2023-07-13 21:35:01,775 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-13 21:35:02,596 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:35:02,765 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-13 21:35:02,776 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:35:02,785 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:35:02,962 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:35:03,183 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:35:03,199 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-13 21:35:03,225 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-13 21:35:03,225 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-13 21:35:03,229 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-13 21:35:03,391 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:35:03,406 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:35:12,185 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-13 21:35:12,831 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:35:13,499 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:35:14,222 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:35:14,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-13 21:35:14,342 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:35:14,342 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-13 21:35:14,343 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-07-13 21:35:14,343 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-13 21:35:14,343 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:35:14,344 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:14,346 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:35:14,354 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:35:14,380 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-13 21:35:14,383 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-13 21:35:14,384 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-07-13 21:35:16,600 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-13 21:35:16,639 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-13 21:35:16,643 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-07-13 21:35:16,646 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:16,652 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:35:16,671 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:35:16,811 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-13 21:35:17,836 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:35:18,006 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:35:18,463 [main] INFO util.log: Logging initialized @32552ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:35:19,544 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-13 21:35:19,586 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:35:19,657 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:35:19,683 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:35:19,683 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:35:43,828 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909: new RaftServerImpl for group-963CDD6D806A:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:35:43,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:35:43,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:35:43,838 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:35:43,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:43,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:35:43,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:35:43,867 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:35:43,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:35:43,904 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:35:43,917 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:35:43,947 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:43,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:35:43,951 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:35:44,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:35:44,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:35:44,098 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:35:44,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:35:44,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:35:44,104 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/80c77ada-959c-48f0-b5a5-963cdd6d806a does not exist. Creating ...
datanode_2          | 2023-07-13 21:35:44,125 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/80c77ada-959c-48f0-b5a5-963cdd6d806a/in_use.lock acquired by nodename 6@5cc2defedb2a
datanode_2          | 2023-07-13 21:35:44,151 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/80c77ada-959c-48f0-b5a5-963cdd6d806a has been successfully formatted.
datanode_2          | 2023-07-13 21:35:44,188 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-963CDD6D806A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:35:44,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:35:44,279 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:35:44,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:35:44,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:35:44,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:35:44,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,331 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:35:44,332 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:35:44,349 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/80c77ada-959c-48f0-b5a5-963cdd6d806a
datanode_2          | 2023-07-13 21:35:44,352 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:35:44,353 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:35:44,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:35:44,357 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:35:44,360 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:35:44,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:35:44,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:35:44,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:44,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:44,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:44,428 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:44,428 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:44,433 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:44,433 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:35:44,435 [pool-22-thread-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState
datanode_3          | 2023-07-13 21:35:19,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:35:20,264 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-13 21:35:20,268 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3          | 2023-07-13 21:35:20,596 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:35:20,596 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:35:20,598 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-07-13 21:35:20,681 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79afa369{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:35:20,740 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79ba0a6f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:35:23,582 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5a4dda2{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-12262962924461227465/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:35:23,646 [main] INFO server.AbstractConnector: Started ServerConnector@4f363abd{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:35:23,646 [main] INFO server.Server: Started @37736ms
datanode_3          | 2023-07-13 21:35:23,656 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-13 21:35:23,656 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-13 21:35:23,663 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:35:23,685 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-13 21:35:23,905 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@579ac36a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-13 21:35:24,515 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.11:9891
datanode_3          | 2023-07-13 21:35:24,799 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:35:27,303 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:27,304 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:28,304 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:28,305 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:29,306 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:29,306 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:30,309 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:30,310 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.11:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:31,310 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:35:35,343 [EndpointStateMachine task thread for recon/172.23.0.11:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 5dc7158196b9/172.23.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.6:59404 remote=recon/172.23.0.11:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.6:59404 remote=recon/172.23.0.11:9891]
datanode_2          | 2023-07-13 21:35:44,443 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:44,443 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:44,452 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-963CDD6D806A,id=5017a172-51c5-4d66-9290-f85658670909
datanode_2          | 2023-07-13 21:35:44,454 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:44,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:35:44,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:35:44,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:35:44,502 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=80c77ada-959c-48f0-b5a5-963cdd6d806a
datanode_2          | 2023-07-13 21:35:44,503 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=80c77ada-959c-48f0-b5a5-963cdd6d806a.
datanode_2          | 2023-07-13 21:35:44,503 [Command processor thread] INFO server.RaftServer: 5017a172-51c5-4d66-9290-f85658670909: addNew group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-C1A24067B8E3:java.util.concurrent.CompletableFuture@62b30268[Not completed]
datanode_2          | 2023-07-13 21:35:44,508 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909: new RaftServerImpl for group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:35:44,510 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:35:44,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:35:44,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:35:44,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:44,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:35:44,515 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:35:44,515 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:35:44,515 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:35:44,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:35:44,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:35:44,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:44,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:35:44,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:35:44,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:35:44,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:35:44,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:35:44,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:35:44,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:35:44,528 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 does not exist. Creating ...
datanode_2          | 2023-07-13 21:35:44,537 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/in_use.lock acquired by nodename 6@5cc2defedb2a
datanode_2          | 2023-07-13 21:35:44,541 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 has been successfully formatted.
datanode_2          | 2023-07-13 21:35:44,542 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C1A24067B8E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:35:44,542 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:35:44,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:35:44,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:35:44,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:35:44,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:35:44,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:35:44,546 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:35:44,547 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:35:43,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:35:43,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:35:43,739 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:43,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:43,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:43,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:43,747 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:43,747 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:43,755 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C1A24067B8E3,id=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:35:43,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:35:43,762 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:43,770 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_1          | 2023-07-13 21:35:43,770 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:46,194 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3.
datanode_1          | 2023-07-13 21:35:46,195 [Command processor thread] INFO server.RaftServer: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: addNew group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-106F6D0C2D2D:java.util.concurrent.CompletableFuture@6f9d0823[Not completed]
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: new RaftServerImpl for group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:35:46,202 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:35:46,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:35:46,204 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d does not exist. Creating ...
datanode_1          | 2023-07-13 21:35:46,212 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/in_use.lock acquired by nodename 7@d80200d16468
datanode_1          | 2023-07-13 21:35:46,218 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d has been successfully formatted.
datanode_1          | 2023-07-13 21:35:46,220 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-106F6D0C2D2D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:35:46,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:35:46,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:35:46,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:46,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:35:46,231 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:35:46,231 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:46,231 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:35:46,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:35:46,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:35:46,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:35:46,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:46,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:46,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:35:46,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:46,281 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:46,288 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:35:46,290 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:46,291 [pool-22-thread-1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:35:46,291 [pool-22-thread-1] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState
datanode_1          | 2023-07-13 21:35:46,301 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-106F6D0C2D2D,id=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:46,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:35:46,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:35:46,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:35:46,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:35:46,303 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:46,342 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d
datanode_2          | 2023-07-13 21:35:44,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:35:44,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:35:44,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:35:44,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:35:44,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:35:44,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:35:44,567 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:35:44,569 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:44,578 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:44,581 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:44,581 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:44,581 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:44,581 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:44,582 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:44,588 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:35:44,590 [pool-22-thread-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState
datanode_2          | 2023-07-13 21:35:44,616 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:44,616 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:44,617 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C1A24067B8E3,id=5017a172-51c5-4d66-9290-f85658670909
datanode_2          | 2023-07-13 21:35:44,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:44,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:35:44,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:35:44,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:35:44,637 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_2          | 2023-07-13 21:35:46,393 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3.
datanode_2          | 2023-07-13 21:35:46,394 [Command processor thread] INFO server.RaftServer: 5017a172-51c5-4d66-9290-f85658670909: addNew group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-106F6D0C2D2D:java.util.concurrent.CompletableFuture@246fe7e0[Not completed]
datanode_2          | 2023-07-13 21:35:46,396 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909: new RaftServerImpl for group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:35:46,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:35:46,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:35:46,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:35:46,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:46,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:35:46,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:35:46,401 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:35:46,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:35:46,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:35:46,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:35:46,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:35:46,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:35:46,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:35:46,404 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:35:46,407 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:35:46,407 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:35:46,408 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:35:46,408 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:35:46,408 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d does not exist. Creating ...
datanode_2          | 2023-07-13 21:35:46,410 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/in_use.lock acquired by nodename 6@5cc2defedb2a
datanode_2          | 2023-07-13 21:35:46,411 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d has been successfully formatted.
datanode_2          | 2023-07-13 21:35:46,412 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-106F6D0C2D2D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:35:46,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:35:46,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:35:46,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:35:46,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:35:46,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:35:46,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:46,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:35:46,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:35:46,444 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d
datanode_2          | 2023-07-13 21:35:46,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:35:46,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:35:46,446 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:35:46,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:35:46,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:46,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:35:46,453 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:46,453 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:46,453 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:35:46,501 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:46,507 [pool-22-thread-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:35:46,507 [pool-22-thread-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState
datanode_2          | 2023-07-13 21:35:46,520 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-106F6D0C2D2D,id=5017a172-51c5-4d66-9290-f85658670909
datanode_2          | 2023-07-13 21:35:46,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:35:46,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:35:46,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:35:46,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:35:46,521 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:46,540 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:46,554 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d
datanode_2          | 2023-07-13 21:35:46,650 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d.
datanode_1          | 2023-07-13 21:35:46,343 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:46,572 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d.
datanode_1          | 2023-07-13 21:35:48,529 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO impl.FollowerState: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5029202243ns, electionTimeout:5012ms
datanode_1          | 2023-07-13 21:35:48,530 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState
datanode_1          | 2023-07-13 21:35:48,530 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:35:48,533 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:35:48,534 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1
datanode_1          | 2023-07-13 21:35:48,546 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:48,547 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-13 21:35:48,548 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1
datanode_1          | 2023-07-13 21:35:48,549 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:35:48,549 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D8FB65656228 with new leaderId: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:48,550 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: change Leader from null to d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 at term 1 for becomeLeader, leader elected after 5487ms
datanode_1          | 2023-07-13 21:35:48,575 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:35:48,583 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:48,583 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:35:48,590 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:35:48,594 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:35:48,595 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:35:48,600 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:48,603 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:35:48,608 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderStateImpl
datanode_1          | 2023-07-13 21:35:48,631 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:35:48,668 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-LeaderElection1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228: set configuration 0: peers:[d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:48,712 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-D8FB65656228-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a49cb362-f1d9-45d8-a3ee-d8fb65656228/current/log_inprogress_0
datanode_1          | 2023-07-13 21:35:48,876 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5120396387ns, electionTimeout:5104ms
datanode_1          | 2023-07-13 21:35:48,877 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState
datanode_1          | 2023-07-13 21:35:48,877 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:35:48,877 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:35:48,878 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-07-13 21:35:36,326 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 5dc7158196b9/172.23.0.6 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.6:40992 remote=scm/172.23.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.6:40992 remote=scm/172.23.0.5:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-07-13 21:35:38,501 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-a0938a9a-fbb3-46d6-bd38-0e51c29b75f1/container.db to cache
datanode_3          | 2023-07-13 21:35:38,501 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/DS-a0938a9a-fbb3-46d6-bd38-0e51c29b75f1/container.db for volume DS-a0938a9a-fbb3-46d6-bd38-0e51c29b75f1
datanode_3          | 2023-07-13 21:35:38,503 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:35:38,507 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-13 21:35:38,655 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:38,703 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.RaftServer: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start RPC server
datanode_3          | 2023-07-13 21:35:38,714 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: GrpcService started, listening on 9858
datanode_3          | 2023-07-13 21:35:38,716 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: GrpcService started, listening on 9856
datanode_3          | 2023-07-13 21:35:38,717 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO server.GrpcService: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: GrpcService started, listening on 9857
datanode_3          | 2023-07-13 21:35:38,774 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c6e41c66-4ffe-43e7-a261-3d6c1c36e141 is started using port 9858 for RATIS
datanode_3          | 2023-07-13 21:35:38,774 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c6e41c66-4ffe-43e7-a261-3d6c1c36e141 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-13 21:35:38,774 [EndpointStateMachine task thread for scm/172.23.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c6e41c66-4ffe-43e7-a261-3d6c1c36e141 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-13 21:35:38,775 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c6e41c66-4ffe-43e7-a261-3d6c1c36e141: Started
datanode_3          | 2023-07-13 21:35:43,139 [Command processor thread] INFO server.RaftServer: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: addNew group-AE18E71EF787:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-AE18E71EF787:java.util.concurrent.CompletableFuture@529ae86b[Not completed]
datanode_3          | 2023-07-13 21:35:43,210 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: new RaftServerImpl for group-AE18E71EF787:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:35:43,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:35:43,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:35:43,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:35:43,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:43,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:35:43,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:35:43,237 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: ConfigurationManager, init=-1: peers:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:35:43,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:35:43,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:35:43,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:35:43,273 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:43,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:35:43,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:35:43,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:35:43,557 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:35:43,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:35:43,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:35:43,586 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:35:43,587 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3832adf0-273d-4d89-b2d5-ae18e71ef787 does not exist. Creating ...
datanode_3          | 2023-07-13 21:35:43,607 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3832adf0-273d-4d89-b2d5-ae18e71ef787/in_use.lock acquired by nodename 7@5dc7158196b9
datanode_3          | 2023-07-13 21:35:43,638 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3832adf0-273d-4d89-b2d5-ae18e71ef787 has been successfully formatted.
datanode_3          | 2023-07-13 21:35:43,659 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-AE18E71EF787: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:35:43,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:35:43,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:35:43,739 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:43,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:35:43,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:35:43,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:43,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:35:43,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:35:43,855 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3832adf0-273d-4d89-b2d5-ae18e71ef787
datanode_3          | 2023-07-13 21:35:43,861 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:35:43,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:43,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:43,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:35:43,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:35:43,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:35:43,874 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:35:43,875 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:35:43,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:43,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:43,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:43,910 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:43,920 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:43,930 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:43,940 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: start as a follower, conf=-1: peers:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:43,941 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:35:43,942 [pool-22-thread-1] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState
datanode_3          | 2023-07-13 21:35:43,943 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:43,957 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:43,958 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AE18E71EF787,id=c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:43,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:43,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:35:43,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:35:43,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:35:44,052 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3832adf0-273d-4d89-b2d5-ae18e71ef787
datanode_3          | 2023-07-13 21:35:44,054 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3832adf0-273d-4d89-b2d5-ae18e71ef787.
datanode_3          | 2023-07-13 21:35:44,062 [Command processor thread] INFO server.RaftServer: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: addNew group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-C1A24067B8E3:java.util.concurrent.CompletableFuture@6c805d09[Not completed]
datanode_3          | 2023-07-13 21:35:44,094 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: new RaftServerImpl for group-C1A24067B8E3:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:35:44,098 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:35:44,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:35:44,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:35:44,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:44,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:35:44,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:35:44,101 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:35:44,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:35:44,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:35:44,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:35:44,107 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:44,107 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:35:44,107 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:35:44,108 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:35:44,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:35:44,120 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:35:44,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:35:44,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:35:44,133 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 does not exist. Creating ...
datanode_3          | 2023-07-13 21:35:44,140 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/in_use.lock acquired by nodename 7@5dc7158196b9
datanode_3          | 2023-07-13 21:35:44,149 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 has been successfully formatted.
datanode_3          | 2023-07-13 21:35:44,170 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C1A24067B8E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:35:44,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:35:44,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:35:44,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:44,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:35:44,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:35:44,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:35:48,882 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:48,892 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:48,892 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:48,892 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 5017a172-51c5-4d66-9290-f85658670909
datanode_1          | 2023-07-13 21:35:48,897 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_1          | 2023-07-13 21:35:49,056 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:35:49,057 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 0: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-5017a172-51c5-4d66-9290-f85658670909#0:OK-t1
datanode_1          | 2023-07-13 21:35:49,057 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 1: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:FAIL-t1
datanode_1          | 2023-07-13 21:35:49,057 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-13 21:35:49,062 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-07-13 21:35:49,062 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2
datanode_1          | 2023-07-13 21:35:49,062 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState
datanode_1          | 2023-07-13 21:35:49,064 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:49,064 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:51,373 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO impl.FollowerState: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5082394758ns, electionTimeout:5030ms
datanode_1          | 2023-07-13 21:35:51,374 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState
datanode_1          | 2023-07-13 21:35:51,374 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:35:51,374 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-13 21:35:51,374 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-FollowerState] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3
datanode_1          | 2023-07-13 21:35:51,377 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:51,379 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:51,379 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:51,393 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.LeaderElection:   Response 0: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:OK-t1
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.LeaderElection: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-106F6D0C2D2D with new leaderId: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: change Leader from null to d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 at term 1 for becomeLeader, leader elected after 5190ms
datanode_1          | 2023-07-13 21:35:51,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:35:51,395 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:51,395 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:34:54,080 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = aa86cb06fbd8/172.23.0.3
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-07-13 21:34:54,151 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:35:02,719 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-13 21:35:05,810 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:35:06,171 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.3:9862
om_1                | 2023-07-13 21:35:06,172 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:35:06,172 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:35:06,334 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:35:07,622 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863]
om_1                | 2023-07-13 21:35:11,611 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:13,613 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:15,620 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:17,621 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:19,623 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:21,624 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:23,632 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:25,634 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:27,636 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:29,639 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From aa86cb06fbd8/172.23.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:35,339 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c59164db-77af-49fe-b118-48e9a6e808f8 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-13 21:35:37,366 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c59164db-77af-49fe-b118-48e9a6e808f8 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70;layoutVersion=3
om_1                | 2023-07-13 21:35:39,450 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at aa86cb06fbd8/172.23.0.3
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:35:41,256 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = aa86cb06fbd8/172.23.0.3
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.3.0
datanode_2          | 2023-07-13 21:35:48,915 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: receive requestVote(ELECTION, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, group-C1A24067B8E3, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:35:48,921 [grpc-default-executor-1] INFO impl.VoteContext: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FOLLOWER: accept ELECTION from d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-13 21:35:48,922 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_2          | 2023-07-13 21:35:48,922 [grpc-default-executor-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState
datanode_2          | 2023-07-13 21:35:48,923 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState was interrupted
datanode_2          | 2023-07-13 21:35:48,923 [grpc-default-executor-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState
datanode_2          | 2023-07-13 21:35:48,932 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:48,932 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:48,950 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3 replies to ELECTION vote request: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-5017a172-51c5-4d66-9290-f85658670909#0:OK-t1. Peer's state: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3:t1, leader=null, voted=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, raftlog=Memoized:5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:49,509 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO impl.FollowerState: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073388159ns, electionTimeout:5064ms
datanode_2          | 2023-07-13 21:35:49,509 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState
datanode_2          | 2023-07-13 21:35:49,510 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:35:49,516 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:35:49,516 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-FollowerState] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1
datanode_2          | 2023-07-13 21:35:49,532 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO impl.LeaderElection: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:49,533 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO impl.LeaderElection: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-13 21:35:49,534 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1
datanode_2          | 2023-07-13 21:35:49,534 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:35:49,535 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-963CDD6D806A with new leaderId: 5017a172-51c5-4d66-9290-f85658670909
datanode_2          | 2023-07-13 21:35:49,536 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: change Leader from null to 5017a172-51c5-4d66-9290-f85658670909 at term 1 for becomeLeader, leader elected after 5587ms
datanode_2          | 2023-07-13 21:35:49,560 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:35:49,565 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:35:49,568 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:35:49,572 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:35:49,573 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:35:49,573 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:35:49,580 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:35:49,587 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-13 21:35:49,589 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderStateImpl
datanode_2          | 2023-07-13 21:35:49,628 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:35:51,396 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:35:51,396 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:35:51,397 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:35:51,397 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:35:51,397 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:35:51,414 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:35:51,414 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:51,414 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:35:51,417 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:35:51,438 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:35:51,439 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:35:51,441 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:35:51,441 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-13 21:35:51,443 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:35:51,444 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:35:51,445 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:35:51,445 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:35:51,446 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:35:51,446 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:35:51,446 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:35:51,447 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-13 21:35:51,448 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderStateImpl
datanode_1          | 2023-07-13 21:35:51,449 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:35:51,451 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/current/log_inprogress_0
datanode_1          | 2023-07-13 21:35:51,459 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D-LeaderElection3] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-106F6D0C2D2D: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:54,090 [grpc-default-executor-0] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: receive requestVote(ELECTION, c6e41c66-4ffe-43e7-a261-3d6c1c36e141, group-C1A24067B8E3, 2, (t:0, i:0))
datanode_1          | 2023-07-13 21:35:54,092 [grpc-default-executor-0] INFO impl.VoteContext: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FOLLOWER: accept ELECTION from c6e41c66-4ffe-43e7-a261-3d6c1c36e141: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:35:54,096 [grpc-default-executor-0] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_1          | 2023-07-13 21:35:54,096 [grpc-default-executor-0] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: shutdown d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState
datanode_1          | 2023-07-13 21:35:54,096 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState was interrupted
datanode_1          | 2023-07-13 21:35:54,097 [grpc-default-executor-0] INFO impl.RoleInfo: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: start d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState
datanode_1          | 2023-07-13 21:35:54,109 [grpc-default-executor-0] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3 replies to ELECTION vote request: c6e41c66-4ffe-43e7-a261-3d6c1c36e141<-d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7#0:OK-t2. Peer's state: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3:t2, leader=null, voted=c6e41c66-4ffe-43e7-a261-3d6c1c36e141, raftlog=Memoized:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:54,111 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:35:54,112 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:35:54,176 [grpc-default-executor-0] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: receive requestVote(ELECTION, 5017a172-51c5-4d66-9290-f85658670909, group-C1A24067B8E3, 2, (t:0, i:0))
datanode_1          | 2023-07-13 21:35:54,176 [grpc-default-executor-0] INFO impl.VoteContext: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-FOLLOWER: reject ELECTION from 5017a172-51c5-4d66-9290-f85658670909: already has voted for c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at current term 2
datanode_1          | 2023-07-13 21:35:54,177 [grpc-default-executor-0] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3 replies to ELECTION vote request: 5017a172-51c5-4d66-9290-f85658670909<-d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7#0:FAIL-t2. Peer's state: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3:t2, leader=null, voted=c6e41c66-4ffe-43e7-a261-3d6c1c36e141, raftlog=Memoized:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:54,368 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C1A24067B8E3 with new leaderId: c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_1          | 2023-07-13 21:35:54,369 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7-server-thread1] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: change Leader from null to c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at term 2 for appendEntries, leader elected after 10677ms
datanode_1          | 2023-07-13 21:35:54,392 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7-server-thread3] INFO server.RaftServer$Division: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:35:54,394 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7-server-thread3] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:35:54,404 [d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7@group-C1A24067B8E3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/current/log_inprogress_0
datanode_3          | 2023-07-13 21:35:44,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:35:44,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:35:44,179 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_3          | 2023-07-13 21:35:44,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:35:44,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:44,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:44,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:35:44,181 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:35:44,181 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:35:44,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:35:44,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:35:44,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:44,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:44,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:44,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:44,189 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:44,190 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:44,190 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:44,190 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:35:44,192 [pool-22-thread-1] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState
datanode_3          | 2023-07-13 21:35:44,209 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C1A24067B8E3,id=c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:44,215 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:44,215 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:35:44,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:35:44,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:35:44,219 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3
datanode_3          | 2023-07-13 21:35:44,270 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:44,270 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:46,228 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3.
datanode_3          | 2023-07-13 21:35:46,241 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: new RaftServerImpl for group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:35:46,241 [Command processor thread] INFO server.RaftServer: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: addNew group-106F6D0C2D2D:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-106F6D0C2D2D:java.util.concurrent.CompletableFuture@342b6dba[Not completed]
datanode_3          | 2023-07-13 21:35:46,247 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:35:46,247 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:35:46,247 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:35:46,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:46,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:35:46,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:35:46,248 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: ConfigurationManager, init=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
datanode_2          | 2023-07-13 21:35:49,669 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-LeaderElection1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:49,722 [5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-963CDD6D806A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/80c77ada-959c-48f0-b5a5-963cdd6d806a/current/log_inprogress_0
datanode_2          | 2023-07-13 21:35:51,394 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: receive requestVote(ELECTION, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, group-106F6D0C2D2D, 1, (t:0, i:0))
datanode_2          | 2023-07-13 21:35:51,395 [grpc-default-executor-1] INFO impl.VoteContext: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FOLLOWER: accept ELECTION from d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:35:51,395 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_2          | 2023-07-13 21:35:51,395 [grpc-default-executor-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState
datanode_2          | 2023-07-13 21:35:51,396 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState] INFO impl.FollowerState: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState was interrupted
datanode_2          | 2023-07-13 21:35:51,396 [grpc-default-executor-1] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState
datanode_2          | 2023-07-13 21:35:51,402 [grpc-default-executor-1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D replies to ELECTION vote request: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-5017a172-51c5-4d66-9290-f85658670909#0:OK-t1. Peer's state: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D:t1, leader=null, voted=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, raftlog=Memoized:5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:51,408 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:51,408 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:51,517 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-106F6D0C2D2D with new leaderId: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_2          | 2023-07-13 21:35:51,518 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: change Leader from null to d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 at term 1 for appendEntries, leader elected after 5115ms
datanode_2          | 2023-07-13 21:35:51,520 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:51,522 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:35:51,524 [5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-106F6D0C2D2D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/current/log_inprogress_0
datanode_2          | 2023-07-13 21:35:54,098 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163270814ns, electionTimeout:5158ms
datanode_2          | 2023-07-13 21:35:54,099 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState
datanode_2          | 2023-07-13 21:35:54,099 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-07-13 21:35:54,099 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-13 21:35:54,099 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2
datanode_2          | 2023-07-13 21:35:54,106 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:54,114 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:54,115 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:54,115 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_2          | 2023-07-13 21:35:54,124 [grpc-default-executor-0] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: receive requestVote(ELECTION, c6e41c66-4ffe-43e7-a261-3d6c1c36e141, group-C1A24067B8E3, 2, (t:0, i:0))
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:34:53,633 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = dafb6e0e5511/172.23.0.11
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.3.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1             | STARTUP_MSG:   java = 11.0.14.1
recon_1             | ************************************************************/
recon_1             | 2023-07-13 21:34:53,741 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-13 21:34:58,225 [main] INFO reflections.Reflections: Reflections took 881 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1             | 2023-07-13 21:35:03,058 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-13 21:35:04,747 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:35:13,845 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:35:15,852 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-13 21:35:15,873 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.02 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-13 21:35:15,947 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:35:16,311 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:35:16,315 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-13 21:35:19,996 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 2023-07-13 21:35:23,638 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
datanode_3          | 2023-07-13 21:35:46,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:35:46,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:35:46,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:35:46,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:35:46,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:35:46,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:35:46,250 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:35:46,250 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:35:46,250 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:35:46,250 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:35:46,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:35:46,251 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d does not exist. Creating ...
datanode_3          | 2023-07-13 21:35:46,254 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/in_use.lock acquired by nodename 7@5dc7158196b9
datanode_3          | 2023-07-13 21:35:46,290 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d has been successfully formatted.
datanode_3          | 2023-07-13 21:35:46,296 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-106F6D0C2D2D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:35:46,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:35:46,313 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:35:46,313 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:46,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:35:46,317 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:35:46,317 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:46,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:35:46,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:35:46,319 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d
datanode_3          | 2023-07-13 21:35:46,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:35:46,320 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:46,320 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:46,320 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:35:46,320 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:35:46,321 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:35:46,321 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:35:46,321 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:35:46,322 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:35:46,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:46,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:35:46,328 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:46,329 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:46,329 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:35:46,330 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: start as a follower, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:46,330 [pool-22-thread-1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:35:46,331 [pool-22-thread-1] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState
datanode_3          | 2023-07-13 21:35:46,334 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-106F6D0C2D2D,id=c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:46,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:35:46,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:35:46,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:35:46,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:35:46,335 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:46,336 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d
recon_1             | 2023-07-13 21:35:23,764 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:35:23,921 [main] INFO util.log: Logging initialized @38268ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:35:24,481 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-07-13 21:35:24,513 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-13 21:35:24,578 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:35:24,592 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:35:24,593 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-13 21:35:24,605 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:35:25,498 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-13 21:35:26,737 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-13 21:35:26,759 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-13 21:35:26,776 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-13 21:35:26,812 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-13 21:35:26,812 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-13 21:35:28,104 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:35:28,299 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:35:28,471 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1             | 2023-07-13 21:35:28,473 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-13 21:35:28,578 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:35:28,844 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1             | 2023-07-13 21:35:28,991 [main] INFO reflections.Reflections: Reflections took 137 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1             | 2023-07-13 21:35:29,060 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-13 21:35:29,154 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-13 21:35:29,169 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-13 21:35:29,174 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-13 21:35:29,231 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-13 21:35:29,322 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:35:29,425 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-13 21:35:29,496 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-13 21:35:29,841 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-13 21:35:29,862 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-13 21:35:30,085 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:35:30,106 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:35:30,106 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-13 21:35:30,603 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-13 21:35:30,605 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1             | 2023-07-13 21:35:30,686 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-13 21:35:30,686 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-13 21:35:30,691 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-07-13 21:35:30,713 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1b37fbec{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:35:30,716 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@357f6391{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:35:35,220 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7d49fe37{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-9132971329002794720/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1             | 2023-07-13 21:35:35,275 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1a1f79ce{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-13 21:35:35,275 [Listener at 0.0.0.0/9891] INFO server.Server: Started @49622ms
recon_1             | 2023-07-13 21:35:35,283 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-13 21:35:35,283 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-13 21:35:35,284 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-13 21:35:35,285 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-13 21:35:35,295 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-13 21:35:35,303 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:35:35,303 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-13 21:35:35,303 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-07-13 21:35:41,263 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:35:42,324 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-13 21:35:42,774 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:35:42,824 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.3:9862
om_1                | 2023-07-13 21:35:42,825 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:35:42,825 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:35:42,849 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:35:42,877 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1                | 2023-07-13 21:35:43,931 [main] INFO reflections.Reflections: Reflections took 974 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1                | 2023-07-13 21:35:44,005 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:35:44,973 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863]
om_1                | 2023-07-13 21:35:45,119 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9863]
om_1                | 2023-07-13 21:35:46,804 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:35:46,953 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-13 21:35:46,954 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-13 21:35:47,299 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-07-13 21:35:47,370 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:35:47,403 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:35:47,403 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-13 21:35:47,420 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-13 21:35:47,429 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:35:47,460 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-13 21:35:47,470 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-13 21:35:47,506 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-13 21:35:47,609 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:35:47,614 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-13 21:35:47,614 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:35:47,614 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-13 21:35:47,615 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-07-13 21:35:47,615 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:35:47,616 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-13 21:35:47,618 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:35:47,619 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-13 21:35:47,619 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-13 21:35:47,629 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-13 21:35:47,634 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-13 21:35:47,635 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-13 21:35:47,829 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-13 21:35:47,831 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-07-13 21:35:47,831 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-07-13 21:35:47,832 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:35:47,832 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:35:47,839 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:35:47,853 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@364fd4ae[Not completed]
om_1                | 2023-07-13 21:35:47,853 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-13 21:35:47,873 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-13 21:35:47,883 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-07-13 21:35:47,884 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-13 21:35:47,885 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-13 21:35:47,885 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-13 21:35:47,885 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:35:47,886 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:35:47,886 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1             | 2023-07-13 21:35:35,303 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-13 21:35:35,307 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:35:37,797 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c59164db-77af-49fe-b118-48e9a6e808f8 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.5:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-07-13 21:35:39,927 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-07-13 21:35:39,928 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:35:39,928 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-07-13 21:35:39,929 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:35:39,956 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:35:40,120 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-13 21:35:40,215 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-13 21:35:40,215 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-13 21:35:40,310 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-13 21:35:40,311 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-13 21:35:40,291 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:35:40,317 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 83 milliseconds.
recon_1             | 2023-07-13 21:35:40,722 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.7:41948: output error
recon_1             | 2023-07-13 21:35:40,726 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.6:37366: output error
recon_1             | 2023-07-13 21:35:40,727 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.8:46410: output error
recon_1             | 2023-07-13 21:35:40,726 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.6:59404: output error
recon_1             | 2023-07-13 21:35:40,728 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:40,729 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:40,729 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:40,732 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:40,733 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.7:55906: output error
recon_1             | 2023-07-13 21:35:40,735 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:40,746 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.8:56602: output error
recon_1             | 2023-07-13 21:35:40,746 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-13 21:35:41,934 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
recon_1             | 2023-07-13 21:35:41,965 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:41,966 [IPC Server handler 10 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c6e41c66-4ffe-43e7-a261-3d6c1c36e141
om_1                | 2023-07-13 21:35:47,896 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-13 21:35:47,897 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:35:47,908 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-13 21:35:47,909 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-13 21:35:47,932 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-13 21:35:47,943 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-13 21:35:47,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-13 21:35:48,108 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-13 21:35:48,109 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-07-13 21:35:48,111 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-07-13 21:35:48,113 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-13 21:35:48,114 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-13 21:35:48,510 [main] INFO reflections.Reflections: Reflections took 595 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om_1                | 2023-07-13 21:35:48,737 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-13 21:35:48,748 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-13 21:35:48,915 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-13 21:35:48,944 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:35:48,944 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-13 21:35:49,128 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.23.0.3:9862
om_1                | 2023-07-13 21:35:49,128 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-13 21:35:49,131 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-13 21:35:49,144 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@aa86cb06fbd8
om_1                | 2023-07-13 21:35:49,175 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-13 21:35:49,179 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:35:49,195 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-13 21:35:49,196 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:35:49,197 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-13 21:35:49,198 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-13 21:35:49,202 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:35:49,218 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-13 21:35:49,218 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-13 21:35:49,227 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-13 21:35:49,228 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:35:49,236 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:35:49,237 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:35:49,239 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-13 21:35:49,239 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-13 21:35:49,240 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-13 21:35:49,240 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-13 21:35:49,244 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-13 21:35:49,260 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-13 21:35:49,261 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-13 21:35:49,261 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-13 21:35:49,262 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:35:49,271 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:35:49,271 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:35:49,274 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:35:49,274 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-13 21:35:49,276 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:35:49,279 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:35:49,280 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:46,361 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:46,559 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d.
datanode_3          | 2023-07-13 21:35:48,956 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: receive requestVote(ELECTION, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, group-C1A24067B8E3, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:35:48,958 [grpc-default-executor-0] INFO impl.VoteContext: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FOLLOWER: reject ELECTION from d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-13 21:35:48,959 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_3          | 2023-07-13 21:35:48,960 [grpc-default-executor-0] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState
datanode_3          | 2023-07-13 21:35:48,962 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState was interrupted
datanode_3          | 2023-07-13 21:35:48,963 [grpc-default-executor-0] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState
datanode_3          | 2023-07-13 21:35:48,965 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO impl.FollowerState: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5023218292ns, electionTimeout:5004ms
datanode_3          | 2023-07-13 21:35:48,971 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:48,972 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:48,972 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState
datanode_3          | 2023-07-13 21:35:48,972 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:35:48,975 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:35:48,975 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-FollowerState] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1
datanode_3          | 2023-07-13 21:35:48,997 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO impl.LeaderElection: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:49,003 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO impl.LeaderElection: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-13 21:35:49,013 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3 replies to ELECTION vote request: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:FAIL-t1. Peer's state: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3:t1, leader=null, voted=null, raftlog=Memoized:c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:49,016 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1
datanode_3          | 2023-07-13 21:35:49,019 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-13 21:35:49,024 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-AE18E71EF787 with new leaderId: c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:49,025 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: change Leader from null to c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at term 1 for becomeLeader, leader elected after 5751ms
datanode_3          | 2023-07-13 21:35:49,067 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:35:49,076 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:49,079 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-13 21:35:49,091 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:35:49,092 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:35:49,092 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:35:49,103 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:49,107 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-13 21:35:49,115 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderStateImpl
datanode_2          | 2023-07-13 21:35:54,124 [grpc-default-executor-0] INFO impl.VoteContext: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-CANDIDATE: reject ELECTION from c6e41c66-4ffe-43e7-a261-3d6c1c36e141: already has voted for 5017a172-51c5-4d66-9290-f85658670909 at current term 2
datanode_2          | 2023-07-13 21:35:54,128 [grpc-default-executor-0] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3 replies to ELECTION vote request: c6e41c66-4ffe-43e7-a261-3d6c1c36e141<-5017a172-51c5-4d66-9290-f85658670909#0:FAIL-t2. Peer's state: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3:t2, leader=null, voted=5017a172-51c5-4d66-9290-f85658670909, raftlog=Memoized:5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:54,117 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_2          | 2023-07-13 21:35:54,276 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:35:54,276 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 0: 5017a172-51c5-4d66-9290-f85658670909<-d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7#0:FAIL-t2
datanode_2          | 2023-07-13 21:35:54,276 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 1: 5017a172-51c5-4d66-9290-f85658670909<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:FAIL-t2
datanode_2          | 2023-07-13 21:35:54,277 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-07-13 21:35:54,277 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_2          | 2023-07-13 21:35:54,277 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: shutdown 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2
datanode_2          | 2023-07-13 21:35:54,278 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: 5017a172-51c5-4d66-9290-f85658670909: start 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState
datanode_2          | 2023-07-13 21:35:54,283 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:35:54,283 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:35:54,307 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C1A24067B8E3 with new leaderId: c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_2          | 2023-07-13 21:35:54,307 [5017a172-51c5-4d66-9290-f85658670909-server-thread1] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: change Leader from null to c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at term 2 for appendEntries, leader elected after 9790ms
datanode_2          | 2023-07-13 21:35:54,379 [5017a172-51c5-4d66-9290-f85658670909-server-thread2] INFO server.RaftServer$Division: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:35:54,380 [5017a172-51c5-4d66-9290-f85658670909-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:35:54,381 [5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5017a172-51c5-4d66-9290-f85658670909@group-C1A24067B8E3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/current/log_inprogress_0
om_1                | 2023-07-13 21:35:49,286 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-13 21:35:49,288 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:35:49,289 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-13 21:35:49,289 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:35:49,290 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-13 21:35:49,293 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-13 21:35:49,334 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-13 21:35:49,336 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-13 21:35:49,338 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-13 21:35:49,381 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:35:49,382 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-13 21:35:49,409 [Listener at om/9862] INFO util.log: Logging initialized @9718ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:35:49,499 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-07-13 21:35:49,506 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:35:49,512 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:35:49,514 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-13 21:35:49,514 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-13 21:35:49,514 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:35:49,559 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-13 21:35:49,560 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1                | 2023-07-13 21:35:49,612 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:35:49,612 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-13 21:35:49,615 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-13 21:35:49,630 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@440ef8d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:35:49,631 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b170235{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-13 21:35:50,028 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6a7a1a0d{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-9501230609325583541/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:35:50,035 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7d66a126{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-13 21:35:50,035 [Listener at om/9862] INFO server.Server: Started @10345ms
om_1                | 2023-07-13 21:35:50,037 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-13 21:35:50,037 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-13 21:35:50,039 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:35:50,047 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-13 21:35:50,039 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-13 21:35:50,070 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-07-13 21:35:50,077 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@919086] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-13 21:35:54,428 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5152223613ns, electionTimeout:5146ms
om_1                | 2023-07-13 21:35:54,430 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:35:54,430 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:35:54,433 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-13 21:35:54,435 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:35:54,445 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:35:54,446 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-13 21:35:54,446 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:35:54,447 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-13 21:35:54,447 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 6514ms
om_1                | 2023-07-13 21:35:54,460 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-13 21:35:54,473 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:35:54,474 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:35:54,488 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
datanode_3          | 2023-07-13 21:35:49,130 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:35:49,166 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-LeaderElection1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787: set configuration 0: peers:[c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:49,210 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-AE18E71EF787-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3832adf0-273d-4d89-b2d5-ae18e71ef787/current/log_inprogress_0
datanode_3          | 2023-07-13 21:35:51,385 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: receive requestVote(ELECTION, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, group-106F6D0C2D2D, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:35:51,385 [grpc-default-executor-0] INFO impl.VoteContext: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FOLLOWER: accept ELECTION from d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:35:51,385 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_3          | 2023-07-13 21:35:51,385 [grpc-default-executor-0] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState
datanode_3          | 2023-07-13 21:35:51,385 [grpc-default-executor-0] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState
datanode_3          | 2023-07-13 21:35:51,385 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState] INFO impl.FollowerState: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState was interrupted
datanode_3          | 2023-07-13 21:35:51,386 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:51,387 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:51,389 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D replies to ELECTION vote request: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:OK-t1. Peer's state: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D:t1, leader=null, voted=d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, raftlog=Memoized:c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:51,507 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-106F6D0C2D2D with new leaderId: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_3          | 2023-07-13 21:35:51,507 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141-server-thread1] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: change Leader from null to d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 at term 1 for appendEntries, leader elected after 5257ms
datanode_3          | 2023-07-13 21:35:51,527 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141-server-thread2] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:51,528 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141-server-thread2] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:35:51,530 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-106F6D0C2D2D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3b0663ac-d918-4fb8-983b-106f6d0c2d2d/current/log_inprogress_0
datanode_3          | 2023-07-13 21:35:54,017 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO impl.FollowerState: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5054480512ns, electionTimeout:5045ms
datanode_3          | 2023-07-13 21:35:54,017 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState
datanode_3          | 2023-07-13 21:35:54,018 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-13 21:35:54,018 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-13 21:35:54,018 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-FollowerState] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2
datanode_3          | 2023-07-13 21:35:54,023 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:54,028 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:35:54,028 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 5017a172-51c5-4d66-9290-f85658670909
datanode_3          | 2023-07-13 21:35:54,032 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:35:54,032 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
datanode_3          | 2023-07-13 21:35:54,140 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2: ELECTION PASSED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-13 21:35:54,142 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 0: c6e41c66-4ffe-43e7-a261-3d6c1c36e141<-5017a172-51c5-4d66-9290-f85658670909#0:FAIL-t2
datanode_3          | 2023-07-13 21:35:54,143 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection:   Response 1: c6e41c66-4ffe-43e7-a261-3d6c1c36e141<-d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7#0:OK-t2
datanode_3          | 2023-07-13 21:35:54,143 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.LeaderElection: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-07-13 21:35:54,143 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: shutdown c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2
datanode_3          | 2023-07-13 21:35:54,143 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-07-13 21:35:54,143 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C1A24067B8E3 with new leaderId: c6e41c66-4ffe-43e7-a261-3d6c1c36e141
datanode_3          | 2023-07-13 21:35:54,144 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: change Leader from null to c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at term 2 for becomeLeader, leader elected after 10036ms
datanode_3          | 2023-07-13 21:35:54,144 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:35:54,160 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:54,164 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-13 21:35:54,164 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:35:54,164 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:35:54,166 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:35:54,167 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:35:54,167 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-13 21:35:54,197 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: receive requestVote(ELECTION, 5017a172-51c5-4d66-9290-f85658670909, group-C1A24067B8E3, 2, (t:0, i:0))
datanode_3          | 2023-07-13 21:35:54,225 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-13 21:35:54,225 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:54,227 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-13 21:35:54,236 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-13 21:35:54,236 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:35:54,240 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:35:54,241 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-13 21:35:54,241 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-13 21:35:54,245 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-13 21:35:54,246 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:35:54,246 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-13 21:35:54,247 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-13 21:35:54,247 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:35:54,248 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:35:54,248 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-13 21:35:54,248 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-13 21:35:54,249 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO impl.RoleInfo: c6e41c66-4ffe-43e7-a261-3d6c1c36e141: start c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderStateImpl
datanode_3          | 2023-07-13 21:35:54,249 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:35:54,251 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b85f5c0-f349-46dd-b6e9-c1a24067b8e3/current/log_inprogress_0
om_1                | 2023-07-13 21:35:54,488 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-13 21:35:54,489 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-13 21:35:54,495 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:35:54,496 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-13 21:35:54,500 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-13 21:35:54,554 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-13 21:35:54,620 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:35:54,671 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-13 21:35:54,792 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-07-13 21:35:56,852 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-13 21:35:56,907 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-07-13 21:36:18,905 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-13 21:36:18,905 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-13 21:36:18,906 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-13 21:36:35,699 [qtp1436610577-48] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-13 21:36:35,733 [qtp1436610577-48] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689284195700 in 32 milliseconds
om_1                | 2023-07-13 21:36:35,795 [qtp1436610577-48] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 60 milliseconds
om_1                | 2023-07-13 21:36:35,795 [qtp1436610577-48] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689284195700
om_1                | 2023-07-13 21:36:45,172 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-13 21:34:54,478 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:34:54,494 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-13 21:34:54,689 [main] INFO util.log: Logging initialized @8887ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-13 21:34:56,173 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-07-13 21:34:56,459 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-13 21:34:56,529 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-13 21:34:56,539 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-13 21:34:56,542 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-13 21:34:56,543 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-13 21:34:57,014 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = cb262b16b092/172.23.0.12
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.3.0
recon_1             | 2023-07-13 21:35:41,966 [IPC Server handler 10 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:42,060 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c6e41c66-4ffe-43e7-a261-3d6c1c36e141 to Node DB.
recon_1             | 2023-07-13 21:35:42,061 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7 to Node DB.
recon_1             | 2023-07-13 21:35:42,507 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5017a172-51c5-4d66-9290-f85658670909
recon_1             | 2023-07-13 21:35:42,507 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:42,508 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5017a172-51c5-4d66-9290-f85658670909 to Node DB.
recon_1             | 2023-07-13 21:35:43,305 [IPC Server handler 7 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-13 21:35:43,308 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a49cb362-f1d9-45d8-a3ee-d8fb65656228. Trying to get from SCM.
recon_1             | 2023-07-13 21:35:43,352 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a49cb362-f1d9-45d8-a3ee-d8fb65656228, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, CreationTimestamp2023-07-13T21:35:40.425Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:35:43,457 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a49cb362-f1d9-45d8-a3ee-d8fb65656228, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, CreationTimestamp2023-07-13T21:35:40.425Z[UTC]].
recon_1             | 2023-07-13 21:35:43,734 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-13 21:35:43,735 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3. Trying to get from SCM.
recon_1             | 2023-07-13 21:35:43,743 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5b85f5c0-f349-46dd-b6e9-c1a24067b8e3, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.622Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:35:43,744 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b85f5c0-f349-46dd-b6e9-c1a24067b8e3, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.622Z[UTC]].
recon_1             | 2023-07-13 21:35:43,744 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:43,776 [IPC Server handler 25 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-13 21:35:43,777 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3832adf0-273d-4d89-b2d5-ae18e71ef787. Trying to get from SCM.
recon_1             | 2023-07-13 21:35:43,784 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3832adf0-273d-4d89-b2d5-ae18e71ef787, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.105Z[UTC]] to Recon pipeline metadata.
datanode_3          | 2023-07-13 21:35:54,268 [c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LeaderElection2] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3: set configuration 0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:35:54,272 [grpc-default-executor-0] INFO impl.VoteContext: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-LEADER: reject ELECTION from 5017a172-51c5-4d66-9290-f85658670909: already has voted for c6e41c66-4ffe-43e7-a261-3d6c1c36e141 at current term 2
datanode_3          | 2023-07-13 21:35:54,273 [grpc-default-executor-0] INFO server.RaftServer$Division: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3 replies to ELECTION vote request: 5017a172-51c5-4d66-9290-f85658670909<-c6e41c66-4ffe-43e7-a261-3d6c1c36e141#0:FAIL-t2. Peer's state: c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3:t2, leader=c6e41c66-4ffe-43e7-a261-3d6c1c36e141, voted=c6e41c66-4ffe-43e7-a261-3d6c1c36e141, raftlog=Memoized:c6e41c66-4ffe-43e7-a261-3d6c1c36e141@group-C1A24067B8E3-SegmentedRaftLog:OPENED:c0, conf=0: peers:[5017a172-51c5-4d66-9290-f85658670909|rpc:172.23.0.7:9856|admin:172.23.0.7:9857|client:172.23.0.7:9858|dataStream:|priority:0|startupRole:FOLLOWER, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7|rpc:172.23.0.8:9856|admin:172.23.0.8:9857|client:172.23.0.8:9858|dataStream:|priority:0|startupRole:FOLLOWER, c6e41c66-4ffe-43e7-a261-3d6c1c36e141|rpc:172.23.0.6:9856|admin:172.23.0.6:9857|client:172.23.0.6:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1               | STARTUP_MSG:   java = 11.0.14.1
s3g_1               | ************************************************************/
s3g_1               | 2023-07-13 21:34:57,047 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-13 21:34:57,188 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-13 21:34:58,122 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-07-13 21:34:59,222 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-13 21:34:59,227 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-07-13 21:34:59,452 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-13 21:34:59,467 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1               | 2023-07-13 21:34:59,677 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:34:59,690 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-13 21:34:59,704 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-07-13 21:35:00,135 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-13 21:35:00,159 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 13, 2023 9:35:24 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-13 21:35:25,001 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-4981331761945775037/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:34:54,373 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 62bbab1eb305/172.23.0.5
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | 2023-07-13 21:35:25,082 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:35:25,085 [main] INFO server.Server: Started @39284ms
s3g_1               | 2023-07-13 21:35:25,105 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-07-13 21:35:25,106 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-07-13 21:35:25,109 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-07-13 21:35:43,784 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3832adf0-273d-4d89-b2d5-ae18e71ef787, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.105Z[UTC]].
recon_1             | 2023-07-13 21:35:43,785 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=3832adf0-273d-4d89-b2d5-ae18e71ef787 reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:43,785 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3832adf0-273d-4d89-b2d5-ae18e71ef787, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c6e41c66-4ffe-43e7-a261-3d6c1c36e141, CreationTimestamp2023-07-13T21:35:40.105Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:35:44,156 [IPC Server handler 97 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-13 21:35:44,157 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:44,231 [IPC Server handler 7 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-13 21:35:44,232 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=80c77ada-959c-48f0-b5a5-963cdd6d806a. Trying to get from SCM.
recon_1             | 2023-07-13 21:35:44,243 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 80c77ada-959c-48f0-b5a5-963cdd6d806a, Nodes: 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:5017a172-51c5-4d66-9290-f85658670909, CreationTimestamp2023-07-13T21:35:40.570Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:35:44,244 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 80c77ada-959c-48f0-b5a5-963cdd6d806a, Nodes: 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:5017a172-51c5-4d66-9290-f85658670909, CreationTimestamp2023-07-13T21:35:40.570Z[UTC]].
recon_1             | 2023-07-13 21:35:44,555 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-13 21:35:44,557 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,226 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,227 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d. Trying to get from SCM.
recon_1             | 2023-07-13 21:35:46,233 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3b0663ac-d918-4fb8-983b-106f6d0c2d2d, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.644Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:35:46,234 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3b0663ac-d918-4fb8-983b-106f6d0c2d2d, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.644Z[UTC]].
recon_1             | 2023-07-13 21:35:46,235 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,314 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,315 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,434 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:46,434 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:48,563 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:48,563 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:49,032 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:49,032 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:49,552 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:49,552 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:51,420 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:51,421 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d reported by d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:51,421 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3b0663ac-d918-4fb8-983b-106f6d0c2d2d, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, CreationTimestamp2023-07-13T21:35:40.644Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:35:54,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 reported by c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:35:54,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b85f5c0-f349-46dd-b6e9-c1a24067b8e3, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c6e41c66-4ffe-43e7-a261-3d6c1c36e141, CreationTimestamp2023-07-13T21:35:40.622Z[UTC]] moved to OPEN state
recon_1             | 2023-07-13 21:35:58,975 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:35:59,134 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-13 21:35:59,172 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:35:59,188 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:36:35,307 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-13 21:36:35,308 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-13 21:36:35,930 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689284195308
recon_1             | 2023-07-13 21:36:35,949 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:36:35,951 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-13 21:36:36,048 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689284195308.
recon_1             | 2023-07-13 21:36:36,105 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-13 21:36:36,116 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-07-13 21:36:36,145 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-07-13 21:36:36,570 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-13 21:36:36,571 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:36:36,572 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-13 21:36:36,580 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-13 21:36:36,643 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:36:36,644 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.072 seconds to process 4 keys.
recon_1             | 2023-07-13 21:36:36,697 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-13 21:36:36,744 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-13 21:36:40,844 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:34:54,444 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:34:54,760 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:34:54,942 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:34:55,023 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:34:56,632 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-13 21:34:57,295 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:34:57,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:34:57,310 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:34:57,310 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:34:57,314 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:34:57,315 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-13 21:34:57,323 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-13 21:34:57,325 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:34:57,335 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-13 21:34:57,336 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:34:57,548 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-13 21:34:57,552 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-13 21:34:57,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-13 21:34:59,277 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-13 21:34:59,291 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-13 21:34:59,300 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-13 21:34:59,307 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:34:59,309 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:34:59,346 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:34:59,411 [main] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: addNew group-7DD24336FB70:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|priority:0|startupRole:FOLLOWER] returns group-7DD24336FB70:java.util.concurrent.CompletableFuture@6e16b8b5[Not completed]
scm_1               | 2023-07-13 21:34:59,492 [pool-2-thread-1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8: new RaftServerImpl for group-7DD24336FB70:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-07-13 21:34:59,499 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-13 21:34:59,509 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-13 21:34:59,511 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:34:59,511 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:34:59,512 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:34:59,512 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:34:59,533 [pool-2-thread-1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: ConfigurationManager, init=-1: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:34:59,535 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:34:59,563 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:34:59,564 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:34:59,640 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-13 21:34:59,643 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-13 21:34:59,655 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:34:59,968 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-13 21:35:01,430 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-13 21:35:01,431 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-13 21:35:01,432 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:35:01,434 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-13 21:35:01,444 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:35:01,445 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70 does not exist. Creating ...
scm_1               | 2023-07-13 21:35:01,483 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/in_use.lock acquired by nodename 14@62bbab1eb305
scm_1               | 2023-07-13 21:35:01,532 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70 has been successfully formatted.
scm_1               | 2023-07-13 21:35:01,550 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-13 21:35:01,631 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-13 21:35:01,633 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:35:01,663 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:35:01,679 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-13 21:35:01,681 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:35:01,730 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-13 21:35:01,739 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:35:01,782 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70
scm_1               | 2023-07-13 21:35:01,783 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:35:01,787 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:01,803 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:35:01,804 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-13 21:35:01,805 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-13 21:35:01,806 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-13 21:35:01,816 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-13 21:35:01,818 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-13 21:35:02,046 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-13 21:35:02,060 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-13 21:35:02,061 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-13 21:35:02,062 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-13 21:35:02,107 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:35:02,108 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:35:02,133 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: start as a follower, conf=-1: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:02,143 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-07-13 21:35:02,146 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState
scm_1               | 2023-07-13 21:35:02,177 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-13 21:35:02,186 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-13 21:35:02,210 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7DD24336FB70,id=c59164db-77af-49fe-b118-48e9a6e808f8
scm_1               | 2023-07-13 21:35:02,213 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-13 21:35:02,222 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-13 21:35:02,223 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-13 21:35:02,224 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-13 21:35:02,255 [main] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: start RPC server
scm_1               | 2023-07-13 21:35:02,764 [main] INFO server.GrpcService: c59164db-77af-49fe-b118-48e9a6e808f8: GrpcService started, listening on 9894
scm_1               | 2023-07-13 21:35:02,799 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c59164db-77af-49fe-b118-48e9a6e808f8: Started
scm_1               | 2023-07-13 21:35:07,271 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.FollowerState: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126576328ns, electionTimeout:5083ms
scm_1               | 2023-07-13 21:35:07,275 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState
scm_1               | 2023-07-13 21:35:07,276 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-13 21:35:07,342 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-07-13 21:35:07,343 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1
scm_1               | 2023-07-13 21:35:07,380 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.LeaderElection: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:07,447 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.LeaderElection: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-13 21:35:07,450 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1
scm_1               | 2023-07-13 21:35:07,451 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-13 21:35:07,466 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: change Leader from null to c59164db-77af-49fe-b118-48e9a6e808f8 at term 1 for becomeLeader, leader elected after 7812ms
scm_1               | 2023-07-13 21:35:07,523 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:35:07,607 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:07,627 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:35:07,696 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:35:07,748 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:35:07,749 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:35:07,830 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:07,866 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:35:07,930 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderStateImpl
scm_1               | 2023-07-13 21:35:08,041 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-13 21:35:08,407 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: set configuration 0: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:08,526 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/log_inprogress_0
scm_1               | 2023-07-13 21:35:08,790 [main] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: close
scm_1               | 2023-07-13 21:35:08,791 [main] INFO server.GrpcService: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown server GrpcServerProtocolService now
scm_1               | 2023-07-13 21:35:08,791 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: shutdown
scm_1               | 2023-07-13 21:35:08,794 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7DD24336FB70,id=c59164db-77af-49fe-b118-48e9a6e808f8
scm_1               | 2023-07-13 21:35:08,794 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderStateImpl
scm_1               | 2023-07-13 21:35:08,818 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO impl.PendingRequests: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-13 21:35:08,911 [main] INFO server.GrpcService: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-07-13 21:35:08,924 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO impl.StateMachineUpdater: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-07-13 21:35:08,934 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO impl.StateMachineUpdater: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-07-13 21:35:08,940 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO impl.StateMachineUpdater: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-07-13 21:35:09,004 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: closes. applyIndex: 0
scm_1               | 2023-07-13 21:35:09,006 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1               | 2023-07-13 21:35:09,014 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker close()
scm_1               | 2023-07-13 21:35:09,024 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c59164db-77af-49fe-b118-48e9a6e808f8: Stopped
scm_1               | 2023-07-13 21:35:09,039 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:35:09,066 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-ad70b6dd-404b-4dd3-b1ab-7dd24336fb70; layoutVersion=4; scmId=c59164db-77af-49fe-b118-48e9a6e808f8
scm_1               | 2023-07-13 21:35:09,130 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 62bbab1eb305/172.23.0.5
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:35:17,838 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 62bbab1eb305/172.23.0.5
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:35:17,877 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:35:18,413 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:35:18,708 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:35:18,779 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:35:23,207 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:35:24,420 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:35:25,655 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm_1               | 2023-07-13 21:35:25,667 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-13 21:35:26,030 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-13 21:35:26,132 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:c59164db-77af-49fe-b118-48e9a6e808f8
scm_1               | 2023-07-13 21:35:26,452 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-13 21:35:26,666 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:35:26,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:35:26,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:35:26,681 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:35:26,681 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:35:26,681 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-13 21:35:26,682 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-13 21:35:26,698 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:35:26,699 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-13 21:35:26,700 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:35:26,722 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-13 21:35:26,733 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-13 21:35:26,733 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-13 21:35:27,135 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-13 21:35:27,138 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-13 21:35:27,139 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-13 21:35:27,139 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:35:27,140 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:35:27,144 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:35:27,150 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: found a subdirectory /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70
scm_1               | 2023-07-13 21:35:27,155 [main] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: addNew group-7DD24336FB70:[] returns group-7DD24336FB70:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm_1               | 2023-07-13 21:35:27,180 [pool-16-thread-1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8: new RaftServerImpl for group-7DD24336FB70:[] with SCMStateMachine:uninitialized
scm_1               | 2023-07-13 21:35:27,181 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-13 21:35:27,182 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-13 21:35:27,184 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:35:27,184 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:35:27,185 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:35:27,185 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:35:27,290 [pool-16-thread-1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:35:27,291 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:35:27,306 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:35:27,307 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:35:27,402 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-13 21:35:27,406 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-13 21:35:27,429 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:35:28,034 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-13 21:35:28,035 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-13 21:35:28,036 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:35:28,038 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-13 21:35:28,055 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:35:28,062 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-07-13 21:35:28,062 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-13 21:35:28,062 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-07-13 21:35:28,134 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1               | 2023-07-13 21:35:28,479 [main] INFO reflections.Reflections: Reflections took 230 ms to scan 3 urls, producing 112 keys and 252 values 
scm_1               | 2023-07-13 21:35:28,600 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-13 21:35:28,601 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-13 21:35:28,604 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-13 21:35:28,615 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-13 21:35:28,675 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:35:28,696 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-13 21:35:28,697 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-13 21:35:28,707 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-13 21:35:28,756 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-13 21:35:28,756 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-13 21:35:28,763 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-13 21:35:28,764 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:35:28,766 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-07-13 21:35:28,767 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-07-13 21:35:28,779 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-07-13 21:35:28,793 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-07-13 21:35:28,875 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-13 21:35:28,910 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-13 21:35:28,967 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-13 21:35:29,007 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-13 21:35:29,015 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-13 21:35:29,030 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-13 21:35:29,034 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:29,039 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:35:30,938 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:35:30,984 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:35:31,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-13 21:35:31,204 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:35:31,212 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:35:31,215 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-13 21:35:31,404 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:35:31,453 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:35:31,460 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:35:31,752 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-07-13 21:35:31,754 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        true
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-07-13 21:35:31,762 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:35:31,763 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-13 21:35:31,783 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:35:31,789 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-07-13 21:35:31,828 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/in_use.lock acquired by nodename 8@62bbab1eb305
scm_1               | 2023-07-13 21:35:31,840 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=c59164db-77af-49fe-b118-48e9a6e808f8} from /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/raft-meta
scm_1               | 2023-07-13 21:35:31,981 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: set configuration 0: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:31,995 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-13 21:35:32,038 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-13 21:35:32,049 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:35:32,051 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:35:32,053 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-13 21:35:32,070 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:35:32,089 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-13 21:35:32,089 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:35:32,124 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70
scm_1               | 2023-07-13 21:35:32,125 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:35:32,125 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:32,129 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:35:32,129 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-13 21:35:32,130 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-13 21:35:32,131 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-13 21:35:32,143 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-13 21:35:32,149 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-13 21:35:32,186 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-13 21:35:32,187 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-13 21:35:32,187 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-13 21:35:32,187 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-13 21:35:32,251 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: set configuration 0: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:32,255 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/log_inprogress_0
scm_1               | 2023-07-13 21:35:32,259 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1               | 2023-07-13 21:35:32,259 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:35:32,623 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: start as a follower, conf=0: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:32,623 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-07-13 21:35:32,634 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState
scm_1               | 2023-07-13 21:35:32,661 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-13 21:35:32,662 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7DD24336FB70,id=c59164db-77af-49fe-b118-48e9a6e808f8
scm_1               | 2023-07-13 21:35:32,675 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-13 21:35:32,681 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-13 21:35:32,688 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-13 21:35:32,690 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-13 21:35:32,695 [c59164db-77af-49fe-b118-48e9a6e808f8-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-13 21:35:32,722 [Listener at 0.0.0.0/9860] INFO server.RaftServer: c59164db-77af-49fe-b118-48e9a6e808f8: start RPC server
scm_1               | 2023-07-13 21:35:33,008 [Listener at 0.0.0.0/9860] INFO server.GrpcService: c59164db-77af-49fe-b118-48e9a6e808f8: GrpcService started, listening on 9894
scm_1               | 2023-07-13 21:35:33,035 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c59164db-77af-49fe-b118-48e9a6e808f8: Started
scm_1               | 2023-07-13 21:35:33,060 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-07-13 21:35:33,066 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-07-13 21:35:33,324 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-13 21:35:33,344 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-13 21:35:33,344 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-13 21:35:33,983 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:35:33,983 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:35:34,109 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-13 21:35:34,202 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:35:34,211 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:35:34,233 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:35:34,238 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-13 21:35:34,495 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@46bb0bdf] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-13 21:35:34,538 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-13 21:35:34,538 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-13 21:35:34,674 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @24439ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:35:35,387 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-13 21:35:35,400 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:35:35,419 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:35:35,421 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:35:35,421 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-13 21:35:35,425 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:35:35,575 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-13 21:35:35,577 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1               | 2023-07-13 21:35:35,706 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:35:35,706 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:35:35,722 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-13 21:35:35,784 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@150d6eaf{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:35:35,788 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6615237{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:35:36,460 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@60dcf9ec{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-10456528935627204696/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm_1               | 2023-07-13 21:35:36,478 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@7336fd8f{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:35:36,479 [Listener at 0.0.0.0/9860] INFO server.Server: Started @26245ms
scm_1               | 2023-07-13 21:35:36,502 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-13 21:35:36,502 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-13 21:35:36,524 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:35:37,835 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.FollowerState: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5201016074ns, electionTimeout:5146ms
scm_1               | 2023-07-13 21:35:37,836 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState
scm_1               | 2023-07-13 21:35:37,837 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-13 21:35:37,840 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-07-13 21:35:37,841 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-FollowerState] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1
scm_1               | 2023-07-13 21:35:37,883 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.LeaderElection: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:37,884 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.LeaderElection: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-13 21:35:37,888 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: shutdown c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1
scm_1               | 2023-07-13 21:35:37,895 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-13 21:35:37,896 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-13 21:35:37,896 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-07-13 21:35:37,901 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: change Leader from null to c59164db-77af-49fe-b118-48e9a6e808f8 at term 2 for becomeLeader, leader elected after 10493ms
scm_1               | 2023-07-13 21:35:37,911 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:35:37,915 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:37,916 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:35:37,927 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:35:37,928 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:35:37,928 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:35:37,935 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:35:37,947 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:35:37,954 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO impl.RoleInfo: c59164db-77af-49fe-b118-48e9a6e808f8: start c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderStateImpl
scm_1               | 2023-07-13 21:35:37,969 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-13 21:35:37,974 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/log_inprogress_0 to /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/log_0-0
scm_1               | 2023-07-13 21:35:37,990 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-LeaderElection1] INFO server.RaftServer$Division: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70: set configuration 1: peers:[c59164db-77af-49fe-b118-48e9a6e808f8|rpc:62bbab1eb305:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:35:37,997 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ad70b6dd-404b-4dd3-b1ab-7dd24336fb70/current/log_inprogress_1
scm_1               | 2023-07-13 21:35:38,004 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-07-13 21:35:38,004 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-13 21:35:38,018 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:38,022 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-13 21:35:38,022 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:35:38,023 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:35:38,035 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:35:38,071 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:35:38,301 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.6:40992: output error
scm_1               | 2023-07-13 21:35:38,302 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-13 21:35:38,306 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.7:33240: output error
scm_1               | 2023-07-13 21:35:38,307 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-13 21:35:38,306 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.8:41510: output error
scm_1               | 2023-07-13 21:35:38,310 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-13 21:35:40,046 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c6e41c66-4ffe-43e7-a261-3d6c1c36e141
scm_1               | 2023-07-13 21:35:40,048 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:35:40,070 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:35:40,071 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:35:40,089 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
scm_1               | 2023-07-13 21:35:40,089 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:35:40,091 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:35:40,087 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:35:40,091 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:35:40,099 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:35:40,108 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3832adf0-273d-4d89-b2d5-ae18e71ef787 to datanode:c6e41c66-4ffe-43e7-a261-3d6c1c36e141
scm_1               | 2023-07-13 21:35:40,385 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3832adf0-273d-4d89-b2d5-ae18e71ef787, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.105Z[UTC]].
scm_1               | 2023-07-13 21:35:40,392 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:40,425 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a49cb362-f1d9-45d8-a3ee-d8fb65656228 to datanode:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
scm_1               | 2023-07-13 21:35:40,442 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a49cb362-f1d9-45d8-a3ee-d8fb65656228, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.425Z[UTC]].
scm_1               | 2023-07-13 21:35:40,447 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:40,567 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5017a172-51c5-4d66-9290-f85658670909
scm_1               | 2023-07-13 21:35:40,569 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:35:40,570 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:35:40,570 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=80c77ada-959c-48f0-b5a5-963cdd6d806a to datanode:5017a172-51c5-4d66-9290-f85658670909
scm_1               | 2023-07-13 21:35:40,578 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:35:40,579 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:35:40,579 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:35:40,579 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:35:40,580 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-13 21:35:40,580 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:35:40,589 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 80c77ada-959c-48f0-b5a5-963cdd6d806a, Nodes: 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.570Z[UTC]].
scm_1               | 2023-07-13 21:35:40,594 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:40,622 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 to datanode:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
scm_1               | 2023-07-13 21:35:40,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 to datanode:c6e41c66-4ffe-43e7-a261-3d6c1c36e141
scm_1               | 2023-07-13 21:35:40,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 to datanode:5017a172-51c5-4d66-9290-f85658670909
scm_1               | 2023-07-13 21:35:40,637 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b85f5c0-f349-46dd-b6e9-c1a24067b8e3, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.622Z[UTC]].
scm_1               | 2023-07-13 21:35:40,642 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:40,644 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d to datanode:c6e41c66-4ffe-43e7-a261-3d6c1c36e141
scm_1               | 2023-07-13 21:35:40,646 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d to datanode:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7
scm_1               | 2023-07-13 21:35:40,646 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d to datanode:5017a172-51c5-4d66-9290-f85658670909
scm_1               | 2023-07-13 21:35:40,652 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3b0663ac-d918-4fb8-983b-106f6d0c2d2d, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:35:40.644Z[UTC]].
scm_1               | 2023-07-13 21:35:40,653 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:40,662 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=3b0663ac-d918-4fb8-983b-106f6d0c2d2d contains same datanodes as previous pipelines: PipelineID=5b85f5c0-f349-46dd-b6e9-c1a24067b8e3 nodeIds: c6e41c66-4ffe-43e7-a261-3d6c1c36e141, d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, 5017a172-51c5-4d66-9290-f85658670909
scm_1               | 2023-07-13 21:35:43,307 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a49cb362-f1d9-45d8-a3ee-d8fb65656228, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, CreationTimestamp2023-07-13T21:35:40.425Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:35:43,318 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:43,322 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:43,749 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:43,786 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3832adf0-273d-4d89-b2d5-ae18e71ef787, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c6e41c66-4ffe-43e7-a261-3d6c1c36e141, CreationTimestamp2023-07-13T21:35:40.105Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:35:43,801 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:43,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:44,165 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:44,225 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 80c77ada-959c-48f0-b5a5-963cdd6d806a, Nodes: 5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5017a172-51c5-4d66-9290-f85658670909, CreationTimestamp2023-07-13T21:35:40.570Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:35:44,229 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:44,236 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:44,558 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:46,236 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:46,316 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:46,417 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:48,562 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:49,041 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:49,555 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:51,413 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3b0663ac-d918-4fb8-983b-106f6d0c2d2d, Nodes: c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7, CreationTimestamp2023-07-13T21:35:40.644Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:35:51,413 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:51,426 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-13 21:35:51,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:35:51,432 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:35:51,437 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-13 21:35:51,440 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-13 21:35:54,185 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b85f5c0-f349-46dd-b6e9-c1a24067b8e3, Nodes: d21a1670-8ef0-4dd8-ac23-2a1f2d6a93f7{ip: 172.23.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c6e41c66-4ffe-43e7-a261-3d6c1c36e141{ip: 172.23.0.6, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5017a172-51c5-4d66-9290-f85658670909{ip: 172.23.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c6e41c66-4ffe-43e7-a261-3d6c1c36e141, CreationTimestamp2023-07-13T21:35:40.622Z[UTC]] moved to OPEN state
scm_1               | 2023-07-13 21:35:56,986 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-13 21:35:57,017 [c59164db-77af-49fe-b118-48e9a6e808f8@group-7DD24336FB70-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-13 21:35:57,022 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-13 21:36:27,105 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.10
scm_1               | 2023-07-13 21:36:37,793 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.10
scm_1               | 2023-07-13 21:37:32,054 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.10
scm_1               | 2023-07-13 21:37:42,501 [IPC Server handler 70 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.10
Attaching to xcompat_old_client_1_0_0_1, xcompat_om_1, xcompat_recon_1, xcompat_s3g_1, xcompat_old_client_1_3_0_1, xcompat_datanode_3, xcompat_datanode_1, xcompat_datanode_5, xcompat_datanode_4, xcompat_datanode_2, xcompat_scm_1, xcompat_new_client_1, xcompat_old_client_1_1_0_1, xcompat_old_client_1_2_1_1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-13 21:38:23,449 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 28b6177c48e8/172.24.0.7
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-13 21:38:26,092 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 3e70c6800834/172.24.0.10
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-07-13 21:38:26,284 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-13 21:38:26,948 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-13 21:38:27,876 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-13 21:38:29,658 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-13 21:38:29,659 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-13 21:38:30,727 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3e70c6800834 ip:172.24.0.10
datanode_1          | 2023-07-13 21:38:32,277 [main] INFO reflections.Reflections: Reflections took 1082 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_1          | 2023-07-13 21:38:37,521 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-07-13 21:38:37,947 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-13 21:38:39,853 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-13 21:38:39,986 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-13 21:38:40,027 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-13 21:38:40,052 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-13 21:38:40,279 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:38:40,330 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:38:40,363 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-13 21:38:40,391 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-07-13 21:38:23,715 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-13 21:38:24,298 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-13 21:38:25,223 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-13 21:38:26,471 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-13 21:38:26,477 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-13 21:38:27,820 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:28b6177c48e8 ip:172.24.0.7
datanode_2          | 2023-07-13 21:38:29,778 [main] INFO reflections.Reflections: Reflections took 1539 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_2          | 2023-07-13 21:38:35,110 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-13 21:38:35,610 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-13 21:38:37,893 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-13 21:38:38,118 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-13 21:38:38,144 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-13 21:38:38,179 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-13 21:38:38,484 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:38:38,510 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:38:38,552 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-13 21:38:38,563 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-13 21:38:38,563 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-13 21:38:38,572 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-13 21:38:38,942 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-13 21:38:38,958 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-13 21:38:52,966 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-13 21:38:54,334 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-13 21:38:55,184 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-13 21:38:56,169 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:38:56,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-13 21:38:56,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-13 21:38:56,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-13 21:38:56,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-13 21:38:56,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-13 21:38:56,252 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-13 21:38:56,268 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:38:56,283 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-13 21:38:56,301 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:38:56,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:38:56,482 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-13 21:38:56,525 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-13 21:39:00,711 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-13 21:39:00,741 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-13 21:39:00,747 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-13 21:39:00,757 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:39:00,770 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:39:00,802 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:39:01,710 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-13 21:39:03,288 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-07-13 21:39:05,019 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:39:05,165 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-13 21:39:05,395 [main] INFO util.log: Logging initialized @57753ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-13 21:39:06,851 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-13 21:39:06,937 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-13 21:39:06,989 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-13 21:39:07,026 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-13 21:39:07,035 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-13 21:39:07,036 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-13 21:39:07,561 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-07-13 21:39:07,602 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-13 21:39:07,619 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-07-13 21:39:07,962 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-13 21:39:07,962 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-13 21:39:07,974 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-13 21:39:08,088 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42684d86{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-13 21:39:08,107 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@39a1e1e6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-13 21:39:09,434 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4cc28ad0{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-792236553542338065/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-13 21:39:09,620 [main] INFO server.AbstractConnector: Started ServerConnector@3c5cb013{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-13 21:39:09,620 [main] INFO server.Server: Started @61979ms
datanode_2          | 2023-07-13 21:39:09,658 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-13 21:39:09,659 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-13 21:39:09,700 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-13 21:39:10,098 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-13 21:39:10,256 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-07-13 21:39:10,286 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-07-13 21:39:12,870 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-07-13 21:39:12,870 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-07-13 21:38:40,395 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-13 21:38:40,399 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-13 21:38:40,572 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-13 21:38:40,572 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-13 21:38:55,248 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-13 21:38:56,370 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-13 21:38:56,814 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-13 21:38:58,223 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:38:58,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-13 21:38:58,246 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-13 21:38:58,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-13 21:38:58,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-13 21:38:58,273 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-13 21:38:58,279 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-13 21:38:58,280 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:38:58,879 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-13 21:38:59,213 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:38:59,324 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:38:59,370 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-13 21:38:59,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-13 21:39:02,531 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-13 21:39:02,591 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-13 21:39:02,603 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-13 21:39:02,604 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:39:02,607 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:39:02,642 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:39:03,489 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-13 21:39:04,315 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-07-13 21:39:06,535 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:39:06,747 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-13 21:39:07,075 [main] INFO util.log: Logging initialized @57555ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-13 21:39:08,146 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-13 21:39:12,901 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-07-13 21:39:12,906 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-07-13 21:39:08,184 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-13 21:39:08,231 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-13 21:39:08,243 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-13 21:39:08,255 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-13 21:39:08,255 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-13 21:39:08,810 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-07-13 21:39:08,836 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-13 21:39:08,864 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-07-13 21:39:09,414 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-13 21:39:09,438 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-13 21:39:09,440 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-13 21:39:09,588 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ad6448e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-13 21:39:09,622 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1abd1a28{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-13 21:39:10,684 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@687d31a9{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16832689887957778713/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-13 21:39:10,758 [main] INFO server.AbstractConnector: Started ServerConnector@1529d534{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:39:10,766 [main] INFO server.Server: Started @61245ms
datanode_1          | 2023-07-13 21:39:10,822 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-13 21:39:10,823 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-13 21:39:10,833 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-13 21:39:11,405 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-07-13 21:39:11,815 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-07-13 21:39:11,831 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-07-13 21:39:14,491 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-07-13 21:39:14,491 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-07-13 21:39:14,491 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-07-13 21:39:14,517 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-07-13 21:39:12,969 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-13 21:39:14,240 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_2          | 2023-07-13 21:39:14,690 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-13 21:39:16,553 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:16,554 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:17,555 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:17,568 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:18,556 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:18,569 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:19,557 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:19,570 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:20,563 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:20,571 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:21,564 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:21,572 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:22,565 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:23,566 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:24,567 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-13 21:39:26,641 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 28b6177c48e8/172.24.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:35394 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:35394 remote=recon/172.24.0.14:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 2023-07-13 21:39:14,554 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-13 21:39:15,238 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_1          | 2023-07-13 21:39:15,812 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-13 21:39:17,958 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:17,959 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:18,959 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:18,960 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:19,960 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:19,961 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:20,961 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:20,962 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:21,962 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:21,963 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:22,963 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:23,964 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-13 21:39:27,093 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 3e70c6800834/172.24.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:48852 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:48852 remote=recon/172.24.0.14:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-13 21:39:29,584 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 28b6177c48e8/172.24.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:48736 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-07-13 21:38:22,695 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | /************************************************************
datanode_5          | STARTUP_MSG: Starting HddsDatanodeService
datanode_5          | STARTUP_MSG:   host = 07f387232595/172.24.0.6
datanode_5          | STARTUP_MSG:   args = []
datanode_5          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_5          | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-13 21:39:28,977 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 3e70c6800834/172.24.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:46392 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:46392 remote=scm/172.24.0.4:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-13 21:39:33,433 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-ed636af7-c49d-4903-a086-04d97c0061db/container.db to cache
datanode_1          | 2023-07-13 21:39:33,434 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-ed636af7-c49d-4903-a086-04d97c0061db/container.db for volume DS-ed636af7-c49d-4903-a086-04d97c0061db
datanode_1          | 2023-07-13 21:39:33,470 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-13 21:39:33,491 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-07-13 21:39:33,899 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-07-13 21:39:33,899 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:39:34,019 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: a1afc4af-4388-40ba-b575-6eb773bd243a: start RPC server
datanode_1          | 2023-07-13 21:39:34,045 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a1afc4af-4388-40ba-b575-6eb773bd243a: GrpcService started, listening on 9858
datanode_1          | 2023-07-13 21:39:34,059 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a1afc4af-4388-40ba-b575-6eb773bd243a: GrpcService started, listening on 9856
datanode_1          | 2023-07-13 21:39:34,068 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a1afc4af-4388-40ba-b575-6eb773bd243a: GrpcService started, listening on 9857
datanode_1          | 2023-07-13 21:39:34,088 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a1afc4af-4388-40ba-b575-6eb773bd243a is started using port 9858 for RATIS
datanode_1          | 2023-07-13 21:39:34,088 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a1afc4af-4388-40ba-b575-6eb773bd243a is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-13 21:39:34,089 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a1afc4af-4388-40ba-b575-6eb773bd243a is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-13 21:39:34,090 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a1afc4af-4388-40ba-b575-6eb773bd243a: Started
datanode_1          | 2023-07-13 21:39:34,184 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:40:07,978 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a1afc4af-4388-40ba-b575-6eb773bd243a: addNew group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-4071CA03B11B:java.util.concurrent.CompletableFuture@40fe8e54[Not completed]
datanode_5          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_5          | ************************************************************/
datanode_5          | 2023-07-13 21:38:22,801 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-07-13 21:38:23,152 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5          | 2023-07-13 21:38:24,290 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5          | 2023-07-13 21:38:25,644 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5          | 2023-07-13 21:38:25,666 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5          | 2023-07-13 21:38:27,028 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:07f387232595 ip:172.24.0.6
datanode_5          | 2023-07-13 21:38:28,480 [main] INFO reflections.Reflections: Reflections took 949 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_5          | 2023-07-13 21:38:33,983 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_5          | 2023-07-13 21:38:34,824 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5          | 2023-07-13 21:38:37,279 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5          | 2023-07-13 21:38:37,384 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-07-13 21:38:37,438 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5          | 2023-07-13 21:38:37,463 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5          | 2023-07-13 21:38:37,854 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5          | 2023-07-13 21:38:37,991 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-13 21:38:38,000 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_5          | 2023-07-13 21:38:38,031 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5          | 2023-07-13 21:38:38,037 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_5          | 2023-07-13 21:38:38,037 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-07-13 21:38:38,262 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5          | 2023-07-13 21:38:38,270 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5          | 2023-07-13 21:38:54,253 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_5          | 2023-07-13 21:38:54,750 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-13 21:38:55,176 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_5          | 2023-07-13 21:38:56,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-07-13 21:38:56,795 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_5          | 2023-07-13 21:38:56,818 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-07-13 21:38:56,846 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_5          | 2023-07-13 21:38:56,847 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_5          | 2023-07-13 21:38:56,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_5          | 2023-07-13 21:38:56,863 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5          | 2023-07-13 21:38:56,864 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:38:56,894 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5          | 2023-07-13 21:38:56,899 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:38:56,994 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-07-13 21:38:57,037 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5          | 2023-07-13 21:38:57,099 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-07-13 21:39:00,903 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-07-13 21:39:00,909 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5          | 2023-07-13 21:39:00,927 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-07-13 21:39:00,938 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:39:00,939 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-13 21:39:00,942 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-07-13 21:39:02,310 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_5          | 2023-07-13 21:39:02,984 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_5          | 2023-07-13 21:39:04,225 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5          | 2023-07-13 21:39:04,401 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_5          | 2023-07-13 21:39:05,027 [main] INFO util.log: Logging initialized @58743ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5          | 2023-07-13 21:39:05,848 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5          | 2023-07-13 21:39:05,893 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5          | 2023-07-13 21:39:05,940 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5          | 2023-07-13 21:39:05,965 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5          | 2023-07-13 21:39:05,965 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5          | 2023-07-13 21:39:05,965 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5          | 2023-07-13 21:39:06,582 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_5          | 2023-07-13 21:39:06,657 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5          | 2023-07-13 21:39:06,686 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_5          | 2023-07-13 21:39:06,949 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5          | 2023-07-13 21:39:06,949 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5          | 2023-07-13 21:39:06,983 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_5          | 2023-07-13 21:39:07,105 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@363a09a2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_5          | 2023-07-13 21:39:07,106 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@66cb9a63{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5          | 2023-07-13 21:39:08,050 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@53086bdc{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11380364894372857015/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5          | 2023-07-13 21:39:08,152 [main] INFO server.AbstractConnector: Started ServerConnector@6447dc25{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-13 21:40:08,071 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a: new RaftServerImpl for group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:40:08,080 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:40:08,081 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:40:08,081 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:40:08,082 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:08,085 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:40:08,085 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:40:08,127 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:40:08,130 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:40:08,154 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:40:08,156 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:40:08,265 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:08,293 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:08,318 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:40:08,328 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:40:08,520 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-13 21:40:08,690 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:40:08,717 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:40:08,718 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:40:08,735 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:40:08,737 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:40:08,738 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:40:08,740 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b does not exist. Creating ...
datanode_1          | 2023-07-13 21:40:08,752 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/in_use.lock acquired by nodename 6@3e70c6800834
datanode_1          | 2023-07-13 21:40:08,787 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b has been successfully formatted.
datanode_1          | 2023-07-13 21:40:08,868 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO ratis.ContainerStateMachine: group-4071CA03B11B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:40:08,902 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:40:09,014 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:40:09,014 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:09,019 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:40:09,023 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:40:09,038 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:09,072 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:40:09,074 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:40:09,078 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:09,109 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b
datanode_1          | 2023-07-13 21:40:09,110 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:40:09,111 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:09,114 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-13 21:38:24,700 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = fe1ff34557f7/172.24.0.9
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_5          | 2023-07-13 21:39:08,152 [main] INFO server.Server: Started @61869ms
datanode_5          | 2023-07-13 21:39:08,187 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5          | 2023-07-13 21:39:08,187 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5          | 2023-07-13 21:39:08,189 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5          | 2023-07-13 21:39:08,531 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_5          | 2023-07-13 21:39:09,237 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_5          | 2023-07-13 21:39:09,345 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_5          | 2023-07-13 21:39:12,319 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_5          | 2023-07-13 21:39:12,319 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_5          | 2023-07-13 21:39:12,331 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | 2023-07-13 21:39:12,402 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_5          | 2023-07-13 21:39:12,442 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5          | 2023-07-13 21:39:13,790 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_5          | 2023-07-13 21:39:14,262 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5          | 2023-07-13 21:39:15,978 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:16,010 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:16,979 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:17,040 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:17,980 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:18,046 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:18,981 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:19,047 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:19,981 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:20,048 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:20,982 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:21,049 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:21,986 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:22,050 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:23,054 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:24,055 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-13 21:39:27,090 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 07f387232595/172.24.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.6:60796 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.6:60796 remote=recon/172.24.0.14:9891]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-07-13 21:38:24,788 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-13 21:38:25,258 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-13 21:38:26,444 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-13 21:38:27,890 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-13 21:38:27,891 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-13 21:38:29,498 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:fe1ff34557f7 ip:172.24.0.9
datanode_3          | 2023-07-13 21:38:30,972 [main] INFO reflections.Reflections: Reflections took 990 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_3          | 2023-07-13 21:38:37,138 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-07-13 21:38:37,762 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-13 21:38:40,077 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-13 21:38:40,372 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-13 21:38:40,426 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-13 21:38:40,427 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-13 21:38:40,659 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:38:40,685 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:38:40,712 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-13 21:38:40,718 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:48736 remote=scm/172.24.0.4:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-13 21:39:33,215 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-40fd012a-f588-45fa-9d25-6c4e7676f361/container.db to cache
datanode_2          | 2023-07-13 21:39:33,219 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-40fd012a-f588-45fa-9d25-6c4e7676f361/container.db for volume DS-40fd012a-f588-45fa-9d25-6c4e7676f361
datanode_2          | 2023-07-13 21:39:33,256 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-13 21:39:33,281 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-07-13 21:39:33,751 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-07-13 21:39:33,752 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_2          | 2023-07-13 21:39:33,829 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start RPC server
datanode_2          | 2023-07-13 21:39:33,854 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a4456cb8-edbd-41c8-9e5d-af9836a313a2: GrpcService started, listening on 9858
datanode_2          | 2023-07-13 21:39:33,861 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a4456cb8-edbd-41c8-9e5d-af9836a313a2: GrpcService started, listening on 9856
datanode_2          | 2023-07-13 21:39:33,867 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: a4456cb8-edbd-41c8-9e5d-af9836a313a2: GrpcService started, listening on 9857
datanode_2          | 2023-07-13 21:39:33,869 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4456cb8-edbd-41c8-9e5d-af9836a313a2 is started using port 9858 for RATIS
datanode_2          | 2023-07-13 21:39:33,869 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4456cb8-edbd-41c8-9e5d-af9836a313a2 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-13 21:39:33,869 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4456cb8-edbd-41c8-9e5d-af9836a313a2 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-13 21:39:33,871 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a4456cb8-edbd-41c8-9e5d-af9836a313a2: Started
datanode_2          | 2023-07-13 21:39:34,028 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:38:40,722 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-13 21:38:40,723 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-13 21:38:41,039 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-13 21:38:41,083 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-13 21:38:55,293 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-13 21:38:56,311 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-13 21:38:56,673 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-13 21:38:58,131 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:38:58,160 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-13 21:38:58,169 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-13 21:38:58,172 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-13 21:38:58,172 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-07-13 21:38:58,194 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-13 21:38:58,195 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-13 21:38:58,225 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:38:58,228 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-13 21:38:58,243 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:38:58,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-13 21:38:58,365 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-13 21:38:58,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-07-13 21:39:02,183 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-13 21:39:02,309 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-13 21:39:02,349 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-07-13 21:39:02,359 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:39:02,359 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:39:02,391 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:39:03,489 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-13 21:39:04,152 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-07-13 21:39:05,731 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:39:05,828 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-13 21:39:06,248 [main] INFO util.log: Logging initialized @56864ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-13 21:39:07,656 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-13 21:39:07,716 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-13 21:39:07,758 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-13 21:39:07,811 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-13 21:39:07,833 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-13 21:39:07,833 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-13 21:39:08,206 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-07-13 21:39:08,265 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-13 21:39:08,296 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-07-13 21:39:08,675 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-13 21:39:08,727 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-13 21:39:08,735 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-13 21:39:08,958 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@f5b4ca6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-13 21:39:09,019 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7cc7e441{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-13 21:39:10,178 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@641255d1{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17700743477569449353/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-13 21:39:10,230 [main] INFO server.AbstractConnector: Started ServerConnector@25762f04{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-13 21:39:10,234 [main] INFO server.Server: Started @60845ms
datanode_3          | 2023-07-13 21:39:10,310 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-13 21:39:10,310 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-13 21:39:10,312 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-13 21:39:10,867 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-07-13 21:39:11,573 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-07-13 21:39:11,649 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-07-13 21:39:14,418 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-07-13 21:39:14,422 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-07-13 21:39:14,423 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-07-13 21:39:14,424 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_4          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4          | 2023-07-13 21:38:24,992 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4          | /************************************************************
datanode_4          | STARTUP_MSG: Starting HddsDatanodeService
datanode_4          | STARTUP_MSG:   host = 71107e799abb/172.24.0.8
datanode_4          | STARTUP_MSG:   args = []
datanode_4          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | 2023-07-13 21:39:14,447 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-13 21:39:15,329 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_3          | 2023-07-13 21:39:16,010 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-13 21:39:18,098 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:18,099 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:19,099 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:19,131 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:20,100 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:20,132 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:21,102 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:21,132 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:22,118 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:22,133 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:23,119 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:24,120 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-13 21:39:24,979 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-13 21:39:27,397 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From fe1ff34557f7/172.24.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.9:36644 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4          | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
datanode_4          | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-13 21:40:09,116 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:40:09,118 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:40:09,145 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:40:09,152 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:09,152 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:40:09,246 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:09,250 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:09,404 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:09,406 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:09,411 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:40:09,460 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:09,460 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:09,476 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:09,477 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:40:09,483 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState
datanode_1          | 2023-07-13 21:40:09,525 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:40:09,533 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:40:09,537 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4071CA03B11B,id=a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:40:09,540 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:40:09,551 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:40:09,551 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:40:09,555 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:40:09,762 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b
datanode_1          | 2023-07-13 21:40:13,314 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a: new RaftServerImpl for group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:40:13,315 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:40:13,315 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:40:13,316 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:40:13,322 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:13,324 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:40:13,325 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:40:13,325 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:40:13,325 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:40:13,326 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:40:13,329 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:40:13,332 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:13,332 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:13,334 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:40:13,334 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:40:13,334 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-13 21:40:13,338 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:40:13,339 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:40:13,339 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:40:13,341 [grpc-default-executor-0] INFO server.RaftServer: a1afc4af-4388-40ba-b575-6eb773bd243a: addNew group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-0FA64AED4490:java.util.concurrent.CompletableFuture@46320420[Not completed]
datanode_1          | 2023-07-13 21:40:13,346 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:40:13,348 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:40:13,350 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:40:13,353 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 does not exist. Creating ...
datanode_1          | 2023-07-13 21:40:13,359 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/in_use.lock acquired by nodename 6@3e70c6800834
datanode_1          | 2023-07-13 21:40:13,366 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 has been successfully formatted.
datanode_1          | 2023-07-13 21:40:13,385 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO ratis.ContainerStateMachine: group-0FA64AED4490: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:40:13,385 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:40:13,385 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-13 21:40:13,386 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:13,386 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:40:13,387 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:40:13,387 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:13,387 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-07-13 21:39:29,067 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 07f387232595/172.24.0.6 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.6:51736 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.6:51736 remote=scm/172.24.0.4:9861]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-07-13 21:39:33,328 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-e928bac9-6b3c-4010-ac18-f89d7e7dd3f8/container.db to cache
datanode_5          | 2023-07-13 21:39:33,328 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-e928bac9-6b3c-4010-ac18-f89d7e7dd3f8/container.db for volume DS-e928bac9-6b3c-4010-ac18-f89d7e7dd3f8
datanode_5          | 2023-07-13 21:39:33,354 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5          | 2023-07-13 21:39:33,374 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_5          | 2023-07-13 21:39:33,788 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5          | 2023-07-13 21:39:33,792 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:39:33,953 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start RPC server
datanode_5          | 2023-07-13 21:39:33,982 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: GrpcService started, listening on 9858
datanode_5          | 2023-07-13 21:39:33,995 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: GrpcService started, listening on 9856
datanode_5          | 2023-07-13 21:39:34,003 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: GrpcService started, listening on 9857
datanode_5          | 2023-07-13 21:39:34,024 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 is started using port 9858 for RATIS
datanode_5          | 2023-07-13 21:39:34,024 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 is started using port 9857 for RATIS_ADMIN
datanode_5          | 2023-07-13 21:39:34,025 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 is started using port 9856 for RATIS_SERVER
datanode_5          | 2023-07-13 21:39:34,031 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: Started
datanode_5          | 2023-07-13 21:39:34,161 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:40:07,807 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: addNew group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-0FA64AED4490:java.util.concurrent.CompletableFuture@32c708dd[Not completed]
datanode_5          | 2023-07-13 21:40:07,941 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: new RaftServerImpl for group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-07-13 21:40:07,945 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-07-13 21:40:07,946 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-07-13 21:40:07,947 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-07-13 21:40:07,947 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:40:08,519 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a4456cb8-edbd-41c8-9e5d-af9836a313a2: addNew group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-FE9FF367F7A0:java.util.concurrent.CompletableFuture@3630c811[Not completed]
datanode_2          | 2023-07-13 21:40:08,731 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2: new RaftServerImpl for group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:40:08,743 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:40:08,749 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:40:08,749 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:40:08,749 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:40:08,750 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:40:08,750 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:40:08,829 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: ConfigurationManager, init=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:40:08,830 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:40:08,882 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:40:08,885 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:40:08,960 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:40:08,982 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-13 21:40:08,996 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:40:09,004 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:40:09,162 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:40:09,362 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:40:09,389 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:40:09,397 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:40:09,400 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:40:09,404 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:40:09,406 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:40:09,409 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 does not exist. Creating ...
datanode_2          | 2023-07-13 21:40:09,445 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/in_use.lock acquired by nodename 7@28b6177c48e8
datanode_2          | 2023-07-13 21:40:09,484 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 has been successfully formatted.
datanode_2          | 2023-07-13 21:40:09,571 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO ratis.ContainerStateMachine: group-FE9FF367F7A0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:40:09,575 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:40:09,643 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:40:09,643 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:09,648 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:40:09,664 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:40:09,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:09,708 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:40:09,709 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:40:09,709 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | ************************************************************/
datanode_4          | 2023-07-13 21:38:25,101 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-07-13 21:38:25,519 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 2023-07-13 21:38:26,872 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-07-13 21:38:28,109 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-07-13 21:38:28,109 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4          | 2023-07-13 21:38:29,500 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:71107e799abb ip:172.24.0.8
datanode_4          | 2023-07-13 21:38:31,144 [main] INFO reflections.Reflections: Reflections took 1161 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_4          | 2023-07-13 21:38:35,848 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-07-13 21:38:36,598 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4          | 2023-07-13 21:38:38,692 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4          | 2023-07-13 21:38:38,872 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4          | 2023-07-13 21:38:38,890 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4          | 2023-07-13 21:38:38,944 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4          | 2023-07-13 21:38:39,412 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4          | 2023-07-13 21:38:39,696 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-07-13 21:38:39,719 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-07-13 21:38:39,783 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-07-13 21:38:39,783 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4          | 2023-07-13 21:38:39,819 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_4          | 2023-07-13 21:38:40,044 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4          | 2023-07-13 21:38:40,052 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4          | 2023-07-13 21:38:55,004 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-07-13 21:38:56,425 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-07-13 21:38:56,754 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-07-13 21:38:58,095 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-07-13 21:38:58,096 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4          | 2023-07-13 21:38:58,171 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-07-13 21:38:58,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4          | 2023-07-13 21:38:58,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 2023-07-13 21:38:58,206 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4          | 2023-07-13 21:38:58,207 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 2023-07-13 21:38:58,235 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:38:58,245 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4          | 2023-07-13 21:38:58,246 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-13 21:38:58,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-07-13 21:38:58,585 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4          | 2023-07-13 21:38:58,612 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4          | 2023-07-13 21:39:01,720 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_4          | 2023-07-13 21:39:01,773 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4          | 2023-07-13 21:39:01,773 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4          | 2023-07-13 21:39:01,774 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:39:01,812 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-13 21:39:01,872 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-13 21:39:03,047 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4          | 2023-07-13 21:39:03,652 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_4          | 2023-07-13 21:39:05,166 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4          | 2023-07-13 21:39:05,284 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4          | 2023-07-13 21:39:05,680 [main] INFO util.log: Logging initialized @56349ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4          | 2023-07-13 21:39:06,998 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4          | 2023-07-13 21:39:07,084 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 2023-07-13 21:39:07,145 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-07-13 21:39:07,152 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4          | 2023-07-13 21:39:07,162 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 2023-07-13 21:39:07,163 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-07-13 21:39:07,575 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_4          | 2023-07-13 21:39:07,603 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4          | 2023-07-13 21:39:07,612 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_4          | 2023-07-13 21:39:07,831 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-07-13 21:39:07,831 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-07-13 21:39:07,839 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_4          | 2023-07-13 21:39:07,933 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2319c1e0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4          | 2023-07-13 21:39:07,936 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@66428512{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-07-13 21:39:09,314 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@73158d35{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4210171926726412966/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4          | 2023-07-13 21:39:09,405 [main] INFO server.AbstractConnector: Started ServerConnector@48224381{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4          | 2023-07-13 21:39:09,415 [main] INFO server.Server: Started @60096ms
datanode_4          | 2023-07-13 21:39:09,437 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-07-13 21:39:09,437 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-07-13 21:39:09,446 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4          | 2023-07-13 21:39:09,808 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-07-13 21:39:10,063 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_4          | 2023-07-13 21:39:10,149 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-07-13 21:40:13,389 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:40:13,390 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:13,395 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490
datanode_1          | 2023-07-13 21:40:13,397 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:40:13,397 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:13,398 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:13,399 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:40:13,399 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:40:13,399 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:40:13,400 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:13,401 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:40:13,404 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:13,409 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:13,476 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:13,477 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:13,478 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.9:36644 remote=recon/172.24.0.14:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-13 21:39:29,139 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From fe1ff34557f7/172.24.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.9:47646 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.9:47646 remote=scm/172.24.0.4:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-13 21:39:32,984 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 2023-07-13 21:40:13,478 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:13,480 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:13,480 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:13,481 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:40:13,481 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState
datanode_1          | 2023-07-13 21:40:13,482 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0FA64AED4490,id=a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:40:13,483 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:40:13,483 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:40:13,483 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:40:13,484 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:40:13,490 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:40:13,495 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:40:14,268 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: receive requestVote(PRE_VOTE, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-0FA64AED4490, 0, (t:0, i:0))
datanode_1          | 2023-07-13 21:40:14,273 [grpc-default-executor-0] INFO impl.VoteContext: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FOLLOWER: accept PRE_VOTE from 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:40:14,305 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490 replies to PRE_VOTE vote request: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:OK-t0. Peer's state: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490:t0, leader=null, voted=, raftlog=Memoized:a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:14,356 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: receive requestVote(ELECTION, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-0FA64AED4490, 1, (t:0, i:0))
datanode_1          | 2023-07-13 21:40:14,357 [grpc-default-executor-0] INFO impl.VoteContext: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FOLLOWER: accept ELECTION from 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-13 21:40:14,376 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_1          | 2023-07-13 21:40:14,382 [grpc-default-executor-0] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: shutdown a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState
datanode_1          | 2023-07-13 21:40:14,383 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState] INFO impl.FollowerState: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState was interrupted
datanode_1          | 2023-07-13 21:40:14,384 [grpc-default-executor-0] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-FollowerState
datanode_1          | 2023-07-13 21:40:14,396 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490 replies to ELECTION vote request: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:OK-t1. Peer's state: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490:t1, leader=null, voted=2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, raftlog=Memoized:a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:14,657 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO impl.FollowerState: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5174353627ns, electionTimeout:5119ms
datanode_1          | 2023-07-13 21:40:14,667 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: shutdown a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState
datanode_1          | 2023-07-13 21:40:14,674 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:40:14,721 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-07-13 21:40:07,947 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-13 21:40:07,947 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-07-13 21:40:07,964 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-07-13 21:40:07,966 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-07-13 21:40:07,980 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-07-13 21:40:07,981 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-07-13 21:40:08,035 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:40:08,047 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:08,060 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-07-13 21:40:08,063 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-13 21:40:08,164 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-07-13 21:40:08,308 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:40:08,332 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-13 21:40:08,339 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-07-13 21:40:08,340 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-07-13 21:40:08,342 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-07-13 21:40:08,344 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-13 21:40:08,348 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 does not exist. Creating ...
datanode_5          | 2023-07-13 21:40:08,392 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/in_use.lock acquired by nodename 7@07f387232595
datanode_5          | 2023-07-13 21:40:08,445 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 has been successfully formatted.
datanode_5          | 2023-07-13 21:40:08,520 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO ratis.ContainerStateMachine: group-0FA64AED4490: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-07-13 21:40:08,545 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-13 21:40:08,696 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-07-13 21:40:08,701 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:08,702 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-07-13 21:40:08,727 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-07-13 21:40:08,737 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:08,776 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-07-13 21:40:08,777 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-07-13 21:40:08,779 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:08,805 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490
datanode_5          | 2023-07-13 21:40:08,806 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-07-13 21:40:08,808 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:08,811 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:08,815 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-07-13 21:40:08,815 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-13 21:40:08,824 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-13 21:40:08,824 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:08,851 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-07-13 21:40:08,941 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:08,942 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:09,009 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:09,024 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:09,030 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:09,066 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:09,066 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:09,071 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:09,073 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-13 21:40:09,078 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState
datanode_5          | 2023-07-13 21:40:09,094 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:09,098 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:09,119 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0FA64AED4490,id=2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:40:09,132 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:09,136 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-07-13 21:40:09,141 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-07-13 21:40:09,143 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-07-13 21:40:09,308 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490
datanode_5          | 2023-07-13 21:40:13,146 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: new RaftServerImpl for group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-07-13 21:40:13,146 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-07-13 21:40:13,147 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-07-13 21:40:13,147 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-07-13 21:40:13,148 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:40:13,148 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 2023-07-13 21:40:09,723 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0
datanode_2          | 2023-07-13 21:40:09,725 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:40:09,726 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:09,732 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:09,737 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:40:09,737 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-13 21:40:09,740 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:40:09,740 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:40:09,743 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:40:09,793 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:09,802 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:09,892 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:40:09,897 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:40:09,901 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:40:09,943 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:40:09,943 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:40:09,950 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: start as a follower, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:09,953 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:40:09,960 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState
datanode_2          | 2023-07-13 21:40:09,995 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:40:09,996 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:40:10,007 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FE9FF367F7A0,id=a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_2          | 2023-07-13 21:40:10,025 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:40:10,030 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:40:10,032 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:40:10,036 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:40:10,144 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0
datanode_2          | 2023-07-13 21:40:14,678 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0.
datanode_2          | 2023-07-13 21:40:14,679 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a4456cb8-edbd-41c8-9e5d-af9836a313a2: addNew group-FE52BEFF02DA:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-FE52BEFF02DA:java.util.concurrent.CompletableFuture@cb7ed76[Not completed]
datanode_2          | 2023-07-13 21:40:14,681 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2: new RaftServerImpl for group-FE52BEFF02DA:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-13 21:40:14,684 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-13 21:40:14,684 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-13 21:40:14,684 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-13 21:40:14,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:40:14,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-13 21:40:14,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-13 21:40:14,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: ConfigurationManager, init=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-13 21:40:14,685 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-13 21:40:14,687 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-13 21:40:14,688 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-13 21:40:14,689 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-13 21:40:14,691 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-13 21:40:14,693 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-13 21:40:14,695 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-13 21:40:14,697 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-13 21:40:14,703 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:40:14,716 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:40:14,719 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-13 21:40:14,719 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-13 21:40:14,720 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-13 21:40:14,721 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-13 21:40:14,721 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1283747c-17ff-47ac-873e-fe52beff02da does not exist. Creating ...
datanode_2          | 2023-07-13 21:40:14,758 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1283747c-17ff-47ac-873e-fe52beff02da/in_use.lock acquired by nodename 7@28b6177c48e8
datanode_2          | 2023-07-13 21:40:14,767 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1283747c-17ff-47ac-873e-fe52beff02da has been successfully formatted.
datanode_2          | 2023-07-13 21:40:14,771 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO ratis.ContainerStateMachine: group-FE52BEFF02DA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-13 21:40:14,772 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-13 21:40:14,772 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-13 21:40:14,777 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:14,780 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-13 21:40:14,780 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-13 21:40:14,782 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:14,784 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:40:14,785 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-13 21:40:14,786 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:14,787 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1283747c-17ff-47ac-873e-fe52beff02da
datanode_2          | 2023-07-13 21:40:14,788 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-13 21:40:14,788 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:14,789 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:14,791 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-13 21:40:14,791 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-07-13 21:39:12,259 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 2023-07-13 21:39:12,260 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_4          | 2023-07-13 21:39:12,269 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_4          | 2023-07-13 21:39:12,307 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-07-13 21:40:14,742 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-FollowerState] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1
datanode_1          | 2023-07-13 21:40:14,819 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:14,908 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_1          | 2023-07-13 21:40:14,940 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:40:14,941 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:40:14,953 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_1          | 2023-07-13 21:40:15,452 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:40:15,452 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection:   Response 0: a1afc4af-4388-40ba-b575-6eb773bd243a<-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0:OK-t0
datanode_1          | 2023-07-13 21:40:15,452 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-07-13 21:40:15,521 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:15,645 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:40:15,645 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:40:15,969 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: receive requestVote(PRE_VOTE, bb5c41c8-12ac-4f33-939d-67f4ed99ba46, group-4071CA03B11B, 0, (t:0, i:0))
datanode_1          | 2023-07-13 21:40:15,972 [grpc-default-executor-0] INFO impl.VoteContext: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-CANDIDATE: reject PRE_VOTE from bb5c41c8-12ac-4f33-939d-67f4ed99ba46: our priority 1 > candidate's priority 0
datanode_1          | 2023-07-13 21:40:15,972 [grpc-default-executor-0] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B replies to PRE_VOTE vote request: bb5c41c8-12ac-4f33-939d-67f4ed99ba46<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:FAIL-t1. Peer's state: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B:t1, leader=null, voted=a1afc4af-4388-40ba-b575-6eb773bd243a, raftlog=Memoized:a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:16,026 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-13 21:40:16,027 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection:   Response 0: a1afc4af-4388-40ba-b575-6eb773bd243a<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t1
datanode_1          | 2023-07-13 21:40:16,027 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1 ELECTION round 0: result PASSED
datanode_1          | 2023-07-13 21:40:16,027 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: shutdown a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1
datanode_1          | 2023-07-13 21:40:16,028 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:40:16,028 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4071CA03B11B with new leaderId: a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:40:16,029 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: change Leader from null to a1afc4af-4388-40ba-b575-6eb773bd243a at term 1 for becomeLeader, leader elected after 7801ms
datanode_1          | 2023-07-13 21:40:16,089 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:40:16,304 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:16,319 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:40:16,380 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:40:16,403 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:40:16,404 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:40:16,413 [a1afc4af-4388-40ba-b575-6eb773bd243a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0FA64AED4490 with new leaderId: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_1          | 2023-07-13 21:40:16,427 [a1afc4af-4388-40ba-b575-6eb773bd243a-server-thread1] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: change Leader from null to 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 at term 1 for appendEntries, leader elected after 3082ms
datanode_1          | 2023-07-13 21:40:16,656 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:16,671 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:40:16,816 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b.
datanode_1          | 2023-07-13 21:40:16,817 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a1afc4af-4388-40ba-b575-6eb773bd243a: addNew group-D2C8FFC48200:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-D2C8FFC48200:java.util.concurrent.CompletableFuture@5fe94e26[Not completed]
datanode_1          | 2023-07-13 21:40:16,825 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a: new RaftServerImpl for group-D2C8FFC48200:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-13 21:40:16,834 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-13 21:40:16,834 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-13 21:40:16,835 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-13 21:40:16,838 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-13 21:40:16,838 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-13 21:40:16,839 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-13 21:40:16,839 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:16,839 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-13 21:40:16,839 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-13 21:40:16,839 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-13 21:40:16,860 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:40:16,860 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:40:16,867 [a1afc4af-4388-40ba-b575-6eb773bd243a-server-thread1] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:16,871 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-13 21:40:16,871 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-13 21:40:16,872 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-13 21:40:16,872 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-13 21:40:16,873 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/635788d3-7f27-4198-8993-d2c8ffc48200 does not exist. Creating ...
datanode_1          | 2023-07-13 21:40:16,882 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/635788d3-7f27-4198-8993-d2c8ffc48200/in_use.lock acquired by nodename 6@3e70c6800834
datanode_1          | 2023-07-13 21:40:16,901 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/635788d3-7f27-4198-8993-d2c8ffc48200 has been successfully formatted.
datanode_1          | 2023-07-13 21:40:16,913 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO ratis.ContainerStateMachine: group-D2C8FFC48200: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-13 21:40:16,964 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-13 21:40:16,964 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-13 21:39:12,349 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4          | 2023-07-13 21:39:13,375 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_4          | 2023-07-13 21:39:14,156 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4          | 2023-07-13 21:39:16,038 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:16,039 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:17,040 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:17,041 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:18,042 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:18,042 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:19,043 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:19,043 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:20,044 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:20,045 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:21,045 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:21,045 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:22,046 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:22,046 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:23,047 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:24,048 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-13 21:39:27,133 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 71107e799abb/172.24.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:50448 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:50448 remote=recon/172.24.0.14:9891]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_4          | 2023-07-13 21:39:29,057 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 71107e799abb/172.24.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:37898 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:37898 remote=scm/172.24.0.4:9861]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-13 21:39:33,364 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-6b49d8db-ee2a-458c-a975-05796762f5b4/container.db to cache
datanode_3          | 2023-07-13 21:39:33,369 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-6b49d8db-ee2a-458c-a975-05796762f5b4/container.db for volume DS-6b49d8db-ee2a-458c-a975-05796762f5b4
datanode_3          | 2023-07-13 21:39:33,426 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-13 21:39:33,456 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-07-13 21:39:33,865 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-07-13 21:39:33,865 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_3          | 2023-07-13 21:39:34,000 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start RPC server
datanode_3          | 2023-07-13 21:39:34,025 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: GrpcService started, listening on 9858
datanode_3          | 2023-07-13 21:39:34,029 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: GrpcService started, listening on 9856
datanode_3          | 2023-07-13 21:39:34,034 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: GrpcService started, listening on 9857
datanode_3          | 2023-07-13 21:39:34,065 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775 is started using port 9858 for RATIS
datanode_3          | 2023-07-13 21:39:34,065 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-13 21:39:34,065 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-13 21:39:34,070 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Started
datanode_3          | 2023-07-13 21:39:34,196 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:40:08,256 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: addNew group-26B0221B21D7:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-26B0221B21D7:java.util.concurrent.CompletableFuture@537c9cdb[Not completed]
datanode_3          | 2023-07-13 21:40:08,376 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: new RaftServerImpl for group-26B0221B21D7:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:40:08,391 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:40:08,395 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:40:08,397 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:40:08,400 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:08,400 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:40:08,401 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:40:08,463 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: ConfigurationManager, init=-1: peers:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:40:08,463 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:40:08,495 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:40:08,496 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:40:08,582 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:08,608 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:08,642 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:40:08,645 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:40:08,792 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:40:08,934 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:40:08,958 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:40:08,963 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:40:08,977 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:40:08,979 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:40:08,984 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:40:08,985 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/627a05b4-e96a-4f2b-b164-26b0221b21d7 does not exist. Creating ...
datanode_3          | 2023-07-13 21:40:09,002 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/627a05b4-e96a-4f2b-b164-26b0221b21d7/in_use.lock acquired by nodename 7@fe1ff34557f7
datanode_3          | 2023-07-13 21:40:09,039 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/627a05b4-e96a-4f2b-b164-26b0221b21d7 has been successfully formatted.
datanode_3          | 2023-07-13 21:40:09,102 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO ratis.ContainerStateMachine: group-26B0221B21D7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:40:09,145 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:40:09,245 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:40:09,245 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:09,276 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:40:09,277 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:40:09,301 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:09,352 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:40:09,361 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:40:09,364 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:09,418 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/627a05b4-e96a-4f2b-b164-26b0221b21d7
datanode_3          | 2023-07-13 21:40:09,425 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:40:09,430 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:40:09,437 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:09,438 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:40:09,442 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:40:09,447 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:40:09,450 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:09,455 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:40:09,529 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:09,541 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:09,703 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:09,705 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:09,709 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-13 21:39:33,473 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-68f816f9-ba7e-4fba-8701-bee6717b5086/container.db to cache
datanode_4          | 2023-07-13 21:39:33,479 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de/DS-68f816f9-ba7e-4fba-8701-bee6717b5086/container.db for volume DS-68f816f9-ba7e-4fba-8701-bee6717b5086
datanode_4          | 2023-07-13 21:39:33,540 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4          | 2023-07-13 21:39:33,561 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4          | 2023-07-13 21:39:33,927 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4          | 2023-07-13 21:39:33,927 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:39:34,036 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start RPC server
datanode_4          | 2023-07-13 21:39:34,051 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: GrpcService started, listening on 9858
datanode_4          | 2023-07-13 21:39:34,056 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: GrpcService started, listening on 9856
datanode_4          | 2023-07-13 21:39:34,062 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: GrpcService started, listening on 9857
datanode_4          | 2023-07-13 21:39:34,110 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bb5c41c8-12ac-4f33-939d-67f4ed99ba46 is started using port 9858 for RATIS
datanode_4          | 2023-07-13 21:39:34,111 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bb5c41c8-12ac-4f33-939d-67f4ed99ba46 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-07-13 21:39:34,111 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bb5c41c8-12ac-4f33-939d-67f4ed99ba46 is started using port 9856 for RATIS_SERVER
datanode_4          | 2023-07-13 21:39:34,115 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-bb5c41c8-12ac-4f33-939d-67f4ed99ba46: Started
datanode_5          | 2023-07-13 21:40:13,148 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-07-13 21:40:13,148 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: ConfigurationManager, init=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-07-13 21:40:13,149 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-07-13 21:40:13,150 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-07-13 21:40:13,150 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-07-13 21:40:13,151 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:40:13,152 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:13,152 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-07-13 21:40:13,153 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-13 21:40:13,156 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-07-13 21:40:13,168 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:40:13,174 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-13 21:40:13,175 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-07-13 21:40:13,175 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-07-13 21:40:13,181 [grpc-default-executor-0] INFO server.RaftServer: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: addNew group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-FE9FF367F7A0:java.util.concurrent.CompletableFuture@77b0c9e6[Not completed]
datanode_5          | 2023-07-13 21:40:13,182 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-07-13 21:40:13,185 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-13 21:40:13,186 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 does not exist. Creating ...
datanode_5          | 2023-07-13 21:40:13,199 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/in_use.lock acquired by nodename 7@07f387232595
datanode_5          | 2023-07-13 21:40:13,206 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 has been successfully formatted.
datanode_5          | 2023-07-13 21:40:13,221 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO ratis.ContainerStateMachine: group-FE9FF367F7A0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-07-13 21:40:13,221 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-13 21:40:13,232 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-07-13 21:40:13,232 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:13,233 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-07-13 21:40:13,233 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-07-13 21:40:13,234 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:13,241 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-07-13 21:40:13,241 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-07-13 21:40:13,247 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-13 21:40:13,248 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-13 21:40:13,248 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:13,248 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-07-13 21:40:13,251 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:13,255 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:13,387 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:13,395 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:13,395 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:13,397 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:13,398 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:13,400 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: start as a follower, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:13,402 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-13 21:40:13,403 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState
datanode_5          | 2023-07-13 21:40:13,414 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:13,420 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FE9FF367F7A0,id=2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:40:13,422 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:13,422 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-07-13 21:40:13,422 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-07-13 21:40:13,423 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-07-13 21:40:13,424 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:14,103 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO impl.FollowerState: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5025044179ns, electionTimeout:5000ms
datanode_5          | 2023-07-13 21:40:14,105 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState
datanode_5          | 2023-07-13 21:40:14,115 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-07-13 21:40:14,132 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-07-13 21:40:14,134 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1
datanode_5          | 2023-07-13 21:40:14,160 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:14,178 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:14,178 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:14,194 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_5          | 2023-07-13 21:40:14,195 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_5          | 2023-07-13 21:40:14,327 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-07-13 21:40:14,330 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:OK-t0
datanode_1          | 2023-07-13 21:40:17,010 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:17,010 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-13 21:40:17,010 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-13 21:40:17,010 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:17,030 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-13 21:40:17,034 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-13 21:40:17,034 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:17,034 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/635788d3-7f27-4198-8993-d2c8ffc48200
datanode_1          | 2023-07-13 21:40:17,034 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-13 21:40:17,035 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-13 21:40:17,026 [a1afc4af-4388-40ba-b575-6eb773bd243a-server-thread1] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:40:17,073 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-13 21:40:17,101 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:17,103 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:40:17,103 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:17,149 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:40:17,186 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-13 21:40:17,194 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:40:17,283 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:40:17,287 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-13 21:40:17,294 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-07-13 21:40:17,294 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:40:17,295 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:40:17,321 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-13 21:40:17,321 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-13 21:40:17,325 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-13 21:40:17,325 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-13 21:40:17,325 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-13 21:40:17,326 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-13 21:40:17,328 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-13 21:40:17,329 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-07-13 21:40:17,333 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-13 21:40:17,333 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-13 21:40:17,343 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderStateImpl
datanode_5          | 2023-07-13 21:40:14,332 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_4          | 2023-07-13 21:39:34,211 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:40:07,955 [PipelineCommandHandlerThread-0] INFO server.RaftServer: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: addNew group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-4071CA03B11B:java.util.concurrent.CompletableFuture@43ed2c61[Not completed]
datanode_4          | 2023-07-13 21:40:08,063 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: new RaftServerImpl for group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-13 21:40:08,071 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-13 21:40:08,072 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-13 21:40:08,075 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-07-13 21:40:08,076 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:40:08,080 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-13 21:40:08,080 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-07-13 21:40:08,152 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-13 21:40:08,154 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-13 21:40:08,198 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-07-13 21:40:08,214 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-13 21:40:08,312 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:40:08,352 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:08,389 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-07-13 21:40:08,395 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-07-13 21:40:08,565 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-07-13 21:40:08,784 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-13 21:40:08,809 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-07-13 21:40:08,835 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-07-13 21:40:08,835 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-07-13 21:40:08,845 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-07-13 21:40:08,846 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-07-13 21:40:08,848 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b does not exist. Creating ...
datanode_4          | 2023-07-13 21:40:08,879 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/in_use.lock acquired by nodename 7@71107e799abb
datanode_4          | 2023-07-13 21:40:08,943 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b has been successfully formatted.
datanode_4          | 2023-07-13 21:40:09,086 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO ratis.ContainerStateMachine: group-4071CA03B11B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-13 21:40:09,099 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-13 21:40:09,226 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-13 21:40:09,229 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:09,232 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-13 21:40:09,238 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-07-13 21:40:09,248 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:09,271 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-07-13 21:40:14,342 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:14,357 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:14,357 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:14,411 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-07-13 21:40:14,415 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:OK-t1
datanode_5          | 2023-07-13 21:40:14,415 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1 ELECTION round 0: result PASSED
datanode_5          | 2023-07-13 21:40:14,416 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1
datanode_5          | 2023-07-13 21:40:14,419 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-07-13 21:40:14,420 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0FA64AED4490 with new leaderId: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:40:14,422 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: change Leader from null to 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 at term 1 for becomeLeader, leader elected after 6386ms
datanode_5          | 2023-07-13 21:40:14,500 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-07-13 21:40:14,535 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:14,537 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-07-13 21:40:14,564 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-07-13 21:40:14,566 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-07-13 21:40:14,568 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-07-13 21:40:14,598 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:14,600 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-07-13 21:40:14,848 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-07-13 21:40:14,851 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:14,851 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-07-13 21:40:14,884 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-07-13 21:40:14,898 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-07-13 21:40:14,913 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:40:14,930 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-07-13 21:40:14,933 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-07-13 21:40:14,942 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-13 21:40:14,942 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-07-13 21:40:14,973 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-07-13 21:40:14,979 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:14,980 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-07-13 21:40:14,982 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-07-13 21:40:14,985 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-07-13 21:40:14,985 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:40:14,987 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-07-13 21:40:14,992 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_4          | 2023-07-13 21:40:09,282 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-07-13 21:40:09,284 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:09,352 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: new bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b
datanode_4          | 2023-07-13 21:40:09,354 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-07-13 21:40:09,359 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-07-13 21:40:09,361 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:09,366 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-07-13 21:40:09,369 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-07-13 21:40:09,375 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-07-13 21:40:09,376 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:09,380 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-07-13 21:40:09,461 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:09,463 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:09,607 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:09,624 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:09,630 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:09,681 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:09,682 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:09,696 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:09,697 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-07-13 21:40:09,698 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState
datanode_4          | 2023-07-13 21:40:09,747 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4071CA03B11B,id=bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:40:09,758 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:09,759 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-07-13 21:40:09,762 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-13 21:40:09,797 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-13 21:40:09,794 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-07-13 21:40:09,806 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-13 21:40:09,899 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b
datanode_4          | 2023-07-13 21:40:13,305 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-bb5c41c8-12ac-4f33-939d-67f4ed99ba46: Detected pause in JVM or host machine approximately 0.105s with 0.081s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=81ms
datanode_4          | 2023-07-13 21:40:14,074 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: new RaftServerImpl for group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-13 21:40:14,076 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-13 21:40:14,077 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-13 21:40:14,077 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-07-13 21:40:14,078 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:09,768 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:09,775 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:09,777 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: start as a follower, conf=-1: peers:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:09,778 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:40:09,796 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState
datanode_3          | 2023-07-13 21:40:09,831 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-26B0221B21D7,id=1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_3          | 2023-07-13 21:40:09,833 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:40:09,849 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:40:09,849 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:40:09,850 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:40:09,848 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:09,851 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:09,980 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=627a05b4-e96a-4f2b-b164-26b0221b21d7
datanode_1          | 2023-07-13 21:40:17,350 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:17,350 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-13 21:40:17,350 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-13 21:40:17,351 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:17,351 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-13 21:40:17,351 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:40:17,352 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:17,352 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-13 21:40:17,352 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState
datanode_1          | 2023-07-13 21:40:17,352 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D2C8FFC48200,id=a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:40:17,353 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-13 21:40:17,353 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-13 21:40:17,353 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-13 21:40:17,353 [a1afc4af-4388-40ba-b575-6eb773bd243a-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-13 21:40:17,379 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-13 21:40:17,379 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-13 21:40:17,454 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200
datanode_1          | 2023-07-13 21:40:17,454 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200.
datanode_1          | 2023-07-13 21:40:17,491 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-LeaderElection1] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:18,254 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-4071CA03B11B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/current/log_inprogress_0
datanode_1          | 2023-07-13 21:40:18,269 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-0FA64AED4490-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/current/log_inprogress_0
datanode_1          | 2023-07-13 21:40:22,409 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO impl.FollowerState: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057238458ns, electionTimeout:5030ms
datanode_1          | 2023-07-13 21:40:22,410 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: shutdown a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState
datanode_1          | 2023-07-13 21:40:22,410 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-13 21:40:22,410 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-13 21:40:22,410 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-FollowerState] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2
datanode_1          | 2023-07-13 21:40:22,412 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:22,412 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-07-13 21:40:22,414 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:22,414 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.LeaderElection: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-13 21:40:22,415 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: shutdown a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2
datanode_1          | 2023-07-13 21:40:22,415 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-13 21:40:22,415 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D2C8FFC48200 with new leaderId: a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_1          | 2023-07-13 21:40:22,427 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: change Leader from null to a1afc4af-4388-40ba-b575-6eb773bd243a at term 1 for becomeLeader, leader elected after 5576ms
datanode_1          | 2023-07-13 21:40:22,433 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-13 21:40:22,435 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:22,436 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-13 21:40:22,438 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-13 21:40:22,438 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-13 21:40:22,439 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-13 21:40:22,439 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-13 21:40:22,439 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-13 21:40:22,440 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO impl.RoleInfo: a1afc4af-4388-40ba-b575-6eb773bd243a: start a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderStateImpl
datanode_1          | 2023-07-13 21:40:22,440 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-13 21:40:22,443 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/635788d3-7f27-4198-8993-d2c8ffc48200/current/log_inprogress_0
datanode_1          | 2023-07-13 21:40:22,449 [a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200-LeaderElection2] INFO server.RaftServer$Division: a1afc4af-4388-40ba-b575-6eb773bd243a@group-D2C8FFC48200: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-13 21:40:34,185 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:41:34,186 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:42:34,186 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:43:34,187 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:44:34,190 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:45:34,190 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:46:34,191 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-13 21:47:34,191 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:40:09,982 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=627a05b4-e96a-4f2b-b164-26b0221b21d7.
datanode_3          | 2023-07-13 21:40:09,985 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: addNew group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-4071CA03B11B:java.util.concurrent.CompletableFuture@5acef227[Not completed]
datanode_3          | 2023-07-13 21:40:10,038 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: new RaftServerImpl for group-4071CA03B11B:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:40:10,047 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:40:10,048 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:40:10,048 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:40:10,048 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:10,048 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:40:10,049 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:40:10,055 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:40:10,067 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:40:10,078 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:40:10,082 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:40:10,084 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:10,088 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:10,089 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:40:10,094 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:40:10,098 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:40:10,110 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:40:10,112 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:40:10,113 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:40:10,114 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:40:10,115 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:40:10,115 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:40:10,115 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b does not exist. Creating ...
datanode_3          | 2023-07-13 21:40:10,119 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/in_use.lock acquired by nodename 7@fe1ff34557f7
datanode_3          | 2023-07-13 21:40:10,124 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b has been successfully formatted.
datanode_3          | 2023-07-13 21:40:10,156 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO ratis.ContainerStateMachine: group-4071CA03B11B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:40:10,160 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:40:10,164 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:40:10,165 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:10,166 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:40:10,167 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:40:10,173 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:10,192 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-07-13 21:40:14,082 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-13 21:40:14,083 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-07-13 21:40:14,084 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: ConfigurationManager, init=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-13 21:40:14,086 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-13 21:40:14,086 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-07-13 21:40:14,087 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-13 21:40:14,088 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:40:14,089 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:14,090 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-07-13 21:40:14,091 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-07-13 21:40:14,092 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-07-13 21:40:14,098 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-13 21:40:14,098 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-07-13 21:40:14,102 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-07-13 21:40:14,102 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-07-13 21:40:14,103 [grpc-default-executor-0] INFO server.RaftServer: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: addNew group-FE9FF367F7A0:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-FE9FF367F7A0:java.util.concurrent.CompletableFuture@3fdab0af[Not completed]
datanode_4          | 2023-07-13 21:40:14,103 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-07-13 21:40:14,106 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-07-13 21:40:14,107 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 does not exist. Creating ...
datanode_4          | 2023-07-13 21:40:14,142 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/in_use.lock acquired by nodename 7@71107e799abb
datanode_4          | 2023-07-13 21:40:14,148 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0 has been successfully formatted.
datanode_4          | 2023-07-13 21:40:14,175 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO ratis.ContainerStateMachine: group-FE9FF367F7A0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-13 21:40:14,189 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-13 21:40:14,190 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-13 21:40:14,190 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:14,190 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-13 21:40:14,190 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-07-13 21:40:14,191 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:14,192 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-07-13 21:40:14,209 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-07-13 21:40:14,210 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: new bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-07-13 21:40:14,211 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:40:10,195 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:40:10,196 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:10,196 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b
datanode_3          | 2023-07-13 21:40:10,197 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:40:10,199 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:40:10,201 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:10,209 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:40:10,209 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:40:10,213 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:40:10,213 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:10,214 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:40:10,216 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:10,230 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:11,011 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Detected pause in JVM or host machine approximately 0.297s with 0.765s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=51ms
datanode_3          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=714ms
datanode_3          | 2023-07-13 21:40:11,033 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:11,045 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:11,047 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:40:11,047 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:11,051 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:11,052 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:11,057 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:40:11,057 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState
datanode_3          | 2023-07-13 21:40:11,065 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4071CA03B11B,id=1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_3          | 2023-07-13 21:40:11,065 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:40:11,072 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:40:11,072 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:40:11,072 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:40:11,074 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:11,094 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:11,094 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b
datanode_3          | 2023-07-13 21:40:13,863 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Detected pause in JVM or host machine approximately 0.336s with 0.560s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=560ms
datanode_3          | 2023-07-13 21:40:14,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: new RaftServerImpl for group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-13 21:40:14,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-13 21:40:14,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-13 21:40:14,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-13 21:40:14,222 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:14,222 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-13 21:40:14,222 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: ConfigurationManager, init=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-13 21:40:14,223 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-13 21:40:14,224 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-13 21:40:14,227 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-13 21:40:14,227 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:40:14,227 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-13 21:40:14,227 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-13 21:40:14,228 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-13 21:40:14,228 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-13 21:40:14,228 [grpc-default-executor-1] INFO server.RaftServer: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: addNew group-0FA64AED4490:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] returns      null 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_3          | 2023-07-13 21:40:14,233 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 does not exist. Creating ...
datanode_3          | 2023-07-13 21:40:14,243 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/in_use.lock acquired by nodename 7@fe1ff34557f7
datanode_3          | 2023-07-13 21:40:14,280 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490 has been successfully formatted.
datanode_3          | 2023-07-13 21:40:14,280 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO ratis.ContainerStateMachine: group-0FA64AED4490: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-13 21:40:14,281 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-13 21:40:14,284 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-13 21:40:14,284 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:14,313 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-13 21:40:14,321 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-13 21:40:14,322 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:14,326 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-13 21:40:14,328 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-13 21:40:14,329 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:14,330 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490
datanode_4          | 2023-07-13 21:40:14,212 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-07-13 21:40:14,212 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:14,213 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-07-13 21:40:14,213 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:14,216 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:14,401 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:14,401 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:14,402 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:14,406 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:14,408 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:14,413 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: start as a follower, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:14,413 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-07-13 21:40:14,413 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState
datanode_4          | 2023-07-13 21:40:14,415 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FE9FF367F7A0,id=bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:40:14,416 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:14,417 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-07-13 21:40:14,418 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-07-13 21:40:14,418 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-13 21:40:14,422 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-13 21:40:14,431 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-13 21:40:14,870 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO impl.FollowerState: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172354840ns, electionTimeout:5065ms
datanode_4          | 2023-07-13 21:40:14,871 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: shutdown bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState
datanode_4          | 2023-07-13 21:40:14,908 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-07-13 21:40:14,963 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-07-13 21:40:14,963 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1
datanode_4          | 2023-07-13 21:40:15,024 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:15,098 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-13 21:40:15,098 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-13 21:40:15,135 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_4          | 2023-07-13 21:40:15,135 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_4          | 2023-07-13 21:40:15,248 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: receive requestVote(PRE_VOTE, a1afc4af-4388-40ba-b575-6eb773bd243a, group-4071CA03B11B, 0, (t:0, i:0))
datanode_4          | 2023-07-13 21:40:15,257 [grpc-default-executor-1] INFO impl.VoteContext: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-CANDIDATE: accept PRE_VOTE from a1afc4af-4388-40ba-b575-6eb773bd243a: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-13 21:40:14,791 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-13 21:40:14,792 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-13 21:40:14,793 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-13 21:40:14,814 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-13 21:40:14,823 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:16,224 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5047ms but took extra 1180160562ns (> threshold = 300ms)
datanode_2          | 2023-07-13 21:40:16,227 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:40:16,227 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:40:16,238 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a4456cb8-edbd-41c8-9e5d-af9836a313a2: Detected pause in JVM or host machine approximately 1.291s with 1.387s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=97ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1290ms
datanode_2          | 2023-07-13 21:40:16,307 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:40:16,311 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-13 21:40:16,311 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-13 21:40:16,312 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:40:16,313 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-13 21:40:16,320 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: start as a follower, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:16,320 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-13 21:40:16,321 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState
datanode_2          | 2023-07-13 21:40:16,324 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FE52BEFF02DA,id=a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_2          | 2023-07-13 21:40:16,325 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-13 21:40:16,325 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-13 21:40:16,325 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-13 21:40:16,325 [a4456cb8-edbd-41c8-9e5d-af9836a313a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-13 21:40:16,331 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:40:16,331 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:40:16,333 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=1283747c-17ff-47ac-873e-fe52beff02da
datanode_2          | 2023-07-13 21:40:16,333 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=1283747c-17ff-47ac-873e-fe52beff02da.
datanode_2          | 2023-07-13 21:40:19,028 [grpc-default-executor-0] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: receive requestVote(PRE_VOTE, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-FE9FF367F7A0, 0, (t:0, i:0))
datanode_2          | 2023-07-13 21:40:19,034 [grpc-default-executor-0] INFO impl.VoteContext: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FOLLOWER: reject PRE_VOTE from 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-13 21:40:19,044 [grpc-default-executor-0] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0 replies to PRE_VOTE vote request: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a4456cb8-edbd-41c8-9e5d-af9836a313a2#0:FAIL-t0. Peer's state: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0:t0, leader=null, voted=, raftlog=Memoized:a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,247 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO impl.FollowerState: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:11288522286ns, electionTimeout:5018ms
datanode_2          | 2023-07-13 21:40:21,247 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: shutdown a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState
datanode_2          | 2023-07-13 21:40:21,248 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:40:21,252 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-07-13 21:40:21,252 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-FollowerState] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1
datanode_2          | 2023-07-13 21:40:21,256 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,259 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:40:15,495 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B replies to PRE_VOTE vote request: a1afc4af-4388-40ba-b575-6eb773bd243a<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t0. Peer's state: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B:t0, leader=null, voted=, raftlog=Memoized:bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:15,969 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: receive requestVote(ELECTION, a1afc4af-4388-40ba-b575-6eb773bd243a, group-4071CA03B11B, 1, (t:0, i:0))
datanode_4          | 2023-07-13 21:40:15,969 [grpc-default-executor-1] INFO impl.VoteContext: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-CANDIDATE: accept ELECTION from a1afc4af-4388-40ba-b575-6eb773bd243a: our priority 0 <= candidate's priority 1
datanode_4          | 2023-07-13 21:40:15,970 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_4          | 2023-07-13 21:40:15,970 [grpc-default-executor-1] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: shutdown bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1
datanode_4          | 2023-07-13 21:40:15,978 [grpc-default-executor-1] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-FollowerState
datanode_4          | 2023-07-13 21:40:16,008 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B replies to ELECTION vote request: a1afc4af-4388-40ba-b575-6eb773bd243a<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t1. Peer's state: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B:t1, leader=null, voted=a1afc4af-4388-40ba-b575-6eb773bd243a, raftlog=Memoized:bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:16,223 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=1) received 1 response(s) and 0 exception(s):
datanode_4          | 2023-07-13 21:40:16,286 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection:   Response 0: bb5c41c8-12ac-4f33-939d-67f4ed99ba46<-a1afc4af-4388-40ba-b575-6eb773bd243a#0:FAIL-t1
datanode_4          | 2023-07-13 21:40:16,287 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=1)
datanode_4          | 2023-07-13 21:40:16,591 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b.
datanode_4          | 2023-07-13 21:40:16,592 [PipelineCommandHandlerThread-0] INFO server.RaftServer: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: addNew group-94578CD12FB5:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-94578CD12FB5:java.util.concurrent.CompletableFuture@478dd43c[Not completed]
datanode_4          | 2023-07-13 21:40:16,597 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: new RaftServerImpl for group-94578CD12FB5:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: ConfigurationManager, init=-1: peers:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-07-13 21:40:16,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-13 21:40:16,608 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-07-13 21:40:16,608 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:16,609 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-07-13 21:40:16,609 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-07-13 21:40:16,609 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-07-13 21:40:16,621 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-13 21:40:16,621 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-13 21:40:14,332 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-13 21:40:14,333 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:40:14,334 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:14,335 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-13 21:40:14,337 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-13 21:40:14,338 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-13 21:40:14,338 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-13 21:40:14,338 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-13 21:40:14,343 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-13 21:40:14,350 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-13 21:40:14,414 [grpc-default-executor-2] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: receive requestVote(ELECTION, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-0FA64AED4490, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:40:14,444 [grpc-default-executor-2] WARN server.GrpcServerProtocolService: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Failed requestVote 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600->1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490 is not in [RUNNING]: current state is STARTING
datanode_3          | 2023-07-13 21:40:14,442 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:14,446 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-13 21:40:14,446 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-13 21:40:14,446 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:14,446 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-13 21:40:14,412 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: receive requestVote(PRE_VOTE, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-0FA64AED4490, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:40:14,446 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Failed requestVote 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600->1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490 is not in [RUNNING]: current state is STARTING
datanode_3          | 2023-07-13 21:40:14,474 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: start as a follower, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:14,474 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-13 21:40:14,475 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState
datanode_3          | 2023-07-13 21:40:14,507 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0FA64AED4490,id=1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_3          | 2023-07-13 21:40:14,508 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-13 21:40:14,508 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-13 21:40:14,508 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-13 21:40:14,508 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-13 21:40:14,548 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:14,818 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:14,991 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO impl.FollowerState: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5194989817ns, electionTimeout:5116ms
datanode_3          | 2023-07-13 21:40:14,992 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: shutdown 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState
datanode_3          | 2023-07-13 21:40:15,008 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-13 21:40:15,067 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-13 21:40:15,073 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-FollowerState] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1
datanode_5          | 2023-07-13 21:40:14,994 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-13 21:40:14,997 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-07-13 21:40:15,029 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderStateImpl
datanode_5          | 2023-07-13 21:40:15,205 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-13 21:40:15,513 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490.
datanode_5          | 2023-07-13 21:40:15,514 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: addNew group-B6ED32824BCA:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-B6ED32824BCA:java.util.concurrent.CompletableFuture@154bb947[Not completed]
datanode_5          | 2023-07-13 21:40:15,519 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: new RaftServerImpl for group-B6ED32824BCA:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-07-13 21:40:15,526 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-07-13 21:40:15,526 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: ConfigurationManager, init=-1: peers:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-07-13 21:40:15,527 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-13 21:40:15,528 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-07-13 21:40:15,569 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-13 21:40:15,573 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9f28966e-e086-4328-ab75-b6ed32824bca does not exist. Creating ...
datanode_5          | 2023-07-13 21:40:15,583 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9f28966e-e086-4328-ab75-b6ed32824bca/in_use.lock acquired by nodename 7@07f387232595
datanode_5          | 2023-07-13 21:40:15,593 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9f28966e-e086-4328-ab75-b6ed32824bca has been successfully formatted.
datanode_5          | 2023-07-13 21:40:15,686 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO ratis.ContainerStateMachine: group-B6ED32824BCA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-07-13 21:40:15,707 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-13 21:40:15,713 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-07-13 21:40:15,716 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:15,717 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-07-13 21:40:15,717 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-07-13 21:40:15,725 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:15,727 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-13 21:40:21,264 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:40:21,272 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:40:21,302 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_2          | 2023-07-13 21:40:21,346 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:40:21,346 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection:   Response 0: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t0
datanode_2          | 2023-07-13 21:40:21,346 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_2          | 2023-07-13 21:40:21,352 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,354 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-13 21:40:21,354 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-13 21:40:21,387 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-13 21:40:21,388 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection:   Response 0: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t1
datanode_2          | 2023-07-13 21:40:21,388 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1 ELECTION round 0: result PASSED
datanode_2          | 2023-07-13 21:40:21,388 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: shutdown a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1
datanode_2          | 2023-07-13 21:40:21,388 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:40:21,389 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FE9FF367F7A0 with new leaderId: a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_2          | 2023-07-13 21:40:21,391 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: change Leader from null to a4456cb8-edbd-41c8-9e5d-af9836a313a2 at term 1 for becomeLeader, leader elected after 12430ms
datanode_2          | 2023-07-13 21:40:21,464 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO impl.FollowerState: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139883551ns, electionTimeout:5129ms
datanode_2          | 2023-07-13 21:40:21,478 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: shutdown a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState
datanode_2          | 2023-07-13 21:40:21,478 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-13 21:40:21,478 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-07-13 21:40:21,478 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-FollowerState] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2
datanode_2          | 2023-07-13 21:40:21,495 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:40:21,496 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,497 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-13 21:40:21,505 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,505 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.LeaderElection: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-13 21:40:21,512 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:21,513 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:40:21,517 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:40:21,518 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-07-13 21:40:16,621 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-07-13 21:40:16,621 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-07-13 21:40:16,622 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-07-13 21:40:16,622 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-07-13 21:40:16,625 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/43147e0e-97a9-4d73-8143-94578cd12fb5 does not exist. Creating ...
datanode_4          | 2023-07-13 21:40:16,668 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/43147e0e-97a9-4d73-8143-94578cd12fb5/in_use.lock acquired by nodename 7@71107e799abb
datanode_4          | 2023-07-13 21:40:16,684 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/43147e0e-97a9-4d73-8143-94578cd12fb5 has been successfully formatted.
datanode_4          | 2023-07-13 21:40:16,685 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO ratis.ContainerStateMachine: group-94578CD12FB5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-13 21:40:16,685 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-07-13 21:40:16,756 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: new bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/43147e0e-97a9-4d73-8143-94578cd12fb5
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-07-13 21:40:16,757 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-07-13 21:40:16,758 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-13 21:40:16,876 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:16,876 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-07-13 21:40:16,876 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:16,877 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:16,877 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-13 21:40:16,877 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: start as a follower, conf=-1: peers:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:16,877 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-07-13 21:40:16,877 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState
datanode_4          | 2023-07-13 21:40:16,880 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-94578CD12FB5,id=bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:40:16,880 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-07-13 21:40:16,880 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-07-13 21:40:16,881 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-07-13 21:40:15,729 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-07-13 21:40:15,729 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:15,734 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9f28966e-e086-4328-ab75-b6ed32824bca
datanode_5          | 2023-07-13 21:40:15,736 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-07-13 21:40:15,736 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:15,738 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:15,738 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-07-13 21:40:15,738 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-13 21:40:15,738 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-13 21:40:15,738 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-07-13 21:40:15,739 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-07-13 21:40:15,739 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-07-13 21:40:15,740 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-13 21:40:15,818 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:15,819 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-07-13 21:40:15,819 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:15,820 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:15,820 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-13 21:40:15,838 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: start as a follower, conf=-1: peers:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:15,878 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-13 21:40:15,878 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState
datanode_5          | 2023-07-13 21:40:15,961 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-LeaderElection1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:16,053 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B6ED32824BCA,id=2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:40:16,054 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-07-13 21:40:16,054 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-07-13 21:40:16,054 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-07-13 21:40:16,054 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-07-13 21:40:16,061 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:16,126 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca
datanode_5          | 2023-07-13 21:40:16,197 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca.
datanode_5          | 2023-07-13 21:40:16,197 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:17,379 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-0FA64AED4490-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/current/log_inprogress_0
datanode_5          | 2023-07-13 21:40:18,435 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO impl.FollowerState: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5032757555ns, electionTimeout:5011ms
datanode_5          | 2023-07-13 21:40:18,437 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState
datanode_5          | 2023-07-13 21:40:18,438 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-07-13 21:40:16,881 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-13 21:40:16,882 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-13 21:40:16,882 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-13 21:40:16,883 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5
datanode_4          | 2023-07-13 21:40:16,883 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5.
datanode_4          | 2023-07-13 21:40:17,707 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4071CA03B11B with new leaderId: a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_4          | 2023-07-13 21:40:17,707 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: change Leader from null to a1afc4af-4388-40ba-b575-6eb773bd243a at term 1 for appendEntries, leader elected after 9401ms
datanode_4          | 2023-07-13 21:40:17,987 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread2] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:18,063 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread2] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-07-13 21:40:18,739 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: receive requestVote(PRE_VOTE, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, group-FE9FF367F7A0, 0, (t:0, i:0))
datanode_4          | 2023-07-13 21:40:18,777 [grpc-default-executor-1] INFO impl.VoteContext: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FOLLOWER: accept PRE_VOTE from 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: our priority 0 <= candidate's priority 0
datanode_4          | 2023-07-13 21:40:18,779 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0 replies to PRE_VOTE vote request: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t0. Peer's state: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0:t0, leader=null, voted=, raftlog=Memoized:bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:18,953 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-4071CA03B11B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/current/log_inprogress_0
datanode_4          | 2023-07-13 21:40:19,599 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-13 21:40:19,600 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-13 21:40:21,312 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: receive requestVote(PRE_VOTE, a4456cb8-edbd-41c8-9e5d-af9836a313a2, group-FE9FF367F7A0, 0, (t:0, i:0))
datanode_4          | 2023-07-13 21:40:21,312 [grpc-default-executor-1] INFO impl.VoteContext: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FOLLOWER: accept PRE_VOTE from a4456cb8-edbd-41c8-9e5d-af9836a313a2: our priority 0 <= candidate's priority 1
datanode_4          | 2023-07-13 21:40:21,314 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0 replies to PRE_VOTE vote request: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t0. Peer's state: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0:t0, leader=null, voted=, raftlog=Memoized:bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:21,370 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: receive requestVote(ELECTION, a4456cb8-edbd-41c8-9e5d-af9836a313a2, group-FE9FF367F7A0, 1, (t:0, i:0))
datanode_4          | 2023-07-13 21:40:21,370 [grpc-default-executor-1] INFO impl.VoteContext: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FOLLOWER: accept ELECTION from a4456cb8-edbd-41c8-9e5d-af9836a313a2: our priority 0 <= candidate's priority 1
datanode_4          | 2023-07-13 21:40:21,370 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_4          | 2023-07-13 21:40:21,370 [grpc-default-executor-1] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: shutdown bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState
datanode_4          | 2023-07-13 21:40:21,371 [grpc-default-executor-1] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState
datanode_4          | 2023-07-13 21:40:21,371 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState] INFO impl.FollowerState: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-FollowerState was interrupted
datanode_2          | 2023-07-13 21:40:21,520 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: shutdown a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2
datanode_2          | 2023-07-13 21:40:21,521 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-13 21:40:21,521 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FE52BEFF02DA with new leaderId: a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_2          | 2023-07-13 21:40:21,522 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:40:21,525 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: change Leader from null to a4456cb8-edbd-41c8-9e5d-af9836a313a2 at term 1 for becomeLeader, leader elected after 6831ms
datanode_2          | 2023-07-13 21:40:21,541 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-13 21:40:21,543 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:21,543 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-13 21:40:21,564 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-13 21:40:21,565 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-13 21:40:21,569 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-13 21:40:21,583 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:21,583 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-13 21:40:21,587 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-13 21:40:21,592 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-13 21:40:21,593 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderStateImpl
datanode_2          | 2023-07-13 21:40:21,616 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:40:21,617 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:21,617 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-13 21:40:21,621 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-13 21:40:21,623 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-13 21:40:21,624 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:40:21,624 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-07-13 21:40:21,625 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-07-13 21:40:21,625 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:40:21,625 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:40:21,632 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-13 21:40:21,632 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-13 21:40:21,632 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-13 21:40:21,633 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-13 21:40:21,633 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-13 21:40:21,633 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-13 21:40:21,634 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-07-13 21:40:21,634 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-07-13 21:40:21,634 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-13 21:40:21,635 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-13 21:40:21,638 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO impl.RoleInfo: a4456cb8-edbd-41c8-9e5d-af9836a313a2: start a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderStateImpl
datanode_2          | 2023-07-13 21:40:21,650 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:40:21,650 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-13 21:40:21,712 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-LeaderElection2] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA: set configuration 0: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:21,724 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-LeaderElection1] INFO server.RaftServer$Division: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0: set configuration 0: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-13 21:40:22,013 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE52BEFF02DA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1283747c-17ff-47ac-873e-fe52beff02da/current/log_inprogress_0
datanode_2          | 2023-07-13 21:40:22,021 [a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a4456cb8-edbd-41c8-9e5d-af9836a313a2@group-FE9FF367F7A0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/current/log_inprogress_0
datanode_2          | 2023-07-13 21:40:34,035 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:41:14,671 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a4456cb8-edbd-41c8-9e5d-af9836a313a2: Detected pause in JVM or host machine approximately 0.377s with 0.437s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=437ms
datanode_2          | 2023-07-13 21:41:34,039 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:42:34,040 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:43:34,040 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:44:34,041 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:45:34,042 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:46:34,042 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-13 21:47:34,043 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:40:21,384 [grpc-default-executor-1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0 replies to ELECTION vote request: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t1. Peer's state: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0:t1, leader=null, voted=a4456cb8-edbd-41c8-9e5d-af9836a313a2, raftlog=Memoized:bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:21,798 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FE9FF367F7A0 with new leaderId: a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_4          | 2023-07-13 21:40:21,798 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: change Leader from null to a4456cb8-edbd-41c8-9e5d-af9836a313a2 at term 1 for appendEntries, leader elected after 7710ms
datanode_4          | 2023-07-13 21:40:21,823 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0: set configuration 0: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:21,834 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46-server-thread1] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-07-13 21:40:21,842 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-FE9FF367F7A0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/current/log_inprogress_0
datanode_4          | 2023-07-13 21:40:21,902 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO impl.FollowerState: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5024119016ns, electionTimeout:5013ms
datanode_4          | 2023-07-13 21:40:21,902 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: shutdown bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState
datanode_4          | 2023-07-13 21:40:21,902 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-07-13 21:40:21,903 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-07-13 21:40:21,903 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-FollowerState] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2
datanode_4          | 2023-07-13 21:40:21,916 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:21,917 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_4          | 2023-07-13 21:40:21,924 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:21,924 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.LeaderElection: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_4          | 2023-07-13 21:40:21,924 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: shutdown bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2
datanode_4          | 2023-07-13 21:40:21,924 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4          | 2023-07-13 21:40:21,924 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-94578CD12FB5 with new leaderId: bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_4          | 2023-07-13 21:40:21,940 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: change Leader from null to bb5c41c8-12ac-4f33-939d-67f4ed99ba46 at term 1 for becomeLeader, leader elected after 5315ms
datanode_4          | 2023-07-13 21:40:21,946 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-07-13 21:40:21,980 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-13 21:40:21,983 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-07-13 21:40:21,998 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4          | 2023-07-13 21:40:21,998 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-07-13 21:40:21,999 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-07-13 21:40:22,007 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-13 21:40:22,015 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4          | 2023-07-13 21:40:22,025 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO impl.RoleInfo: bb5c41c8-12ac-4f33-939d-67f4ed99ba46: start bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderStateImpl
datanode_5          | 2023-07-13 21:40:18,438 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-07-13 21:40:18,438 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2
datanode_5          | 2023-07-13 21:40:18,446 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:18,458 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_5          | 2023-07-13 21:40:18,521 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-13 21:40:18,588 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-13 21:40:18,591 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for bb5c41c8-12ac-4f33-939d-67f4ed99ba46
datanode_5          | 2023-07-13 21:40:19,097 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_5          | 2023-07-13 21:40:19,098 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.LeaderElection:   Response 0: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-a4456cb8-edbd-41c8-9e5d-af9836a313a2#0:FAIL-t0
datanode_5          | 2023-07-13 21:40:19,098 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.LeaderElection:   Response 1: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600<-bb5c41c8-12ac-4f33-939d-67f4ed99ba46#0:OK-t0
datanode_5          | 2023-07-13 21:40:19,099 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode_5          | 2023-07-13 21:40:19,100 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_5          | 2023-07-13 21:40:19,102 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2
datanode_5          | 2023-07-13 21:40:19,102 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-LeaderElection2] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState
datanode_5          | 2023-07-13 21:40:21,310 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO impl.FollowerState: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5431681628ns, electionTimeout:5112ms
datanode_5          | 2023-07-13 21:40:21,310 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState
datanode_5          | 2023-07-13 21:40:21,310 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-07-13 21:40:21,311 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-07-13 21:40:21,311 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-FollowerState] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3
datanode_5          | 2023-07-13 21:40:21,313 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,313 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
datanode_5          | 2023-07-13 21:40:21,316 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,316 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.LeaderElection: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-07-13 21:40:21,316 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3
datanode_5          | 2023-07-13 21:40:21,316 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-07-13 21:40:21,316 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B6ED32824BCA with new leaderId: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_5          | 2023-07-13 21:40:21,317 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: change Leader from null to 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 at term 1 for becomeLeader, leader elected after 5788ms
datanode_5          | 2023-07-13 21:40:21,318 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-07-13 21:40:21,322 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:21,322 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-07-13 21:40:22,027 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-07-13 21:40:22,030 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/43147e0e-97a9-4d73-8143-94578cd12fb5/current/log_inprogress_0
datanode_4          | 2023-07-13 21:40:22,053 [bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5-LeaderElection2] INFO server.RaftServer$Division: bb5c41c8-12ac-4f33-939d-67f4ed99ba46@group-94578CD12FB5: set configuration 0: peers:[bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-13 21:40:34,215 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:41:34,216 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:42:34,216 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:43:34,217 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:44:34,218 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:45:34,219 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:46:34,219 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-13 21:47:34,220 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:40:15,140 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.LeaderElection: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:15,224 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: receive requestVote(PRE_VOTE, a1afc4af-4388-40ba-b575-6eb773bd243a, group-4071CA03B11B, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:40:15,237 [grpc-default-executor-0] INFO impl.VoteContext: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FOLLOWER: accept PRE_VOTE from a1afc4af-4388-40ba-b575-6eb773bd243a: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:40:15,282 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.LeaderElection: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-07-13 21:40:15,366 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B replies to PRE_VOTE vote request: a1afc4af-4388-40ba-b575-6eb773bd243a<-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0:OK-t0. Peer's state: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B:t0, leader=null, voted=, raftlog=Memoized:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:15,381 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.LeaderElection: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:15,387 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.LeaderElection: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-13 21:40:15,387 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: shutdown 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1
datanode_3          | 2023-07-13 21:40:15,387 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-13 21:40:15,387 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-26B0221B21D7 with new leaderId: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
datanode_3          | 2023-07-13 21:40:15,476 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: change Leader from null to 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775 at term 1 for becomeLeader, leader elected after 6811ms
datanode_3          | 2023-07-13 21:40:15,760 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-13 21:40:15,886 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:40:15,919 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-13 21:40:15,952 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: receive requestVote(ELECTION, a1afc4af-4388-40ba-b575-6eb773bd243a, group-4071CA03B11B, 1, (t:0, i:0))
datanode_3          | 2023-07-13 21:40:16,033 [grpc-default-executor-0] INFO impl.VoteContext: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FOLLOWER: accept ELECTION from a1afc4af-4388-40ba-b575-6eb773bd243a: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-13 21:40:16,033 [grpc-default-executor-1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: receive requestVote(PRE_VOTE, bb5c41c8-12ac-4f33-939d-67f4ed99ba46, group-4071CA03B11B, 0, (t:0, i:0))
datanode_3          | 2023-07-13 21:40:16,024 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-13 21:40:16,067 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_3          | 2023-07-13 21:40:16,068 [grpc-default-executor-0] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: shutdown 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState
datanode_3          | 2023-07-13 21:40:16,068 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-13 21:40:16,112 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b.
datanode_3          | 2023-07-13 21:40:16,136 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-13 21:40:16,143 [grpc-default-executor-0] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState
datanode_3          | 2023-07-13 21:40:16,068 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState] INFO impl.FollowerState: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FollowerState was interrupted
datanode_3          | 2023-07-13 21:40:16,220 [grpc-default-executor-0] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B replies to ELECTION vote request: a1afc4af-4388-40ba-b575-6eb773bd243a<-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0:OK-t1. Peer's state: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B:t1, leader=null, voted=a1afc4af-4388-40ba-b575-6eb773bd243a, raftlog=Memoized:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,325 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-07-13 21:40:21,333 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-07-13 21:40:21,333 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-07-13 21:40:21,334 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-07-13 21:40:21,334 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-07-13 21:40:21,334 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderStateImpl
datanode_5          | 2023-07-13 21:40:21,334 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-13 21:40:21,347 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9f28966e-e086-4328-ab75-b6ed32824bca/current/log_inprogress_0
datanode_5          | 2023-07-13 21:40:21,376 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA-LeaderElection3] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-B6ED32824BCA: set configuration 0: peers:[2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,471 [grpc-default-executor-0] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: receive requestVote(PRE_VOTE, a4456cb8-edbd-41c8-9e5d-af9836a313a2, group-FE9FF367F7A0, 0, (t:0, i:0))
datanode_5          | 2023-07-13 21:40:21,474 [grpc-default-executor-0] INFO impl.VoteContext: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FOLLOWER: accept PRE_VOTE from a4456cb8-edbd-41c8-9e5d-af9836a313a2: our priority 0 <= candidate's priority 1
datanode_5          | 2023-07-13 21:40:21,475 [grpc-default-executor-0] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0 replies to PRE_VOTE vote request: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-2ba100c8-b16f-4f8d-b8fc-edf40b2b8600#0:OK-t0. Peer's state: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0:t0, leader=null, voted=, raftlog=Memoized:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,490 [grpc-default-executor-1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: receive requestVote(ELECTION, a4456cb8-edbd-41c8-9e5d-af9836a313a2, group-FE9FF367F7A0, 1, (t:0, i:0))
datanode_5          | 2023-07-13 21:40:21,491 [grpc-default-executor-1] INFO impl.VoteContext: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FOLLOWER: accept ELECTION from a4456cb8-edbd-41c8-9e5d-af9836a313a2: our priority 0 <= candidate's priority 1
datanode_5          | 2023-07-13 21:40:21,501 [grpc-default-executor-1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_5          | 2023-07-13 21:40:21,502 [grpc-default-executor-1] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: shutdown 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState
datanode_5          | 2023-07-13 21:40:21,503 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState] INFO impl.FollowerState: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState was interrupted
datanode_5          | 2023-07-13 21:40:21,504 [grpc-default-executor-1] INFO impl.RoleInfo: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600: start 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-FollowerState
datanode_5          | 2023-07-13 21:40:21,506 [grpc-default-executor-1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0 replies to ELECTION vote request: a4456cb8-edbd-41c8-9e5d-af9836a313a2<-2ba100c8-b16f-4f8d-b8fc-edf40b2b8600#0:OK-t1. Peer's state: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0:t1, leader=null, voted=a4456cb8-edbd-41c8-9e5d-af9836a313a2, raftlog=Memoized:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,808 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FE9FF367F7A0 with new leaderId: a4456cb8-edbd-41c8-9e5d-af9836a313a2
datanode_5          | 2023-07-13 21:40:21,808 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-server-thread1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: change Leader from null to a4456cb8-edbd-41c8-9e5d-af9836a313a2 at term 1 for appendEntries, leader elected after 8656ms
datanode_5          | 2023-07-13 21:40:21,816 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-server-thread1] INFO server.RaftServer$Division: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0: set configuration 0: peers:[a4456cb8-edbd-41c8-9e5d-af9836a313a2|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-13 21:40:21,817 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600-server-thread1] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-13 21:40:21,839 [2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600@group-FE9FF367F7A0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5903532c-fa1f-46c8-9b10-fe9ff367f7a0/current/log_inprogress_0
datanode_5          | 2023-07-13 21:40:34,166 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:41:34,167 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:42:34,168 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:43:34,168 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:44:34,169 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:45:34,170 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:46:34,171 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-13 21:47:34,171 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:40:16,221 [grpc-default-executor-1] INFO impl.VoteContext: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-FOLLOWER: accept PRE_VOTE from bb5c41c8-12ac-4f33-939d-67f4ed99ba46: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-13 21:40:16,241 [grpc-default-executor-1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B replies to PRE_VOTE vote request: bb5c41c8-12ac-4f33-939d-67f4ed99ba46<-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775#0:OK-t1. Peer's state: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B:t1, leader=null, voted=a1afc4af-4388-40ba-b575-6eb773bd243a, raftlog=Memoized:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:16,390 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0FA64AED4490 with new leaderId: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
datanode_3          | 2023-07-13 21:40:16,418 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: change Leader from null to 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 at term 1 for appendEntries, leader elected after 2166ms
datanode_3          | 2023-07-13 21:40:16,466 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-13 21:40:16,553 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-13 21:40:16,557 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO impl.RoleInfo: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: start 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderStateImpl
datanode_3          | 2023-07-13 21:40:16,824 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600|rpc:172.24.0.6:9856|admin:172.24.0.6:9857|client:172.24.0.6:9858|dataStream:172.24.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:16,843 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:40:16,861 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:40:17,063 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-LeaderElection1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7: set configuration 0: peers:[1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:17,697 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-26B0221B21D7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/627a05b4-e96a-4f2b-b164-26b0221b21d7/current/log_inprogress_0
datanode_3          | 2023-07-13 21:40:17,747 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a30e7d5b-40a3-4d01-bd10-0fa64aed4490/current/log_inprogress_0
datanode_3          | 2023-07-13 21:40:18,036 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4071CA03B11B with new leaderId: a1afc4af-4388-40ba-b575-6eb773bd243a
datanode_3          | 2023-07-13 21:40:18,036 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread1] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: change Leader from null to a1afc4af-4388-40ba-b575-6eb773bd243a at term 1 for appendEntries, leader elected after 7952ms
datanode_3          | 2023-07-13 21:40:18,042 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread2] INFO server.RaftServer$Division: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B: set configuration 0: peers:[a1afc4af-4388-40ba-b575-6eb773bd243a|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER, 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775|rpc:172.24.0.9:9856|admin:172.24.0.9:9857|client:172.24.0.9:9858|dataStream:172.24.0.9:9858|priority:0|startupRole:FOLLOWER, bb5c41c8-12ac-4f33-939d-67f4ed99ba46|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-13 21:40:18,043 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775-server-thread2] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-13 21:40:18,053 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-4071CA03B11B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/55669a65-4b6b-4715-8550-4071ca03b11b/current/log_inprogress_0
datanode_3          | 2023-07-13 21:40:19,935 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:19,935 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:25,090 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:25,091 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:30,136 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:30,136 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:34,196 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:40:35,296 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:35,296 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:40,364 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:40,364 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:45,541 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:45,541 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:50,637 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:50,637 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:40:56,115 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1bbf5b6c-17c4-4aff-a715-8dff0fd5a775: Detected pause in JVM or host machine approximately 0.168s with 0.599s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=599ms
datanode_3          | 2023-07-13 21:40:56,119 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5035ms but took extra 446499675ns (> threshold = 300ms)
datanode_3          | 2023-07-13 21:40:56,124 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:40:56,125 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:01,295 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:01,296 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:06,401 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:06,402 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:11,554 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:11,555 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:16,664 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:16,664 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:21,776 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:21,776 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:26,933 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:26,933 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:31,995 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:31,996 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:34,198 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:41:37,185 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:37,185 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:42,229 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:42,229 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:47,229 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:47,229 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:52,285 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:52,285 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:41:57,337 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:41:57,337 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:02,533 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:02,533 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:07,591 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:07,592 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:12,782 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:12,782 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:17,921 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:17,922 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:22,978 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:22,980 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:28,152 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:28,152 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:33,190 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:33,190 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:34,198 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:42:38,347 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:38,347 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:43,370 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:43,370 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:48,517 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:48,517 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:53,706 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:53,707 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:42:58,755 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:42:58,755 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:03,841 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:03,841 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:08,878 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:08,878 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:13,985 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:13,986 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:19,070 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:19,071 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:38:25,840 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = b2cf55b1fcd9/172.24.0.12
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-07-13 21:38:25,952 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:38:37,093 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-13 21:38:41,685 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:38:42,278 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.12:9862
om_1                | 2023-07-13 21:38:42,279 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:38:42,279 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:38:42,573 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:38:45,100 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
om_1                | 2023-07-13 21:38:49,072 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-07-13 21:38:51,074 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-07-13 21:38:53,075 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-07-13 21:38:55,077 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-07-13 21:38:57,079 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
datanode_3          | 2023-07-13 21:43:24,270 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:24,270 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:29,448 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:29,448 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:34,199 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:43:34,583 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:34,584 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:39,694 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:39,694 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:44,744 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:44,745 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:49,857 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:49,857 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:43:54,913 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:43:54,913 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:00,077 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:00,077 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:05,222 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:05,222 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:10,276 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:10,276 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:15,467 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:15,468 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:20,589 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:20,590 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:25,769 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:25,769 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:30,805 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:30,806 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:34,199 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:44:35,824 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:35,825 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:40,835 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:40,836 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:45,884 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:45,884 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:50,907 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:44:50,907 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:44:56,006 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:38:59,084 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1                | 2023-07-13 21:39:01,088 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-07-13 21:39:03,140 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-07-13 21:39:05,143 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-07-13 21:39:07,144 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-07-13 21:39:09,146 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-07-13 21:39:11,156 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-07-13 21:39:13,158 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-07-13 21:39:15,160 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-07-13 21:39:17,165 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-07-13 21:39:19,169 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | 2023-07-13 21:39:21,172 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om_1                | 2023-07-13 21:39:23,174 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b2cf55b1fcd9/172.24.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 18.
om_1                | 2023-07-13 21:39:29,808 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e83d8b1d-d539-4110-bdbf-ca352359fa10 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3          | 2023-07-13 21:44:56,007 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:01,199 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:01,200 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:06,251 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:06,251 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:11,385 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:11,385 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:16,475 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:16,475 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:21,554 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:21,554 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:26,619 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:26,619 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:31,695 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:31,695 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:34,200 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:45:36,782 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:36,782 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:41,970 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:41,971 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:47,060 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:47,061 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:52,171 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:52,171 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:45:57,241 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:45:57,241 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:02,310 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:02,311 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:07,471 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:07,472 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:12,489 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 19.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de;layoutVersion=6
om_1                | 2023-07-13 21:39:32,519 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at b2cf55b1fcd9/172.24.0.12
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-13 21:39:39,076 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = b2cf55b1fcd9/172.24.0.12
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | 2023-07-13 21:46:12,490 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:17,506 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:17,507 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:22,677 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:22,678 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:27,817 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:27,818 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:32,923 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:32,923 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:34,200 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:46:37,936 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:37,936 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:43,055 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:43,056 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:48,254 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:48,254 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:53,424 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:53,424 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:46:58,496 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:46:58,496 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:03,591 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:03,591 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:08,747 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:08,747 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:13,753 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:13,754 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:18,875 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:18,876 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:24,053 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:24,053 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:29,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:29,221 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:34,201 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-13 21:47:34,421 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:34,422 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:39,462 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:39,462 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:44,656 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:44,657 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:49,817 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:49,817 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:47:54,888 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:47:54,889 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-13 21:48:00,007 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-13 21:48:00,007 [1bbf5b6c-17c4-4aff-a715-8dff0fd5a775@group-0FA64AED4490-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
om_1                | STARTUP_MSG:   java = 11.0.19
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-13 21:38:25,862 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 8701c2111563/172.24.0.14
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
recon_1             | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-07-13 21:39:39,095 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-13 21:39:40,962 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-13 21:39:41,649 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-13 21:39:41,833 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.12:9862
om_1                | 2023-07-13 21:39:41,833 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-13 21:39:41,833 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-13 21:39:41,989 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:39:42,131 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-07-13 21:39:42,737 [main] INFO reflections.Reflections: Reflections took 503 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-07-13 21:39:42,814 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-07-13 21:39:42,973 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:39:43,775 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
om_1                | 2023-07-13 21:39:43,850 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
om_1                | 2023-07-13 21:39:44,434 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-07-13 21:39:44,468 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:39:44,752 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-07-13 21:39:45,270 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-07-13 21:39:45,334 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-07-13 21:39:45,345 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:39:45,401 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-07-13 21:39:45,402 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-07-13 21:39:45,599 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-13 21:39:45,730 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-13 21:39:45,731 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-13 21:39:45,754 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-13 21:39:45,764 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:38:20,771 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = acbb772f7582/172.24.0.4
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:38:20,868 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:38:22,058 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:38:24,439 [main] INFO reflections.Reflections: Reflections took 1625 ms to scan 3 urls, producing 132 keys and 288 values 
scm_1               | 2023-07-13 21:38:25,792 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:38:25,925 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:38:28,375 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-13 21:38:29,559 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:39:45,800 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-13 21:39:45,814 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-13 21:39:45,886 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-13 21:39:45,895 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:39:45,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-13 21:39:45,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-13 21:39:45,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-13 21:39:45,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-07-13 21:39:45,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-13 21:39:45,899 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-13 21:39:45,901 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:39:45,901 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-13 21:39:45,902 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-13 21:39:45,915 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-13 21:39:45,919 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-13 21:39:45,919 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-13 21:39:46,207 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-13 21:39:46,210 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-07-13 21:39:46,211 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-07-13 21:39:46,211 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:39:46,211 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:39:46,214 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:39:46,223 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@65314e4[Not completed]
om_1                | 2023-07-13 21:39:46,223 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-13 21:39:46,228 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-07-13 21:39:46,261 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-13 21:39:46,274 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-13 21:39:46,275 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-13 21:39:46,275 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-13 21:39:46,275 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-13 21:39:46,275 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-13 21:39:46,275 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-13 21:39:46,291 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-13 21:39:46,293 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-13 21:39:46,318 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-13 21:39:46,318 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-13 21:39:46,353 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-13 21:39:46,359 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-07-13 21:39:46,369 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-13 21:39:46,369 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-13 21:39:46,415 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-07-13 21:39:46,542 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-13 21:39:46,553 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-13 21:39:46,554 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-07-13 21:39:46,556 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-07-13 21:39:46,557 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-13 21:39:46,557 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-13 21:39:47,006 [main] INFO reflections.Reflections: Reflections took 600 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1                | 2023-07-13 21:39:47,359 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-13 21:39:47,369 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-07-13 21:39:47,372 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
scm_1               | 2023-07-13 21:38:29,603 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:38:29,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:38:29,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:38:29,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:38:29,631 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-13 21:38:29,640 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-13 21:38:29,679 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:38:29,687 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-13 21:38:29,711 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:38:29,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-13 21:38:30,006 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-13 21:38:30,008 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-13 21:38:32,900 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-13 21:38:32,964 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-13 21:38:32,976 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-13 21:38:32,978 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:38:32,978 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:38:33,036 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:38:33,126 [main] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: addNew group-3B9DD2E479DE:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER] returns group-3B9DD2E479DE:java.util.concurrent.CompletableFuture@68ab0936[Not completed]
scm_1               | 2023-07-13 21:38:33,525 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10: new RaftServerImpl for group-3B9DD2E479DE:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:38:33,624 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:38:33,701 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: ConfigurationManager, init=-1: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:38:33,730 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:38:33,876 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:38:33,895 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:38:34,145 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-13 21:38:34,234 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-13 21:38:34,359 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-13 21:38:34,391 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:38:34,771 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-13 21:38:34,853 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-13 21:38:35,952 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:38:36,020 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-13 21:38:36,027 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-13 21:38:36,027 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:38:36,028 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-13 21:38:36,028 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:38:36,055 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de does not exist. Creating ...
scm_1               | 2023-07-13 21:38:36,142 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/in_use.lock acquired by nodename 12@acbb772f7582
scm_1               | 2023-07-13 21:38:36,202 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de has been successfully formatted.
scm_1               | 2023-07-13 21:38:36,299 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:39:48,336 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-13 21:39:48,354 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-13 21:39:48,355 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-13 21:39:48,464 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.12:9862
om_1                | 2023-07-13 21:39:48,465 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-13 21:39:48,468 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
scm_1               | 2023-07-13 21:38:36,398 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-13 21:38:36,454 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:38:36,511 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:38:36,519 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-13 21:38:36,586 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:38:36,782 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-13 21:38:36,784 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:38:36,784 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:38:36,934 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de
scm_1               | 2023-07-13 21:38:36,947 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:38:36,948 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:38:36,949 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:38:36,951 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-13 21:38:36,952 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-13 21:38:36,953 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-13 21:38:36,954 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-13 21:38:36,954 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-13 21:38:37,181 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-13 21:38:37,207 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:38:37,447 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-13 21:38:37,451 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-13 21:38:37,452 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-13 21:38:37,560 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:38:37,591 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:38:37,594 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: start as a follower, conf=-1: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:38:37,674 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-07-13 21:38:37,688 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState
scm_1               | 2023-07-13 21:38:37,735 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-13 21:38:37,735 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-13 21:38:37,775 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B9DD2E479DE,id=e83d8b1d-d539-4110-bdbf-ca352359fa10
scm_1               | 2023-07-13 21:38:37,869 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-13 21:38:37,939 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-13 21:38:37,940 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-13 21:38:37,940 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-13 21:38:38,038 [main] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: start RPC server
scm_1               | 2023-07-13 21:38:38,704 [main] INFO server.GrpcService: e83d8b1d-d539-4110-bdbf-ca352359fa10: GrpcService started, listening on 9894
scm_1               | 2023-07-13 21:38:38,806 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e83d8b1d-d539-4110-bdbf-ca352359fa10: Started
scm_1               | 2023-07-13 21:38:42,826 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.FollowerState: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5138613951ns, electionTimeout:5090ms
scm_1               | 2023-07-13 21:38:42,827 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState
scm_1               | 2023-07-13 21:38:42,828 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-13 21:38:42,831 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1             | 2023-07-13 21:38:25,995 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-07-13 21:38:32,590 [main] INFO reflections.Reflections: Reflections took 609 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-07-13 21:38:40,175 [main] INFO reflections.Reflections: Reflections took 638 ms to scan 3 urls, producing 132 keys and 288 values 
recon_1             | 2023-07-13 21:38:41,013 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-13 21:38:45,541 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:38:56,544 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-13 21:38:59,598 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-13 21:38:59,624 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-13 21:39:00,036 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-13 21:39:00,327 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-13 21:39:00,334 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-13 21:39:07,180 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-13 21:39:07,337 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-13 21:39:07,539 [main] INFO util.log: Logging initialized @57711ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-13 21:39:08,366 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-07-13 21:39:08,451 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-07-13 21:39:08,670 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-13 21:39:08,673 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-13 21:39:08,700 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-13 21:39:08,706 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-13 21:39:09,536 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-07-13 21:39:09,585 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-13 21:39:12,071 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-13 21:39:12,152 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-07-13 21:39:12,249 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-13 21:39:12,640 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-13 21:39:12,648 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-13 21:39:17,723 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:39:18,802 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:39:18,983 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-07-13 21:39:18,988 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-13 21:39:19,268 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:39:19,554 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-07-13 21:39:19,617 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-13 21:39:19,774 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-13 21:39:19,845 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-13 21:39:19,958 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-07-13 21:39:21,064 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-13 21:39:21,127 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-13 21:39:21,216 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-07-13 21:39:21,272 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-13 21:39:21,394 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-13 21:39:21,617 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-07-13 21:39:21,634 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-13 21:39:21,634 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-13 21:39:21,771 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-13 21:39:21,808 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-13 21:39:21,808 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-13 21:39:23,237 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-13 21:39:23,242 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-07-13 21:39:23,389 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-13 21:39:23,391 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:38:42,832 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1
scm_1               | 2023-07-13 21:38:42,852 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:38:42,858 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-07-13 21:38:42,871 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:38:42,871 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-13 21:38:42,872 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1
scm_1               | 2023-07-13 21:38:42,872 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-13 21:38:42,880 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: change Leader from null to e83d8b1d-d539-4110-bdbf-ca352359fa10 at term 1 for becomeLeader, leader elected after 8736ms
scm_1               | 2023-07-13 21:38:42,973 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:38:43,012 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:38:43,013 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:38:43,124 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:38:43,210 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:38:43,235 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:38:43,326 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:38:43,475 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:38:43,500 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderStateImpl
scm_1               | 2023-07-13 21:38:44,247 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-13 21:38:44,849 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: set configuration 0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:38:45,406 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/log_inprogress_0
scm_1               | 2023-07-13 21:38:46,771 [main] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: close
scm_1               | 2023-07-13 21:38:46,772 [main] INFO server.GrpcService: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown server GrpcServerProtocolService now
scm_1               | 2023-07-13 21:38:46,779 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: shutdown
scm_1               | 2023-07-13 21:38:46,779 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B9DD2E479DE,id=e83d8b1d-d539-4110-bdbf-ca352359fa10
scm_1               | 2023-07-13 21:38:46,779 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderStateImpl
scm_1               | 2023-07-13 21:38:46,809 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO impl.PendingRequests: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-13 21:38:46,900 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO impl.StateMachineUpdater: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-07-13 21:38:46,963 [main] INFO server.GrpcService: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-07-13 21:38:46,974 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO impl.StateMachineUpdater: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-07-13 21:38:47,016 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO impl.StateMachineUpdater: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-07-13 21:38:47,082 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: closes. applyIndex: 0
scm_1               | 2023-07-13 21:38:47,498 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker close()
scm_1               | 2023-07-13 21:38:47,501 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e83d8b1d-d539-4110-bdbf-ca352359fa10: Stopped
scm_1               | 2023-07-13 21:38:47,501 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:39:48,480 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@b2cf55b1fcd9
om_1                | 2023-07-13 21:39:48,487 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-13 21:39:48,491 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-13 21:39:48,501 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-13 21:39:48,501 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:39:48,503 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-13 21:39:48,504 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-13 21:39:48,512 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:39:48,519 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-13 21:39:48,519 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-13 21:39:48,519 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:39:48,526 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-13 21:39:48,526 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:39:48,527 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-13 21:39:48,528 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-13 21:39:48,529 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-13 21:39:48,531 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-13 21:39:48,532 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-13 21:39:48,535 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-13 21:39:48,536 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-13 21:39:48,546 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-13 21:39:48,546 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-13 21:39:48,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-13 21:39:48,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-13 21:39:48,571 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-13 21:39:48,582 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:39:48,582 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-13 21:39:48,588 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:39:48,591 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-13 21:39:48,593 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:39:48,598 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-13 21:39:48,601 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-13 21:39:48,602 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-13 21:39:48,602 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-13 21:39:48,603 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-13 21:39:48,605 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-13 21:39:48,607 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-13 21:39:48,614 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-13 21:39:48,677 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-13 21:39:48,683 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-13 21:39:48,691 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-13 21:39:48,768 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-13 21:39:48,768 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-13 21:39:48,795 [main] INFO util.log: Logging initialized @14840ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-13 21:39:48,964 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-07-13 21:39:48,985 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-13 21:39:48,999 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-13 21:39:49,008 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-13 21:39:49,009 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-13 21:39:49,009 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-13 21:39:49,057 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-07-13 21:39:49,062 [main] INFO http.HttpServer2: Jetty bound to port 9874
scm_1               | 2023-07-13 21:38:47,505 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-190c981a-dbc7-49fd-8a00-3b9dd2e479de; layoutVersion=7; scmId=e83d8b1d-d539-4110-bdbf-ca352359fa10
scm_1               | 2023-07-13 21:38:47,545 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at acbb772f7582/172.24.0.4
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-13 21:39:05,278 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = acbb772f7582/172.24.0.4
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 2023-07-13 21:39:23,397 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-13 21:39:23,436 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@8a11a19{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-13 21:39:23,440 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ecf4b3e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-13 21:39:28,568 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1f4fc447{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-5013497330026981552/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-07-13 21:39:28,587 [main] INFO server.AbstractConnector: Started ServerConnector@1f7b8d59{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-13 21:39:28,588 [main] INFO server.Server: Started @78760ms
recon_1             | 2023-07-13 21:39:28,599 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-13 21:39:28,599 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-13 21:39:28,601 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-13 21:39:28,601 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-13 21:39:28,619 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-13 21:39:28,630 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-13 21:39:28,631 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-13 21:39:28,631 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-13 21:39:28,633 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-13 21:39:28,639 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:39:35,812 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-07-13 21:39:35,813 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-13 21:39:35,813 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-07-13 21:39:35,822 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-13 21:39:35,829 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-13 21:39:36,144 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-13 21:39:36,483 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:60796 / 172.24.0.6:60796
recon_1             | 2023-07-13 21:39:37,723 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:50448 / 172.24.0.8:50448: output error
recon_1             | 2023-07-13 21:39:37,724 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:41362 / 172.24.0.10:41362: output error
recon_1             | 2023-07-13 21:39:37,729 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:58826 / 172.24.0.6:58826: output error
recon_1             | 2023-07-13 21:39:37,731 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:48852 / 172.24.0.10:48852: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:44980 / 172.24.0.7:44980: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:58834 / 172.24.0.6:58834: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:53112 / 172.24.0.8:53112: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:44992 / 172.24.0.7:44992: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:59408 / 172.24.0.9:59408: output error
recon_1             | 2023-07-13 21:39:37,730 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:41346 / 172.24.0.10:41346: output error
recon_1             | 2023-07-13 21:39:37,733 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:35394 / 172.24.0.7:35394: output error
recon_1             | 2023-07-13 21:39:37,733 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:59400 / 172.24.0.9:59400: output error
recon_1             | 2023-07-13 21:39:37,745 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:36644 / 172.24.0.9:36644: output error
recon_1             | 2023-07-13 21:39:37,746 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:53114 / 172.24.0.8:53114: output error
recon_1             | 2023-07-13 21:39:37,749 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-13 21:38:25,471 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:38:25,478 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-13 21:38:25,859 [main] INFO util.log: Logging initialized @17519ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-13 21:38:27,648 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-07-13 21:38:27,857 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-13 21:38:27,936 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-13 21:38:27,959 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-13 21:38:27,959 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-13 21:38:27,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-13 21:38:28,387 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir4857377677148947414
s3g_1               | 2023-07-13 21:38:30,184 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 1bbc2f9f2cda/172.24.0.13
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:31Z
scm_1               | STARTUP_MSG:   java = 11.0.19
om_1                | 2023-07-13 21:39:49,063 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-07-13 21:39:49,124 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-13 21:39:49,124 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-13 21:39:49,126 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-13 21:39:49,140 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d0e7e6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-13 21:39:49,140 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@64e443c1{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-13 21:39:49,297 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2e185af0{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-8672170799892115619/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-07-13 21:39:49,315 [main] INFO server.AbstractConnector: Started ServerConnector@14a97e7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-13 21:39:49,315 [main] INFO server.Server: Started @15360ms
om_1                | 2023-07-13 21:39:49,325 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-13 21:39:49,325 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-13 21:39:49,327 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-13 21:39:49,333 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-13 21:39:49,334 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-13 21:39:49,428 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-07-13 21:39:49,563 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-07-13 21:39:53,748 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5155330742ns, electionTimeout:5139ms
om_1                | 2023-07-13 21:39:53,749 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-13 21:39:53,750 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-13 21:39:53,758 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-07-13 21:39:53,758 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:39:53,770 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:39:53,772 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-07-13 21:39:53,776 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:39:53,776 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-13 21:39:53,777 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-13 21:39:53,777 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-13 21:39:53,784 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 7424ms
om_1                | 2023-07-13 21:39:53,793 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-13 21:39:53,805 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:39:53,807 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-13 21:39:53,816 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-13 21:39:53,816 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-13 21:39:53,817 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-13 21:39:53,840 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-13 21:39:53,843 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-13 21:39:53,848 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-13 21:39:53,893 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-13 21:39:53,997 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-13 21:39:54,086 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-13 21:39:54,194 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-07-13 21:40:28,333 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol7m84v for user:hadoop
om_1                | 2023-07-13 21:40:29,573 [qtp1406754442-52] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-07-13 21:40:30,487 [qtp1406754442-52] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1689284430381 in 105 milliseconds
om_1                | 2023-07-13 21:40:30,541 [qtp1406754442-52] INFO db.RDBCheckpointUtils: Waited for 52 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1689284430381 availability.
om_1                | 2023-07-13 21:40:30,607 [qtp1406754442-52] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 60 milliseconds
om_1                | 2023-07-13 21:40:30,607 [qtp1406754442-52] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/2fbf702a9ccea1521e48eb5c4d692b673e69df1c ; compiled by 'runner' on 2023-07-13T20:32Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-07-13 21:39:05,338 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-13 21:39:06,183 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:39:08,177 [main] INFO reflections.Reflections: Reflections took 1312 ms to scan 3 urls, producing 132 keys and 288 values 
scm_1               | 2023-07-13 21:39:09,739 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-13 21:39:09,894 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-13 21:39:16,680 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-13 21:40:30,607 [qtp1406754442-52] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689284430381
om_1                | 2023-07-13 21:40:35,330 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: default of layout LEGACY in volume: vol7m84v
om_1                | 2023-07-13 21:40:41,038 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ratis of layout LEGACY in volume: vol7m84v
om_1                | 2023-07-13 21:40:46,612 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ecbucket of layout LEGACY in volume: vol7m84v
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir4857377677148947414, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-07-13 21:38:30,279 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-13 21:38:30,429 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-13 21:38:31,633 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-07-13 21:38:32,863 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-13 21:38:32,863 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-07-13 21:38:33,178 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-07-13 21:38:33,209 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-13 21:38:33,229 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-07-13 21:38:33,608 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-13 21:38:33,609 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-13 21:38:33,666 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-07-13 21:38:33,973 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@d816dde{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-13 21:38:34,031 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2a7b6f69{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-07-13 21:38:43,267 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.115s with 0.252s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=145ms
s3g_1               | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=107ms
s3g_1               | 2023-07-13 21:38:49,763 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.296s with 0.343s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=343ms
s3g_1               | 2023-07-13 21:39:01,183 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.253s with 0.255s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=255ms
s3g_1               | 2023-07-13 21:39:10,537 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6bfdaa7a{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir4857377677148947414/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-12518607774322070900/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-07-13 21:39:10,633 [main] INFO server.AbstractConnector: Started ServerConnector@32c0915e{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-13 21:39:10,645 [main] INFO server.Server: Started @62304ms
s3g_1               | 2023-07-13 21:39:10,673 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-07-13 21:39:10,673 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-07-13 21:39:10,686 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1               | 2023-07-13 21:39:11,783 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.360s with 0.256s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=256ms
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,768 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,768 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,769 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-13 21:39:17,336 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-13 21:39:18,335 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-07-13 21:39:18,336 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-13 21:39:18,509 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-13 21:39:18,929 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e83d8b1d-d539-4110-bdbf-ca352359fa10
scm_1               | 2023-07-13 21:39:19,173 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-13 21:39:19,192 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:39:19,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:39:19,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-13 21:39:19,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-13 21:39:19,196 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-13 21:39:19,196 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-13 21:39:19,197 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-13 21:39:19,204 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:39:19,204 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-13 21:39:19,205 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:39:19,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-13 21:39:19,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-13 21:39:19,232 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-13 21:39:19,613 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-13 21:39:19,622 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-13 21:39:19,622 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-13 21:39:19,623 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:39:19,623 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:39:19,636 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:39:19,666 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: found a subdirectory /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de
scm_1               | 2023-07-13 21:39:19,712 [main] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: addNew group-3B9DD2E479DE:[] returns group-3B9DD2E479DE:java.util.concurrent.CompletableFuture@77e9dca8[Not completed]
scm_1               | 2023-07-13 21:39:19,928 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10: new RaftServerImpl for group-3B9DD2E479DE:[] with SCMStateMachine:uninitialized
scm_1               | 2023-07-13 21:39:19,929 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-13 21:39:19,930 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-13 21:39:19,930 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-13 21:39:19,930 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-13 21:39:19,930 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-13 21:39:19,933 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-13 21:39:19,967 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-13 21:39:19,971 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-13 21:39:19,995 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-13 21:39:19,996 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-13 21:39:20,036 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-13 21:39:20,051 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-13 21:39:20,071 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-13 21:39:20,072 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-13 21:39:20,148 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-13 21:39:20,603 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-13 21:39:20,606 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-13 21:39:20,615 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-13 21:39:20,619 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-13 21:39:20,619 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-13 21:39:20,619 [e83d8b1d-d539-4110-bdbf-ca352359fa10-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-13 21:39:20,622 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-13 21:39:20,622 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-13 21:39:20,627 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-07-13 21:39:20,730 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-07-13 21:39:20,861 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-13 21:39:20,863 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-13 21:39:20,887 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-13 21:39:20,890 [main] INFO ha.SequenceIdGenerator: upgrade rootCertificateId to 1
scm_1               | 2023-07-13 21:39:20,899 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-13 21:39:21,136 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-13 21:39:21,173 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-07-13 21:39:21,179 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-13 21:39:21,193 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-13 21:39:21,237 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-13 21:39:21,237 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-13 21:39:21,249 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-13 21:39:21,251 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:21,256 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-07-13 21:39:21,272 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-07-13 21:39:21,288 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-07-13 21:39:21,288 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-07-13 21:39:21,368 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-13 21:39:21,370 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-13 21:39:21,424 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-13 21:39:21,646 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-13 21:39:21,679 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-13 21:39:21,682 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-13 21:39:21,719 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-13 21:39:21,726 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:21,733 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:39:21,814 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-07-13 21:39:23,683 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:39:23,755 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:39:23,823 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-07-13 21:39:23,829 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-13 21:39:23,940 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:39:23,956 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:39:23,957 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-07-13 21:39:24,031 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-13 21:39:24,259 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-13 21:39:24,337 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-13 21:39:24,351 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-07-13 21:39:24,359 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-13 21:39:24,804 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-07-13 21:39:24,805 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-07-13 21:39:24,810 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-13 21:39:24,841 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:39:24,865 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-07-13 21:39:24,891 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/in_use.lock acquired by nodename 6@acbb772f7582
scm_1               | 2023-07-13 21:39:24,932 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=e83d8b1d-d539-4110-bdbf-ca352359fa10} from /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/raft-meta
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-13 21:39:25,141 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: set configuration 0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:25,157 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-13 21:39:25,213 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-13 21:39:25,220 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:39:25,227 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-13 21:39:25,230 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-13 21:39:25,242 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:39:25,269 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-13 21:39:25,270 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-13 21:39:25,272 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:39:25,312 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de
scm_1               | 2023-07-13 21:39:25,339 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:39:25,340 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:39:25,341 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-13 21:39:25,359 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-13 21:39:25,359 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-13 21:39:25,367 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-13 21:39:25,368 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-13 21:39:25,368 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-13 21:39:25,438 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-13 21:39:25,442 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-13 21:39:25,929 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-13 21:39:25,934 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-13 21:39:25,934 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-13 21:39:26,028 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: set configuration 0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:26,041 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/log_inprogress_0
scm_1               | 2023-07-13 21:39:26,046 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-13 21:39:26,180 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: start as a follower, conf=0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:26,180 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-07-13 21:39:26,182 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState
scm_1               | 2023-07-13 21:39:26,188 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-13 21:39:26,188 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-13 21:39:26,188 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B9DD2E479DE,id=e83d8b1d-d539-4110-bdbf-ca352359fa10
scm_1               | 2023-07-13 21:39:26,191 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-13 21:39:26,195 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-13 21:39:26,196 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-13 21:39:26,197 [e83d8b1d-d539-4110-bdbf-ca352359fa10-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-13 21:39:26,217 [main] INFO server.RaftServer: e83d8b1d-d539-4110-bdbf-ca352359fa10: start RPC server
scm_1               | 2023-07-13 21:39:26,340 [main] INFO server.GrpcService: e83d8b1d-d539-4110-bdbf-ca352359fa10: GrpcService started, listening on 9894
scm_1               | 2023-07-13 21:39:26,363 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e83d8b1d-d539-4110-bdbf-ca352359fa10: Started
scm_1               | 2023-07-13 21:39:26,375 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
scm_1               | 2023-07-13 21:39:26,375 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-07-13 21:39:26,378 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-07-13 21:39:26,414 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-07-13 21:39:26,414 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-07-13 21:39:26,962 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-13 21:39:27,114 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-13 21:39:27,115 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-13 21:39:27,951 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-13 21:39:27,973 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:39:28,382 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-13 21:39:28,413 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:39:28,415 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-13 21:39:28,421 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:39:28,426 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-13 21:39:28,987 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-13 21:39:28,995 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-13 21:39:29,215 [main] INFO util.log: Logging initialized @39435ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-13 21:39:29,849 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-13 21:39:29,874 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-13 21:39:29,898 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-13 21:39:29,902 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-13 21:39:29,904 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-13 21:39:29,905 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-13 21:39:30,057 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-07-13 21:39:30,061 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-13 21:39:30,065 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-07-13 21:39:30,196 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-13 21:39:30,196 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-13 21:39:30,199 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-07-13 21:39:30,227 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@419dbd6d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-13 21:39:30,229 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@582b6362{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-13 21:39:30,557 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@72426ced{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-1008677660246485036/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-07-13 21:39:30,581 [main] INFO server.AbstractConnector: Started ServerConnector@1ec8afc4{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-13 21:39:30,591 [main] INFO server.Server: Started @40811ms
scm_1               | 2023-07-13 21:39:30,595 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-13 21:39:30,595 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-13 21:39:30,597 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-13 21:39:31,254 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.FollowerState: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072538970ns, electionTimeout:5063ms
scm_1               | 2023-07-13 21:39:31,257 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState
scm_1               | 2023-07-13 21:39:31,259 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-13 21:39:31,267 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-13 21:39:31,268 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-FollowerState] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1
scm_1               | 2023-07-13 21:39:31,283 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:31,287 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-07-13 21:39:31,320 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:31,323 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.LeaderElection: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-13 21:39:31,323 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: shutdown e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:39:37,771 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-07-13 21:39:31,327 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-13 21:39:31,328 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-13 21:39:31,328 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-07-13 21:39:31,336 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: change Leader from null to e83d8b1d-d539-4110-bdbf-ca352359fa10 at term 2 for becomeLeader, leader elected after 11292ms
scm_1               | 2023-07-13 21:39:31,363 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-13 21:39:31,375 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:39:31,377 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-13 21:39:31,389 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-13 21:39:31,393 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-13 21:39:31,395 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-13 21:39:31,410 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-13 21:39:31,415 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-13 21:39:31,420 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO impl.RoleInfo: e83d8b1d-d539-4110-bdbf-ca352359fa10: start e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderStateImpl
scm_1               | 2023-07-13 21:39:31,433 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-13 21:39:31,441 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/log_inprogress_0 to /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/log_0-0
scm_1               | 2023-07-13 21:39:31,474 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-LeaderElection1] INFO server.RaftServer$Division: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE: set configuration 1: peers:[e83d8b1d-d539-4110-bdbf-ca352359fa10|rpc:acbb772f7582:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-13 21:39:31,482 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/190c981a-dbc7-49fd-8a00-3b9dd2e479de/current/log_inprogress_1
scm_1               | 2023-07-13 21:39:31,496 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-07-13 21:39:31,498 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-13 21:39:31,508 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:31,508 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-13 21:39:31,508 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-13 21:39:31,509 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-13 21:39:31,520 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-13 21:39:31,549 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-13 21:39:32,342 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:46392 / 172.24.0.10:46392: output error
scm_1               | 2023-07-13 21:39:32,343 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:51736 / 172.24.0.6:51736: output error
scm_1               | 2023-07-13 21:39:32,344 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:48736 / 172.24.0.7:48736: output error
scm_1               | 2023-07-13 21:39:32,358 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:47646 / 172.24.0.9:47646: output error
scm_1               | 2023-07-13 21:39:32,393 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-13 21:40:06,775 [IPC Server handler 2 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
recon_1             | 2023-07-13 21:40:06,804 [IPC Server handler 62 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/bb5c41c8-12ac-4f33-939d-67f4ed99ba46
recon_1             | 2023-07-13 21:40:06,811 [IPC Server handler 17 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a1afc4af-4388-40ba-b575-6eb773bd243a
recon_1             | 2023-07-13 21:40:06,811 [IPC Server handler 2 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600{ip: 172.24.0.6, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:40:06,837 [IPC Server handler 17 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a1afc4af-4388-40ba-b575-6eb773bd243a{ip: 172.24.0.10, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:39:32,397 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:39:32,397 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:39:32,397 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 2023-07-13 21:40:06,840 [IPC Server handler 62 on default port 9891] INFO node.SCMNodeManager: Registered Data node : bb5c41c8-12ac-4f33-939d-67f4ed99ba46{ip: 172.24.0.8, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:40:07,101 [IPC Server handler 44 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
recon_1             | 2023-07-13 21:40:07,109 [IPC Server handler 44 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775{ip: 172.24.0.9, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:40:07,218 [IPC Server handler 63 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a4456cb8-edbd-41c8-9e5d-af9836a313a2
recon_1             | 2023-07-13 21:40:07,224 [IPC Server handler 63 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a4456cb8-edbd-41c8-9e5d-af9836a313a2{ip: 172.24.0.7, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-13 21:40:07,546 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600 to Node DB.
recon_1             | 2023-07-13 21:40:07,553 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a1afc4af-4388-40ba-b575-6eb773bd243a to Node DB.
recon_1             | 2023-07-13 21:40:07,553 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node bb5c41c8-12ac-4f33-939d-67f4ed99ba46 to Node DB.
recon_1             | 2023-07-13 21:40:07,554 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775 to Node DB.
recon_1             | 2023-07-13 21:40:07,555 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a4456cb8-edbd-41c8-9e5d-af9836a313a2 to Node DB.
recon_1             | 2023-07-13 21:40:08,663 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:08,865 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a30e7d5b-40a3-4d01-bd10-0fa64aed4490, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.105Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:09,246 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:09,261 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:09,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 55669a65-4b6b-4715-8550-4071ca03b11b, Nodes: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:36.975Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:09,324 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)
recon_1             | 2023-07-13 21:40:09,329 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)
recon_1             | 2023-07-13 21:40:09,330 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=627a05b4-e96a-4f2b-b164-26b0221b21d7. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:09,343 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 627a05b4-e96a-4f2b-b164-26b0221b21d7, Nodes: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775, CreationTimestamp2023-07-13T21:39:35.613Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:09,651 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:09,659 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5903532c-fa1f-46c8-9b10-fe9ff367f7a0, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.282Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:09,663 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7)
recon_1             | 2023-07-13 21:40:10,156 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)
recon_1             | 2023-07-13 21:40:13,326 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:13,326 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:13,382 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:39:32,435 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:37898 / 172.24.0.8:37898: output error
scm_1               | 2023-07-13 21:39:32,435 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-13 21:39:35,142 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
scm_1               | 2023-07-13 21:39:35,168 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/bb5c41c8-12ac-4f33-939d-67f4ed99ba46
scm_1               | 2023-07-13 21:39:35,260 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a1afc4af-4388-40ba-b575-6eb773bd243a
scm_1               | 2023-07-13 21:39:35,261 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : bb5c41c8-12ac-4f33-939d-67f4ed99ba46{ip: 172.24.0.8, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:39:35,292 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600{ip: 172.24.0.6, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:39:35,282 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a1afc4af-4388-40ba-b575-6eb773bd243a{ip: 172.24.0.10, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:39:35,367 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
scm_1               | 2023-07-13 21:39:35,368 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775{ip: 172.24.0.9, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:39:35,358 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,413 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,427 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,428 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,439 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a4456cb8-edbd-41c8-9e5d-af9836a313a2
scm_1               | 2023-07-13 21:39:35,440 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a4456cb8-edbd-41c8-9e5d-af9836a313a2{ip: 172.24.0.7, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-13 21:39:35,460 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,535 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:39:35,564 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:39:35,620 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-13 21:39:35,621 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-13 21:39:35,624 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=627a05b4-e96a-4f2b-b164-26b0221b21d7 to datanode:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
scm_1               | 2023-07-13 21:39:36,731 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:36,906 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 627a05b4-e96a-4f2b-b164-26b0221b21d7, Nodes: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:35.613658Z[UTC]]
scm_1               | 2023-07-13 21:39:36,975 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b to datanode:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
scm_1               | 2023-07-13 21:39:36,983 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b to datanode:bb5c41c8-12ac-4f33-939d-67f4ed99ba46
scm_1               | 2023-07-13 21:39:36,985 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b to datanode:a1afc4af-4388-40ba-b575-6eb773bd243a
scm_1               | 2023-07-13 21:39:37,052 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,063 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 55669a65-4b6b-4715-8550-4071ca03b11b, Nodes: 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:36.975493Z[UTC]]
scm_1               | 2023-07-13 21:39:37,068 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200 to datanode:a1afc4af-4388-40ba-b575-6eb773bd243a
scm_1               | 2023-07-13 21:39:37,085 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-07-13 21:40:13,382 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 reported by a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)
recon_1             | 2023-07-13 21:40:14,177 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)
recon_1             | 2023-07-13 21:40:14,177 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)
recon_1             | 2023-07-13 21:40:14,315 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)
recon_1             | 2023-07-13 21:40:14,315 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 reported by 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)
recon_1             | 2023-07-13 21:40:14,432 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:14,433 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:14,807 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7)
recon_1             | 2023-07-13 21:40:14,807 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=1283747c-17ff-47ac-873e-fe52beff02da. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:14,865 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 1283747c-17ff-47ac-873e-fe52beff02da, Nodes: a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a4456cb8-edbd-41c8-9e5d-af9836a313a2, CreationTimestamp2023-07-13T21:39:37.316Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:14,867 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=1283747c-17ff-47ac-873e-fe52beff02da reported by a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7)
recon_1             | 2023-07-13 21:40:15,588 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by 1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)
recon_1             | 2023-07-13 21:40:15,901 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:15,901 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:15,932 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9f28966e-e086-4328-ab75-b6ed32824bca, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600, CreationTimestamp2023-07-13T21:39:37.230Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:15,939 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:16,084 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b reported by a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)
recon_1             | 2023-07-13 21:40:16,701 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)
recon_1             | 2023-07-13 21:40:16,701 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:16,715 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 43147e0e-97a9-4d73-8143-94578cd12fb5, Nodes: bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:bb5c41c8-12ac-4f33-939d-67f4ed99ba46, CreationTimestamp2023-07-13T21:39:37.413Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:16,751 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5 reported by bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)
recon_1             | 2023-07-13 21:40:16,954 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200. Trying to get from SCM.
recon_1             | 2023-07-13 21:40:16,964 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 635788d3-7f27-4198-8993-d2c8ffc48200, Nodes: a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.068Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-13 21:40:16,967 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200 reported by a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)
recon_1             | 2023-07-13 21:40:21,331 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)
recon_1             | 2023-07-13 21:40:21,405 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 reported by a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7)
recon_1             | 2023-07-13 21:40:28,635 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-13 21:40:28,636 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-13 21:40:30,815 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689284428636
recon_1             | 2023-07-13 21:40:30,852 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-07-13 21:40:31,656 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689284428636.
recon_1             | 2023-07-13 21:40:31,831 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-13 21:40:32,462 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-07-13 21:40:32,479 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-07-13 21:40:32,479 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-07-13 21:40:32,480 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:40:32,480 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-13 21:40:32,482 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-13 21:40:32,490 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-13 21:40:32,492 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.011 seconds to process 0 keys.
recon_1             | 2023-07-13 21:40:32,532 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-13 21:40:32,536 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-13 21:40:55,775 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-07-13 21:40:55,915 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-13 21:41:06,364 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-13 21:41:06,364 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-13 21:41:06,372 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-07-13 21:41:06,374 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-07-13 21:41:06,394 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
recon_1             | 2023-07-13 21:41:06,397 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-13 21:41:06,397 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-13 21:41:06,406 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 35 milliseconds.
recon_1             | 2023-07-13 21:41:06,411 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-07-13 21:41:06,462 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 57 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:41:06,513 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 50 milliseconds for processing 1 containers.
recon_1             | 2023-07-13 21:41:14,057 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-13 21:41:14,064 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d not found. Cannot add container #2
recon_1             | 2023-07-13 21:41:14,065 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-07-13 21:41:14,102 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-07-13 21:41:14,116 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d not found. Cannot add container #2
recon_1             | 2023-07-13 21:41:14,117 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-07-13 21:41:14,173 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:41:14,201 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d not found. Cannot add container #2
recon_1             | 2023-07-13 21:41:14,206 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-07-13 21:41:14,780 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_4.xcompat_default.
recon_1             | 2023-07-13 21:41:14,784 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d not found. Cannot add container #2
recon_1             | 2023-07-13 21:41:14,785 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-07-13 21:41:15,110 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-07-13 21:41:15,139 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d not found. Cannot add container #2
recon_1             | 2023-07-13 21:41:15,140 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-07-13 21:42:06,446 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-07-13 21:42:06,523 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:42:06,525 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 76
recon_1             | 2023-07-13 21:42:18,958 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-13 21:42:18,983 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
scm_1               | 2023-07-13 21:39:37,092 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 635788d3-7f27-4198-8993-d2c8ffc48200, Nodes: a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.068155Z[UTC]]
scm_1               | 2023-07-13 21:39:37,105 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 to datanode:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
scm_1               | 2023-07-13 21:39:37,105 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 to datanode:a1afc4af-4388-40ba-b575-6eb773bd243a
scm_1               | 2023-07-13 21:39:37,105 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490 to datanode:1bbf5b6c-17c4-4aff-a715-8dff0fd5a775
scm_1               | 2023-07-13 21:39:37,126 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,229 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: a30e7d5b-40a3-4d01-bd10-0fa64aed4490, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.105265Z[UTC]]
scm_1               | 2023-07-13 21:39:37,230 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca to datanode:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
scm_1               | 2023-07-13 21:39:37,240 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,278 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9f28966e-e086-4328-ab75-b6ed32824bca, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.230224Z[UTC]]
scm_1               | 2023-07-13 21:39:37,282 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 to datanode:2ba100c8-b16f-4f8d-b8fc-edf40b2b8600
scm_1               | 2023-07-13 21:39:37,283 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 to datanode:bb5c41c8-12ac-4f33-939d-67f4ed99ba46
scm_1               | 2023-07-13 21:39:37,283 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0 to datanode:a4456cb8-edbd-41c8-9e5d-af9836a313a2
scm_1               | 2023-07-13 21:39:37,307 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,312 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5903532c-fa1f-46c8-9b10-fe9ff367f7a0, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.282273Z[UTC]]
scm_1               | 2023-07-13 21:39:37,316 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1283747c-17ff-47ac-873e-fe52beff02da to datanode:a4456cb8-edbd-41c8-9e5d-af9836a313a2
scm_1               | 2023-07-13 21:39:37,337 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,400 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 1283747c-17ff-47ac-873e-fe52beff02da, Nodes: a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.316476Z[UTC]]
scm_1               | 2023-07-13 21:39:37,402 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-13 21:39:37,413 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5 to datanode:bb5c41c8-12ac-4f33-939d-67f4ed99ba46
scm_1               | 2023-07-13 21:39:37,436 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:39:37,437 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 43147e0e-97a9-4d73-8143-94578cd12fb5, Nodes: bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:39:37.413408Z[UTC]]
scm_1               | 2023-07-13 21:39:37,489 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-13 21:40:09,296 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:40:09,299 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=627a05b4-e96a-4f2b-b164-26b0221b21d7
scm_1               | 2023-07-13 21:40:09,321 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:40:10,153 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:40:14,312 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:40:14,458 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-07-13 21:40:14,459 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=a30e7d5b-40a3-4d01-bd10-0fa64aed4490
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
recon_1             | 2023-07-13 21:43:06,536 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:43:06,537 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 10
recon_1             | 2023-07-13 21:44:06,537 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:44:06,537 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-07-13 21:44:29,575 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #4 got from xcompat_datanode_4.xcompat_default.
recon_1             | 2023-07-13 21:44:29,625 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #4 to Recon.
recon_1             | 2023-07-13 21:45:06,544 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:45:06,544 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 4
recon_1             | 2023-07-13 21:46:06,424 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
recon_1             | 2023-07-13 21:46:06,453 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=82406618-7255-4b96-9c2e-cbccf4d9f79d from SCM.
recon_1             | 2023-07-13 21:46:06,487 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 77 milliseconds.
recon_1             | 2023-07-13 21:46:06,514 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
recon_1             | 2023-07-13 21:46:06,523 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 9 milliseconds for processing 3 containers.
recon_1             | 2023-07-13 21:46:06,545 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:46:06,545 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-07-13 21:47:06,546 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-13 21:47:06,546 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-13 21:40:14,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-13 21:40:14,526 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-13 21:40:14,526 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-07-13 21:40:14,855 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=1283747c-17ff-47ac-873e-fe52beff02da
scm_1               | 2023-07-13 21:40:15,932 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9f28966e-e086-4328-ab75-b6ed32824bca
scm_1               | 2023-07-13 21:40:16,092 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=55669a65-4b6b-4715-8550-4071ca03b11b
scm_1               | 2023-07-13 21:40:16,766 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=43147e0e-97a9-4d73-8143-94578cd12fb5
scm_1               | 2023-07-13 21:40:17,023 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=635788d3-7f27-4198-8993-d2c8ffc48200
scm_1               | 2023-07-13 21:40:21,411 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=5903532c-fa1f-46c8-9b10-fe9ff367f7a0
scm_1               | 2023-07-13 21:40:52,205 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-13 21:40:52,247 [e83d8b1d-d539-4110-bdbf-ca352359fa10@group-3B9DD2E479DE-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-13 21:40:52,251 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-13 21:41:11,624 [IPC Server handler 16 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 82406618-7255-4b96-9c2e-cbccf4d9f79d, Nodes: 2ba100c8-b16f-4f8d-b8fc-edf40b2b8600(xcompat_datanode_5.xcompat_default/172.24.0.6)1bbf5b6c-17c4-4aff-a715-8dff0fd5a775(xcompat_datanode_3.xcompat_default/172.24.0.9)bb5c41c8-12ac-4f33-939d-67f4ed99ba46(xcompat_datanode_4.xcompat_default/172.24.0.8)a1afc4af-4388-40ba-b575-6eb773bd243a(xcompat_datanode_1.xcompat_default/172.24.0.10)a4456cb8-edbd-41c8-9e5d-af9836a313a2(xcompat_datanode_2.xcompat_default/172.24.0.7), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-13T21:41:11.604369Z[UTC]]
scm_1               | 2023-07-13 21:41:37,494 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-13 21:42:48,848 [IPC Server handler 63 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm_1               | 2023-07-13 21:43:37,495 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-13 21:44:21,691 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-13 21:45:37,498 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-13 21:47:37,500 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
Attaching to 
