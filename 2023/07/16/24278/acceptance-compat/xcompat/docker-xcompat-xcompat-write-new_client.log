Attaching to xcompat_new_client_1, xcompat_datanode_1, xcompat_datanode_3, xcompat_datanode_2, xcompat_old_client_1_3_0_1, xcompat_old_client_1_1_0_1, xcompat_s3g_1, xcompat_old_client_1_2_1_1, xcompat_scm_1, xcompat_recon_1, xcompat_old_client_1_0_0_1, xcompat_om_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 04:42:51,754 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 8d1bfd0f2b9d/172.19.0.13
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 04:42:51,838 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 04:42:52,155 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 04:42:53,281 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 04:42:54,191 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 04:42:54,192 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-16 04:42:55,263 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:8d1bfd0f2b9d ip:172.19.0.13
datanode_1          | 2023-07-16 04:42:56,593 [main] INFO reflections.Reflections: Reflections took 768 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_1          | 2023-07-16 04:43:01,048 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-07-16 04:43:01,464 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-16 04:43:03,513 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 04:43:03,664 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-16 04:43:03,732 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 04:43:03,734 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 04:43:04,122 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:43:04,183 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:43:04,185 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-16 04:43:04,217 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-16 04:43:04,217 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-16 04:43:04,253 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-16 04:43:04,429 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:43:04,440 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-16 04:43:16,499 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-16 04:43:16,884 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:43:17,673 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 04:43:18,895 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 04:43:18,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-16 04:43:18,941 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 04:43:18,946 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-16 04:43:18,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-16 04:43:18,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-16 04:43:18,956 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 04:43:18,986 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:43:18,993 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 04:43:18,996 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:43:19,094 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 04:43:19,122 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-16 04:43:19,141 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-16 04:43:21,767 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-16 04:43:21,780 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-16 04:43:21,791 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-16 04:43:21,809 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:43:21,810 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:43:21,840 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:43:22,393 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-16 04:43:23,112 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-07-16 04:43:24,046 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:43:24,144 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 04:43:24,314 [main] INFO util.log: Logging initialized @45116ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 04:43:25,452 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-16 04:43:25,525 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 04:43:25,586 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 04:43:25,626 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 04:43:25,631 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 04:43:25,631 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 04:43:25,908 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-07-16 04:43:25,937 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-16 04:43:25,945 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-07-16 04:43:26,180 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 04:43:26,187 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 04:43:26,188 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-16 04:43:26,315 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e111aeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 04:43:26,320 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@20ad64c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 04:43:27,359 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5418a659{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16017819360511995276/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-16 04:43:27,435 [main] INFO server.AbstractConnector: Started ServerConnector@2318651f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-16 04:43:27,444 [main] INFO server.Server: Started @48246ms
datanode_1          | 2023-07-16 04:43:27,463 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-16 04:43:27,463 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 04:43:27,467 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:43:27,745 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-07-16 04:43:27,951 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-07-16 04:43:27,991 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-07-16 04:43:30,035 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-07-16 04:43:30,035 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-07-16 04:43:30,173 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-07-16 04:43:30,244 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-16 04:42:52,228 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = ba39a9eb6daa/172.19.0.9
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 04:42:52,346 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 87fd3ab5b15c/172.19.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-07-16 04:42:52,349 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 04:42:52,830 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 04:42:53,888 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 04:42:55,028 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-16 04:42:55,028 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-16 04:42:56,231 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ba39a9eb6daa ip:172.19.0.9
datanode_3          | 2023-07-16 04:42:57,356 [main] INFO reflections.Reflections: Reflections took 879 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_3          | 2023-07-16 04:43:01,521 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-07-16 04:43:02,089 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-16 04:43:03,979 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 04:43:04,124 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-16 04:43:04,140 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 04:43:04,154 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 04:43:04,472 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:43:04,526 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:43:04,537 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-16 04:43:04,557 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 04:42:52,479 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 04:42:52,971 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 04:42:53,973 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 04:42:55,220 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 04:42:55,220 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-16 04:42:56,399 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:87fd3ab5b15c ip:172.19.0.10
datanode_2          | 2023-07-16 04:42:58,034 [main] INFO reflections.Reflections: Reflections took 1206 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_2          | 2023-07-16 04:43:01,671 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-16 04:43:02,040 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-16 04:43:04,109 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 04:43:04,289 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-16 04:43:04,305 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 04:43:04,329 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 04:43:04,614 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:43:04,689 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:43:04,699 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-16 04:43:04,710 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-16 04:43:04,562 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-16 04:43:04,562 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-16 04:43:04,958 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:43:04,997 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-16 04:43:16,668 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-16 04:43:17,415 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:43:17,982 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 04:43:18,949 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-16 04:43:18,964 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-16 04:43:18,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-16 04:43:18,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-16 04:43:18,980 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-07-16 04:43:18,981 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-16 04:43:18,982 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-16 04:43:18,987 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:43:19,053 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-16 04:43:19,081 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:43:19,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 04:43:19,208 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-16 04:43:19,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-07-16 04:43:21,951 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-16 04:43:21,987 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-16 04:43:22,048 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-07-16 04:43:22,049 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:43:22,050 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:43:22,087 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:43:22,719 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-16 04:43:23,415 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-07-16 04:43:24,886 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:43:25,050 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 04:43:25,433 [main] INFO util.log: Logging initialized @46783ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 04:43:26,169 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-16 04:43:26,233 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-16 04:43:26,309 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-16 04:43:26,312 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 04:43:26,312 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-16 04:43:26,312 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-16 04:43:26,606 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-07-16 04:43:26,678 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 04:43:26,687 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-07-16 04:43:26,986 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 04:43:27,018 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 04:43:27,038 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-07-16 04:43:27,249 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1abd1a28{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 04:43:27,260 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@53fd59d4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-16 04:43:27,885 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3abb6aa{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1961686229260123662/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 04:43:27,976 [main] INFO server.AbstractConnector: Started ServerConnector@52a605c3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-16 04:43:27,976 [main] INFO server.Server: Started @49325ms
datanode_3          | 2023-07-16 04:43:27,988 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-16 04:43:27,988 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-16 04:43:27,990 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:43:28,273 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-07-16 04:43:28,456 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-07-16 04:43:28,478 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-07-16 04:43:30,528 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-07-16 04:43:30,534 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-07-16 04:43:30,540 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-07-16 04:43:30,543 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-07-16 04:43:04,716 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-16 04:43:04,724 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-16 04:43:05,085 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:43:05,086 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-16 04:43:17,187 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-16 04:43:18,035 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:43:18,521 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 04:43:20,243 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 04:43:20,261 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-16 04:43:20,268 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 04:43:20,269 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-16 04:43:20,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-16 04:43:20,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-16 04:43:20,273 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 04:43:20,276 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:43:20,300 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 04:43:20,301 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:43:20,383 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 04:43:20,443 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-16 04:43:20,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-16 04:43:23,583 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-16 04:43:23,641 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-16 04:43:23,642 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-16 04:43:23,642 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:43:23,642 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:43:23,672 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:43:23,997 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-16 04:43:24,325 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-07-16 04:43:25,901 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:43:25,995 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 04:43:26,300 [main] INFO util.log: Logging initialized @47263ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 04:43:27,444 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-16 04:43:27,487 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 04:43:27,519 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 04:43:27,541 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 04:43:27,542 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 04:43:27,543 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 04:43:27,826 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-07-16 04:43:27,914 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-16 04:43:27,915 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-07-16 04:43:28,128 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 04:43:28,152 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 04:43:28,154 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-16 04:43:28,234 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@74691677{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 04:43:28,241 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@50fe5df2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 04:43:28,852 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@e30a8ef{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8843731594170907929/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 04:43:28,955 [main] INFO server.AbstractConnector: Started ServerConnector@560c3758{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-16 04:43:28,955 [main] INFO server.Server: Started @49918ms
datanode_2          | 2023-07-16 04:43:28,984 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 04:43:29,201 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-16 04:43:29,203 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:43:29,687 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-16 04:43:29,966 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-07-16 04:43:30,008 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-07-16 04:43:31,373 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-07-16 04:43:31,373 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-07-16 04:43:31,383 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-07-16 04:43:31,384 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-07-16 04:43:30,568 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-16 04:43:31,024 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.4:9891
datanode_3          | 2023-07-16 04:43:31,440 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 04:43:33,976 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:33,977 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:34,985 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:34,987 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:35,988 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:35,992 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:36,989 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:36,992 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:37,989 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:38,990 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:39,991 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:40,994 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:41,995 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:42,038 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From ba39a9eb6daa/172.19.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:60656 remote=recon/172.19.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:60656 remote=recon/172.19.0.4:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 2023-07-16 04:43:31,438 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-16 04:43:32,096 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.4:9891
datanode_2          | 2023-07-16 04:43:32,473 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 04:43:35,045 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:35,064 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:36,047 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:36,065 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:37,049 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:37,066 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:38,067 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:39,068 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:40,070 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:41,073 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:42,077 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:42,113 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 87fd3ab5b15c/172.19.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:55224 remote=recon/172.19.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:55224 remote=recon/172.19.0.4:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-16 04:43:43,084 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:43:47,713 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-16 04:43:42,996 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:43:48,019 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From ba39a9eb6daa/172.19.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:56376 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:56376 remote=scm/172.19.0.5:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-07-16 04:43:50,654 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-fc4d9fe6-69e6-431f-8f81-668ab2be527e/container.db to cache
datanode_3          | 2023-07-16 04:43:50,655 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-fc4d9fe6-69e6-431f-8f81-668ab2be527e/container.db for volume DS-fc4d9fe6-69e6-431f-8f81-668ab2be527e
datanode_3          | 2023-07-16 04:43:50,678 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-16 04:43:50,692 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-07-16 04:43:50,976 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-07-16 04:43:50,977 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:43:51,059 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: 984f9786-6250-4b94-8d30-f65fab4a7cff: start RPC server
datanode_3          | 2023-07-16 04:43:51,073 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 984f9786-6250-4b94-8d30-f65fab4a7cff: GrpcService started, listening on 9858
datanode_3          | 2023-07-16 04:43:51,079 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 984f9786-6250-4b94-8d30-f65fab4a7cff: GrpcService started, listening on 9856
datanode_3          | 2023-07-16 04:43:51,083 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 984f9786-6250-4b94-8d30-f65fab4a7cff: GrpcService started, listening on 9857
datanode_3          | 2023-07-16 04:43:51,095 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 984f9786-6250-4b94-8d30-f65fab4a7cff is started using port 9858 for RATIS
datanode_3          | 2023-07-16 04:43:51,095 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 984f9786-6250-4b94-8d30-f65fab4a7cff is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-16 04:43:51,095 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 984f9786-6250-4b94-8d30-f65fab4a7cff is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-16 04:43:51,097 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-984f9786-6250-4b94-8d30-f65fab4a7cff: Started
datanode_3          | 2023-07-16 04:43:51,163 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:43:52,848 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-16 04:43:56,204 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 984f9786-6250-4b94-8d30-f65fab4a7cff: addNew group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-4CAA690A8BC7:java.util.concurrent.CompletableFuture@73129051[Not completed]
datanode_3          | 2023-07-16 04:43:56,291 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff: new RaftServerImpl for group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:43:56,321 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:43:56,325 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:43:56,325 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:43:56,328 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:43:56,329 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:43:56,329 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:43:56,360 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: ConfigurationManager, init=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:43:56,361 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:43:56,379 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:43:56,381 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:43:56,447 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:43:56,468 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-16 04:43:56,485 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:43:56,491 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:43:56,576 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 04:43:56,669 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:43:56,687 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:43:56,700 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 04:43:56,701 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 04:43:56,716 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 04:43:56,719 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 04:43:56,719 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 does not exist. Creating ...
datanode_3          | 2023-07-16 04:43:56,734 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/in_use.lock acquired by nodename 7@ba39a9eb6daa
datanode_3          | 2023-07-16 04:43:56,765 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 has been successfully formatted.
datanode_3          | 2023-07-16 04:43:56,881 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO ratis.ContainerStateMachine: group-4CAA690A8BC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:43:56,889 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:43:57,006 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:43:57,013 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:43:57,018 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 04:43:57,021 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 04:43:57,038 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:43:57,096 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:43:57,096 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:43:57,098 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:43:57,143 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7
datanode_3          | 2023-07-16 04:43:57,144 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 04:43:57,147 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:43:57,161 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:43:57,162 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:43:57,162 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:43:57,165 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:43:57,169 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:43:57,171 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:43:57,223 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:43:57,223 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:43:57,281 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:43:57,282 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:43:57,283 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:43:57,295 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:43:57,295 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:43:57,298 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: start as a follower, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:43:57,298 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:43:57,301 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState
datanode_3          | 2023-07-16 04:43:57,315 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4CAA690A8BC7,id=984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:43:57,320 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:43:57,320 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:43:57,321 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:43:57,321 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:43:57,322 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:43:57,323 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:43:57,361 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7
datanode_3          | 2023-07-16 04:44:02,437 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO impl.FollowerState: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137524904ns, electionTimeout:5112ms
datanode_1          | 2023-07-16 04:43:30,237 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-16 04:43:30,902 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.4:9891
datanode_1          | 2023-07-16 04:43:31,263 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 04:43:33,792 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:33,796 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:34,793 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:34,797 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:35,794 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:35,797 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:36,795 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:36,800 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:37,796 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.4:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:37,801 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:38,801 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:39,802 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:40,803 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:41,805 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:42,806 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:43:42,854 [EndpointStateMachine task thread for recon/172.19.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 8d1bfd0f2b9d/172.19.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:50988 remote=recon/172.19.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:50988 remote=recon/172.19.0.4:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-07-16 04:43:48,099 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 87fd3ab5b15c/172.19.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:60552 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 04:43:47,816 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 8d1bfd0f2b9d/172.19.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:44252 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:44252 remote=scm/172.19.0.5:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 04:43:50,710 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-3746f9a6-15f2-48d6-9570-d1c2d226de39/container.db to cache
datanode_1          | 2023-07-16 04:43:50,712 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-3746f9a6-15f2-48d6-9570-d1c2d226de39/container.db for volume DS-3746f9a6-15f2-48d6-9570-d1c2d226de39
datanode_1          | 2023-07-16 04:43:50,730 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 04:43:50,738 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-07-16 04:43:50,948 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-07-16 04:43:50,950 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:43:51,049 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start RPC server
datanode_1          | 2023-07-16 04:43:51,057 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 04:43:51,061 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: GrpcService started, listening on 9856
datanode_1          | 2023-07-16 04:43:51,063 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: GrpcService started, listening on 9857
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:60552 remote=scm/172.19.0.5:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-16 04:43:49,716 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_1          | 2023-07-16 04:43:51,083 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb is started using port 9858 for RATIS
datanode_1          | 2023-07-16 04:43:51,083 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 04:43:51,083 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 04:43:51,086 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: Started
datanode_1          | 2023-07-16 04:43:51,162 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:44:02,438 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState
datanode_3          | 2023-07-16 04:44:02,438 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:44:02,442 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 04:44:02,443 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1
datanode_3          | 2023-07-16 04:44:02,448 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:02,466 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:44:02,466 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:44:02,481 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_3          | 2023-07-16 04:44:02,482 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_3          | 2023-07-16 04:44:02,523 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: receive requestVote(PRE_VOTE, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-4CAA690A8BC7, 0, (t:0, i:0))
datanode_3          | 2023-07-16 04:44:02,541 [grpc-default-executor-0] INFO impl.VoteContext: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-CANDIDATE: accept PRE_VOTE from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-16 04:44:02,581 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff: new RaftServerImpl for group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:44:02,581 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:44:02,581 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:44:02,581 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:44:02,581 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:44:02,582 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:44:02,583 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:44:02,583 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: ConfigurationManager, init=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:44:02,584 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:44:02,585 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:44:02,586 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:44:02,586 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:44:02,587 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-16 04:44:02,587 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:44:02,587 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:44:02,589 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 04:44:02,590 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:44:02,593 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:44:02,594 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 04:44:02,594 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 04:44:02,636 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 04:44:02,636 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 04:44:02,636 [grpc-default-executor-1] INFO server.RaftServer: 984f9786-6250-4b94-8d30-f65fab4a7cff: addNew group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] returns      null 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_3          | 2023-07-16 04:44:02,654 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 does not exist. Creating ...
datanode_3          | 2023-07-16 04:44:02,658 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/in_use.lock acquired by nodename 7@ba39a9eb6daa
datanode_3          | 2023-07-16 04:44:02,661 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 has been successfully formatted.
datanode_3          | 2023-07-16 04:44:02,663 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO ratis.ContainerStateMachine: group-3E77D073C3D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:44:02,663 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:44:02,663 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:44:02,663 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:02,664 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 04:44:02,664 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 04:44:02,664 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:02,674 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7 replies to PRE_VOTE vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-984f9786-6250-4b94-8d30-f65fab4a7cff#0:OK-t0. Peer's state: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7:t0, leader=null, voted=, raftlog=Memoized:984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:02,715 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:44:02,716 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:44:02,732 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:44:02,733 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:44:02,733 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:44:02,734 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:02,735 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:02,890 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:44:02,895 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:44:02,895 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:44:02,896 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:44:02,896 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:43:50,677 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-518449eb-85f1-4b4f-9694-b5c9c9801b05/container.db to cache
datanode_2          | 2023-07-16 04:43:50,677 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6d5e37af-423b-4dae-b444-8f69d29deb95/DS-518449eb-85f1-4b4f-9694-b5c9c9801b05/container.db for volume DS-518449eb-85f1-4b4f-9694-b5c9c9801b05
datanode_2          | 2023-07-16 04:43:50,695 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-16 04:43:50,711 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-07-16 04:43:50,981 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-07-16 04:43:50,982 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_2          | 2023-07-16 04:43:51,083 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start RPC server
datanode_2          | 2023-07-16 04:43:51,094 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: d0d64a7d-3e54-43d9-a2ff-589187cfae66: GrpcService started, listening on 9858
datanode_2          | 2023-07-16 04:43:51,102 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: d0d64a7d-3e54-43d9-a2ff-589187cfae66: GrpcService started, listening on 9856
datanode_2          | 2023-07-16 04:43:51,105 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: d0d64a7d-3e54-43d9-a2ff-589187cfae66: GrpcService started, listening on 9857
datanode_2          | 2023-07-16 04:43:51,120 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d0d64a7d-3e54-43d9-a2ff-589187cfae66 is started using port 9858 for RATIS
datanode_2          | 2023-07-16 04:43:51,121 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d0d64a7d-3e54-43d9-a2ff-589187cfae66 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-16 04:43:51,122 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d0d64a7d-3e54-43d9-a2ff-589187cfae66 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-16 04:43:51,121 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d0d64a7d-3e54-43d9-a2ff-589187cfae66: Started
datanode_2          | 2023-07-16 04:43:51,218 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:43:51,719 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-07-16 04:44:00,879 [grpc-default-executor-0] INFO server.RaftServer: d0d64a7d-3e54-43d9-a2ff-589187cfae66: addNew group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-4CAA690A8BC7:java.util.concurrent.CompletableFuture@58a9b215[Not completed]
datanode_2          | 2023-07-16 04:44:01,017 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66: new RaftServerImpl for group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:44:01,019 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:44:01,029 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:44:01,030 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:44:01,031 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:01,040 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:44:01,044 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:44:02,896 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: start as a follower, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:02,900 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:44:02,900 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState
datanode_3          | 2023-07-16 04:44:02,900 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E77D073C3D8,id=984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:44:02,908 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:44:02,908 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:44:02,910 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:44:02,910 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:44:02,912 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:44:02,955 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:44:03,838 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: receive requestVote(ELECTION, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-4CAA690A8BC7, 1, (t:0, i:0))
datanode_3          | 2023-07-16 04:44:03,839 [grpc-default-executor-0] INFO impl.VoteContext: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-CANDIDATE: accept ELECTION from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-16 04:44:03,841 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_3          | 2023-07-16 04:44:03,841 [grpc-default-executor-0] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1
datanode_3          | 2023-07-16 04:44:03,842 [grpc-default-executor-0] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-FollowerState
datanode_3          | 2023-07-16 04:44:03,846 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7 replies to ELECTION vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-984f9786-6250-4b94-8d30-f65fab4a7cff#0:OK-t1. Peer's state: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7:t1, leader=null, voted=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, raftlog=Memoized:984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:04,408 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7.
datanode_3          | 2023-07-16 04:44:04,409 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 984f9786-6250-4b94-8d30-f65fab4a7cff: addNew group-58C9B116B294:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-58C9B116B294:java.util.concurrent.CompletableFuture@d2d9520[Not completed]
datanode_3          | 2023-07-16 04:44:04,423 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff: new RaftServerImpl for group-58C9B116B294:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:44:04,424 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:44:04,425 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:44:04,426 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:44:04,427 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:44:04,427 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:44:04,427 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:44:04,429 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: ConfigurationManager, init=-1: peers:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:44:04,429 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:44:04,433 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:44:04,434 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:44:04,434 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:44:04,436 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-16 04:44:04,436 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:44:04,436 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:44:04,436 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 04:44:04,446 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a1928e58-e59a-421f-a41b-58c9b116b294 does not exist. Creating ...
datanode_3          | 2023-07-16 04:44:04,481 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a1928e58-e59a-421f-a41b-58c9b116b294/in_use.lock acquired by nodename 7@ba39a9eb6daa
datanode_3          | 2023-07-16 04:44:04,497 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a1928e58-e59a-421f-a41b-58c9b116b294 has been successfully formatted.
datanode_3          | 2023-07-16 04:44:04,499 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO ratis.ContainerStateMachine: group-58C9B116B294: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:44:04,503 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:44:04,512 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:44:04,512 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:04,513 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 04:44:04,517 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 04:44:04,519 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:04,526 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:44:04,526 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:44:04,531 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:04,600 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a1928e58-e59a-421f-a41b-58c9b116b294
datanode_3          | 2023-07-16 04:44:04,600 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 04:44:04,601 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:04,601 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:04,601 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:44:04,601 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:44:04,602 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:44:04,603 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:44:04,604 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:44:04,604 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:44:04,607 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:04,820 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:44:04,825 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:44:04,826 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:44:04,833 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:44:04,837 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:44:04,839 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: start as a follower, conf=-1: peers:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:04,841 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:44:01,120 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: ConfigurationManager, init=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:44:01,135 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:44:01,169 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:44:01,174 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:44:01,328 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:01,356 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:01,381 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:44:01,382 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:44:01,514 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 04:44:01,594 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:44:01,613 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:44:01,616 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 04:44:01,620 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 04:44:01,624 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 04:44:01,626 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 04:44:01,627 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 does not exist. Creating ...
datanode_2          | 2023-07-16 04:44:01,659 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/in_use.lock acquired by nodename 7@87fd3ab5b15c
datanode_2          | 2023-07-16 04:44:01,702 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 has been successfully formatted.
datanode_2          | 2023-07-16 04:44:01,756 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO ratis.ContainerStateMachine: group-4CAA690A8BC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:44:01,763 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:44:01,817 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:44:01,825 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:01,831 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 04:44:01,835 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 04:44:01,855 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:01,898 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:44:01,904 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:44:01,908 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:01,942 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7
datanode_2          | 2023-07-16 04:44:01,946 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 04:44:01,947 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:44:01,953 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:01,955 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:44:01,964 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:44:01,969 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:44:01,969 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:01,971 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:44:02,026 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:02,028 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:02,145 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 04:44:02,148 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 04:44:04,843 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState
datanode_3          | 2023-07-16 04:44:04,863 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-58C9B116B294,id=984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:44:04,863 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:44:04,863 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:44:04,864 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:44:04,864 [984f9786-6250-4b94-8d30-f65fab4a7cff-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:44:04,865 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:44:04,865 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:44:04,867 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294
datanode_3          | 2023-07-16 04:44:04,868 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294.
datanode_3          | 2023-07-16 04:44:05,203 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=1) received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 04:44:05,207 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection:   Response 0: 984f9786-6250-4b94-8d30-f65fab4a7cff<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0
datanode_3          | 2023-07-16 04:44:05,207 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection:   Response 1: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:FAIL-t1
datanode_3          | 2023-07-16 04:44:05,207 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=1)
datanode_3          | 2023-07-16 04:44:05,457 [984f9786-6250-4b94-8d30-f65fab4a7cff-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4CAA690A8BC7 with new leaderId: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_3          | 2023-07-16 04:44:05,463 [984f9786-6250-4b94-8d30-f65fab4a7cff-server-thread1] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: change Leader from null to e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb at term 1 for appendEntries, leader elected after 9014ms
datanode_3          | 2023-07-16 04:44:05,562 [984f9786-6250-4b94-8d30-f65fab4a7cff-server-thread3] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7: set configuration 0: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:05,627 [984f9786-6250-4b94-8d30-f65fab4a7cff-server-thread3] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:44:05,884 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-4CAA690A8BC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/current/log_inprogress_0
datanode_3          | 2023-07-16 04:44:07,492 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: receive requestVote(PRE_VOTE, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-3E77D073C3D8, 0, (t:0, i:0))
datanode_3          | 2023-07-16 04:44:07,493 [grpc-default-executor-0] INFO impl.VoteContext: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FOLLOWER: reject PRE_VOTE from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-16 04:44:07,493 [grpc-default-executor-0] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8 replies to PRE_VOTE vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-984f9786-6250-4b94-8d30-f65fab4a7cff#0:FAIL-t0. Peer's state: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8:t0, leader=null, voted=, raftlog=Memoized:984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:08,033 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO impl.FollowerState: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5133307225ns, electionTimeout:5076ms
datanode_3          | 2023-07-16 04:44:08,037 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState
datanode_3          | 2023-07-16 04:44:08,037 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:44:08,037 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 04:44:08,037 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2
datanode_3          | 2023-07-16 04:44:08,043 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:08,081 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:44:08,081 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:44:08,091 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 04:44:08,091 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection:   Response 0: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:OK-t0
datanode_3          | 2023-07-16 04:44:08,092 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-07-16 04:44:08,097 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:08,114 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 04:44:08,131 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 04:44:08,139 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 04:44:08,139 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection:   Response 0: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:OK-t1
datanode_3          | 2023-07-16 04:44:08,139 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-07-16 04:44:08,139 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2
datanode_3          | 2023-07-16 04:44:08,140 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:44:08,168 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E77D073C3D8 with new leaderId: 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:44:08,169 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: change Leader from null to 984f9786-6250-4b94-8d30-f65fab4a7cff at term 1 for becomeLeader, leader elected after 5553ms
datanode_3          | 2023-07-16 04:44:08,192 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:44:08,216 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:08,218 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 04:44:08,240 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:44:08,241 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:44:08,242 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:44:08,276 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:08,285 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 04:44:08,355 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:44:08,358 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:08,360 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:44:08,372 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-16 04:44:08,375 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:44:08,376 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:44:08,378 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-07-16 04:44:08,378 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-07-16 04:44:08,378 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:44:02,148 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:02,170 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:02,170 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:02,173 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: start as a follower, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:02,176 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:44:02,180 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState
datanode_2          | 2023-07-16 04:44:02,204 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 04:44:02,212 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 04:44:02,220 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4CAA690A8BC7,id=d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_2          | 2023-07-16 04:44:02,223 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:02,225 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:44:02,225 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:44:02,227 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:44:02,559 [grpc-default-executor-0] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: receive requestVote(PRE_VOTE, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-4CAA690A8BC7, 0, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:02,601 [grpc-default-executor-0] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FOLLOWER: accept PRE_VOTE from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:44:02,653 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: receive requestVote(PRE_VOTE, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-4CAA690A8BC7, 0, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:02,661 [grpc-default-executor-0] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7 replies to PRE_VOTE vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7:t0, leader=null, voted=, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:02,662 [grpc-default-executor-1] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FOLLOWER: accept PRE_VOTE from 984f9786-6250-4b94-8d30-f65fab4a7cff: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-16 04:44:02,662 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7 replies to PRE_VOTE vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7:t0, leader=null, voted=, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:03,823 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: receive requestVote(ELECTION, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-4CAA690A8BC7, 1, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:03,824 [grpc-default-executor-1] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FOLLOWER: accept ELECTION from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:44:03,826 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_2          | 2023-07-16 04:44:03,827 [grpc-default-executor-1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: shutdown d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState
datanode_2          | 2023-07-16 04:44:03,828 [grpc-default-executor-1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState
datanode_2          | 2023-07-16 04:44:03,828 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState] INFO impl.FollowerState: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-FollowerState was interrupted
datanode_2          | 2023-07-16 04:44:03,855 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7 replies to ELECTION vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t1. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7:t1, leader=null, voted=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:04,204 [grpc-default-executor-1] INFO server.RaftServer: d0d64a7d-3e54-43d9-a2ff-589187cfae66: addNew group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-3E77D073C3D8:java.util.concurrent.CompletableFuture@4ed95eae[Not completed]
datanode_2          | 2023-07-16 04:44:04,206 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66: new RaftServerImpl for group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:44:04,208 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:44:04,208 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:44:04,208 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:44:04,208 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:04,208 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:44:04,209 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:44:04,209 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: ConfigurationManager, init=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:44:04,209 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:44:04,209 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:44:04,209 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:44:04,210 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:04,210 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:04,211 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:44:04,211 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:44:04,212 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 04:44:04,219 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:44:04,219 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:44:04,220 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 04:44:04,221 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 04:44:04,221 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 04:44:04,225 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 04:44:04,225 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 does not exist. Creating ...
datanode_2          | 2023-07-16 04:44:04,240 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/in_use.lock acquired by nodename 7@87fd3ab5b15c
datanode_2          | 2023-07-16 04:44:04,247 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 has been successfully formatted.
datanode_2          | 2023-07-16 04:44:04,255 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO ratis.ContainerStateMachine: group-3E77D073C3D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:44:04,268 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:44:04,268 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:44:04,268 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:04,268 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 04:44:04,269 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 04:43:55,760 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: addNew group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-4CAA690A8BC7:java.util.concurrent.CompletableFuture@6ed127d6[Not completed]
datanode_1          | 2023-07-16 04:43:55,897 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: new RaftServerImpl for group-4CAA690A8BC7:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:43:55,903 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:43:55,903 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:43:55,907 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:43:55,916 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:43:55,917 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:43:55,917 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:43:55,954 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: ConfigurationManager, init=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:43:55,958 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:43:55,997 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:43:55,999 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:43:56,067 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:43:56,098 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-16 04:43:56,121 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:43:56,125 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:43:56,248 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-16 04:43:56,395 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:43:56,415 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:43:56,416 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 04:43:56,419 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 04:43:56,422 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 04:43:56,424 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 04:43:56,425 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 does not exist. Creating ...
datanode_1          | 2023-07-16 04:43:56,451 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/in_use.lock acquired by nodename 8@8d1bfd0f2b9d
datanode_1          | 2023-07-16 04:43:56,480 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7 has been successfully formatted.
datanode_1          | 2023-07-16 04:43:56,555 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO ratis.ContainerStateMachine: group-4CAA690A8BC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:43:56,569 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:43:56,656 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:43:56,656 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:43:56,712 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 04:43:56,714 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 04:43:56,718 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:43:56,750 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:43:56,752 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:43:56,754 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:08,379 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 04:44:08,390 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:44:08,390 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:44:08,390 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:44:08,391 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-16 04:44:08,391 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:44:08,392 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:44:08,393 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-07-16 04:44:08,393 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-07-16 04:44:08,393 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:44:08,393 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 04:44:08,397 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderStateImpl
datanode_3          | 2023-07-16 04:44:08,402 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:44:08,406 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/current/log_inprogress_0
datanode_3          | 2023-07-16 04:44:08,489 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8-LeaderElection2] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-3E77D073C3D8: set configuration 0: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:09,998 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO impl.FollowerState: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5154502105ns, electionTimeout:5105ms
datanode_3          | 2023-07-16 04:44:09,998 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState
datanode_3          | 2023-07-16 04:44:09,998 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:44:09,998 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 04:44:09,999 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-FollowerState] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3
datanode_3          | 2023-07-16 04:44:10,002 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:10,002 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-07-16 04:44:10,007 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:10,007 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.LeaderElection: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-16 04:44:10,007 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: shutdown 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3
datanode_3          | 2023-07-16 04:44:10,008 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:44:10,008 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-58C9B116B294 with new leaderId: 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_3          | 2023-07-16 04:44:10,008 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: change Leader from null to 984f9786-6250-4b94-8d30-f65fab4a7cff at term 1 for becomeLeader, leader elected after 5573ms
datanode_3          | 2023-07-16 04:44:10,009 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:44:10,009 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:10,009 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 04:44:10,010 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:44:10,010 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:44:10,011 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:44:10,028 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:44:10,028 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 04:44:10,029 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO impl.RoleInfo: 984f9786-6250-4b94-8d30-f65fab4a7cff: start 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderStateImpl
datanode_3          | 2023-07-16 04:44:10,029 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:44:10,049 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a1928e58-e59a-421f-a41b-58c9b116b294/current/log_inprogress_0
datanode_3          | 2023-07-16 04:44:10,063 [984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294-LeaderElection3] INFO server.RaftServer$Division: 984f9786-6250-4b94-8d30-f65fab4a7cff@group-58C9B116B294: set configuration 0: peers:[984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 04:44:51,163 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:45:51,165 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:46:51,165 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:47:51,166 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:48:51,167 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 04:49:51,168 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:44:04,269 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:04,278 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:44:04,278 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:44:04,280 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:04,280 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8
datanode_2          | 2023-07-16 04:44:04,282 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 04:44:04,282 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:44:04,283 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:04,283 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:44:04,284 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:44:04,284 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:44:04,287 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:04,289 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:44:04,289 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:04,293 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:04,878 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d0d64a7d-3e54-43d9-a2ff-589187cfae66: Detected pause in JVM or host machine approximately 0.192s with 0.577s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=76ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=501ms
datanode_2          | 2023-07-16 04:44:04,910 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 04:44:04,911 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 04:44:04,911 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:04,914 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:04,914 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:04,924 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: start as a follower, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:04,924 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:44:04,924 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState
datanode_2          | 2023-07-16 04:44:04,925 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E77D073C3D8,id=d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_2          | 2023-07-16 04:44:04,927 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:04,927 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:44:04,931 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:44:04,931 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:44:04,934 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 04:44:04,945 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 04:44:05,478 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4CAA690A8BC7 with new leaderId: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_2          | 2023-07-16 04:44:05,485 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: change Leader from null to e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb at term 1 for appendEntries, leader elected after 4150ms
datanode_2          | 2023-07-16 04:44:05,544 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread2] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7: set configuration 0: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:05,584 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread2] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:44:05,906 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-4CAA690A8BC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/current/log_inprogress_0
datanode_2          | 2023-07-16 04:44:07,452 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: receive requestVote(PRE_VOTE, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, group-3E77D073C3D8, 0, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:07,453 [grpc-default-executor-1] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FOLLOWER: accept PRE_VOTE from e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-16 04:44:07,453 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8 replies to PRE_VOTE vote request: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8:t0, leader=null, voted=, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:08,103 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: receive requestVote(PRE_VOTE, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-3E77D073C3D8, 0, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:08,104 [grpc-default-executor-1] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FOLLOWER: accept PRE_VOTE from 984f9786-6250-4b94-8d30-f65fab4a7cff: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:44:08,104 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8 replies to PRE_VOTE vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8:t0, leader=null, voted=, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:08,147 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: receive requestVote(ELECTION, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-3E77D073C3D8, 1, (t:0, i:0))
datanode_2          | 2023-07-16 04:44:08,148 [grpc-default-executor-1] INFO impl.VoteContext: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FOLLOWER: accept ELECTION from 984f9786-6250-4b94-8d30-f65fab4a7cff: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:44:08,148 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_2          | 2023-07-16 04:44:08,148 [grpc-default-executor-1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: shutdown d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState
datanode_2          | 2023-07-16 04:44:08,148 [grpc-default-executor-1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState
datanode_2          | 2023-07-16 04:44:08,148 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState] INFO impl.FollowerState: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-FollowerState was interrupted
datanode_2          | 2023-07-16 04:44:08,161 [grpc-default-executor-1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8 replies to ELECTION vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t1. Peer's state: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8:t1, leader=null, voted=984f9786-6250-4b94-8d30-f65fab4a7cff, raftlog=Memoized:d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:08,644 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E77D073C3D8 with new leaderId: 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_2          | 2023-07-16 04:44:08,644 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: change Leader from null to 984f9786-6250-4b94-8d30-f65fab4a7cff at term 1 for appendEntries, leader elected after 4433ms
datanode_2          | 2023-07-16 04:44:08,654 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread2] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8: set configuration 0: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:08,656 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-server-thread2] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:44:08,658 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-3E77D073C3D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/current/log_inprogress_0
datanode_1          | 2023-07-16 04:43:56,781 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7
datanode_1          | 2023-07-16 04:43:56,787 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 04:43:56,788 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:43:56,798 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:43:56,800 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:43:56,806 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:43:56,808 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:43:56,808 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:43:56,812 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:43:56,860 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:43:56,868 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:43:56,984 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:43:56,997 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:43:56,997 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:43:57,023 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:43:57,024 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:43:57,030 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: start as a follower, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:43:57,032 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:43:57,037 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState
datanode_1          | 2023-07-16 04:43:57,068 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:43:57,071 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:43:57,071 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4CAA690A8BC7,id=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:43:57,082 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:43:57,085 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:43:57,085 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:43:57,088 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:43:57,140 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7
datanode_1          | 2023-07-16 04:44:01,352 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7.
datanode_1          | 2023-07-16 04:44:01,353 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: addNew group-F300D3F45A0F:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-F300D3F45A0F:java.util.concurrent.CompletableFuture@4d01fa9d[Not completed]
datanode_1          | 2023-07-16 04:44:01,359 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: new RaftServerImpl for group-F300D3F45A0F:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:44:01,359 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:44:01,361 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:44:01,361 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:44:01,361 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:44:01,361 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:44:01,361 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:44:24,758 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66: new RaftServerImpl for group-897E24564BB6:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:44:24,759 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:44:24,760 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:44:24,761 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:44:24,762 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:24,762 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:44:24,762 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:44:24,762 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: ConfigurationManager, init=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:44:24,763 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:44:24,764 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:44:24,765 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:44:24,765 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:44:24,765 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:24,765 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:44:24,765 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:44:24,766 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 04:44:24,768 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:44:24,769 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d0d64a7d-3e54-43d9-a2ff-589187cfae66: addNew group-897E24564BB6:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-897E24564BB6:java.util.concurrent.CompletableFuture@3134a685[Not completed]
datanode_2          | 2023-07-16 04:44:24,769 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:44:24,769 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 04:44:24,770 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 04:44:24,770 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 04:44:24,770 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 04:44:24,771 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ced029d5-cefb-47a1-a5da-897e24564bb6 does not exist. Creating ...
datanode_2          | 2023-07-16 04:44:24,773 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ced029d5-cefb-47a1-a5da-897e24564bb6/in_use.lock acquired by nodename 7@87fd3ab5b15c
datanode_2          | 2023-07-16 04:44:24,775 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ced029d5-cefb-47a1-a5da-897e24564bb6 has been successfully formatted.
datanode_2          | 2023-07-16 04:44:24,782 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO ratis.ContainerStateMachine: group-897E24564BB6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:44:24,783 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:44:24,783 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:44:24,784 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:24,784 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 04:44:24,784 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 04:44:24,784 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:24,785 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:44:24,785 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:44:24,785 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:24,785 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ced029d5-cefb-47a1-a5da-897e24564bb6
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:44:24,786 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:44:24,787 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:44:24,787 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:44:24,788 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:44:24,788 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:44:24,970 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 04:44:24,971 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 04:44:24,972 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:24,972 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:24,972 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:44:24,973 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: start as a follower, conf=-1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:24,973 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:44:24,973 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState
datanode_2          | 2023-07-16 04:44:24,980 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-897E24564BB6,id=d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_2          | 2023-07-16 04:44:24,980 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:44:24,981 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:44:24,981 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:44:01,362 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: ConfigurationManager, init=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:44:01,362 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:44:01,364 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:44:01,364 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:44:01,364 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:44:01,367 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-16 04:44:01,367 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:44:01,367 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:44:01,368 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-16 04:44:01,377 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:44:01,377 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:44:01,377 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 04:44:01,380 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 04:44:01,381 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 04:44:01,381 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 04:44:01,384 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/952e8d54-8734-4347-ab9a-f300d3f45a0f does not exist. Creating ...
datanode_1          | 2023-07-16 04:44:01,395 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/952e8d54-8734-4347-ab9a-f300d3f45a0f/in_use.lock acquired by nodename 8@8d1bfd0f2b9d
datanode_1          | 2023-07-16 04:44:01,407 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/952e8d54-8734-4347-ab9a-f300d3f45a0f has been successfully formatted.
datanode_1          | 2023-07-16 04:44:01,411 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO ratis.ContainerStateMachine: group-F300D3F45A0F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:44:01,416 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:44:01,419 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:44:01,419 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:01,419 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 04:44:01,419 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 04:44:01,429 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:01,432 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:44:01,439 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:44:01,443 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/952e8d54-8734-4347-ab9a-f300d3f45a0f
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:44:01,444 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:44:01,450 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:44:01,456 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:44:01,456 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:44:01,459 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:01,460 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:01,954 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: Detected pause in JVM or host machine approximately 0.259s with 0.487s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=65ms
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=422ms
datanode_1          | 2023-07-16 04:44:01,971 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:44:01,971 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:44:01,984 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:44:01,985 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:44:01,988 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:44:01,997 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: start as a follower, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:01,997 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:44:01,998 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState
datanode_1          | 2023-07-16 04:44:02,003 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F300D3F45A0F,id=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:44:02,003 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:44:02,003 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:44:02,004 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:44:02,004 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:44:24,981 [d0d64a7d-3e54-43d9-a2ff-589187cfae66-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:44:24,983 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 04:44:24,983 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 04:44:24,989 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ced029d5-cefb-47a1-a5da-897e24564bb6
datanode_2          | 2023-07-16 04:44:24,990 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=ced029d5-cefb-47a1-a5da-897e24564bb6.
datanode_2          | 2023-07-16 04:44:30,149 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO impl.FollowerState: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5175908733ns, electionTimeout:5165ms
datanode_2          | 2023-07-16 04:44:30,150 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: shutdown d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState
datanode_2          | 2023-07-16 04:44:30,150 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:44:30,154 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-07-16 04:44:30,154 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-FollowerState] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1
datanode_2          | 2023-07-16 04:44:30,233 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.LeaderElection: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:30,234 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.LeaderElection: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-16 04:44:30,242 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.LeaderElection: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:30,242 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.LeaderElection: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-16 04:44:30,243 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: shutdown d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1
datanode_2          | 2023-07-16 04:44:30,243 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-16 04:44:30,245 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-897E24564BB6 with new leaderId: d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_2          | 2023-07-16 04:44:30,252 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: change Leader from null to d0d64a7d-3e54-43d9-a2ff-589187cfae66 at term 1 for becomeLeader, leader elected after 5480ms
datanode_2          | 2023-07-16 04:44:30,265 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 04:44:30,292 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:44:30,293 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 04:44:30,310 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 04:44:30,315 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 04:44:30,316 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 04:44:30,331 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:44:30,335 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 04:44:30,348 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO impl.RoleInfo: d0d64a7d-3e54-43d9-a2ff-589187cfae66: start d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderStateImpl
datanode_2          | 2023-07-16 04:44:30,352 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:44:30,357 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ced029d5-cefb-47a1-a5da-897e24564bb6/current/log_inprogress_0
datanode_2          | 2023-07-16 04:44:30,364 [d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6-LeaderElection1] INFO server.RaftServer$Division: d0d64a7d-3e54-43d9-a2ff-589187cfae66@group-897E24564BB6: set configuration 0: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 04:44:32,757 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d0d64a7d-3e54-43d9-a2ff-589187cfae66: Detected pause in JVM or host machine approximately 0.291s with 0.701s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=701ms
datanode_2          | 2023-07-16 04:44:51,221 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:45:51,222 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:46:51,223 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:47:51,223 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:48:51,224 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 04:49:51,224 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:44:02,004 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:44:02,005 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:44:02,010 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f
datanode_1          | 2023-07-16 04:44:02,010 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f.
datanode_1          | 2023-07-16 04:44:02,011 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: addNew group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-3E77D073C3D8:java.util.concurrent.CompletableFuture@6f17bb0[Not completed]
datanode_1          | 2023-07-16 04:44:02,019 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: new RaftServerImpl for group-3E77D073C3D8:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:44:02,020 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:44:02,020 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:44:02,021 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:44:02,021 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:44:02,021 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:44:02,022 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:44:02,023 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: ConfigurationManager, init=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:44:02,025 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:44:02,027 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:44:02,028 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:44:02,029 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:44:02,033 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-16 04:44:02,033 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:42:52,101 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = d2cf9daf66d6/172.19.0.2
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 04:44:02,034 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:44:02,034 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-16 04:44:02,040 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:44:02,041 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:44:02,041 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 04:44:02,041 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 04:44:02,042 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 04:44:02,045 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 04:44:02,046 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 does not exist. Creating ...
datanode_1          | 2023-07-16 04:44:02,054 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/in_use.lock acquired by nodename 8@8d1bfd0f2b9d
datanode_1          | 2023-07-16 04:44:02,057 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 has been successfully formatted.
datanode_1          | 2023-07-16 04:44:02,083 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO ratis.ContainerStateMachine: group-3E77D073C3D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:44:02,084 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:44:02,086 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:44:02,086 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:02,087 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 04:44:02,087 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 04:44:02,088 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:02,086 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO impl.FollowerState: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5048946010ns, electionTimeout:5004ms
datanode_1          | 2023-07-16 04:44:02,093 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState
datanode_1          | 2023-07-16 04:44:02,095 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:44:02,118 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-16 04:44:02,121 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1
datanode_1          | 2023-07-16 04:44:02,130 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:44:02,131 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:44:02,131 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:02,131 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8
datanode_1          | 2023-07-16 04:44:02,131 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 04:44:02,131 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:02,132 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:02,132 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:44:02,134 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:44:02,134 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:44:02,136 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:44:02,137 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:44:02,137 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:44:02,145 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:02,151 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:42:51,962 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = d333c4ab6667/172.19.0.5
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:42:52,057 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:42:52,963 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:42:55,016 [main] INFO reflections.Reflections: Reflections took 1204 ms to scan 3 urls, producing 132 keys and 288 values 
scm_1               | 2023-07-16 04:42:56,482 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 04:42:56,551 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 04:42:58,463 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 04:42:59,191 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-16 04:42:52,768 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-16 04:42:52,772 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-16 04:42:53,366 [main] INFO util.log: Logging initialized @14623ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-16 04:42:54,466 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-07-16 04:42:54,589 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-16 04:42:54,681 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-16 04:42:54,695 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-16 04:42:54,695 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-16 04:42:54,700 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-16 04:42:55,295 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir7810648810838884560
s3g_1               | 2023-07-16 04:42:56,664 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 8b4a01769ba4/172.19.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-07-16 04:42:52,202 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:43:01,667 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-16 04:43:04,988 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:43:06,000 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.2:9862
om_1                | 2023-07-16 04:43:06,008 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:43:06,008 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:43:06,478 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:43:08,221 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-07-16 04:43:11,965 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-07-16 04:43:13,967 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-07-16 04:43:15,969 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-07-16 04:43:17,970 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-07-16 04:43:19,972 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-07-16 04:43:21,989 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1                | 2023-07-16 04:43:23,991 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-07-16 04:43:25,993 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-07-16 04:43:27,996 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-07-16 04:43:29,998 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-07-16 04:43:32,000 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-07-16 04:43:34,002 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-07-16 04:43:36,004 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-07-16 04:43:38,007 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-07-16 04:43:40,013 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-07-16 04:43:42,015 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d2cf9daf66d6/172.19.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | 2023-07-16 04:43:46,771 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b0ddd29c-8a6e-4603-a5ef-dbb4e249f117 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | 2023-07-16 04:42:59,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 04:42:59,216 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 04:42:59,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 04:42:59,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-16 04:42:59,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-16 04:42:59,230 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-16 04:42:59,238 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:42:59,257 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-16 04:42:59,266 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 04:42:59,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-16 04:42:59,432 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-16 04:42:59,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-16 04:43:01,650 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-16 04:43:01,666 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-16 04:43:01,677 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-16 04:43:01,679 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 04:43:01,681 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 04:43:01,684 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 04:43:01,733 [main] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: addNew group-8F69D29DEB95:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER] returns group-8F69D29DEB95:java.util.concurrent.CompletableFuture@1716e8c5[Not completed]
scm_1               | 2023-07-16 04:43:02,026 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: new RaftServerImpl for group-8F69D29DEB95:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-07-16 04:43:02,050 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-16 04:43:02,057 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-16 04:43:02,064 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-16 04:43:02,065 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 04:43:02,065 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 04:43:02,065 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 04:43:02,103 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: ConfigurationManager, init=-1: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 04:43:02,136 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 04:43:02,215 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 04:43:02,220 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 04:43:02,392 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-16 04:43:02,422 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:02,463 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-16 04:43:02,480 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 04:43:02,683 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-16 04:43:02,874 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 04:43:04,156 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 04:43:04,178 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 04:43:04,205 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 04:43:04,212 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 04:43:04,218 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-16 04:43:04,229 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 04:43:04,234 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95 does not exist. Creating ...
scm_1               | 2023-07-16 04:43:04,281 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/in_use.lock acquired by nodename 13@d333c4ab6667
scm_1               | 2023-07-16 04:43:04,350 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95 has been successfully formatted.
scm_1               | 2023-07-16 04:43:04,372 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 04:43:04,408 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-16 04:43:04,409 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:04,411 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-16 04:43:04,420 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-16 04:43:04,427 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 04:43:04,475 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 04:43:04,476 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-16 04:43:04,478 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:04,499 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95
scm_1               | 2023-07-16 04:43:04,502 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 04:43:04,520 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:04,521 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 04:43:04,521 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 04:43:04,525 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-16 04:43:04,527 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 04:43:04,538 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:04,538 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-16 04:43:04,636 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-16 04:43:04,644 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:04,725 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 04:43:04,732 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 04:43:04,733 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-16 04:43:04,849 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 04:43:04,849 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 04:43:04,852 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: start as a follower, conf=-1: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:04,883 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-07-16 04:43:04,885 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState
scm_1               | 2023-07-16 04:43:05,003 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 04:43:05,003 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 04:43:05,006 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F69D29DEB95,id=b0ddd29c-8a6e-4603-a5ef-dbb4e249f117
scm_1               | 2023-07-16 04:43:05,022 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-16 04:43:05,022 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-16 04:43:05,035 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-16 04:43:05,044 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-16 04:43:05,092 [main] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start RPC server
scm_1               | 2023-07-16 04:43:05,937 [main] INFO server.GrpcService: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: GrpcService started, listening on 9894
scm_1               | 2023-07-16 04:43:06,041 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: Started
scm_1               | 2023-07-16 04:43:10,024 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.FollowerState: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139560588ns, electionTimeout:5016ms
scm_1               | 2023-07-16 04:43:10,027 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState
scm_1               | 2023-07-16 04:43:10,039 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-16 04:43:10,077 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-16 04:43:10,077 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1
scm_1               | 2023-07-16 04:43:10,380 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:10,381 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-07-16 04:43:10,390 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:10,390 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-16 04:43:10,390 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1
scm_1               | 2023-07-16 04:43:10,391 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-16 04:43:10,391 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: change Leader from null to b0ddd29c-8a6e-4603-a5ef-dbb4e249f117 at term 1 for becomeLeader, leader elected after 8021ms
scm_1               | 2023-07-16 04:43:10,642 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-16 04:43:10,713 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:10,725 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 04:43:10,802 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:10,858 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-16 04:43:10,860 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-16 04:43:10,981 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:11,031 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 04:43:11,038 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderStateImpl
scm_1               | 2023-07-16 04:43:11,306 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-16 04:43:11,505 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: set configuration 0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:11,859 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/log_inprogress_0
scm_1               | 2023-07-16 04:43:12,038 [main] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: close
scm_1               | 2023-07-16 04:43:12,040 [main] INFO server.GrpcService: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown server GrpcServerProtocolService now
scm_1               | 2023-07-16 04:43:12,040 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: shutdown
scm_1               | 2023-07-16 04:43:12,040 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F69D29DEB95,id=b0ddd29c-8a6e-4603-a5ef-dbb4e249f117
scm_1               | 2023-07-16 04:43:12,040 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderStateImpl
scm_1               | 2023-07-16 04:43:12,050 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO impl.PendingRequests: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-16 04:43:12,083 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO impl.StateMachineUpdater: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-07-16 04:43:12,084 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO impl.StateMachineUpdater: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-07-16 04:43:12,086 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO impl.StateMachineUpdater: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-07-16 04:43:12,148 [main] INFO server.GrpcService: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-07-16 04:43:12,164 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: closes. applyIndex: 0
scm_1               | 2023-07-16 04:43:12,919 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker close()
scm_1               | 2023-07-16 04:43:12,921 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: Stopped
scm_1               | 2023-07-16 04:43:12,921 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir7810648810838884560, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-07-16 04:42:56,722 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-16 04:42:56,837 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-16 04:42:57,946 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-07-16 04:42:59,116 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-16 04:42:59,117 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-07-16 04:42:59,415 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-07-16 04:42:59,492 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-16 04:42:59,500 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-07-16 04:42:59,794 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-16 04:42:59,795 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-16 04:42:59,797 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-07-16 04:43:00,032 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@213e3629{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 04:43:00,044 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5528a42c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-07-16 04:43:09,533 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.127s with 0.233s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=233ms
s3g_1               | 2023-07-16 04:43:30,226 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6d7865d6{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir7810648810838884560/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-9981697432949372464/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-07-16 04:43:30,318 [main] INFO server.AbstractConnector: Started ServerConnector@22ee2d0{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-16 04:43:30,327 [main] INFO server.Server: Started @51584ms
s3g_1               | 2023-07-16 04:43:30,364 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-07-16 04:43:30,364 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-07-16 04:43:30,366 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1               | 2023-07-16 04:43:31,078 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.152s without any GCs.
datanode_1          | 2023-07-16 04:44:02,242 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:44:02,250 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:44:02,249 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:44:02,254 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 04:44:02,254 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:44:02,255 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:44:02,258 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:44:02,276 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: start as a follower, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:02,276 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:44:02,277 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState
datanode_1          | 2023-07-16 04:44:02,277 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E77D073C3D8,id=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:44:02,278 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:44:02,278 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:44:02,278 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:44:02,278 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:44:02,282 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:44:02,293 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_1          | 2023-07-16 04:44:02,282 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8
datanode_1          | 2023-07-16 04:44:02,292 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for d0d64a7d-3e54-43d9-a2ff-589187cfae66
datanode_1          | 2023-07-16 04:44:02,364 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:44:03,679 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: Detected pause in JVM or host machine approximately 0.724s with 1.071s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=1071ms
datanode_1          | 2023-07-16 04:44:03,747 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:44:03,748 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection:   Response 0: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0
datanode_1          | 2023-07-16 04:44:03,749 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-07-16 04:44:03,757 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:03,769 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:44:03,769 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:44:03,880 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:44:03,880 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection:   Response 0: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-984f9786-6250-4b94-8d30-f65fab4a7cff#0:OK-t1
datanode_1          | 2023-07-16 04:44:03,880 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1 ELECTION round 0: result PASSED
datanode_1          | 2023-07-16 04:44:03,881 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om_1                | 2023-07-16 04:43:48,777 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b0ddd29c-8a6e-4603-a5ef-dbb4e249f117 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 18.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-6d5e37af-423b-4dae-b444-8f69d29deb95;layoutVersion=6
om_1                | 2023-07-16 04:43:51,417 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at d2cf9daf66d6/172.19.0.2
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:43:58,936 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = d2cf9daf66d6/172.19.0.2
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-07-16 04:44:03,888 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 04:44:03,888 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4CAA690A8BC7 with new leaderId: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:44:03,889 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: change Leader from null to e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb at term 1 for becomeLeader, leader elected after 7821ms
datanode_1          | 2023-07-16 04:44:03,963 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:44:04,006 [grpc-default-executor-1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: receive requestVote(PRE_VOTE, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-4CAA690A8BC7, 0, (t:0, i:0))
datanode_1          | 2023-07-16 04:44:04,088 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:04,089 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 04:44:04,161 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:44:04,161 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:44:04,235 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:44:04,503 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:04,541 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-16 04:44:04,750 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:44:04,751 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:04,757 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:44:04,803 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-16 04:44:04,805 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:44:04,805 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:44:04,836 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-16 04:44:04,838 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-07-16 04:44:04,839 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:44:04,839 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 04:44:04,852 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:44:04,852 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:44:04,853 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:44:04,854 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-07-16 04:44:04,854 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:44:04,855 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:44:04,855 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-16 04:44:04,855 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-07-16 04:44:04,856 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:44:04,856 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 04:44:04,861 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderStateImpl
datanode_1          | 2023-07-16 04:44:04,924 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:44:04,980 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8.
datanode_1          | 2023-07-16 04:44:05,161 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LeaderElection1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7: set configuration 0: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 04:42:49,646 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 1c45133c7f25/172.19.0.4
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | 2023-07-16 04:43:12,934 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-6d5e37af-423b-4dae-b444-8f69d29deb95; layoutVersion=7; scmId=b0ddd29c-8a6e-4603-a5ef-dbb4e249f117
scm_1               | 2023-07-16 04:43:13,201 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at d333c4ab6667/172.19.0.5
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:43:28,291 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = d333c4ab6667/172.19.0.5
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-07-16 04:43:58,979 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:44:03,764 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-16 04:44:05,398 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:44:05,928 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.2:9862
om_1                | 2023-07-16 04:44:05,929 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:44:05,929 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:44:06,082 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:44:06,287 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-07-16 04:44:06,805 [main] INFO reflections.Reflections: Reflections took 451 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-07-16 04:44:06,893 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-07-16 04:44:07,032 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:44:08,291 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-07-16 04:44:08,558 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-07-16 04:44:10,177 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-07-16 04:44:10,261 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:44:10,746 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-07-16 04:44:11,826 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-07-16 04:44:11,976 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-07-16 04:44:12,025 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:44:12,150 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-07-16 04:44:12,162 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-07-16 04:44:12,846 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-16 04:44:13,066 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 04:44:13,070 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-16 04:44:13,174 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-16 04:44:13,210 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 04:44:05,170 [grpc-default-executor-1] INFO impl.VoteContext: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-LEADER: reject PRE_VOTE from 984f9786-6250-4b94-8d30-f65fab4a7cff: this server is the leader and still has leadership
datanode_1          | 2023-07-16 04:44:05,172 [grpc-default-executor-1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7 replies to PRE_VOTE vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:FAIL-t1. Peer's state: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7:t1, leader=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, voted=e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, raftlog=Memoized:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLog:OPENED:c-1, conf=0: peers:[d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:05,647 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-4CAA690A8BC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef7467ff-7a52-4835-9cf9-4caa690a8bc7/current/log_inprogress_0
datanode_1          | 2023-07-16 04:44:07,111 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO impl.FollowerState: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5113456644ns, electionTimeout:5106ms
datanode_1          | 2023-07-16 04:44:07,112 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState
datanode_1          | 2023-07-16 04:44:07,112 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:44:07,112 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-16 04:44:07,112 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2
datanode_1          | 2023-07-16 04:44:07,125 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:07,125 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-07-16 04:44:07,146 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:07,146 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-16 04:44:07,146 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2
datanode_1          | 2023-07-16 04:44:07,149 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 04:44:07,149 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F300D3F45A0F with new leaderId: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
datanode_1          | 2023-07-16 04:44:07,152 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: change Leader from null to e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb at term 1 for becomeLeader, leader elected after 5784ms
datanode_1          | 2023-07-16 04:44:07,161 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:44:07,166 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:07,166 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 04:44:07,167 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:44:07,167 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:44:07,171 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:44:07,172 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:44:07,173 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-16 04:44:07,174 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderStateImpl
datanode_1          | 2023-07-16 04:44:07,176 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:44:07,197 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-LeaderElection2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F: set configuration 0: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:07,221 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-F300D3F45A0F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/952e8d54-8734-4347-ab9a-f300d3f45a0f/current/log_inprogress_0
om_1                | 2023-07-16 04:44:13,324 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-16 04:44:13,363 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-16 04:44:13,537 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-16 04:44:13,559 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-16 04:44:13,562 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-16 04:44:13,563 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-16 04:44:13,564 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-16 04:44:13,565 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-07-16 04:44:13,566 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:44:13,566 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-16 04:44:13,571 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:44:13,575 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-16 04:44:13,578 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-16 04:44:13,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-16 04:44:13,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-16 04:44:13,641 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-16 04:44:14,403 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-16 04:44:14,413 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-07-16 04:44:14,418 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-07-16 04:44:14,418 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:44:14,420 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:44:14,429 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:44:14,467 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@1aa945f6[Not completed]
om_1                | 2023-07-16 04:44:14,468 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-16 04:44:14,483 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-07-16 04:44:14,533 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-16 04:44:14,541 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-16 04:44:14,543 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-16 04:44:14,543 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-16 04:44:14,544 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:44:14,545 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:44:14,545 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 04:44:14,570 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-16 04:44:14,570 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:44:14,594 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-16 04:44:14,596 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-16 04:44:14,679 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-16 04:44:14,705 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-07-16 04:44:14,729 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-16 04:44:14,733 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-16 04:44:14,870 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-07-16 04:44:15,149 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-16 04:44:15,158 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:44:07,396 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO impl.FollowerState: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5119258968ns, electionTimeout:5028ms
datanode_1          | 2023-07-16 04:44:07,396 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState
datanode_1          | 2023-07-16 04:44:07,396 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:44:07,397 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-07-16 04:44:07,397 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3
datanode_1          | 2023-07-16 04:44:07,415 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:07,430 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 04:44:07,432 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 04:44:07,525 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:44:07,525 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.LeaderElection:   Response 0: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-d0d64a7d-3e54-43d9-a2ff-589187cfae66#0:OK-t0
datanode_1          | 2023-07-16 04:44:07,525 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.LeaderElection:   Response 1: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb<-984f9786-6250-4b94-8d30-f65fab4a7cff#0:FAIL-t0
datanode_1          | 2023-07-16 04:44:07,525 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.LeaderElection: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3 PRE_VOTE round 0: result REJECTED
datanode_1          | 2023-07-16 04:44:07,532 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_1          | 2023-07-16 04:44:07,533 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3
datanode_1          | 2023-07-16 04:44:07,535 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-LeaderElection3] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState
datanode_1          | 2023-07-16 04:44:08,057 [grpc-default-executor-2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: receive requestVote(PRE_VOTE, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-3E77D073C3D8, 0, (t:0, i:0))
datanode_1          | 2023-07-16 04:44:08,058 [grpc-default-executor-2] INFO impl.VoteContext: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FOLLOWER: accept PRE_VOTE from 984f9786-6250-4b94-8d30-f65fab4a7cff: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-16 04:44:08,061 [grpc-default-executor-2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8 replies to PRE_VOTE vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:OK-t0. Peer's state: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8:t0, leader=null, voted=, raftlog=Memoized:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:08,110 [grpc-default-executor-2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: receive requestVote(ELECTION, 984f9786-6250-4b94-8d30-f65fab4a7cff, group-3E77D073C3D8, 1, (t:0, i:0))
datanode_1          | 2023-07-16 04:44:08,110 [grpc-default-executor-2] INFO impl.VoteContext: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FOLLOWER: accept ELECTION from 984f9786-6250-4b94-8d30-f65fab4a7cff: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-16 04:44:08,111 [grpc-default-executor-2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_1          | 2023-07-16 04:44:08,111 [grpc-default-executor-2] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: shutdown e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState
datanode_1          | 2023-07-16 04:44:08,111 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState] INFO impl.FollowerState: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState was interrupted
datanode_1          | 2023-07-16 04:44:08,112 [grpc-default-executor-2] INFO impl.RoleInfo: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb: start e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-FollowerState
datanode_1          | 2023-07-16 04:44:08,120 [grpc-default-executor-2] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8 replies to ELECTION vote request: 984f9786-6250-4b94-8d30-f65fab4a7cff<-e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb#0:OK-t1. Peer's state: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8:t1, leader=null, voted=984f9786-6250-4b94-8d30-f65fab4a7cff, raftlog=Memoized:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 04:44:15,160 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-07-16 04:44:15,161 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-07-16 04:44:15,161 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-16 04:44:15,164 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-16 04:44:16,256 [main] INFO reflections.Reflections: Reflections took 1479 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
om_1                | 2023-07-16 04:44:16,839 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 04:44:16,855 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-07-16 04:44:16,864 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 04:44:18,461 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-16 04:44:18,501 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 04:44:18,501 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-16 04:44:18,650 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.19.0.2:9862
om_1                | 2023-07-16 04:44:18,650 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-16 04:44:18,656 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
recon_1             | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 04:44:08,624 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E77D073C3D8 with new leaderId: 984f9786-6250-4b94-8d30-f65fab4a7cff
datanode_1          | 2023-07-16 04:44:08,624 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-server-thread1] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: change Leader from null to 984f9786-6250-4b94-8d30-f65fab4a7cff at term 1 for appendEntries, leader elected after 6595ms
datanode_1          | 2023-07-16 04:44:08,721 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-server-thread3] INFO server.RaftServer$Division: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8: set configuration 0: peers:[e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, d0d64a7d-3e54-43d9-a2ff-589187cfae66|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER, 984f9786-6250-4b94-8d30-f65fab4a7cff|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 04:44:08,725 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb-server-thread3] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:44:08,732 [e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb@group-3E77D073C3D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8/current/log_inprogress_0
datanode_1          | 2023-07-16 04:44:51,162 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:45:51,165 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:46:51,168 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:47:51,168 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:48:51,169 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 04:49:51,169 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-07-16 04:44:18,671 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@d2cf9daf66d6
om_1                | 2023-07-16 04:44:18,689 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-16 04:44:18,696 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-16 04:44:18,721 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-16 04:44:18,722 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:44:18,727 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-16 04:44:18,729 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-16 04:44:18,738 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:44:18,762 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-16 04:44:18,764 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-16 04:44:18,764 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:44:18,780 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-16 04:44:18,782 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:44:18,783 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-16 04:44:18,786 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:44:18,788 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-16 04:44:18,792 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-16 04:44:18,795 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-16 04:44:18,804 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 04:44:18,805 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 04:44:18,830 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-16 04:44:18,831 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:44:18,902 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-16 04:44:18,903 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-16 04:44:18,903 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-16 04:44:18,925 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:44:18,925 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:44:18,933 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 04:44:18,934 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-16 04:44:18,939 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:44:18,950 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 04:44:18,951 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-16 04:44:18,955 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-16 04:44:18,958 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 04:44:18,959 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-16 04:44:18,959 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-16 04:44:18,961 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-16 04:44:18,973 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-16 04:44:19,158 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-16 04:44:19,178 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-16 04:44:19,179 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-16 04:44:19,347 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-16 04:44:19,347 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 04:44:19,379 [main] INFO util.log: Logging initialized @27172ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 04:44:19,650 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-07-16 04:44:19,659 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-16 04:44:19,676 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-16 04:44:19,681 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-16 04:44:19,681 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-16 04:44:19,682 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-16 04:44:19,746 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-07-16 04:44:19,752 [main] INFO http.HttpServer2: Jetty bound to port 9874
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
om_1                | 2023-07-16 04:44:19,753 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-07-16 04:44:19,812 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-16 04:44:19,813 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-16 04:44:19,818 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-16 04:44:19,834 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@590db06a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 04:44:19,835 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3ac2300f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-16 04:44:20,011 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@371a3078{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-7382465154324709299/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-07-16 04:44:20,030 [main] INFO server.AbstractConnector: Started ServerConnector@6d30d79b{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-16 04:44:20,030 [main] INFO server.Server: Started @27824ms
om_1                | 2023-07-16 04:44:20,039 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-16 04:44:20,040 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-16 04:44:20,042 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-16 04:44:20,046 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-16 04:44:20,062 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-16 04:44:20,177 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-07-16 04:44:20,467 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-07-16 04:44:24,119 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5180737245ns, electionTimeout:5166ms
om_1                | 2023-07-16 04:44:24,121 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:44:24,121 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-16 04:44:24,125 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-07-16 04:44:24,125 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:44:24,133 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 04:44:24,134 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-07-16 04:44:24,136 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 04:44:24,137 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-16 04:44:24,137 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:44:24,137 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-16 04:44:24,141 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 9460ms
om_1                | 2023-07-16 04:44:24,150 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-16 04:44:24,156 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 04:44:24,157 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:44:24,164 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-16 04:44:24,165 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-16 04:44:24,168 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-16 04:44:24,175 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 04:44:24,178 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-16 04:44:24,181 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-16 04:44:24,252 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-16 04:44:24,290 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 04:44:24,371 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-16 04:44:24,495 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-07-16 04:44:27,771 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-16 04:44:27,951 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-07-16 04:44:46,073 [qtp1374165330-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-07-16 04:44:46,456 [qtp1374165330-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1689482686384 in 72 milliseconds
om_1                | 2023-07-16 04:44:46,497 [qtp1374165330-54] INFO db.RDBCheckpointUtils: Waited for 39 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1689482686384 availability.
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:43:28,354 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:43:28,826 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:43:30,551 [main] INFO reflections.Reflections: Reflections took 1334 ms to scan 3 urls, producing 132 keys and 288 values 
scm_1               | 2023-07-16 04:43:31,312 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 04:43:31,383 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 04:43:34,280 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:44:46,679 [qtp1374165330-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 180 milliseconds
om_1                | 2023-07-16 04:44:46,680 [qtp1374165330-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1                | 2023-07-16 04:44:46,683 [qtp1374165330-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689482686384
recon_1             | 2023-07-16 04:42:49,742 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-07-16 04:42:56,086 [main] INFO reflections.Reflections: Reflections took 632 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-07-16 04:43:02,384 [main] INFO reflections.Reflections: Reflections took 585 ms to scan 3 urls, producing 132 keys and 288 values 
recon_1             | 2023-07-16 04:43:03,481 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-16 04:43:06,849 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:43:16,062 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-16 04:43:18,517 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-16 04:43:18,518 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-16 04:43:18,874 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:43:18,975 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:43:18,981 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-16 04:43:23,230 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-16 04:43:23,335 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-16 04:43:23,530 [main] INFO util.log: Logging initialized @46360ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-16 04:43:24,277 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-07-16 04:43:24,378 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-07-16 04:43:24,433 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-16 04:43:24,477 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-16 04:43:24,481 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-16 04:43:24,497 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-16 04:43:25,150 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-07-16 04:43:25,193 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-16 04:43:26,894 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-16 04:43:27,032 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-07-16 04:43:27,184 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-16 04:43:27,609 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-16 04:43:27,609 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-16 04:43:33,220 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:43:33,784 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:43:34,362 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-07-16 04:43:34,378 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-16 04:43:34,951 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:43:35,379 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-07-16 04:43:35,429 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-16 04:43:35,537 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-16 04:43:35,564 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-16 04:43:35,585 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-07-16 04:43:36,668 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-16 04:43:36,892 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 04:43:36,961 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-07-16 04:43:36,974 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-16 04:43:37,114 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-16 04:43:37,441 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-07-16 04:43:37,456 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-16 04:43:37,463 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-16 04:43:37,677 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-16 04:43:37,719 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 04:43:37,719 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-16 04:43:38,696 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-16 04:43:38,706 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-07-16 04:43:38,902 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 04:43:38,903 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 04:43:35,165 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:43:35,928 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-07-16 04:43:35,930 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-16 04:43:36,221 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 04:43:36,923 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:b0ddd29c-8a6e-4603-a5ef-dbb4e249f117
scm_1               | 2023-07-16 04:43:37,277 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 04:43:37,292 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 04:43:37,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 04:43:37,304 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 04:43:37,307 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 04:43:37,308 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-16 04:43:37,308 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-16 04:43:37,309 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-16 04:43:37,310 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:37,311 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-16 04:43:37,312 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 04:43:37,343 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-16 04:43:37,367 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-16 04:43:37,368 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-16 04:43:37,725 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-16 04:43:37,728 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-16 04:43:37,729 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-16 04:43:37,729 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 04:43:37,739 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 04:43:37,773 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 04:43:37,783 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: found a subdirectory /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95
scm_1               | 2023-07-16 04:43:37,796 [main] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: addNew group-8F69D29DEB95:[] returns group-8F69D29DEB95:java.util.concurrent.CompletableFuture@595814a1[Not completed]
scm_1               | 2023-07-16 04:43:37,847 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: new RaftServerImpl for group-8F69D29DEB95:[] with SCMStateMachine:uninitialized
scm_1               | 2023-07-16 04:43:37,862 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-16 04:43:37,862 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-16 04:43:37,862 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-16 04:43:37,864 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 04:43:37,864 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 04:43:37,864 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 04:43:37,882 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 04:43:37,882 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 04:43:37,894 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 04:43:37,902 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 04:43:37,933 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-16 04:43:37,937 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:37,946 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-16 04:43:37,948 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 04:43:37,983 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-16 04:43:38,285 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 04:43:38,287 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 04:43:38,302 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 04:43:38,302 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 04:43:38,314 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-16 04:43:38,315 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 04:43:38,320 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1             | 2023-07-16 04:43:38,910 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-07-16 04:43:38,980 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@240f712e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 04:43:38,987 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7995a589{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 04:43:45,022 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4eee43a{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-17353949272551024049/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-07-16 04:43:45,053 [main] INFO server.AbstractConnector: Started ServerConnector@242c4a94{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-16 04:43:45,054 [main] INFO server.Server: Started @67886ms
recon_1             | 2023-07-16 04:43:45,065 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-16 04:43:45,065 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 04:43:45,068 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 04:43:45,068 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-16 04:43:45,086 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-16 04:43:45,098 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 04:43:45,098 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-16 04:43:45,098 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:43:45,099 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-16 04:43:45,103 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:43:49,175 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b0ddd29c-8a6e-4603-a5ef-dbb4e249f117 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-07-16 04:43:52,795 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-07-16 04:43:52,795 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 04:43:52,795 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-07-16 04:43:52,796 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:43:52,813 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-16 04:43:52,844 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-16 04:43:53,153 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:55224 / 172.19.0.10:55224
recon_1             | 2023-07-16 04:43:53,153 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:50988 / 172.19.0.13:50988
recon_1             | 2023-07-16 04:43:53,960 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:60656 / 172.19.0.9:60656: output error
recon_1             | 2023-07-16 04:43:53,965 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:59734 / 172.19.0.9:59734: output error
recon_1             | 2023-07-16 04:43:53,964 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:46566 / 172.19.0.13:46566: output error
recon_1             | 2023-07-16 04:43:53,964 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:39196 / 172.19.0.10:39196: output error
recon_1             | 2023-07-16 04:43:53,968 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:39192 / 172.19.0.10:39192: output error
recon_1             | 2023-07-16 04:43:53,972 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:46556 / 172.19.0.13:46556: output error
recon_1             | 2023-07-16 04:43:53,974 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:59730 / 172.19.0.9:59730: output error
recon_1             | 2023-07-16 04:43:53,975 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-16 04:43:38,321 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-16 04:43:38,321 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-07-16 04:43:38,469 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-07-16 04:43:38,648 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-16 04:43:38,650 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-16 04:43:38,684 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-16 04:43:38,687 [main] INFO ha.SequenceIdGenerator: upgrade rootCertificateId to 1
scm_1               | 2023-07-16 04:43:38,695 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-16 04:43:39,061 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-16 04:43:39,161 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-07-16 04:43:39,178 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-16 04:43:39,237 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-16 04:43:39,344 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-16 04:43:39,345 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-16 04:43:39,354 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-16 04:43:39,358 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:43:39,368 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-07-16 04:43:39,372 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-07-16 04:43:39,381 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-07-16 04:43:39,386 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-07-16 04:43:39,464 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-16 04:43:39,464 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-16 04:43:39,527 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-16 04:43:39,771 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-16 04:43:39,836 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-16 04:43:39,840 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-16 04:43:39,872 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-16 04:43:39,884 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:43:39,898 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 04:43:40,023 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-07-16 04:43:41,911 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 04:43:42,006 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:43:42,146 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-07-16 04:43:42,152 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 04:43:42,346 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 04:43:42,355 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:43:42,357 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-07-16 04:43:42,363 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 04:43:42,468 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 04:43:42,518 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:43:42,519 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-07-16 04:43:42,530 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-16 04:43:42,990 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-07-16 04:43:42,992 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,977 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,977 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 
scm_1               | 2023-07-16 04:43:42,993 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-16 04:43:43,012 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:43:43,031 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-07-16 04:43:43,052 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/in_use.lock acquired by nodename 7@d333c4ab6667
scm_1               | 2023-07-16 04:43:43,067 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=b0ddd29c-8a6e-4603-a5ef-dbb4e249f117} from /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/raft-meta
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,977 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,977 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-07-16 04:43:43,199 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: set configuration 0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:43,215 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 04:43:43,262 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-16 04:43:43,262 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:43,272 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-16 04:43:43,274 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-16 04:43:43,289 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 04:43:43,313 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 04:43:43,313 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-16 04:43:43,321 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:43,339 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95
scm_1               | 2023-07-16 04:43:43,341 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 04:43:43,345 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:43,350 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 04:43:43,351 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 04:43:43,352 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-16 04:43:43,353 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 04:43:43,355 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:43,356 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-16 04:43:43,388 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-16 04:43:43,389 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 04:43:43,469 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 04:43:43,477 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 04:43:43,477 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-16 04:43:43,621 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: set configuration 0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:43,637 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/log_inprogress_0
scm_1               | 2023-07-16 04:43:43,653 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 04:43:44,040 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: start as a follower, conf=0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:44,040 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-07-16 04:43:44,041 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState
scm_1               | 2023-07-16 04:43:44,061 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F69D29DEB95,id=b0ddd29c-8a6e-4603-a5ef-dbb4e249f117
scm_1               | 2023-07-16 04:43:44,066 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 04:43:44,066 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 04:43:44,068 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-16 04:43:44,074 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-16 04:43:44,075 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-16 04:43:44,078 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-16 04:43:44,099 [main] INFO server.RaftServer: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start RPC server
scm_1               | 2023-07-16 04:43:44,629 [main] INFO server.GrpcService: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: GrpcService started, listening on 9894
scm_1               | 2023-07-16 04:43:44,637 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: Started
scm_1               | 2023-07-16 04:43:44,681 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,977 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:53,981 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 04:43:54,633 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
recon_1             | 2023-07-16 04:43:54,658 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb{ip: 172.19.0.13, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:43:54,862 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/984f9786-6250-4b94-8d30-f65fab4a7cff
recon_1             | 2023-07-16 04:43:54,867 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 984f9786-6250-4b94-8d30-f65fab4a7cff{ip: 172.19.0.9, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:43:56,007 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb to Node DB.
recon_1             | 2023-07-16 04:43:56,031 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 984f9786-6250-4b94-8d30-f65fab4a7cff to Node DB.
recon_1             | 2023-07-16 04:43:56,675 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7. Trying to get from SCM.
scm_1               | 2023-07-16 04:43:44,681 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-07-16 04:43:44,695 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-07-16 04:43:44,698 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-07-16 04:43:44,699 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-07-16 04:43:44,946 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 04:43:44,985 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 04:43:44,985 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-16 04:43:45,728 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:43:45,728 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:43:45,730 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-16 04:43:46,297 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:43:46,300 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:43:46,316 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:43:46,318 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-16 04:43:47,034 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-16 04:43:47,034 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-16 04:43:47,175 [main] INFO util.log: Logging initialized @32450ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-16 04:43:47,496 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-16 04:43:47,515 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 04:43:47,533 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 04:43:47,538 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 04:43:47,538 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 04:43:47,538 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 04:43:47,630 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-07-16 04:43:47,633 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-16 04:43:47,636 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-07-16 04:43:47,699 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-16 04:43:47,700 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 04:43:47,702 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-16 04:43:47,738 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d569c10{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 04:43:47,740 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5cdd5ff9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 04:43:48,144 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4bdec54{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7248091746167639922/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-07-16 04:43:48,159 [main] INFO server.AbstractConnector: Started ServerConnector@cb29b75{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-16 04:43:48,160 [main] INFO server.Server: Started @33435ms
scm_1               | 2023-07-16 04:43:48,164 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-16 04:43:48,164 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-16 04:43:48,168 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 04:43:49,218 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.FollowerState: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5176687985ns, electionTimeout:5150ms
scm_1               | 2023-07-16 04:43:49,219 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState
scm_1               | 2023-07-16 04:43:49,221 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-16 04:43:49,227 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-16 04:43:49,228 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-FollowerState] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1
scm_1               | 2023-07-16 04:43:49,238 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:49,239 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-07-16 04:43:49,286 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:49,286 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.LeaderElection: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-16 04:43:49,286 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: shutdown b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1
recon_1             | 2023-07-16 04:43:56,930 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ef7467ff-7a52-4835-9cf9-4caa690a8bc7, Nodes: 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10)e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.583Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:43:57,154 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:43:57,168 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:01,422 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:01,423 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f. Trying to get from SCM.
recon_1             | 2023-07-16 04:44:01,433 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 952e8d54-8734-4347-ab9a-f300d3f45a0f, Nodes: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.617Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:44:01,437 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:01,759 [IPC Server handler 15 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d0d64a7d-3e54-43d9-a2ff-589187cfae66
scm_1               | 2023-07-16 04:43:49,287 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-16 04:43:49,287 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-16 04:43:49,287 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-07-16 04:43:49,306 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: change Leader from null to b0ddd29c-8a6e-4603-a5ef-dbb4e249f117 at term 2 for becomeLeader, leader elected after 11359ms
scm_1               | 2023-07-16 04:43:49,337 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-16 04:43:49,355 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:49,357 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 04:43:49,376 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-16 04:43:49,380 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-16 04:43:49,383 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-16 04:43:49,409 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 04:43:49,418 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 04:43:49,426 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO impl.RoleInfo: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117: start b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderStateImpl
scm_1               | 2023-07-16 04:43:49,449 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-16 04:43:49,467 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/log_inprogress_0 to /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/log_0-0
scm_1               | 2023-07-16 04:43:49,494 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-LeaderElection1] INFO server.RaftServer$Division: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95: set configuration 1: peers:[b0ddd29c-8a6e-4603-a5ef-dbb4e249f117|rpc:d333c4ab6667:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 04:43:49,509 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/6d5e37af-423b-4dae-b444-8f69d29deb95/current/log_inprogress_1
scm_1               | 2023-07-16 04:43:49,517 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-07-16 04:43:49,518 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-16 04:43:49,531 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:43:49,532 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-16 04:43:49,534 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 04:43:49,538 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:43:49,566 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-16 04:43:49,585 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:43:50,177 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:56376 / 172.19.0.9:56376: output error
scm_1               | 2023-07-16 04:43:50,178 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 04:43:50,184 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:44252 / 172.19.0.13:44252: output error
scm_1               | 2023-07-16 04:43:50,194 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
recon_1             | 2023-07-16 04:44:01,760 [IPC Server handler 15 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d0d64a7d-3e54-43d9-a2ff-589187cfae66{ip: 172.19.0.10, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:44:01,761 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d0d64a7d-3e54-43d9-a2ff-589187cfae66 to Node DB.
recon_1             | 2023-07-16 04:44:01,761 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10)
recon_1             | 2023-07-16 04:44:02,079 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:02,080 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8. Trying to get from SCM.
recon_1             | 2023-07-16 04:44:02,103 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8, Nodes: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.642Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:44:02,106 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:02,682 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:02,683 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:03,921 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:03,922 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:04,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10)
recon_1             | 2023-07-16 04:44:04,506 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:04,508 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294. Trying to get from SCM.
recon_1             | 2023-07-16 04:44:04,534 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a1928e58-e59a-421f-a41b-58c9b116b294, Nodes: 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.671Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:44:04,535 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:07,204 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)
recon_1             | 2023-07-16 04:44:08,183 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 reported by 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)
recon_1             | 2023-07-16 04:44:25,020 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ced029d5-cefb-47a1-a5da-897e24564bb6. Trying to get from SCM.
recon_1             | 2023-07-16 04:44:25,062 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ced029d5-cefb-47a1-a5da-897e24564bb6, Nodes: d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d0d64a7d-3e54-43d9-a2ff-589187cfae66, CreationTimestamp2023-07-16T04:43:52.421Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:44:32,087 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-07-16 04:44:32,105 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 04:44:32,311 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 04:44:32,312 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 04:44:45,100 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-16 04:44:45,101 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-16 04:44:46,807 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689482685101
recon_1             | 2023-07-16 04:44:46,830 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-07-16 04:44:47,498 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689482685101.
recon_1             | 2023-07-16 04:44:47,610 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-16 04:44:48,273 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-07-16 04:44:48,289 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 04:43:50,205 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:60552 / 172.19.0.10:60552: output error
scm_1               | 2023-07-16 04:43:50,207 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 04:43:52,232 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d0d64a7d-3e54-43d9-a2ff-589187cfae66
scm_1               | 2023-07-16 04:43:52,293 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d0d64a7d-3e54-43d9-a2ff-589187cfae66{ip: 172.19.0.10, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:43:52,328 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:43:52,354 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:43:52,367 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:43:52,417 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:43:52,432 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ced029d5-cefb-47a1-a5da-897e24564bb6 to datanode:d0d64a7d-3e54-43d9-a2ff-589187cfae66
scm_1               | 2023-07-16 04:43:52,791 [IPC Server handler 62 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
scm_1               | 2023-07-16 04:43:52,798 [IPC Server handler 62 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb{ip: 172.19.0.13, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:43:52,802 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:43:52,802 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:43:53,216 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/984f9786-6250-4b94-8d30-f65fab4a7cff
scm_1               | 2023-07-16 04:43:53,218 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 984f9786-6250-4b94-8d30-f65fab4a7cff{ip: 172.19.0.9, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:43:53,218 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1             | 2023-07-16 04:44:48,292 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
scm_1               | 2023-07-16 04:43:53,226 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
recon_1             | 2023-07-16 04:44:48,292 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
scm_1               | 2023-07-16 04:43:53,230 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
recon_1             | 2023-07-16 04:44:48,293 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
scm_1               | 2023-07-16 04:43:53,230 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
recon_1             | 2023-07-16 04:44:48,293 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-16 04:44:48,367 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm_1               | 2023-07-16 04:43:53,230 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
recon_1             | 2023-07-16 04:44:48,367 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.075 seconds to process 1 keys.
scm_1               | 2023-07-16 04:43:53,231 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1             | 2023-07-16 04:44:48,399 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm_1               | 2023-07-16 04:43:53,491 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-07-16 04:44:48,428 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | 2023-07-16 04:43:53,547 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: ced029d5-cefb-47a1-a5da-897e24564bb6, Nodes: d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:52.421244Z[UTC]]
recon_1             | 2023-07-16 04:44:53,065 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
scm_1               | 2023-07-16 04:43:53,584 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 to datanode:984f9786-6250-4b94-8d30-f65fab4a7cff
recon_1             | 2023-07-16 04:44:53,068 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
scm_1               | 2023-07-16 04:43:53,585 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 to datanode:d0d64a7d-3e54-43d9-a2ff-589187cfae66
scm_1               | 2023-07-16 04:43:53,585 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 to datanode:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
scm_1               | 2023-07-16 04:43:53,603 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:43:53,604 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: ef7467ff-7a52-4835-9cf9-4caa690a8bc7, Nodes: 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10)e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.583947Z[UTC]]
recon_1             | 2023-07-16 04:44:53,091 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
scm_1               | 2023-07-16 04:43:53,619 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f to datanode:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
scm_1               | 2023-07-16 04:43:53,628 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-07-16 04:44:53,091 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm_1               | 2023-07-16 04:43:53,636 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 952e8d54-8734-4347-ab9a-f300d3f45a0f, Nodes: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.617012Z[UTC]]
recon_1             | 2023-07-16 04:44:53,098 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
scm_1               | 2023-07-16 04:43:53,642 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 to datanode:e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb
recon_1             | 2023-07-16 04:44:53,098 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm_1               | 2023-07-16 04:43:53,650 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 to datanode:984f9786-6250-4b94-8d30-f65fab4a7cff
recon_1             | 2023-07-16 04:44:53,130 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-07-16 04:44:53,141 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
scm_1               | 2023-07-16 04:43:53,650 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 to datanode:d0d64a7d-3e54-43d9-a2ff-589187cfae66
recon_1             | 2023-07-16 04:44:53,166 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 58 milliseconds.
scm_1               | 2023-07-16 04:43:53,660 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-07-16 04:44:53,208 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 108 milliseconds to process 0 existing database records.
scm_1               | 2023-07-16 04:43:53,665 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8 contains same datanodes as previous pipelines: PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7 nodeIds: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb, 984f9786-6250-4b94-8d30-f65fab4a7cff, d0d64a7d-3e54-43d9-a2ff-589187cfae66
recon_1             | 2023-07-16 04:44:53,264 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 55 milliseconds for processing 1 containers.
scm_1               | 2023-07-16 04:43:53,666 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8, Nodes: e39d62a9-434e-4ddc-bebe-ecffd5ac8bcb(xcompat_datanode_1.xcompat_default/172.19.0.13)984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9)d0d64a7d-3e54-43d9-a2ff-589187cfae66(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.642855Z[UTC]]
recon_1             | 2023-07-16 04:45:00,885 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
scm_1               | 2023-07-16 04:43:53,671 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294 to datanode:984f9786-6250-4b94-8d30-f65fab4a7cff
recon_1             | 2023-07-16 04:45:00,899 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-16 04:45:53,124 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-07-16 04:45:53,157 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 04:43:53,676 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-07-16 04:45:53,157 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 32
scm_1               | 2023-07-16 04:43:53,690 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: a1928e58-e59a-421f-a41b-58c9b116b294, Nodes: 984f9786-6250-4b94-8d30-f65fab4a7cff(xcompat_datanode_3.xcompat_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:43:53.671922Z[UTC]]
recon_1             | 2023-07-16 04:46:53,158 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 04:43:53,691 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
recon_1             | 2023-07-16 04:46:53,158 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-16 04:43:53,712 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
recon_1             | 2023-07-16 04:47:53,159 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-16 04:47:53,159 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-07-16 04:48:53,160 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-07-16 04:48:53,160 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-16 04:44:01,505 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:44:01,516 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=952e8d54-8734-4347-ab9a-f300d3f45a0f
scm_1               | 2023-07-16 04:44:01,545 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:44:02,095 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:44:03,937 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
recon_1             | 2023-07-16 04:49:53,160 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 04:44:03,943 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=ef7467ff-7a52-4835-9cf9-4caa690a8bc7
recon_1             | 2023-07-16 04:49:53,160 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-16 04:44:03,943 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
recon_1             | 2023-07-16 04:49:53,197 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
scm_1               | 2023-07-16 04:44:03,944 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
recon_1             | 2023-07-16 04:49:53,202 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 14 milliseconds.
scm_1               | 2023-07-16 04:44:03,944 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1             | 2023-07-16 04:49:53,266 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 2 milliseconds to process 0 existing database records.
scm_1               | 2023-07-16 04:44:03,944 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
recon_1             | 2023-07-16 04:49:53,271 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 5 milliseconds for processing 2 containers.
scm_1               | 2023-07-16 04:44:03,944 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-16 04:44:03,945 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-16 04:44:03,947 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-16 04:44:03,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-16 04:44:03,993 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-16 04:44:03,995 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-07-16 04:44:04,535 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=a1928e58-e59a-421f-a41b-58c9b116b294
scm_1               | 2023-07-16 04:44:08,199 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=8ddd5eaa-ca0c-4d91-9a48-3e77d073c3d8
scm_1               | 2023-07-16 04:44:25,009 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=ced029d5-cefb-47a1-a5da-897e24564bb6
scm_1               | 2023-07-16 04:44:28,204 [IPC Server handler 63 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-16 04:44:28,239 [b0ddd29c-8a6e-4603-a5ef-dbb4e249f117@group-8F69D29DEB95-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-16 04:44:28,253 [IPC Server handler 63 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-16 04:45:11,750 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:45:26,162 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:45:53,727 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-16 04:46:24,440 [IPC Server handler 10 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:46:38,235 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:47:39,511 [IPC Server handler 18 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:47:53,081 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:47:53,729 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-16 04:48:39,840 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-16 04:48:56,361 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:49:09,116 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:49:53,730 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-07-16 04:50:14,016 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
scm_1               | 2023-07-16 04:50:27,051 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.7
Attaching to xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_1_0_1, xcompat_recon_1, xcompat_new_client_1, xcompat_old_client_1_0_0_1, xcompat_old_client_1_2_1_1, xcompat_old_client_1_3_0_1, xcompat_scm_1, xcompat_s3g_1, xcompat_om_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 04:51:01 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = aa77f7b356bd/172.20.0.12
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.0.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_1          | STARTUP_MSG:   java = 11.0.3
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 04:51:01 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 04:51:04 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 04:51:04 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 04:51:06 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 04:51:06 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1          | 2023-07-16 04:51:07 INFO  HddsDatanodeService:209 - HddsDatanodeService host:aa77f7b356bd ip:172.20.0.12
datanode_1          | 2023-07-16 04:51:08 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 04:51:08 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-07-16 04:51:08 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 04:51:08 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 04:51:08 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:51:09 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:51:09 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:51:09 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_1          | 2023-07-16 04:51:15 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:51:15 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 04:51:16 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_1          | 2023-07-16 04:51:16 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 04:51:16 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:51:16 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 04:51:16 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:51:18 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:51:18 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:51:18 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1          | 2023-07-16 04:51:18 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:51:18 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 04:51:19 INFO  log:169 - Logging initialized @24978ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 04:51:20 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-16 04:51:20 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 04:51:20 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 04:51:20 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 04:51:20 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 04:51:20 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 04:51:20 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_1          | 2023-07-16 04:51:20 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1          | 2023-07-16 04:51:20 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 04:51:20 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 04:51:20 INFO  session:140 - node0 Scavenging every 660000ms
datanode_1          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 04:51:22 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-9547003635445382529.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-16 04:51:22 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1          | 2023-07-16 04:51:22 INFO  Server:399 - Started @28384ms
datanode_1          | 2023-07-16 04:51:22 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1          | 2023-07-16 04:51:22 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1          | 2023-07-16 04:51:22 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:51:22 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1          | 2023-07-16 04:51:23 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.9:9891
datanode_1          | 2023-07-16 04:51:24 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 04:51:25 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From aa77f7b356bd/172.20.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.12:48998 remote=scm/172.20.0.6:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 04:51:00 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = b78acf0a01b0/172.20.0.13
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.0.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_2          | STARTUP_MSG:   java = 11.0.3
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 04:51:00 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 04:51:02 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 04:51:03 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 04:51:05 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 04:51:05 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2          | 2023-07-16 04:51:05 INFO  HddsDatanodeService:209 - HddsDatanodeService host:b78acf0a01b0 ip:172.20.0.13
datanode_2          | 2023-07-16 04:51:06 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 04:51:06 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-07-16 04:51:06 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 04:51:06 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 04:51:06 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:51:07 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:51:07 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:51:07 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_2          | 2023-07-16 04:51:13 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:51:14 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 04:51:14 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_2          | 2023-07-16 04:51:14 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 04:51:15 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:51:15 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 04:51:15 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:51:17 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:51:17 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:51:17 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_2          | 2023-07-16 04:51:18 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:51:18 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 04:51:18 INFO  log:169 - Logging initialized @24696ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 04:51:18 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-16 04:51:18 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 04:51:18 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 04:51:18 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 04:51:18 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 04:51:18 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 04:51:19 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_2          | 2023-07-16 04:51:19 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2          | 2023-07-16 04:51:20 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 04:51:20 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 04:51:20 INFO  session:140 - node0 Scavenging every 600000ms
datanode_2          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 04:51:22 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-5752287129547984066.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 04:51:22 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2          | 2023-07-16 04:51:22 INFO  Server:399 - Started @28790ms
datanode_2          | 2023-07-16 04:51:22 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2          | 2023-07-16 04:51:22 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2          | 2023-07-16 04:51:22 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:51:22 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2          | 2023-07-16 04:51:23 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.9:9891
datanode_2          | 2023-07-16 04:51:23 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 04:51:25 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From b78acf0a01b0/172.20.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.13:52180 remote=scm/172.20.0.6:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.12:48998 remote=scm/172.20.0.6:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_1          | 2023-07-16 04:51:28 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-07-16 04:51:48 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_1          | 2023-07-16 04:52:08 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_1          | 2023-07-16 04:52:24 INFO  Client:958 - Retrying connect to server: recon/172.20.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_1          | 2023-07-16 04:52:25 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_1          | 2023-07-16 04:52:25 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_1          | 2023-07-16 04:52:25 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis ddceee06-c999-4888-8c11-9381bedc9794 at port 9858
datanode_1          | 2023-07-16 04:52:25 INFO  RaftServerProxy:304 - ddceee06-c999-4888-8c11-9381bedc9794: start RPC server
datanode_1          | 2023-07-16 04:52:25 INFO  GrpcService:160 - ddceee06-c999-4888-8c11-9381bedc9794: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1          | 2023-07-16 04:52:29 INFO  RaftServerProxy:89 - ddceee06-c999-4888-8c11-9381bedc9794: addNew group-D52D84264B32:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858] returns group-D52D84264B32:java.util.concurrent.CompletableFuture@701d2b4f[Not completed]
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:107 - ddceee06-c999-4888-8c11-9381bedc9794: new RaftServerImpl for group-D52D84264B32:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:103 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: ConfigurationManager, init=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/f27c09ae-63ac-4fe4-bee8-d52d84264b32 does not exist. Creating ...
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/f27c09ae-63ac-4fe4-bee8-d52d84264b32/in_use.lock acquired by nodename 7@aa77f7b356bd
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/f27c09ae-63ac-4fe4-bee8-d52d84264b32 has been successfully formatted.
datanode_1          | 2023-07-16 04:52:30 INFO  ContainerStateMachine:225 - group-D52D84264B32: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:180 - new ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/f27c09ae-63ac-4fe4-bee8-d52d84264b32
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.13:52180 remote=scm/172.20.0.6:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_2          | 2023-07-16 04:51:28 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_2          | 2023-07-16 04:51:48 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_2          | 2023-07-16 04:52:08 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_2          | 2023-07-16 04:52:24 INFO  Client:958 - Retrying connect to server: recon/172.20.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_2          | 2023-07-16 04:52:24 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_2          | 2023-07-16 04:52:24 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_2          | 2023-07-16 04:52:24 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis fb643f47-389c-499f-862f-3e694a4a939e at port 9858
datanode_2          | 2023-07-16 04:52:24 INFO  RaftServerProxy:304 - fb643f47-389c-499f-862f-3e694a4a939e: start RPC server
datanode_2          | 2023-07-16 04:52:24 INFO  GrpcService:160 - fb643f47-389c-499f-862f-3e694a4a939e: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerProxy:89 - fb643f47-389c-499f-862f-3e694a4a939e: addNew group-6315458DCA71:[fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858] returns group-6315458DCA71:java.util.concurrent.CompletableFuture@30a7d93b[Not completed]
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerImpl:107 - fb643f47-389c-499f-862f-3e694a4a939e: new RaftServerImpl for group-6315458DCA71:[fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerImpl:103 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: ConfigurationManager, init=-1: [fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/b8f7421a-bcff-4342-96c2-6315458dca71 does not exist. Creating ...
datanode_2          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/b8f7421a-bcff-4342-96c2-6315458dca71/in_use.lock acquired by nodename 7@b78acf0a01b0
datanode_2          | 2023-07-16 04:52:29 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/b8f7421a-bcff-4342-96c2-6315458dca71 has been successfully formatted.
datanode_2          | 2023-07-16 04:52:29 INFO  ContainerStateMachine:225 - group-6315458DCA71: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:180 - new fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/b8f7421a-bcff-4342-96c2-6315458dca71
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:196 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: start as a follower, conf=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858], old=null
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:185 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:52:30 INFO  RoleInfo:143 - ddceee06-c999-4888-8c11-9381bedc9794: start FollowerState
datanode_1          | 2023-07-16 04:52:30 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D52D84264B32,id=ddceee06-c999-4888-8c11-9381bedc9794
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerProxy:89 - ddceee06-c999-4888-8c11-9381bedc9794: addNew group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] returns group-BA3D81F9BCC3:java.util.concurrent.CompletableFuture@5c8e3416[Not completed]
datanode_1          | 2023-07-16 04:52:30 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "f27c09ae-63ac-4fe4-bee8-d52d84264b32"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: -973892774817673244
datanode_1          |   leastSigBits: -4690264620439024846
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-07-16 04:52:30 ERROR CreatePipelineCommandHandler:114 - Can't create pipeline RATIS THREE #id: "2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 3185722521696551315
datanode_1          |   leastSigBits: -7323211170721448765
datanode_1          | }
datanode_1          | 
datanode_1          | java.io.IOException: ddceee06-c999-4888-8c11-9381bedc9794: Failed to add group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] since the group already exists in the map.
datanode_1          | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.addGroup(XceiverServerRatis.java:725)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:96)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:484)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: org.apache.ratis.protocol.AlreadyExistsException: ddceee06-c999-4888-8c11-9381bedc9794: Failed to add group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] since the group already exists in the map.
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:383)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:368)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.addGroup(XceiverServerRatis.java:723)
datanode_1          | 	... 4 more
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:107 - ddceee06-c999-4888-8c11-9381bedc9794: new RaftServerImpl for group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:103 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: ConfigurationManager, init=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 does not exist. Creating ...
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/in_use.lock acquired by nodename 7@aa77f7b356bd
datanode_1          | 2023-07-16 04:52:30 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 has been successfully formatted.
datanode_1          | 2023-07-16 04:52:30 INFO  ContainerStateMachine:225 - group-BA3D81F9BCC3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:180 - new ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:196 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: start as a follower, conf=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_1          | 2023-07-16 04:52:30 INFO  RaftServerImpl:185 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:52:30 INFO  RoleInfo:143 - ddceee06-c999-4888-8c11-9381bedc9794: start FollowerState
datanode_1          | 2023-07-16 04:52:30 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BA3D81F9BCC3,id=ddceee06-c999-4888-8c11-9381bedc9794
datanode_1          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3
datanode_1          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_1          | 2023-07-16 04:52:34 INFO  RoleInfo:121 - ddceee06-c999-4888-8c11-9381bedc9794: shutdown FollowerState
datanode_1          | 2023-07-16 04:52:34 INFO  FollowerState:117 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - ddceee06-c999-4888-8c11-9381bedc9794: start FollowerState
datanode_1          | 2023-07-16 04:52:34 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-BA3D81F9BCC3 with new leaderId: 8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_1          | 2023-07-16 04:52:34 INFO  RaftServerImpl:255 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: change Leader from null to 8ce16940-0905-4a9c-a5eb-adaf222452d2 at term 1 for appendEntries, leader elected after 4069ms
datanode_1          | 2023-07-16 04:52:34 INFO  RaftServerImpl:356 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3: set configuration 0: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null at 0
datanode_1          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:397 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:596 - ddceee06-c999-4888-8c11-9381bedc9794@group-BA3D81F9BCC3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/current/log_inprogress_0
datanode_1          | 2023-07-16 04:52:35 INFO  FollowerState:108 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-FollowerState: change to CANDIDATE, lastRpcTime:5059ms, electionTimeout:5030ms
datanode_1          | 2023-07-16 04:52:35 INFO  RoleInfo:121 - ddceee06-c999-4888-8c11-9381bedc9794: shutdown FollowerState
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerImpl:185 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:52:35 INFO  RoleInfo:143 - ddceee06-c999-4888-8c11-9381bedc9794: start LeaderElection
datanode_1          | 2023-07-16 04:52:35 INFO  LeaderElection:209 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-LeaderElection1: begin an election at term 1 for -1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858], old=null
datanode_1          | 2023-07-16 04:52:35 INFO  RoleInfo:134 - ddceee06-c999-4888-8c11-9381bedc9794: shutdown LeaderElection
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerImpl:185 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 04:52:35 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-D52D84264B32 with new leaderId: ddceee06-c999-4888-8c11-9381bedc9794
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerImpl:255 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: change Leader from null to ddceee06-c999-4888-8c11-9381bedc9794 at term 1 for becomeLeader, leader elected after 5623ms
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:52:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-16 04:51:00 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 349f3d69c1c5/172.20.0.11
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.0.0
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:52:35 INFO  RoleInfo:143 - ddceee06-c999-4888-8c11-9381bedc9794: start LeaderState
datanode_1          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:397 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:596 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f27c09ae-63ac-4fe4-bee8-d52d84264b32/current/log_inprogress_0
datanode_1          | 2023-07-16 04:52:35 INFO  RaftServerImpl:356 - ddceee06-c999-4888-8c11-9381bedc9794@group-D52D84264B32: set configuration 0: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858], old=null at 0
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:196 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: start as a follower, conf=-1: [fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858], old=null
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:185 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:52:30 INFO  RoleInfo:143 - fb643f47-389c-499f-862f-3e694a4a939e: start FollowerState
datanode_2          | 2023-07-16 04:52:30 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6315458DCA71,id=fb643f47-389c-499f-862f-3e694a4a939e
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71
datanode_2          | 2023-07-16 04:52:30 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "b8f7421a-bcff-4342-96c2-6315458dca71"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: -5118549768875850942
datanode_2          |   leastSigBits: -7583389879526110607
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerProxy:89 - fb643f47-389c-499f-862f-3e694a4a939e: addNew group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] returns group-BA3D81F9BCC3:java.util.concurrent.CompletableFuture@3f07b5a5[Not completed]
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:107 - fb643f47-389c-499f-862f-3e694a4a939e: new RaftServerImpl for group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:103 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: ConfigurationManager, init=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 does not exist. Creating ...
datanode_2          | 2023-07-16 04:52:30 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/in_use.lock acquired by nodename 7@b78acf0a01b0
datanode_2          | 2023-07-16 04:52:30 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 has been successfully formatted.
datanode_2          | 2023-07-16 04:52:30 INFO  ContainerStateMachine:225 - group-BA3D81F9BCC3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:180 - new fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:52:30 INFO  SegmentedRaftLogWorker:129 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:196 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: start as a follower, conf=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_2          | 2023-07-16 04:52:30 INFO  RaftServerImpl:185 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:52:30 INFO  RoleInfo:143 - fb643f47-389c-499f-862f-3e694a4a939e: start FollowerState
datanode_2          | 2023-07-16 04:52:30 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BA3D81F9BCC3,id=fb643f47-389c-499f-862f-3e694a4a939e
datanode_2          | 2023-07-16 04:52:30 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3
datanode_2          | 2023-07-16 04:52:31 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 3185722521696551315
datanode_2          |   leastSigBits: -7323211170721448765
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_2          | 2023-07-16 04:52:34 INFO  RoleInfo:121 - fb643f47-389c-499f-862f-3e694a4a939e: shutdown FollowerState
datanode_2          | 2023-07-16 04:52:34 INFO  FollowerState:117 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - fb643f47-389c-499f-862f-3e694a4a939e: start FollowerState
datanode_2          | 2023-07-16 04:52:34 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-BA3D81F9BCC3 with new leaderId: 8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_2          | 2023-07-16 04:52:34 INFO  RaftServerImpl:255 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: change Leader from null to 8ce16940-0905-4a9c-a5eb-adaf222452d2 at term 1 for appendEntries, leader elected after 4513ms
datanode_2          | 2023-07-16 04:52:34 INFO  RaftServerImpl:356 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3: set configuration 0: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null at 0
datanode_2          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:397 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:596 - fb643f47-389c-499f-862f-3e694a4a939e@group-BA3D81F9BCC3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/current/log_inprogress_0
datanode_2          | 2023-07-16 04:52:35 INFO  FollowerState:108 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-FollowerState: change to CANDIDATE, lastRpcTime:5121ms, electionTimeout:5107ms
datanode_2          | 2023-07-16 04:52:35 INFO  RoleInfo:121 - fb643f47-389c-499f-862f-3e694a4a939e: shutdown FollowerState
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerImpl:185 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:52:35 INFO  RoleInfo:143 - fb643f47-389c-499f-862f-3e694a4a939e: start LeaderElection
datanode_2          | 2023-07-16 04:52:35 INFO  LeaderElection:209 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-LeaderElection1: begin an election at term 1 for -1: [fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858], old=null
datanode_2          | 2023-07-16 04:52:35 INFO  RoleInfo:134 - fb643f47-389c-499f-862f-3e694a4a939e: shutdown LeaderElection
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerImpl:185 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-16 04:52:35 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-6315458DCA71 with new leaderId: fb643f47-389c-499f-862f-3e694a4a939e
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerImpl:255 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: change Leader from null to fb643f47-389c-499f-862f-3e694a4a939e at term 1 for becomeLeader, leader elected after 5500ms
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:52:35 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 04:52:35 INFO  RoleInfo:143 - fb643f47-389c-499f-862f-3e694a4a939e: start LeaderState
datanode_2          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:397 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:52:35 INFO  SegmentedRaftLogWorker:596 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b8f7421a-bcff-4342-96c2-6315458dca71/current/log_inprogress_0
datanode_2          | 2023-07-16 04:52:35 INFO  RaftServerImpl:356 - fb643f47-389c-499f-862f-3e694a4a939e@group-6315458DCA71: set configuration 0: [fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858], old=null at 0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_3          | STARTUP_MSG:   java = 11.0.3
datanode_3          | ************************************************************/
datanode_3          | 2023-07-16 04:51:00 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 04:51:03 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 04:51:03 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 04:51:05 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-16 04:51:05 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3          | 2023-07-16 04:51:06 INFO  HddsDatanodeService:209 - HddsDatanodeService host:349f3d69c1c5 ip:172.20.0.11
datanode_3          | 2023-07-16 04:51:06 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 04:51:06 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-07-16 04:51:06 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 04:51:06 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 04:51:07 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:51:08 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:51:08 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:51:08 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_3          | 2023-07-16 04:51:14 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:51:14 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 04:51:15 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_3          | 2023-07-16 04:51:15 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-16 04:51:15 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:51:15 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-16 04:51:15 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:51:17 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:51:17 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:51:18 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_3          | 2023-07-16 04:51:18 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:51:18 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 04:51:18 INFO  log:169 - Logging initialized @24785ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 04:51:19 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-16 04:51:19 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-16 04:51:19 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-16 04:51:19 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 04:51:19 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-16 04:51:19 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-16 04:51:20 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_3          | 2023-07-16 04:51:20 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_3          | 2023-07-16 04:51:20 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 04:51:20 INFO  session:338 - No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 04:51:20 INFO  session:140 - node0 Scavenging every 660000ms
datanode_3          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 04:51:20 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-16 04:51:21 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-11269591548911380615.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 04:51:21 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3          | 2023-07-16 04:51:21 INFO  Server:399 - Started @28275ms
datanode_3          | 2023-07-16 04:51:21 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3          | 2023-07-16 04:51:21 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3          | 2023-07-16 04:51:21 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:51:22 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3          | 2023-07-16 04:51:22 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.9:9891
datanode_3          | 2023-07-16 04:51:23 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 04:51:25 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 349f3d69c1c5/172.20.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:46246 remote=scm/172.20.0.6:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:46246 remote=scm/172.20.0.6:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_3          | 2023-07-16 04:51:28 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-07-16 04:51:48 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_3          | 2023-07-16 04:52:08 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_3          | 2023-07-16 04:52:24 INFO  Client:958 - Retrying connect to server: recon/172.20.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_3          | 2023-07-16 04:52:24 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_3          | 2023-07-16 04:52:24 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_3          | 2023-07-16 04:52:24 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 8ce16940-0905-4a9c-a5eb-adaf222452d2 at port 9858
datanode_3          | 2023-07-16 04:52:24 INFO  RaftServerProxy:304 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start RPC server
datanode_3          | 2023-07-16 04:52:24 INFO  GrpcService:160 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerProxy:89 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: addNew group-BA2C6BC3E242:[8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] returns group-BA2C6BC3E242:java.util.concurrent.CompletableFuture@399da5e5[Not completed]
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:107 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: new RaftServerImpl for group-BA2C6BC3E242:[8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:103 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: ConfigurationManager, init=-1: [8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/0318a2b7-f619-40a2-b429-ba2c6bc3e242 does not exist. Creating ...
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/0318a2b7-f619-40a2-b429-ba2c6bc3e242/in_use.lock acquired by nodename 7@349f3d69c1c5
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/0318a2b7-f619-40a2-b429-ba2c6bc3e242 has been successfully formatted.
datanode_3          | 2023-07-16 04:52:29 INFO  ContainerStateMachine:225 - group-BA2C6BC3E242: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:180 - new 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/0318a2b7-f619-40a2-b429-ba2c6bc3e242
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:129 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:129 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:196 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: start as a follower, conf=-1: [8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:52:29 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start FollowerState
datanode_3          | 2023-07-16 04:52:29 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BA2C6BC3E242,id=8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242
datanode_3          | 2023-07-16 04:52:29 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "0318a2b7-f619-40a2-b429-ba2c6bc3e242"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 223107092546404514
datanode_3          |   leastSigBits: -5464631972888059326
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerProxy:89 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: addNew group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] returns group-BA3D81F9BCC3:java.util.concurrent.CompletableFuture@2d037c9a[Not completed]
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:107 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: new RaftServerImpl for group-BA3D81F9BCC3:[ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:103 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: ConfigurationManager, init=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 does not exist. Creating ...
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/in_use.lock acquired by nodename 7@349f3d69c1c5
datanode_3          | 2023-07-16 04:52:29 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 has been successfully formatted.
datanode_3          | 2023-07-16 04:52:29 INFO  ContainerStateMachine:225 - group-BA3D81F9BCC3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:180 - new 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:129 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:52:29 INFO  SegmentedRaftLogWorker:129 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:196 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: start as a follower, conf=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-16 04:52:29 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:52:29 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start FollowerState
datanode_3          | 2023-07-16 04:52:29 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BA3D81F9BCC3,id=8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_3          | 2023-07-16 04:52:29 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:31 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 3185722521696551315
datanode_3          |   leastSigBits: -7323211170721448765
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-07-16 04:52:34 INFO  FollowerState:108 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-FollowerState: change to CANDIDATE, lastRpcTime:5038ms, electionTimeout:5033ms
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:121 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: shutdown FollowerState
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start LeaderElection
datanode_3          | 2023-07-16 04:52:34 INFO  LeaderElection:209 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-LeaderElection1: begin an election at term 1 for -1: [8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:134 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: shutdown LeaderElection
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:52:34 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-BA2C6BC3E242 with new leaderId: 8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:255 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: change Leader from null to 8ce16940-0905-4a9c-a5eb-adaf222452d2 at term 1 for becomeLeader, leader elected after 5173ms
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start LeaderState
datanode_3          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:397 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:356 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242: set configuration 0: [8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null at 0
datanode_3          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:596 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA2C6BC3E242-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0318a2b7-f619-40a2-b429-ba2c6bc3e242/current/log_inprogress_0
datanode_3          | 2023-07-16 04:52:34 INFO  FollowerState:108 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-FollowerState: change to CANDIDATE, lastRpcTime:5192ms, electionTimeout:5191ms
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:121 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: shutdown FollowerState
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start LeaderElection
datanode_3          | 2023-07-16 04:52:34 INFO  LeaderElection:209 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-LeaderElection2: begin an election at term 1 for -1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-16 04:52:34 INFO  LeaderElection:61 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-LeaderElection2: Election PASSED; received 1 response(s) [8ce16940-0905-4a9c-a5eb-adaf222452d2<-ddceee06-c999-4888-8c11-9381bedc9794#0:OK-t1] and 0 exception(s); 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3:t1, leader=null, voted=8ce16940-0905-4a9c-a5eb-adaf222452d2, raftlog=8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:134 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: shutdown LeaderElection
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:185 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:52:34 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-BA3D81F9BCC3 with new leaderId: 8ce16940-0905-4a9c-a5eb-adaf222452d2
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:255 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: change Leader from null to 8ce16940-0905-4a9c-a5eb-adaf222452d2 at term 1 for becomeLeader, leader elected after 5307ms
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis_grpc.log_appender.8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:52:34 INFO  RoleInfo:143 - 8ce16940-0905-4a9c-a5eb-adaf222452d2: start LeaderState
datanode_3          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:397 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:52:34 INFO  SegmentedRaftLogWorker:596 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3/current/log_inprogress_0
datanode_3          | 2023-07-16 04:52:34 INFO  RaftServerImpl:356 - 8ce16940-0905-4a9c-a5eb-adaf222452d2@group-BA3D81F9BCC3: set configuration 0: [ddceee06-c999-4888-8c11-9381bedc9794:172.20.0.12:9858, fb643f47-389c-499f-862f-3e694a4a939e:172.20.0.13:9858, 8ce16940-0905-4a9c-a5eb-adaf222452d2:172.20.0.11:9858], old=null at 0
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 04:50:59 INFO  ReconServer:112 - STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 666a9de6bbcc/172.20.0.9
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.0.0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:50:59 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c4d17bd4217e/172.20.0.2
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
recon_1             | STARTUP_MSG:   java = 11.0.3
recon_1             | ************************************************************/
recon_1             | 2023-07-16 04:50:59 INFO  ReconServer:90 - registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-16 04:51:03 INFO  ReconRestServletModule:75 - rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-07-16 04:51:06 INFO  ReconServer:93 - Initializing Recon server...
recon_1             | 2023-07-16 04:51:08 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:51:16 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:51:18 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:51:18 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:51:18 INFO  ReconServer:101 - Creating Recon Schema.
recon_1             | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1             | 2023-07-16 04:51:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
recon_1             | 2023-07-16 04:51:24 INFO  BaseHttpServer:207 - Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-16 04:51:24 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-16 04:51:24 INFO  log:169 - Logging initialized @30438ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-16 04:51:25 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-16 04:51:25 WARN  HttpRequestLog:103 - Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-16 04:51:25 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-16 04:51:25 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-16 04:51:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-16 04:51:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-16 04:51:25 INFO  ReconTaskControllerImpl:79 - Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-16 04:51:26 INFO  ReconTaskControllerImpl:79 - Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-16 04:51:26 INFO  OmUtils:550 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-16 04:51:26 INFO  OmUtils:569 - No OzoneManager ServiceID configured.
recon_1             | 2023-07-16 04:51:26 INFO  deprecation:1395 - No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-07-16 04:51:27 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:51:27 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:51:27 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@5002fde9
recon_1             | 2023-07-16 04:51:27 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
recon_1             | 2023-07-16 04:51:27 WARN  DBStoreBuilder:277 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:51:27 INFO  SCMNodeManager:116 - Entering startup safe mode.
recon_1             | 2023-07-16 04:51:27 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:51:27 INFO  ReconNodeManager:100 - Loaded 0 nodes from node DB.
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-07-16 04:50:59 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:51:06 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:51:06 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.2:9862
om_1                | 2023-07-16 04:51:06 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:51:06 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:51:06 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-07-16 04:51:09 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:10 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:11 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:12 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:13 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:14 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:15 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:16 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:17 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:18 INFO  Client:958 - Retrying connect to server: scm/172.20.0.6:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:51:18 INFO  RetriableTask:62 - Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-33284611-60c7-4428-a5be-320df0bc24f2;layoutVersion=0
om_1                | 2023-07-16 04:51:28 INFO  OzoneManagerStarter:124 - SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at c4d17bd4217e/172.20.0.2
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:51:31 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c4d17bd4217e/172.20.0.2
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.0.0
recon_1             | 2023-07-16 04:51:27 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-16 04:51:27 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 04:51:27 INFO  Server:1219 - Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-16 04:51:27 INFO  SCMPipelineManager:161 - No pipeline exists in current db
recon_1             | 2023-07-16 04:51:28 INFO  ReconServer:109 - Recon server initialized successfully!
recon_1             | 2023-07-16 04:51:28 INFO  ReconServer:134 - Starting Recon server
recon_1             | 2023-07-16 04:51:28 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-16 04:51:28 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 04:51:28 INFO  MetricsSystemImpl:191 - Recon metrics system started
recon_1             | 2023-07-16 04:51:28 INFO  HttpServer2:1237 - Jetty bound to port 9888
recon_1             | 2023-07-16 04:51:28 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
recon_1             | 2023-07-16 04:51:29 INFO  session:333 - DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 04:51:29 INFO  session:338 - No SessionScavenger set, using defaults
recon_1             | 2023-07-16 04:51:29 INFO  session:140 - node0 Scavenging every 600000ms
recon_1             | 2023-07-16 04:51:29 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@723877dd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 04:51:29 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@6d229b1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 04:51:32 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@40b01718{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_0_0_jar-_-any-8255902803585584326.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/recon}
recon_1             | 2023-07-16 04:51:32 INFO  AbstractConnector:330 - Started ServerConnector@62765aec{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1             | 2023-07-16 04:51:32 INFO  Server:399 - Started @38150ms
recon_1             | 2023-07-16 04:51:32 INFO  MetricsSinkAdapter:204 - Sink prometheus started
recon_1             | 2023-07-16 04:51:32 INFO  MetricsSystemImpl:301 - Registered sink prometheus
recon_1             | 2023-07-16 04:51:32 INFO  BaseHttpServer:327 - HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 04:51:32 INFO  OzoneManagerServiceProviderImpl:198 - Starting Ozone Manager Service Provider.
recon_1             | 2023-07-16 04:51:32 INFO  OzoneManagerServiceProviderImpl:176 - Registered OmDeltaRequest task 
recon_1             | 2023-07-16 04:51:32 INFO  OzoneManagerServiceProviderImpl:186 - Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 04:51:32 INFO  ReconOmMetadataManagerImpl:65 - Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-16 04:51:32 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:51:32 INFO  ReconTaskControllerImpl:221 - Starting Recon Task Controller.
recon_1             | 2023-07-16 04:51:32 INFO  ReconStorageContainerManagerFacade:206 - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:51:32 INFO  ReconStorageContainerManagerFacade:256 - Obtained 0 pipelines from SCM.
recon_1             | 2023-07-16 04:51:32 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-07-16 04:51:32 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:51:32 INFO  Server:1460 - IPC Server Responder: starting
recon_1             | 2023-07-16 04:51:32 INFO  Server:1298 - IPC Server listener on 9891: starting
recon_1             | 2023-07-16 04:51:32 INFO  ReconScmTask:46 - Registered PipelineSyncTask task 
recon_1             | 2023-07-16 04:51:32 INFO  ReconScmTask:56 - Starting PipelineSyncTask Thread.
recon_1             | 2023-07-16 04:51:32 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-07-16 04:51:32 INFO  PipelineSyncTask:61 - Pipeline sync Thread took 11 milliseconds.
recon_1             | 2023-07-16 04:51:32 INFO  ReconScmTask:46 - Registered ContainerHealthTask task 
recon_1             | 2023-07-16 04:51:32 INFO  ReconScmTask:56 - Starting ContainerHealthTask Thread.
recon_1             | 2023-07-16 04:51:33 INFO  ContainerHealthTask:77 - Container Health task thread took 142 milliseconds to process 0 existing database records.
recon_1             | 2023-07-16 04:51:33 INFO  ContainerHealthTask:86 - Container Health task thread took 13 milliseconds for processing 0 containers.
recon_1             | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/8ce16940-0905-4a9c-a5eb-adaf222452d2
recon_1             | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:26 INFO  ReconNodeManager:116 - Adding new node 8ce16940-0905-4a9c-a5eb-adaf222452d2 to Node DB.
recon_1             | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/fb643f47-389c-499f-862f-3e694a4a939e
recon_1             | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:26 INFO  ReconNodeManager:116 - Adding new node fb643f47-389c-499f-862f-3e694a4a939e to Node DB.
recon_1             | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/ddceee06-c999-4888-8c11-9381bedc9794
recon_1             | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:26 INFO  ReconNodeManager:116 - Adding new node ddceee06-c999-4888-8c11-9381bedc9794 to Node DB.
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=0318a2b7-f619-40a2-b429-ba2c6bc3e242. Trying to get from SCM.
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 0318a2b7-f619-40a2-b429-ba2c6bc3e242, Nodes: 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:8ce16940-0905-4a9c-a5eb-adaf222452d2, CreationTimestamp2023-07-16T04:52:26.177Z] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:52:29 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 0318a2b7-f619-40a2-b429-ba2c6bc3e242, Nodes: 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:8ce16940-0905-4a9c-a5eb-adaf222452d2, CreationTimestamp2023-07-16T04:52:26.177Z]
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3. Trying to get from SCM.
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.793Z] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:52:29 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.793Z]
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 reported by 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=b8f7421a-bcff-4342-96c2-6315458dca71. Trying to get from SCM.
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: b8f7421a-bcff-4342-96c2-6315458dca71, Nodes: fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.645Z] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:52:29 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: b8f7421a-bcff-4342-96c2-6315458dca71, Nodes: fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.645Z]
recon_1             | 2023-07-16 04:52:29 INFO  ReconPipelineReportHandler:83 - Pipeline ONE PipelineID=b8f7421a-bcff-4342-96c2-6315458dca71 reported by fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:29 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: b8f7421a-bcff-4342-96c2-6315458dca71, Nodes: fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:fb643f47-389c-499f-862f-3e694a4a939e, CreationTimestamp2023-07-16T04:52:26.645Z] moved to OPEN state
recon_1             | 2023-07-16 04:52:30 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=f27c09ae-63ac-4fe4-bee8-d52d84264b32. Trying to get from SCM.
recon_1             | 2023-07-16 04:52:30 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: f27c09ae-63ac-4fe4-bee8-d52d84264b32, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:ddceee06-c999-4888-8c11-9381bedc9794, CreationTimestamp2023-07-16T04:52:26.787Z] to Recon pipeline metadata.
recon_1             | 2023-07-16 04:52:30 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: f27c09ae-63ac-4fe4-bee8-d52d84264b32, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:ddceee06-c999-4888-8c11-9381bedc9794, CreationTimestamp2023-07-16T04:52:26.787Z]
recon_1             | 2023-07-16 04:52:30 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 reported by fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:30 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 reported by ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:32 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-07-16 04:52:32 INFO  OzoneManagerServiceProviderImpl:409 - Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-16 04:52:32 INFO  OzoneManagerServiceProviderImpl:316 - Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689483152531
recon_1             | 2023-07-16 04:52:32 INFO  ReconOmMetadataManagerImpl:91 - Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689483152531.
recon_1             | 2023-07-16 04:52:32 INFO  OzoneManagerServiceProviderImpl:421 - Calling reprocess on Recon tasks.
recon_1             | 2023-07-16 04:52:32 INFO  ContainerKeyMapperTask:73 - Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:52:32 INFO  ContainerDBServiceProviderImpl:117 - Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1689483152820
recon_1             | 2023-07-16 04:52:32 INFO  ContainerDBServiceProviderImpl:122 - Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1689483066955.
recon_1             | 2023-07-16 04:52:32 INFO  ContainerKeyMapperTask:89 - Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:52:32 INFO  ContainerKeyMapperTask:92 - It took me 0.081 seconds to process 0 keys.
recon_1             | 2023-07-16 04:52:32 INFO  FileSizeCountTask:102 - Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-16 04:52:34 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 reported by 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:34 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 reported by 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-07-16 04:52:34 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:8ce16940-0905-4a9c-a5eb-adaf222452d2, CreationTimestamp2023-07-16T04:52:26.793Z] moved to OPEN state
recon_1             | 2023-07-16 04:52:41 INFO  ReconContainerManager:89 - New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 04:52:41 INFO  ReconContainerManager:157 - Successfully added container #1 to Recon.
recon_1             | 2023-07-16 04:52:51 INFO  ReconContainerManager:89 - New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 04:52:51 INFO  ReconContainerManager:157 - Successfully added container #2 to Recon.
recon_1             | 2023-07-16 04:53:32 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-07-16 04:53:32 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-07-16 04:53:33 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 12
recon_1             | 2023-07-16 04:53:33 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 7 OM DB update event(s).
recon_1             | 2023-07-16 04:53:33 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-07-16 04:51:31 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:51:32 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:51:32 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.2:9862
om_1                | 2023-07-16 04:51:32 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:51:32 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:51:32 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:51:32 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-07-16 04:51:33 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:51:34 INFO  OzoneManager:3574 - Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-16 04:51:34 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 04:51:34 INFO  Server:1219 - Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 04:51:34 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-16 04:51:34 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 04:51:34 INFO  MetricsSystemImpl:191 - OzoneManager metrics system started
om_1                | 2023-07-16 04:51:34 INFO  OzoneManager:1114 - OzoneManager RPC server is listening at om/172.20.0.2:9862
om_1                | 2023-07-16 04:51:34 INFO  BaseHttpServer:207 - Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-16 04:51:34 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 04:51:34 INFO  log:169 - Logging initialized @5752ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 04:51:34 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-16 04:51:34 INFO  HttpRequestLog:86 - Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-16 04:51:34 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-16 04:51:34 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-16 04:51:34 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-16 04:51:34 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-16 04:51:34 INFO  HttpServer2:1237 - Jetty bound to port 9874
om_1                | 2023-07-16 04:51:34 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
om_1                | 2023-07-16 04:51:34 INFO  session:333 - DefaultSessionIdManager workerName=node0
om_1                | 2023-07-16 04:51:34 INFO  session:338 - No SessionScavenger set, using defaults
om_1                | 2023-07-16 04:51:34 INFO  session:140 - node0 Scavenging every 660000ms
om_1                | 2023-07-16 04:51:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@2100d047{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 04:51:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@47be0f9b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-16 04:51:34 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@1d247525{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_0_0_jar-_-any-15643431222065870472.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-16 04:51:34 INFO  AbstractConnector:330 - Started ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1                | 2023-07-16 04:51:34 INFO  Server:399 - Started @6323ms
om_1                | 2023-07-16 04:51:34 INFO  MetricsSinkAdapter:204 - Sink prometheus started
om_1                | 2023-07-16 04:51:34 INFO  MetricsSystemImpl:301 - Registered sink prometheus
om_1                | 2023-07-16 04:51:35 INFO  BaseHttpServer:327 - HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-16 04:51:35 INFO  Server:1460 - IPC Server Responder: starting
om_1                | 2023-07-16 04:51:35 INFO  Server:1298 - IPC Server listener on 9862: starting
om_1                | 2023-07-16 04:51:35 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
om_1                | 2023-07-16 04:52:32 INFO  OMDBCheckpointServlet:101 - Received request to obtain OM DB checkpoint snapshot
om_1                | 2023-07-16 04:52:32 INFO  RDBCheckpointManager:86 - Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1689483152640 in 14 milliseconds
om_1                | 2023-07-16 04:52:32 INFO  OMDBCheckpointServlet:144 - Time taken to write the checkpoint to response output stream: 25 milliseconds
om_1                | 2023-07-16 04:52:32 INFO  RocksDBCheckpoint:78 - Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1689483152640
om_1                | 2023-07-16 04:52:39 INFO  OMVolumeCreateRequest:195 - created volume:vol1 for user:hadoop
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-16 04:50:55 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
s3g_1               | 2023-07-16 04:50:56 INFO  BaseHttpServer:207 - Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-16 04:50:56 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-16 04:50:56 INFO  log:169 - Logging initialized @6036ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-16 04:50:57 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-16 04:50:57 INFO  HttpRequestLog:86 - Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-16 04:50:57 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-16 04:50:57 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-16 04:50:57 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-16 04:50:57 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-16 04:50:57 INFO  Gateway:112 - STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 02a1083112c8/172.20.0.3
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.0.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.10.3.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
s3g_1               | STARTUP_MSG:   java = 11.0.3
s3g_1               | ************************************************************/
s3g_1               | 2023-07-16 04:50:57 INFO  Gateway:90 - registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-16 04:50:58 INFO  Gateway:68 - Starting Ozone S3 gateway
s3g_1               | 2023-07-16 04:50:58 INFO  HttpServer2:1237 - Jetty bound to port 9878
s3g_1               | 2023-07-16 04:50:58 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
s3g_1               | 2023-07-16 04:50:59 INFO  session:333 - DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-16 04:50:59 INFO  session:338 - No SessionScavenger set, using defaults
s3g_1               | 2023-07-16 04:50:59 INFO  session:140 - node0 Scavenging every 660000ms
s3g_1               | 2023-07-16 04:50:59 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@24c1b2d2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 04:50:59 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@1df8da7a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/static,AVAILABLE}
s3g_1               | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 16, 2023 4:51:22 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-16 04:51:23 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@195580ba{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_0_0_jar-_-any-12603007409481965511.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/s3gateway}
s3g_1               | 2023-07-16 04:51:23 INFO  AbstractConnector:330 - Started ServerConnector@b86de0d{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1               | 2023-07-16 04:51:23 INFO  Server:399 - Started @32799ms
s3g_1               | 2023-07-16 04:51:23 INFO  BaseHttpServer:327 - HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:51:02 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 9d0bc6fb4aeb/172.20.0.6
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:51:02 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:51:02 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:51:03 INFO  StorageContainerManager:644 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-33284611-60c7-4428-a5be-320df0bc24f2;layoutVersion=0
scm_1               | 2023-07-16 04:51:03 INFO  StorageContainerManagerStarter:124 - SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 9d0bc6fb4aeb/172.20.0.6
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:51:11 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 9d0bc6fb4aeb/172.20.0.6
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:51:11 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:51:12 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:51:13 WARN  DBStoreBuilder:277 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:51:13 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@7b4c50bc
scm_1               | 2023-07-16 04:51:13 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1               | 2023-07-16 04:51:14 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1               | 2023-07-16 04:51:15 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-16 04:51:15 INFO  SCMPipelineManager:161 - No pipeline exists in current db
scm_1               | 2023-07-16 04:51:15 INFO  HealthyPipelineSafeModeRule:89 - Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:51:15 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 04:51:16 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
scm_1               | 2023-07-16 04:51:20 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:51:20 INFO  Server:1219 - Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 04:51:21 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:51:21 INFO  Server:1219 - Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 04:51:21 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:51:21 INFO  Server:1219 - Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-16 04:51:21 INFO  BaseHttpServer:207 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-16 04:51:21 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-16 04:51:21 INFO  log:169 - Logging initialized @17204ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-16 04:51:22 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-16 04:51:22 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 04:51:22 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 04:51:22 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 04:51:22 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 04:51:22 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 04:51:23 INFO  StorageContainerManager:784 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:51:24 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 04:51:24 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 04:51:24 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1               | 2023-07-16 04:51:25 INFO  SCMClientProtocolServer:156 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:51:25 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-16 04:51:26 INFO  Server:1298 - IPC Server listener on 9860: starting
scm_1               | 2023-07-16 04:51:26 INFO  StorageContainerManager:796 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:51:26 INFO  SCMBlockProtocolServer:149 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:51:26 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-16 04:51:26 INFO  Server:1298 - IPC Server listener on 9863: starting
scm_1               | 2023-07-16 04:51:27 INFO  StorageContainerManager:802 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:51:27 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:51:27 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-07-16 04:51:27 INFO  Server:1298 - IPC Server listener on 9861: starting
scm_1               | 2023-07-16 04:51:27 INFO  Server:990 - IPC Server handler 0 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.12:48998
scm_1               | 2023-07-16 04:51:27 INFO  HttpServer2:1237 - Jetty bound to port 9876
scm_1               | 2023-07-16 04:51:27 INFO  Server:990 - IPC Server handler 1 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.11:46246
scm_1               | 2023-07-16 04:51:27 INFO  Server:990 - IPC Server handler 2 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.13:52180
scm_1               | 2023-07-16 04:51:27 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1               | 2023-07-16 04:51:28 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-16 04:51:28 INFO  session:338 - No SessionScavenger set, using defaults
scm_1               | 2023-07-16 04:51:28 INFO  session:140 - node0 Scavenging every 600000ms
scm_1               | 2023-07-16 04:51:28 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@66ba7e45{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 04:51:28 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7573e12f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 04:51:29 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@e9ef5b6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_0_0_jar-_-any-2221361448153612584.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/scm}
scm_1               | 2023-07-16 04:51:29 INFO  AbstractConnector:330 - Started ServerConnector@5f80fa43{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1               | 2023-07-16 04:51:29 INFO  Server:399 - Started @25483ms
scm_1               | 2023-07-16 04:51:29 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1               | 2023-07-16 04:51:29 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1               | 2023-07-16 04:51:29 INFO  BaseHttpServer:327 - HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 04:51:29 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1               | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/8ce16940-0905-4a9c-a5eb-adaf222452d2
scm_1               | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=0318a2b7-f619-40a2-b429-ba2c6bc3e242 to datanode:8ce16940-0905-4a9c-a5eb-adaf222452d2
scm_1               | 2023-07-16 04:52:26 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 0318a2b7-f619-40a2-b429-ba2c6bc3e242, Nodes: 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.177357Z]
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/fb643f47-389c-499f-862f-3e694a4a939e
scm_1               | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:71 - SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=b8f7421a-bcff-4342-96c2-6315458dca71 to datanode:fb643f47-389c-499f-862f-3e694a4a939e
scm_1               | 2023-07-16 04:52:26 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: b8f7421a-bcff-4342-96c2-6315458dca71, Nodes: fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.645553Z]
scm_1               | 2023-07-16 04:52:26 INFO  NetworkTopology:111 - Added a new node: /default-rack/ddceee06-c999-4888-8c11-9381bedc9794
scm_1               | 2023-07-16 04:52:26 INFO  SCMNodeManager:273 - Registered Data node : ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:71 - SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:214 - DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:52:26 INFO  SCMSafeModeManager:242 - All SCM safe mode pre check rules have passed
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=f27c09ae-63ac-4fe4-bee8-d52d84264b32 to datanode:ddceee06-c999-4888-8c11-9381bedc9794
scm_1               | 2023-07-16 04:52:26 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: f27c09ae-63ac-4fe4-bee8-d52d84264b32, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.787124Z]
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 to datanode:ddceee06-c999-4888-8c11-9381bedc9794
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 to datanode:fb643f47-389c-499f-862f-3e694a4a939e
scm_1               | 2023-07-16 04:52:26 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3 to datanode:8ce16940-0905-4a9c-a5eb-adaf222452d2
scm_1               | 2023-07-16 04:52:26 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:52:26.793779Z]
scm_1               | 2023-07-16 04:52:29 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 0318a2b7-f619-40a2-b429-ba2c6bc3e242, Nodes: 8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:8ce16940-0905-4a9c-a5eb-adaf222452d2, CreationTimestamp2023-07-16T04:52:26.177357Z] moved to OPEN state
scm_1               | 2023-07-16 04:52:29 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:52:29 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:52:29 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: b8f7421a-bcff-4342-96c2-6315458dca71, Nodes: fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:fb643f47-389c-499f-862f-3e694a4a939e, CreationTimestamp2023-07-16T04:52:26.645553Z] moved to OPEN state
scm_1               | 2023-07-16 04:52:29 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:52:29 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:52:30 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: f27c09ae-63ac-4fe4-bee8-d52d84264b32, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:ddceee06-c999-4888-8c11-9381bedc9794, CreationTimestamp2023-07-16T04:52:26.787124Z] moved to OPEN state
scm_1               | 2023-07-16 04:52:30 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:52:30 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:52:34 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 2c35f5c1-3aaf-4193-9a5e-ba3d81f9bcc3, Nodes: ddceee06-c999-4888-8c11-9381bedc9794{ip: 172.20.0.12, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}fb643f47-389c-499f-862f-3e694a4a939e{ip: 172.20.0.13, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}8ce16940-0905-4a9c-a5eb-adaf222452d2{ip: 172.20.0.11, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:8ce16940-0905-4a9c-a5eb-adaf222452d2, CreationTimestamp2023-07-16T04:52:26.793779Z] moved to OPEN state
scm_1               | 2023-07-16 04:52:34 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:52:34 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:52:34 INFO  SCMSafeModeManager:214 - HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:52:34 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-16 04:52:34 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-07-16 04:53:11 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.10
scm_1               | 2023-07-16 04:53:22 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.10
scm_1               | 2023-07-16 04:54:11 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.10
scm_1               | 2023-07-16 04:54:22 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.10
Attaching to xcompat_s3g_1, xcompat_datanode_2, xcompat_datanode_3, xcompat_recon_1, xcompat_om_1, xcompat_old_client_1_2_1_1, xcompat_old_client_1_0_0_1, xcompat_new_client_1, xcompat_old_client_1_3_0_1, xcompat_datanode_1, xcompat_scm_1, xcompat_old_client_1_1_0_1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 04:54:48,487 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 01bd1f734d75/172.21.0.11
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.1.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_2          | STARTUP_MSG:   java = 11.0.10
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 04:54:48,577 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 04:54:51,112 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 04:54:51,728 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 04:54:52,487 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 04:54:52,489 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-16 04:54:53,333 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:01bd1f734d75 ip:172.21.0.11
datanode_2          | 2023-07-16 04:54:55,089 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 04:54:55,112 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-07-16 04:54:55,122 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 04:54:55,165 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 04:54:55,366 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:54:55,944 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:54:55,945 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:54:55,945 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-16 04:55:06,507 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:55:07,224 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 04:55:07,966 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-16 04:55:07,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-16 04:55:07,975 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-16 04:55:08,009 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 04:55:08,077 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:55:08,078 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 04:55:08,079 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:55:10,850 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-16 04:55:10,900 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:10,901 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:55:11,195 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:55:11,262 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:55:12,101 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:55:12,242 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 04:55:12,480 [main] INFO util.log: Logging initialized @31187ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 04:55:13,494 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-16 04:55:13,643 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 04:55:13,804 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 04:55:13,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 04:55:13,848 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 04:55:13,848 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 04:55:14,329 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-16 04:55:14,398 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_2          | 2023-07-16 04:55:14,887 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 04:55:14,887 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 04:55:14,946 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-16 04:55:15,333 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43bdaa1b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 04:55:15,333 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ea52184{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 04:55:17,233 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2db33feb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-11211220786188543296/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 04:55:17,285 [main] INFO server.AbstractConnector: Started ServerConnector@608b1fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-16 04:55:17,286 [main] INFO server.Server: Started @35993ms
datanode_2          | 2023-07-16 04:55:17,307 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 04:55:17,308 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-16 04:55:17,312 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:55:17,442 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2c566d27] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-16 04:55:17,988 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_2          | 2023-07-16 04:55:18,401 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 04:55:20,676 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 04:54:48,192 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = b02dc580e89b/172.21.0.4
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.1.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_1          | STARTUP_MSG:   java = 11.0.10
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 04:54:48,288 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 04:54:50,620 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 04:54:51,471 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 04:54:52,221 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 04:54:52,222 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-16 04:54:52,773 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b02dc580e89b ip:172.21.0.4
datanode_1          | 2023-07-16 04:54:54,528 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 04:54:54,537 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-07-16 04:54:54,538 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 04:54:54,591 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 04:54:54,694 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:54:54,952 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:54:54,968 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:54:54,968 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-16 04:55:05,937 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:55:06,752 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 04:55:07,502 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-16 04:55:07,521 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-16 04:55:07,522 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-16 04:55:07,523 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 04:55:07,562 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:07,576 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 04:55:07,577 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:55:10,381 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-16 04:55:10,423 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:10,424 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:55:10,758 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:55:10,775 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:55:11,754 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:55:11,947 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 04:55:12,167 [main] INFO util.log: Logging initialized @31683ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 04:55:12,985 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-16 04:55:13,036 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 04:55:13,059 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 04:55:13,110 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 04:55:13,198 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 04:55:13,199 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 04:55:14,054 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-16 04:55:14,065 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_1          | 2023-07-16 04:55:14,359 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 04:55:14,359 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 04:55:14,366 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-16 04:55:14,529 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 04:55:14,603 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 04:55:17,354 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-4152741038371499705/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-16 04:55:17,473 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-16 04:55:17,473 [main] INFO server.Server: Started @36989ms
datanode_1          | 2023-07-16 04:55:17,502 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-16 04:55:17,502 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 04:55:17,504 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:55:17,676 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@21990b79] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-16 04:55:18,331 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_1          | 2023-07-16 04:55:18,566 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 04:55:20,907 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-16 04:54:48,678 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = e3efed581a0f/172.21.0.10
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.1.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_3          | STARTUP_MSG:   java = 11.0.10
datanode_3          | ************************************************************/
datanode_3          | 2023-07-16 04:54:48,753 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 04:54:51,210 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 04:54:51,838 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 04:54:52,513 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-16 04:54:52,513 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-16 04:54:53,480 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e3efed581a0f ip:172.21.0.10
datanode_3          | 2023-07-16 04:54:54,908 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 04:54:54,917 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-07-16 04:54:54,927 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 04:54:54,986 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 04:54:55,206 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:54:55,531 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:54:55,559 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:54:55,559 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-16 04:55:06,659 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:55:07,185 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 04:55:07,926 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-16 04:55:07,957 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-16 04:55:07,976 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-16 04:55:07,992 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-16 04:55:07,993 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:08,000 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-16 04:55:08,030 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:55:10,518 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-16 04:55:10,549 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:10,550 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:55:10,963 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:55:11,093 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:55:11,870 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:55:12,137 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 04:55:12,283 [main] INFO util.log: Logging initialized @31474ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 04:55:12,921 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-16 04:55:12,935 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-16 04:55:12,969 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-16 04:55:12,980 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 04:55:12,985 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-16 04:55:12,989 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-16 04:55:13,723 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 04:55:13,827 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_3          | 2023-07-16 04:55:14,166 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 04:55:14,166 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 04:55:14,173 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-16 04:55:14,254 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 04:55:14,276 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-16 04:55:16,633 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-15338922617496883895/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 04:55:16,699 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-16 04:55:16,699 [main] INFO server.Server: Started @35890ms
datanode_3          | 2023-07-16 04:55:16,764 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-16 04:55:16,764 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 04:55:20,952 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 04:55:20,956 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-16 04:55:21,327 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:21,512 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start RPC server
datanode_1          | 2023-07-16 04:55:21,531 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: GrpcService started, listening on 9856
datanode_1          | 2023-07-16 04:55:21,542 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: GrpcService started, listening on 9857
datanode_1          | 2023-07-16 04:55:21,549 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 04:55:21,588 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678e2808-f01e-45ca-8f6c-3b0a4669caa8 is started using port 9858 for RATIS
datanode_1          | 2023-07-16 04:55:21,589 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678e2808-f01e-45ca-8f6c-3b0a4669caa8 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 04:55:21,590 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678e2808-f01e-45ca-8f6c-3b0a4669caa8 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 04:55:21,602 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@628bb9c1] INFO util.JvmPauseMonitor: JvmPauseMonitor-678e2808-f01e-45ca-8f6c-3b0a4669caa8: Started
datanode_1          | 2023-07-16 04:55:24,734 [Command processor thread] INFO server.RaftServer: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: addNew group-B13B9B9CADC2:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] returns group-B13B9B9CADC2:java.util.concurrent.CompletableFuture@5d854f71[Not completed]
datanode_1          | 2023-07-16 04:55:24,762 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: new RaftServerImpl for group-B13B9B9CADC2:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:55:24,772 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:55:24,776 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:55:24,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:55:24,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:24,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:55:24,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:55:24,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:55:24,800 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:55:24,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:55:24,808 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:55:24,817 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2ab68bb5-b826-4da3-98c0-b13b9b9cadc2 does not exist. Creating ...
datanode_1          | 2023-07-16 04:55:24,845 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ab68bb5-b826-4da3-98c0-b13b9b9cadc2/in_use.lock acquired by nodename 7@b02dc580e89b
datanode_1          | 2023-07-16 04:55:24,868 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2ab68bb5-b826-4da3-98c0-b13b9b9cadc2 has been successfully formatted.
datanode_1          | 2023-07-16 04:55:24,892 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B13B9B9CADC2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:55:24,902 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:24,909 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:55:24,938 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:55:24,943 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:24,957 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2
datanode_1          | 2023-07-16 04:55:24,985 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,000 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:55:25,001 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:55:25,006 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2ab68bb5-b826-4da3-98c0-b13b9b9cadc2
datanode_1          | 2023-07-16 04:55:25,028 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:55:25,029 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:55:25,031 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,031 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:55:25,032 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:55:25,033 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:55:25,040 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:55:25,041 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:55:25,080 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,086 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:25,099 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:25,102 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:25,111 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:25,118 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:55:25,119 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:55:25,120 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:55:25,125 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:55:25,126 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:55:25,239 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2
datanode_1          | 2023-07-16 04:55:25,259 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2
datanode_1          | 2023-07-16 04:55:25,302 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:55:25,311 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:55:25,313 [pool-19-thread-1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState
datanode_1          | 2023-07-16 04:55:25,383 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B13B9B9CADC2,id=678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:25,385 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2
datanode_1          | 2023-07-16 04:55:25,464 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=2ab68bb5-b826-4da3-98c0-b13b9b9cadc2.
datanode_1          | 2023-07-16 04:55:25,467 [Command processor thread] INFO server.RaftServer: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: addNew group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-36C0689D72BE:java.util.concurrent.CompletableFuture@7b6f8fd2[Not completed]
datanode_1          | 2023-07-16 04:55:25,515 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: new RaftServerImpl for group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:55:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:55:25,527 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:55:25,527 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:55:25,528 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:25,528 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:55:25,528 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:55:25,531 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:55:20,870 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-16 04:55:20,873 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-16 04:55:21,206 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:21,363 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 4059d340-ca55-4574-83cb-35af8dac7525: start RPC server
datanode_2          | 2023-07-16 04:55:21,401 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 4059d340-ca55-4574-83cb-35af8dac7525: GrpcService started, listening on 9856
datanode_2          | 2023-07-16 04:55:21,425 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 4059d340-ca55-4574-83cb-35af8dac7525: GrpcService started, listening on 9857
datanode_2          | 2023-07-16 04:55:21,426 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 4059d340-ca55-4574-83cb-35af8dac7525: GrpcService started, listening on 9858
datanode_2          | 2023-07-16 04:55:21,459 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@4e3fbec4] INFO util.JvmPauseMonitor: JvmPauseMonitor-4059d340-ca55-4574-83cb-35af8dac7525: Started
datanode_2          | 2023-07-16 04:55:21,459 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4059d340-ca55-4574-83cb-35af8dac7525 is started using port 9858 for RATIS
datanode_2          | 2023-07-16 04:55:21,462 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4059d340-ca55-4574-83cb-35af8dac7525 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-16 04:55:21,468 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4059d340-ca55-4574-83cb-35af8dac7525 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-16 04:55:21,493 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_2          | java.util.ConcurrentModificationException
datanode_2          | 	at java.base/java.util.ArrayList$Itr.checkForComodification(ArrayList.java:1043)
datanode_2          | 	at java.base/java.util.ArrayList$Itr.next(ArrayList.java:997)
datanode_2          | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.toProtoBuilder(DatanodeDetails.java:418)
datanode_2          | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.toProto(DatanodeDetails.java:379)
datanode_2          | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.getProtoBufMessage(DatanodeDetails.java:375)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.HeartbeatEndpointTask$Builder.build(HeartbeatEndpointTask.java:430)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.initEndPointTask(RunningDatanodeState.java:102)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.<init>(RunningDatanodeState.java:67)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.getTask(StateContext.java:508)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:544)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | 2023-07-16 04:55:25,467 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-07-16 04:55:25,699 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 01bd1f734d75/172.21.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.11:34132 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.11:34132 remote=recon/172.21.0.12:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_2          | 2023-07-16 04:55:26,561 [Command processor thread] INFO server.RaftServer: 4059d340-ca55-4574-83cb-35af8dac7525: addNew group-932902A73F74:[4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1] returns group-932902A73F74:java.util.concurrent.CompletableFuture@40251a12[Not completed]
datanode_2          | 2023-07-16 04:55:26,636 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525: new RaftServerImpl for group-932902A73F74:[4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:55:26,649 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:55:26,650 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:55:26,652 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:55:26,652 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:26,652 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:55:26,653 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:55:26,654 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:55:26,678 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: ConfigurationManager, init=-1: [4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:55:26,680 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:55:26,716 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:55:26,718 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/34f3b6b3-6b48-425a-9038-932902a73f74 does not exist. Creating ...
datanode_2          | 2023-07-16 04:55:26,777 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/34f3b6b3-6b48-425a-9038-932902a73f74/in_use.lock acquired by nodename 7@01bd1f734d75
datanode_2          | 2023-07-16 04:55:26,804 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/34f3b6b3-6b48-425a-9038-932902a73f74 has been successfully formatted.
datanode_2          | 2023-07-16 04:55:26,833 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-932902A73F74: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:55:26,836 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:26,861 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:55:26,915 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:55:26,915 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:55:26,919 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74
datanode_2          | 2023-07-16 04:55:26,995 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:27,054 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:55:27,097 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:55:27,161 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/34f3b6b3-6b48-425a-9038-932902a73f74
datanode_2          | 2023-07-16 04:55:27,176 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:55:27,184 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:55:27,185 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:27,201 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:55:27,207 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:55:27,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:55:27,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:55:27,255 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:55:27,342 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:27,350 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:27,423 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:27,423 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:27,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:27,468 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:55:27,469 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:55:27,470 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:55:27,471 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:55:27,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:55:27,716 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74
datanode_2          | 2023-07-16 04:55:27,759 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74
datanode_2          | 2023-07-16 04:55:27,830 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: start as a follower, conf=-1: [4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:55:27,873 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:55:27,878 [pool-19-thread-1] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState
datanode_2          | 2023-07-16 04:55:27,987 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-932902A73F74,id=4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:27,989 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74
datanode_2          | 2023-07-16 04:55:28,107 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=34f3b6b3-6b48-425a-9038-932902a73f74.
datanode_2          | 2023-07-16 04:55:28,108 [Command processor thread] INFO server.RaftServer: 4059d340-ca55-4574-83cb-35af8dac7525: addNew group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-36C0689D72BE:java.util.concurrent.CompletableFuture@216d9ecc[Not completed]
datanode_2          | 2023-07-16 04:55:28,196 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525: new RaftServerImpl for group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:55:28,197 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:55:28,197 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:55:28,198 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:55:28,198 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:28,198 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:55:28,198 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:55:28,199 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:55:28,199 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:55:28,204 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:55:28,212 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:55:28,212 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be does not exist. Creating ...
datanode_2          | 2023-07-16 04:55:28,214 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/in_use.lock acquired by nodename 7@01bd1f734d75
datanode_2          | 2023-07-16 04:55:28,226 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be has been successfully formatted.
datanode_2          | 2023-07-16 04:55:28,226 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-36C0689D72BE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:55:28,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:28,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:55:28,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:55:28,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:55:28,227 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE
datanode_2          | 2023-07-16 04:55:28,228 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:28,228 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:55:28,229 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:55:28,232 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be
datanode_2          | 2023-07-16 04:55:28,242 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:55:28,242 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:55:28,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:55:28,249 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:28,252 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:28,264 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:28,264 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:28,296 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:28,297 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:55:28,297 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:55:28,297 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:55:28,297 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:55:28,298 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:55:28,298 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE
datanode_2          | 2023-07-16 04:55:28,299 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE
datanode_2          | 2023-07-16 04:55:28,323 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:55:28,330 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:55:28,340 [pool-19-thread-1] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState
datanode_2          | 2023-07-16 04:55:28,352 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-36C0689D72BE,id=4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:28,352 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE
datanode_2          | 2023-07-16 04:55:28,606 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:28,619 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A85A6DA9C501->678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_2          | 2023-07-16 04:55:29,875 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-AED4DF870614->96fc65c5-22fc-48dd-984b-d88286c71717
datanode_2          | 2023-07-16 04:55:30,249 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be.
datanode_2          | 2023-07-16 04:55:30,259 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525: new RaftServerImpl for group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:55:30,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:55:30,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:55:30,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:55:30,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:30,264 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:55:30,264 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:55:30,265 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:55:30,265 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:55:30,266 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:55:30,266 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:55:30,269 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 does not exist. Creating ...
datanode_2          | 2023-07-16 04:55:30,270 [Command processor thread] INFO server.RaftServer: 4059d340-ca55-4574-83cb-35af8dac7525: addNew group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-B076E8F2FF81:java.util.concurrent.CompletableFuture@7e521024[Not completed]
datanode_2          | 2023-07-16 04:55:30,273 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/in_use.lock acquired by nodename 7@01bd1f734d75
datanode_2          | 2023-07-16 04:55:30,280 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 has been successfully formatted.
datanode_2          | 2023-07-16 04:55:30,285 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B076E8F2FF81: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:55:30,289 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:55:30,289 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:55:30,289 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:55:30,290 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:55:30,290 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81
datanode_2          | 2023-07-16 04:55:30,290 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:30,291 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:55:30,291 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:55:30,291 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81
datanode_2          | 2023-07-16 04:55:30,294 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-16 04:55:30,316 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:55:30,336 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:55:30,337 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:30,337 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:55:30,337 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:55:30,337 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:55:30,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:55:30,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:55:30,339 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:30,340 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:30,340 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:30,340 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:55:30,380 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:55:30,381 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:55:30,384 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:55:30,384 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:55:30,384 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:55:30,385 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:55:30,385 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81
datanode_2          | 2023-07-16 04:55:30,386 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81
datanode_2          | 2023-07-16 04:55:30,391 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:55:30,391 [pool-19-thread-1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:55:30,391 [pool-19-thread-1] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FollowerState
datanode_2          | 2023-07-16 04:55:30,405 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B076E8F2FF81,id=4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:30,408 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81
datanode_2          | 2023-07-16 04:55:30,423 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-AD2F127F01CD->678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_2          | 2023-07-16 04:55:30,665 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-63D6D97E776D->96fc65c5-22fc-48dd-984b-d88286c71717
datanode_2          | 2023-07-16 04:55:30,819 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81.
datanode_2          | 2023-07-16 04:55:30,965 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: receive requestVote(ELECTION, 678e2808-f01e-45ca-8f6c-3b0a4669caa8, group-36C0689D72BE, 1, (t:0, i:0))
datanode_2          | 2023-07-16 04:55:30,971 [grpc-default-executor-0] INFO impl.VoteContext: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FOLLOWER: accept ELECTION from 678e2808-f01e-45ca-8f6c-3b0a4669caa8: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-16 04:55:16,773 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:55:16,955 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@64b4fc6f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-16 04:55:17,384 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.12:9891
datanode_3          | 2023-07-16 04:55:17,814 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 04:55:20,149 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:55:20,921 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-16 04:55:20,930 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-16 04:55:21,154 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:55:21,387 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:21,514 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 96fc65c5-22fc-48dd-984b-d88286c71717: start RPC server
datanode_3          | 2023-07-16 04:55:21,536 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 96fc65c5-22fc-48dd-984b-d88286c71717: GrpcService started, listening on 9856
datanode_3          | 2023-07-16 04:55:21,578 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 96fc65c5-22fc-48dd-984b-d88286c71717: GrpcService started, listening on 9857
datanode_3          | 2023-07-16 04:55:21,603 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 96fc65c5-22fc-48dd-984b-d88286c71717: GrpcService started, listening on 9858
datanode_3          | 2023-07-16 04:55:21,619 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$283/0x000000084046ec40@60c8afcc] INFO util.JvmPauseMonitor: JvmPauseMonitor-96fc65c5-22fc-48dd-984b-d88286c71717: Started
datanode_3          | 2023-07-16 04:55:21,620 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 96fc65c5-22fc-48dd-984b-d88286c71717 is started using port 9858 for RATIS
datanode_3          | 2023-07-16 04:55:21,621 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 96fc65c5-22fc-48dd-984b-d88286c71717 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-16 04:55:21,621 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 96fc65c5-22fc-48dd-984b-d88286c71717 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-16 04:55:26,013 [Command processor thread] INFO server.RaftServer: 96fc65c5-22fc-48dd-984b-d88286c71717: addNew group-5B824CF0FE6A:[96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1] returns group-5B824CF0FE6A:java.util.concurrent.CompletableFuture@75a8307d[Not completed]
datanode_3          | 2023-07-16 04:55:26,120 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717: new RaftServerImpl for group-5B824CF0FE6A:[96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:55:26,124 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:55:26,125 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:55:26,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:55:26,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:26,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:55:26,131 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:55:26,145 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:55:26,155 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: ConfigurationManager, init=-1: [96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:55:26,164 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:55:26,187 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From e3efed581a0f/172.21.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.10:34014 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.10:34014 remote=recon/172.21.0.12:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 2023-07-16 04:55:25,532 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:55:25,532 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:55:25,534 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:55:25,534 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be does not exist. Creating ...
datanode_1          | 2023-07-16 04:55:25,544 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/in_use.lock acquired by nodename 7@b02dc580e89b
datanode_1          | 2023-07-16 04:55:25,546 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be has been successfully formatted.
datanode_1          | 2023-07-16 04:55:25,547 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-36C0689D72BE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:55:25,549 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:25,550 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:55:25,550 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:55:25,550 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:25,550 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE
datanode_1          | 2023-07-16 04:55:25,551 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:55:25,553 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:55:30,971 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_2          | 2023-07-16 04:55:30,972 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: shutdown 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState
datanode_2          | 2023-07-16 04:55:30,972 [4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-16 04:55:30,974 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState
datanode_2          | 2023-07-16 04:55:31,003 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE replies to ELECTION vote request: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t1. Peer's state: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE:t1, leader=null, voted=678e2808-f01e-45ca-8f6c-3b0a4669caa8, raftlog=4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:55:33,017 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState] INFO impl.FollowerState: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139419456ns, electionTimeout:5128ms
datanode_2          | 2023-07-16 04:55:33,018 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: shutdown 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState
datanode_2          | 2023-07-16 04:55:33,018 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:55:33,021 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 04:55:33,021 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-FollowerState] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1
datanode_2          | 2023-07-16 04:55:33,028 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO impl.LeaderElection: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:55:33,029 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO impl.LeaderElection: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-16 04:55:33,029 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: shutdown 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1
datanode_2          | 2023-07-16 04:55:33,030 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-16 04:55:33,030 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-932902A73F74 with new leaderId: 4059d340-ca55-4574-83cb-35af8dac7525
datanode_2          | 2023-07-16 04:55:33,031 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: change Leader from null to 4059d340-ca55-4574-83cb-35af8dac7525 at term 1 for becomeLeader, leader elected after 6194ms
datanode_2          | 2023-07-16 04:55:33,060 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 04:55:33,067 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74
datanode_2          | 2023-07-16 04:55:33,072 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:55:33,072 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-07-16 04:55:33,106 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 04:55:33,106 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 04:55:33,108 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 04:55:33,123 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderStateImpl
datanode_2          | 2023-07-16 04:55:33,142 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:55:33,179 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-LeaderElection1] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74: set configuration 0: [4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-07-16 04:55:33,241 [4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-932902A73F74-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/34f3b6b3-6b48-425a-9038-932902a73f74/current/log_inprogress_0
datanode_2          | 2023-07-16 04:55:35,319 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: receive requestVote(ELECTION, 678e2808-f01e-45ca-8f6c-3b0a4669caa8, group-B076E8F2FF81, 1, (t:0, i:0))
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_3          | 2023-07-16 04:55:26,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:55:26,218 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/779c010a-321c-47c6-b40f-5b824cf0fe6a does not exist. Creating ...
datanode_3          | 2023-07-16 04:55:26,237 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/779c010a-321c-47c6-b40f-5b824cf0fe6a/in_use.lock acquired by nodename 6@e3efed581a0f
datanode_3          | 2023-07-16 04:55:26,278 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/779c010a-321c-47c6-b40f-5b824cf0fe6a has been successfully formatted.
datanode_3          | 2023-07-16 04:55:26,284 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-5B824CF0FE6A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:55:26,302 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:26,330 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:55:26,380 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:55:26,381 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:26,385 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A
datanode_3          | 2023-07-16 04:55:26,427 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:26,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:55:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:55:26,513 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/779c010a-321c-47c6-b40f-5b824cf0fe6a
datanode_3          | 2023-07-16 04:55:26,514 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:55:26,515 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:55:26,529 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:26,531 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:55:26,539 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:55:26,545 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:55:26,548 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:55:26,549 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:55:26,606 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:26,609 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:26,645 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:26,645 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:26,677 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:26,718 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:55:26,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:55:26,723 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:55:26,724 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:55:26,732 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:55:26,869 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A
datanode_3          | 2023-07-16 04:55:26,924 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A
datanode_3          | 2023-07-16 04:55:26,967 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: start as a follower, conf=-1: [96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null
datanode_3          | 2023-07-16 04:55:26,977 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:55:26,996 [pool-19-thread-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState
datanode_3          | 2023-07-16 04:55:27,072 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B824CF0FE6A,id=96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:27,074 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A
datanode_3          | 2023-07-16 04:55:27,181 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=779c010a-321c-47c6-b40f-5b824cf0fe6a.
datanode_3          | 2023-07-16 04:55:27,186 [Command processor thread] INFO server.RaftServer: 96fc65c5-22fc-48dd-984b-d88286c71717: addNew group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-36C0689D72BE:java.util.concurrent.CompletableFuture@5864eed4[Not completed]
datanode_3          | 2023-07-16 04:55:27,189 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717: new RaftServerImpl for group-36C0689D72BE:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:55:27,212 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:55:27,216 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:55:27,216 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:55:27,216 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:27,217 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:55:27,218 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:55:27,218 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:55:27,218 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:55:27,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:55:27,226 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:55:27,229 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be does not exist. Creating ...
datanode_3          | 2023-07-16 04:55:27,236 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/in_use.lock acquired by nodename 6@e3efed581a0f
datanode_3          | 2023-07-16 04:55:27,238 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be has been successfully formatted.
datanode_3          | 2023-07-16 04:55:27,248 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-36C0689D72BE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:55:27,253 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:27,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:55:27,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:55:27,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:27,255 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_3          | 2023-07-16 04:55:27,258 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:27,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:55:27,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:55:27,260 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:55:27,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:55:27,262 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:27,262 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:27,262 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:27,262 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:27,300 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:55:27,301 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_3          | 2023-07-16 04:55:27,302 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_2          | 2023-07-16 04:55:35,321 [grpc-default-executor-0] INFO impl.VoteContext: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FOLLOWER: accept ELECTION from 678e2808-f01e-45ca-8f6c-3b0a4669caa8: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:55:35,321 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_2          | 2023-07-16 04:55:35,321 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: shutdown 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FollowerState
datanode_2          | 2023-07-16 04:55:35,321 [4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FollowerState] INFO impl.FollowerState: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-16 04:55:35,321 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-FollowerState
datanode_2          | 2023-07-16 04:55:35,325 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81 replies to ELECTION vote request: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t1. Peer's state: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81:t1, leader=null, voted=678e2808-f01e-45ca-8f6c-3b0a4669caa8, raftlog=4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:55:35,455 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B076E8F2FF81 with new leaderId: 678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_2          | 2023-07-16 04:55:35,455 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: change Leader from null to 678e2808-f01e-45ca-8f6c-3b0a4669caa8 at term 1 for appendEntries, leader elected after 5166ms
datanode_2          | 2023-07-16 04:55:35,485 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-16 04:55:35,490 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:55:35,492 [4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-B076E8F2FF81-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/current/log_inprogress_0
datanode_2          | 2023-07-16 04:55:36,385 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: receive requestVote(ELECTION, 96fc65c5-22fc-48dd-984b-d88286c71717, group-36C0689D72BE, 2, (t:0, i:0))
datanode_2          | 2023-07-16 04:55:36,386 [grpc-default-executor-0] INFO impl.VoteContext: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FOLLOWER: accept ELECTION from 96fc65c5-22fc-48dd-984b-d88286c71717: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:55:36,386 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:96fc65c5-22fc-48dd-984b-d88286c71717
datanode_2          | 2023-07-16 04:55:36,387 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: shutdown 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState
datanode_2          | 2023-07-16 04:55:36,387 [4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-16 04:55:36,387 [grpc-default-executor-0] INFO impl.RoleInfo: 4059d340-ca55-4574-83cb-35af8dac7525: start 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-FollowerState
datanode_2          | 2023-07-16 04:55:36,391 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE replies to ELECTION vote request: 96fc65c5-22fc-48dd-984b-d88286c71717<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t2. Peer's state: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE:t2, leader=null, voted=96fc65c5-22fc-48dd-984b-d88286c71717, raftlog=4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:55:36,566 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-36C0689D72BE with new leaderId: 96fc65c5-22fc-48dd-984b-d88286c71717
datanode_2          | 2023-07-16 04:55:36,567 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: change Leader from null to 96fc65c5-22fc-48dd-984b-d88286c71717 at term 2 for appendEntries, leader elected after 8339ms
datanode_2          | 2023-07-16 04:55:36,615 [grpc-default-executor-0] INFO server.RaftServer$Division: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-16 04:55:36,615 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:55:25,554 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be
datanode_1          | 2023-07-16 04:55:25,554 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:55:25,556 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:55:25,559 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,560 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:55:25,560 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:55:25,564 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:55:25,564 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:55:25,564 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:55:25,565 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:25,566 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:25,566 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:25,568 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:25,585 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:25,590 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:55:25,591 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:55:25,599 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:55:25,600 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:55:25,600 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:55:25,600 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE
datanode_1          | 2023-07-16 04:55:25,601 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE
datanode_1          | 2023-07-16 04:55:25,610 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:55:25,610 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:55:25,611 [pool-19-thread-1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState
datanode_1          | 2023-07-16 04:55:25,616 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-36C0689D72BE,id=678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:25,624 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE
datanode_1          | 2023-07-16 04:55:25,823 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DD0E773AF5A5->4059d340-ca55-4574-83cb-35af8dac7525
datanode_1          | 2023-07-16 04:55:25,953 [EndpointStateMachine task thread for recon/172.21.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From b02dc580e89b/172.21.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.4:58392 remote=recon/172.21.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.4:58392 remote=recon/172.21.0.12:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 2023-07-16 04:55:27,303 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:55:27,303 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:55:27,303 [pool-19-thread-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState
datanode_3          | 2023-07-16 04:55:27,326 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-36C0689D72BE,id=96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:27,327 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_3          | 2023-07-16 04:55:27,552 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-023A045DB3EE->678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_3          | 2023-07-16 04:55:29,884 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-776DA19EB34C->4059d340-ca55-4574-83cb-35af8dac7525
datanode_3          | 2023-07-16 04:55:29,949 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:30,143 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be.
datanode_3          | 2023-07-16 04:55:30,161 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717: new RaftServerImpl for group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:55:30,172 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:55:30,172 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:55:30,173 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:55:30,174 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:30,174 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:55:30,194 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:55:30,189 [Command processor thread] INFO server.RaftServer: 96fc65c5-22fc-48dd-984b-d88286c71717: addNew group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-B076E8F2FF81:java.util.concurrent.CompletableFuture@4f805745[Not completed]
datanode_3          | 2023-07-16 04:55:30,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:55:30,202 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:55:30,203 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:55:30,203 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:55:30,204 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 does not exist. Creating ...
datanode_3          | 2023-07-16 04:55:30,206 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/in_use.lock acquired by nodename 6@e3efed581a0f
datanode_3          | 2023-07-16 04:55:30,210 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 has been successfully formatted.
datanode_3          | 2023-07-16 04:55:30,220 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B076E8F2FF81: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:55:30,223 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:55:30,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:55:30,228 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:55:30,228 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:30,228 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81
datanode_3          | 2023-07-16 04:55:30,229 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:30,229 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81
datanode_3          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:55:30,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:30,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:55:30,235 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:55:30,235 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:55:30,235 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:55:30,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_1          | 2023-07-16 04:55:29,193 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:29,656 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2C7C2DF015E3->96fc65c5-22fc-48dd-984b-d88286c71717
datanode_1          | 2023-07-16 04:55:30,170 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be.
datanode_1          | 2023-07-16 04:55:30,173 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: new RaftServerImpl for group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:55:30,173 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:55:30,173 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:55:30,173 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:55:30,174 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:30,174 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:55:30,175 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:55:30,175 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:55:30,180 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: ConfigurationManager, init=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:55:30,180 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:55:30,181 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:55:30,184 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 does not exist. Creating ...
datanode_1          | 2023-07-16 04:55:30,180 [Command processor thread] INFO server.RaftServer: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: addNew group-B076E8F2FF81:[678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0] returns group-B076E8F2FF81:java.util.concurrent.CompletableFuture@61122410[Not completed]
datanode_1          | 2023-07-16 04:55:30,191 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/in_use.lock acquired by nodename 7@b02dc580e89b
datanode_1          | 2023-07-16 04:55:30,201 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81 has been successfully formatted.
datanode_1          | 2023-07-16 04:55:30,204 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:55:30,206 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B076E8F2FF81: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:55:30,208 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:55:30,210 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:55:30,210 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:55:30,211 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:30,221 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:30,223 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:30,227 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:55:30,231 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81
datanode_1          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:55:30,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:55:30,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:55:30,235 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:55:30,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:55:30,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:55:30,240 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:55:30,250 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:55:36,619 [4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4059d340-ca55-4574-83cb-35af8dac7525@group-36C0689D72BE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/current/log_inprogress_0
datanode_3          | 2023-07-16 04:55:30,244 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:55:30,223 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:55:30,246 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:30,248 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:30,248 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:55:30,249 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:55:30,253 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:55:30,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:55:30,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:55:30,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:55:30,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:55:30,264 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81
datanode_3          | 2023-07-16 04:55:30,265 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81
datanode_3          | 2023-07-16 04:55:30,274 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:55:30,280 [pool-19-thread-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:55:30,281 [pool-19-thread-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FollowerState
datanode_3          | 2023-07-16 04:55:30,290 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B076E8F2FF81,id=96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:30,296 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81
datanode_3          | 2023-07-16 04:55:30,322 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-41BF9C00031F->678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_3          | 2023-07-16 04:55:30,459 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-BCCE50A240F6->4059d340-ca55-4574-83cb-35af8dac7525
datanode_3          | 2023-07-16 04:55:30,648 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81.
datanode_3          | 2023-07-16 04:55:31,130 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: receive requestVote(ELECTION, 678e2808-f01e-45ca-8f6c-3b0a4669caa8, group-36C0689D72BE, 1, (t:0, i:0))
datanode_3          | 2023-07-16 04:55:31,132 [grpc-default-executor-1] INFO impl.VoteContext: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FOLLOWER: reject ELECTION from 678e2808-f01e-45ca-8f6c-3b0a4669caa8: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-16 04:55:31,137 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_3          | 2023-07-16 04:55:31,137 [grpc-default-executor-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState
datanode_3          | 2023-07-16 04:55:31,138 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-16 04:55:31,140 [grpc-default-executor-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState
datanode_3          | 2023-07-16 04:55:31,175 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE replies to ELECTION vote request: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-96fc65c5-22fc-48dd-984b-d88286c71717#0:FAIL-t1. Peer's state: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE:t1, leader=null, voted=null, raftlog=96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:55:32,140 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState] INFO impl.FollowerState: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5143797997ns, electionTimeout:5111ms
datanode_3          | 2023-07-16 04:55:32,141 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState
datanode_3          | 2023-07-16 04:55:32,141 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:55:32,144 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 04:55:32,144 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-FollowerState] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1
datanode_1          | 2023-07-16 04:55:30,254 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:30,254 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:30,256 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:55:30,267 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:55:30,270 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:55:30,271 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:55:30,271 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:55:30,272 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:55:30,273 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:55:30,273 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:30,275 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:30,277 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: start as a follower, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:55:30,284 [pool-19-thread-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:55:30,284 [pool-19-thread-1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState
datanode_1          | 2023-07-16 04:55:30,288 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B076E8F2FF81,id=678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:30,288 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:30,294 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E28A9D9F9D9D->96fc65c5-22fc-48dd-984b-d88286c71717
datanode_1          | 2023-07-16 04:55:30,489 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState] INFO impl.FollowerState: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5176299834ns, electionTimeout:5155ms
datanode_1          | 2023-07-16 04:55:30,491 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState
datanode_1          | 2023-07-16 04:55:30,510 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:55:30,522 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-AE38724EDBF0->4059d340-ca55-4574-83cb-35af8dac7525
datanode_1          | 2023-07-16 04:55:30,566 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 04:55:30,566 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1
datanode_1          | 2023-07-16 04:55:30,614 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:55:30,615 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-16 04:55:30,616 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1
datanode_1          | 2023-07-16 04:55:30,616 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 04:55:30,617 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B13B9B9CADC2 with new leaderId: 678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:30,618 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: change Leader from null to 678e2808-f01e-45ca-8f6c-3b0a4669caa8 at term 1 for becomeLeader, leader elected after 5715ms
datanode_1          | 2023-07-16 04:55:30,671 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:55:30,727 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2
datanode_1          | 2023-07-16 04:55:30,729 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:55:30,753 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-16 04:55:30,761 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:55:30,780 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:55:30,781 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:55:30,802 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5191550104ns, electionTimeout:5186ms
datanode_1          | 2023-07-16 04:55:30,821 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState
datanode_1          | 2023-07-16 04:55:30,821 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:55:30,824 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 04:55:30,825 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2
datanode_1          | 2023-07-16 04:55:30,846 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderStateImpl
datanode_1          | 2023-07-16 04:55:30,868 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81.
datanode_1          | 2023-07-16 04:55:30,884 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:55:30,985 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:55:31,185 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-LeaderElection1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-16 04:55:31,236 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:55:31,241 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection:   Response 0: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-96fc65c5-22fc-48dd-984b-d88286c71717#0:FAIL-t1
datanode_1          | 2023-07-16 04:55:31,242 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection:   Response 1: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t1
datanode_1          | 2023-07-16 04:55:31,249 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-16 04:55:31,250 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-07-16 04:55:31,256 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2
datanode_1          | 2023-07-16 04:55:31,257 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-LeaderElection2] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState
datanode_1          | 2023-07-16 04:55:31,370 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B13B9B9CADC2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ab68bb5-b826-4da3-98c0-b13b9b9cadc2/current/log_inprogress_0
datanode_1          | 2023-07-16 04:55:35,300 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState] INFO impl.FollowerState: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5015806860ns, electionTimeout:5008ms
datanode_1          | 2023-07-16 04:55:35,301 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState
datanode_1          | 2023-07-16 04:55:35,301 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:55:35,301 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 04:55:35,301 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-FollowerState] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3
datanode_1          | 2023-07-16 04:55:35,310 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:55:35,336 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:55:35,336 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.LeaderElection:   Response 0: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t1
datanode_1          | 2023-07-16 04:55:35,337 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.LeaderElection: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-07-16 04:55:35,337 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3
datanode_1          | 2023-07-16 04:55:35,337 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:55:32,152 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO impl.LeaderElection: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null
datanode_3          | 2023-07-16 04:55:32,153 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO impl.LeaderElection: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-16 04:55:32,155 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1
datanode_3          | 2023-07-16 04:55:32,156 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:55:32,156 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5B824CF0FE6A with new leaderId: 96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:32,157 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: change Leader from null to 96fc65c5-22fc-48dd-984b-d88286c71717 at term 1 for becomeLeader, leader elected after 5855ms
datanode_3          | 2023-07-16 04:55:32,167 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:55:32,187 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A
datanode_3          | 2023-07-16 04:55:32,201 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:55:32,201 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-16 04:55:32,215 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:55:32,215 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:55:32,216 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:55:32,241 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderStateImpl
datanode_3          | 2023-07-16 04:55:32,275 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:55:32,328 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-LeaderElection1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A: set configuration 0: [96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:55:32,410 [96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-5B824CF0FE6A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/779c010a-321c-47c6-b40f-5b824cf0fe6a/current/log_inprogress_0
datanode_3          | 2023-07-16 04:55:35,316 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: receive requestVote(ELECTION, 678e2808-f01e-45ca-8f6c-3b0a4669caa8, group-B076E8F2FF81, 1, (t:0, i:0))
datanode_3          | 2023-07-16 04:55:35,316 [grpc-default-executor-1] INFO impl.VoteContext: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FOLLOWER: accept ELECTION from 678e2808-f01e-45ca-8f6c-3b0a4669caa8: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-16 04:55:35,316 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_3          | 2023-07-16 04:55:35,317 [grpc-default-executor-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FollowerState
datanode_3          | 2023-07-16 04:55:35,317 [grpc-default-executor-1] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FollowerState
datanode_3          | 2023-07-16 04:55:35,317 [96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FollowerState] INFO impl.FollowerState: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-16 04:55:35,339 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81 replies to ELECTION vote request: 678e2808-f01e-45ca-8f6c-3b0a4669caa8<-96fc65c5-22fc-48dd-984b-d88286c71717#0:OK-t1. Peer's state: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81:t1, leader=null, voted=678e2808-f01e-45ca-8f6c-3b0a4669caa8, raftlog=96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:55:35,403 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B076E8F2FF81 with new leaderId: 678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_3          | 2023-07-16 04:55:35,403 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: change Leader from null to 678e2808-f01e-45ca-8f6c-3b0a4669caa8 at term 1 for appendEntries, leader elected after 5179ms
datanode_3          | 2023-07-16 04:55:35,426 [grpc-default-executor-1] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-16 04:55:35,337 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B076E8F2FF81 with new leaderId: 678e2808-f01e-45ca-8f6c-3b0a4669caa8
datanode_1          | 2023-07-16 04:55:35,337 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: change Leader from null to 678e2808-f01e-45ca-8f6c-3b0a4669caa8 at term 1 for becomeLeader, leader elected after 5129ms
datanode_1          | 2023-07-16 04:55:35,338 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:55:35,338 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:35,340 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:55:35,340 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-07-16 04:55:35,340 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:55:35,341 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:55:35,342 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:55:35,368 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:55:35,369 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:35,369 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:55:35,372 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:55:35,373 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:55:35,373 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:55:35,373 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81
datanode_1          | 2023-07-16 04:55:35,378 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:55:35,379 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:55:35,379 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:55:35,379 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:55:35,380 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:55:35,380 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:55:35,382 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderStateImpl
datanode_1          | 2023-07-16 04:55:35,382 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:55:35,385 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-LeaderElection3] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-16 04:55:35,385 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-B076E8F2FF81-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/current/log_inprogress_0
datanode_1          | 2023-07-16 04:55:36,362 [grpc-default-executor-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: receive requestVote(ELECTION, 96fc65c5-22fc-48dd-984b-d88286c71717, group-36C0689D72BE, 2, (t:0, i:0))
datanode_1          | 2023-07-16 04:55:36,364 [grpc-default-executor-1] INFO impl.VoteContext: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FOLLOWER: accept ELECTION from 96fc65c5-22fc-48dd-984b-d88286c71717: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-16 04:55:36,364 [grpc-default-executor-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:96fc65c5-22fc-48dd-984b-d88286c71717
datanode_1          | 2023-07-16 04:55:36,365 [grpc-default-executor-1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: shutdown 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState
datanode_1          | 2023-07-16 04:55:36,365 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-16 04:55:36,366 [grpc-default-executor-1] INFO impl.RoleInfo: 678e2808-f01e-45ca-8f6c-3b0a4669caa8: start 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-FollowerState
datanode_3          | 2023-07-16 04:55:35,426 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:55:35,429 [96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-B076E8F2FF81-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0c624948-5297-45ad-b3be-b076e8f2ff81/current/log_inprogress_0
datanode_3          | 2023-07-16 04:55:36,298 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO impl.FollowerState: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5157316662ns, electionTimeout:5155ms
datanode_3          | 2023-07-16 04:55:36,298 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState
datanode_3          | 2023-07-16 04:55:36,299 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-16 04:55:36,300 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 04:55:36,301 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-FollowerState] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2
datanode_3          | 2023-07-16 04:55:36,308 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:55:36,404 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 04:55:36,404 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection:   Response 0: 96fc65c5-22fc-48dd-984b-d88286c71717<-4059d340-ca55-4574-83cb-35af8dac7525#0:OK-t2
datanode_3          | 2023-07-16 04:55:36,405 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.LeaderElection: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-07-16 04:55:36,405 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: shutdown 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2
datanode_3          | 2023-07-16 04:55:36,405 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-07-16 04:55:36,405 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-36C0689D72BE with new leaderId: 96fc65c5-22fc-48dd-984b-d88286c71717
datanode_3          | 2023-07-16 04:55:36,407 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: change Leader from null to 96fc65c5-22fc-48dd-984b-d88286c71717 at term 2 for becomeLeader, leader elected after 9152ms
datanode_3          | 2023-07-16 04:55:36,407 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:55:36,408 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_3          | 2023-07-16 04:55:36,408 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:55:36,408 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-07-16 04:55:36,409 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:55:36,409 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:55:36,409 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:55:36,447 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:55:36,447 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:36,458 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:55:36,461 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:55:36,464 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:55:36,464 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:55:36,465 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE
datanode_3          | 2023-07-16 04:55:36,470 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:55:36,470 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:55:36,470 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:55:36,470 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:55:36,470 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:55:36,471 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:55:36,473 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO impl.RoleInfo: 96fc65c5-22fc-48dd-984b-d88286c71717: start 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderStateImpl
datanode_3          | 2023-07-16 04:55:36,474 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:55:36,477 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/current/log_inprogress_0
datanode_3          | 2023-07-16 04:55:36,519 [96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE-LeaderElection2] INFO server.RaftServer$Division: 96fc65c5-22fc-48dd-984b-d88286c71717@group-36C0689D72BE: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-16 04:55:36,412 [grpc-default-executor-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE replies to ELECTION vote request: 96fc65c5-22fc-48dd-984b-d88286c71717<-678e2808-f01e-45ca-8f6c-3b0a4669caa8#0:OK-t2. Peer's state: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE:t2, leader=null, voted=96fc65c5-22fc-48dd-984b-d88286c71717, raftlog=678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:55:36,535 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-36C0689D72BE with new leaderId: 96fc65c5-22fc-48dd-984b-d88286c71717
datanode_1          | 2023-07-16 04:55:36,537 [grpc-default-executor-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: change Leader from null to 96fc65c5-22fc-48dd-984b-d88286c71717 at term 2 for appendEntries, leader elected after 10984ms
datanode_1          | 2023-07-16 04:55:36,633 [grpc-default-executor-1] INFO server.RaftServer$Division: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE: set configuration 0: [678e2808-f01e-45ca-8f6c-3b0a4669caa8|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 96fc65c5-22fc-48dd-984b-d88286c71717|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, 4059d340-ca55-4574-83cb-35af8dac7525|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-16 04:55:36,637 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:55:36,643 [678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 678e2808-f01e-45ca-8f6c-3b0a4669caa8@group-36C0689D72BE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8776ef61-cd93-4ea2-af55-36c0689d72be/current/log_inprogress_0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:54:47,978 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8f4648fda50d/172.21.0.6
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-07-16 04:54:48,050 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:54:58,200 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:54:59,151 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.6:9862
om_1                | 2023-07-16 04:54:59,151 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:54:59,164 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:54:59,258 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:55:02,339 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:03,348 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:04,349 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:05,350 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:06,353 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:07,354 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:08,356 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:09,368 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:10,369 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:11,371 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 04:55:11,374 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-4567e335-3963-41d3-8d19-a06cee7833df;layoutVersion=0
om_1                | 2023-07-16 04:55:21,284 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 8f4648fda50d/172.21.0.6
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:55:25,244 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8f4648fda50d/172.21.0.6
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-07-16 04:55:25,289 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:55:31,254 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:55:31,500 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.6:9862
om_1                | 2023-07-16 04:55:31,505 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:55:31,505 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:55:31,528 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:55:31,553 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:55:32,919 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:55:33,098 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 04:55:33,099 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 04:55:33,317 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-16 04:55:33,330 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 04:55:33,331 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-16 04:55:33,360 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-16 04:55:33,438 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 04:55:33,471 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872
om_1                | 2023-07-16 04:55:33,482 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-16 04:55:33,517 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-16 04:55:33,595 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-07-16 04:55:33,596 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:55:33,598 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-07-16 04:55:33,598 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:55:33,598 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:55:33,599 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-16 04:55:33,605 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:55:33,605 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-16 04:55:33,606 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 04:54:46,202 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = f2ffcbc28977/172.21.0.12
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.1.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.1.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | STARTUP_MSG:   java = 11.0.10
recon_1             | ************************************************************/
recon_1             | 2023-07-16 04:54:46,272 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-16 04:54:51,134 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-07-16 04:54:53,643 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-16 04:54:55,663 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:55:04,433 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:55:06,953 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:55:07,132 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:55:07,134 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-16 04:55:14,017 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-16 04:55:14,200 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-16 04:55:14,263 [main] INFO util.log: Logging initialized @33657ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-16 04:55:15,651 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-16 04:55:15,666 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-16 04:55:15,771 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-16 04:55:15,808 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-16 04:55:15,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-16 04:55:15,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-16 04:55:16,797 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-16 04:55:18,170 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-16 04:55:18,189 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-16 04:55:18,302 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-16 04:55:18,302 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-16 04:55:18,760 [main] INFO Configuration.deprecation: No unit for ozone.recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-07-16 04:55:19,402 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:55:19,724 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:55:19,816 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
recon_1             | 2023-07-16 04:55:19,835 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-16 04:55:20,191 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:55:20,341 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-16 04:55:20,393 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-16 04:55:20,396 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
om_1                | 2023-07-16 04:55:33,856 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-16 04:55:33,858 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:55:33,858 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:55:33,870 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:55:33,878 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@3f598450[Not completed]
om_1                | 2023-07-16 04:55:33,878 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-16 04:55:33,919 [pool-17-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-16 04:55:33,930 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 04:55:33,931 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-16 04:55:33,931 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-16 04:55:33,931 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-16 04:55:33,932 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:55:33,932 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:55:33,932 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 04:55:33,933 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-16 04:55:33,941 [pool-17-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-16 04:55:33,942 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:55:33,947 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-16 04:55:33,949 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-16 04:55:33,961 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 04:55:33,968 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@8f4648fda50d
om_1                | 2023-07-16 04:55:33,997 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-16 04:55:34,000 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-16 04:55:34,003 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-16 04:55:34,015 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-16 04:55:34,018 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:55:34,035 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-C5BA1605619E
om_1                | 2023-07-16 04:55:34,062 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:55:34,083 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-16 04:55:34,083 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-16 04:55:34,089 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-16 04:55:34,093 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:55:34,093 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-16 04:55:34,095 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:55:34,115 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-16 04:55:34,116 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-16 04:55:34,117 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-16 04:55:34,118 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 04:55:34,119 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 04:55:34,128 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-16 04:55:34,129 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-16 04:55:34,136 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:55:34,136 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:55:34,142 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 04:55:34,143 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-16 04:55:34,144 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-16 04:55:34,145 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-16 04:55:34,146 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-16 04:55:34,146 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-16 04:55:34,190 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-C5BA1605619E
om_1                | 2023-07-16 04:55:34,201 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-16 04:55:34,204 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-C5BA1605619E
recon_1             | 2023-07-16 04:55:20,460 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 04:55:20,514 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-16 04:55:20,573 [Listener at 0.0.0.0/9891] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1             | 2023-07-16 04:55:20,637 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-16 04:55:20,637 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-16 04:55:21,193 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-16 04:55:21,260 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 04:55:21,260 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-16 04:55:22,355 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-16 04:55:22,365 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1             | 2023-07-16 04:55:22,497 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 04:55:22,497 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-16 04:55:22,527 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-07-16 04:55:22,598 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7bc6935c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 04:55:22,608 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47248a48{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 04:55:28,491 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@33e8694b{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_1_0_jar-_-any-11375280614785658731/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/recon}
recon_1             | 2023-07-16 04:55:28,536 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6418e39e{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-16 04:55:28,540 [Listener at 0.0.0.0/9891] INFO server.Server: Started @47934ms
recon_1             | 2023-07-16 04:55:28,550 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-16 04:55:28,550 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 04:55:28,562 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 04:55:28,564 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-16 04:55:28,643 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-16 04:55:28,702 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 04:55:28,713 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-16 04:55:28,713 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:55:28,717 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-16 04:55:28,720 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:55:30,549 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-07-16 04:55:30,552 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 04:55:30,552 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=34f3b6b3-6b48-425a-9038-932902a73f74 from SCM.
recon_1             | 2023-07-16 04:55:30,584 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 34f3b6b3-6b48-425a-9038-932902a73f74, Nodes: 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:4059d340-ca55-4574-83cb-35af8dac7525, CreationTimestamp2023-07-16T04:55:23.562Z]
recon_1             | 2023-07-16 04:55:30,628 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be from SCM.
recon_1             | 2023-07-16 04:55:30,635 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8776ef61-cd93-4ea2-af55-36c0689d72be, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.610Z]
recon_1             | 2023-07-16 04:55:30,653 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=2ab68bb5-b826-4da3-98c0-b13b9b9cadc2 from SCM.
recon_1             | 2023-07-16 04:55:30,655 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2ab68bb5-b826-4da3-98c0-b13b9b9cadc2, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:678e2808-f01e-45ca-8f6c-3b0a4669caa8, CreationTimestamp2023-07-16T04:55:22.161Z]
recon_1             | 2023-07-16 04:55:30,660 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=779c010a-321c-47c6-b40f-5b824cf0fe6a from SCM.
recon_1             | 2023-07-16 04:55:30,660 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 779c010a-321c-47c6-b40f-5b824cf0fe6a, Nodes: 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:96fc65c5-22fc-48dd-984b-d88286c71717, CreationTimestamp2023-07-16T04:55:23.020Z]
recon_1             | 2023-07-16 04:55:30,661 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 from SCM.
recon_1             | 2023-07-16 04:55:30,668 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 0c624948-5297-45ad-b3be-b076e8f2ff81, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.631Z]
recon_1             | 2023-07-16 04:55:30,668 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:55:30,685 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-16 04:55:30,687 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-16 04:55:30,915 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.10:34014
recon_1             | 2023-07-16 04:55:30,915 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.11:34132
recon_1             | 2023-07-16 04:55:30,957 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-16 04:55:30,958 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-16 04:55:31,065 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-16 04:55:31,065 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-16 04:55:31,054 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-07-16 04:55:31,077 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 106 milliseconds.
recon_1             | 2023-07-16 04:55:31,515 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 411 milliseconds to process 0 existing database records.
recon_1             | 2023-07-16 04:55:31,609 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 89 milliseconds for processing 0 containers.
recon_1             | 2023-07-16 04:55:31,771 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.11:34148: output error
recon_1             | 2023-07-16 04:55:31,771 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.10:34026: output error
recon_1             | 2023-07-16 04:55:31,775 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.4:58392: output error
recon_1             | 2023-07-16 04:55:31,775 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-16 04:55:31,809 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.4:58400: output error
recon_1             | 2023-07-16 04:55:31,809 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-16 04:55:31,776 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
om_1                | 2023-07-16 04:55:34,218 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 04:55:34,221 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-16 04:55:34,252 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.21.0.6:9862
om_1                | 2023-07-16 04:55:34,252 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-16 04:55:34,254 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-16 04:55:34,256 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-16 04:55:34,257 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:55:34,259 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-16 04:55:34,261 [Listener at om/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-C5BA1605619E
om_1                | 2023-07-16 04:55:34,267 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-16 04:55:34,315 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-16 04:55:34,323 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$354/0x0000000840484440@c1050f2] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-16 04:55:34,353 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-16 04:55:34,353 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 04:55:34,376 [Listener at om/9862] INFO util.log: Logging initialized @12290ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 04:55:34,470 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-16 04:55:34,473 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-16 04:55:34,478 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-16 04:55:34,479 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-16 04:55:34,480 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-16 04:55:34,480 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-16 04:55:34,513 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-16 04:55:34,514 [Listener at om/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om_1                | 2023-07-16 04:55:34,542 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-16 04:55:34,543 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-16 04:55:34,544 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-07-16 04:55:34,559 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c25cfe1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 04:55:34,561 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a083b96{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-16 04:55:34,938 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@740a0d5e{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_1_0_jar-_-any-17181931649722193916/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-16 04:55:34,950 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7c96c85{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-16 04:55:34,951 [Listener at om/9862] INFO server.Server: Started @12865ms
om_1                | 2023-07-16 04:55:34,958 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-16 04:55:34,958 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-16 04:55:34,961 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-16 04:55:34,966 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-16 04:55:34,981 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-16 04:55:35,018 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-07-16 04:55:35,025 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5c313224] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-16 04:55:39,269 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5011841183ns, electionTimeout:5010ms
om_1                | 2023-07-16 04:55:39,275 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:55:39,276 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-16 04:55:39,279 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-16 04:55:39,279 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:55:39,296 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-16 04:55:39,297 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-16 04:55:39,297 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:55:39,304 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-16 04:55:39,305 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5304ms
om_1                | 2023-07-16 04:55:39,312 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-16 04:55:39,330 [om1@group-C5BA1605619E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-C5BA1605619E
om_1                | 2023-07-16 04:55:39,350 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 04:55:39,351 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:55:39,371 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-16 04:55:39,372 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-16 04:55:39,373 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-16 04:55:39,402 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-16 04:55:39,453 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-16 04:55:39,577 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-07-16 04:55:39,641 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-16 04:55:40,203 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-16 04:56:29,175 [qtp628402659-39] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-16 04:56:29,227 [qtp628402659-39] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483389180 in 46 milliseconds
om_1                | 2023-07-16 04:56:29,268 [qtp628402659-39] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 39 milliseconds
om_1                | 2023-07-16 04:56:29,268 [qtp628402659-39] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483389180
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-16 04:55:31,776 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-07-16 04:55:32,194 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/96fc65c5-22fc-48dd-984b-d88286c71717
recon_1             | 2023-07-16 04:55:32,199 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:32,214 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 96fc65c5-22fc-48dd-984b-d88286c71717 to Node DB.
recon_1             | 2023-07-16 04:55:32,221 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be reported by 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:32,221 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 reported by 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:33,049 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4059d340-ca55-4574-83cb-35af8dac7525
recon_1             | 2023-07-16 04:55:33,050 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:33,050 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 4059d340-ca55-4574-83cb-35af8dac7525 to Node DB.
recon_1             | 2023-07-16 04:55:33,052 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be reported by 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:33,052 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 reported by 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:35,356 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678e2808-f01e-45ca-8f6c-3b0a4669caa8
recon_1             | 2023-07-16 04:55:35,357 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:35,358 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 678e2808-f01e-45ca-8f6c-3b0a4669caa8 to Node DB.
recon_1             | 2023-07-16 04:55:35,358 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be reported by 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:35,359 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 reported by 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:35,359 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 0c624948-5297-45ad-b3be-b076e8f2ff81, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:678e2808-f01e-45ca-8f6c-3b0a4669caa8, CreationTimestamp2023-07-16T04:55:23.631Z] moved to OPEN state
recon_1             | 2023-07-16 04:55:36,455 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be reported by 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:55:36,456 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 8776ef61-cd93-4ea2-af55-36c0689d72be, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:96fc65c5-22fc-48dd-984b-d88286c71717, CreationTimestamp2023-07-16T04:55:23.610Z] moved to OPEN state
recon_1             | 2023-07-16 04:55:42,616 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-16 04:55:42,655 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 04:55:54,192 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-07-16 04:55:54,211 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-16 04:56:05,427 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 04:56:05,439 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-07-16 04:56:28,719 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-16 04:56:28,720 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-16 04:56:29,445 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689483388720
recon_1             | 2023-07-16 04:56:29,496 [pool-14-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 04:56:29,498 [pool-14-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 04:56:29,571 [pool-14-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689483388720.
recon_1             | 2023-07-16 04:56:29,646 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-16 04:56:29,909 [pool-15-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-16 04:56:29,914 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:56:29,950 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1689483389919
recon_1             | 2023-07-16 04:56:29,952 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container key DB at /data/metadata/recon/recon-container-key.db_1689483294027.
recon_1             | 2023-07-16 04:56:30,111 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:56:30,111 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.197 seconds to process 4 keys.
recon_1             | 2023-07-16 04:56:30,150 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-16 04:56:30,187 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-16 04:54:48,997 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-16 04:54:48,997 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-16 04:54:49,218 [main] INFO util.log: Logging initialized @8585ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-16 04:54:49,822 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-16 04:54:49,984 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-16 04:54:50,002 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-16 04:54:50,021 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-16 04:54:50,023 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-16 04:54:50,024 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-16 04:54:51,257 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 52289cb0be6e/172.21.0.13
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.1.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
s3g_1               | STARTUP_MSG:   java = 11.0.10
s3g_1               | ************************************************************/
s3g_1               | 2023-07-16 04:54:51,298 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-16 04:54:51,510 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-16 04:54:51,600 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-16 04:54:51,632 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1               | 2023-07-16 04:54:51,887 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-16 04:54:51,887 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-16 04:54:51,891 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-07-16 04:54:51,970 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@19b843ba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 04:54:51,981 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b0d80ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 16, 2023 4:55:18 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-16 04:55:18,555 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7fe8c7db{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_1_0_jar-_-any-17125376606924173999/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/s3gateway}
s3g_1               | 2023-07-16 04:55:18,582 [main] INFO server.AbstractConnector: Started ServerConnector@51c693d{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-16 04:55:18,582 [main] INFO server.Server: Started @37949ms
s3g_1               | 2023-07-16 04:55:18,589 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:54:46,933 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 687bdf2ea3b3/172.21.0.2
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:54:46,974 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:54:47,279 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:54:47,381 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-4567e335-3963-41d3-8d19-a06cee7833df;layoutVersion=0
scm_1               | 2023-07-16 04:54:47,403 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 687bdf2ea3b3/172.21.0.2
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:54:57,315 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 687bdf2ea3b3/172.21.0.2
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:54:57,363 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:54:58,006 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:54:59,671 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:55:00,185 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
scm_1               | 2023-07-16 04:55:00,207 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-16 04:55:01,053 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-16 04:55:02,595 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-16 04:55:02,683 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-16 04:55:02,691 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1               | 2023-07-16 04:55:02,783 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-16 04:55:03,039 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:55:03,052 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 04:55:10,870 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:55:11,317 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 04:55:11,689 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:55:11,708 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 04:55:12,168 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:55:12,169 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-16 04:55:12,267 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:55:12,988 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 04:55:13,316 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 04:55:13,316 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-16 04:55:16,320 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:55:16,330 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:55:16,342 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-16 04:55:17,368 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:55:17,401 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:55:17,425 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:55:17,448 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-16 04:55:18,456 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:55:18,472 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:55:18,513 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:55:18,514 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-16 04:55:18,693 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1d540566] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-16 04:55:18,805 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-16 04:55:18,810 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-16 04:55:18,867 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @30641ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-16 04:55:20,455 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-16 04:55:20,528 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 04:55:20,547 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 04:55:20,548 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 04:55:20,576 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 04:55:20,576 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 04:55:21,184 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-16 04:55:21,208 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm_1               | 2023-07-16 04:55:21,798 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-16 04:55:21,849 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 04:55:21,860 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-16 04:55:21,925 [IPC Server handler 90 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678e2808-f01e-45ca-8f6c-3b0a4669caa8
scm_1               | 2023-07-16 04:55:22,008 [IPC Server handler 90 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:55:22,059 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:55:22,066 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:55:22,137 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d5c04f9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 04:55:22,137 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:22,161 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61cd1c71{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 04:55:22,167 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2ab68bb5-b826-4da3-98c0-b13b9b9cadc2 to datanode:678e2808-f01e-45ca-8f6c-3b0a4669caa8
scm_1               | 2023-07-16 04:55:22,271 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2ab68bb5-b826-4da3-98c0-b13b9b9cadc2, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:22.161425Z]
scm_1               | 2023-07-16 04:55:23,004 [IPC Server handler 90 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/96fc65c5-22fc-48dd-984b-d88286c71717
scm_1               | 2023-07-16 04:55:23,005 [IPC Server handler 90 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:55:23,017 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:55:23,054 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:23,054 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=779c010a-321c-47c6-b40f-5b824cf0fe6a to datanode:96fc65c5-22fc-48dd-984b-d88286c71717
scm_1               | 2023-07-16 04:55:23,017 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:55:23,069 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 779c010a-321c-47c6-b40f-5b824cf0fe6a, Nodes: 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.020303Z]
scm_1               | 2023-07-16 04:55:23,547 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4059d340-ca55-4574-83cb-35af8dac7525
scm_1               | 2023-07-16 04:55:23,563 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=34f3b6b3-6b48-425a-9038-932902a73f74 to datanode:4059d340-ca55-4574-83cb-35af8dac7525
scm_1               | 2023-07-16 04:55:23,563 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:55:23,568 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 34f3b6b3-6b48-425a-9038-932902a73f74, Nodes: 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.562542Z]
scm_1               | 2023-07-16 04:55:23,570 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:55:23,570 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:23,570 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:55:23,594 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:55:23,600 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-16 04:55:23,610 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be to datanode:678e2808-f01e-45ca-8f6c-3b0a4669caa8
scm_1               | 2023-07-16 04:55:23,611 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be to datanode:4059d340-ca55-4574-83cb-35af8dac7525
scm_1               | 2023-07-16 04:55:23,611 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be to datanode:96fc65c5-22fc-48dd-984b-d88286c71717
scm_1               | 2023-07-16 04:55:23,630 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8776ef61-cd93-4ea2-af55-36c0689d72be, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.610874Z]
scm_1               | 2023-07-16 04:55:23,631 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 to datanode:678e2808-f01e-45ca-8f6c-3b0a4669caa8
scm_1               | 2023-07-16 04:55:23,661 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 to datanode:96fc65c5-22fc-48dd-984b-d88286c71717
scm_1               | 2023-07-16 04:55:23,662 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 to datanode:4059d340-ca55-4574-83cb-35af8dac7525
scm_1               | 2023-07-16 04:55:23,663 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 0c624948-5297-45ad-b3be-b076e8f2ff81, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:55:23.631022Z]
scm_1               | 2023-07-16 04:55:23,684 [RatisPipelineUtilsThread] INFO pipeline.SCMPipelineManager: Pipeline: PipelineID=0c624948-5297-45ad-b3be-b076e8f2ff81 contains same datanodes as previous pipelines: PipelineID=8776ef61-cd93-4ea2-af55-36c0689d72be nodeIds: 678e2808-f01e-45ca-8f6c-3b0a4669caa8, 96fc65c5-22fc-48dd-984b-d88286c71717, 4059d340-ca55-4574-83cb-35af8dac7525
scm_1               | 2023-07-16 04:55:24,544 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@37b52340{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_1_0_jar-_-any-3012873660933314157/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/scm}
scm_1               | 2023-07-16 04:55:24,636 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@6981f8f3{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-16 04:55:24,636 [Listener at 0.0.0.0/9860] INFO server.Server: Started @36412ms
scm_1               | 2023-07-16 04:55:24,648 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-16 04:55:24,648 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-16 04:55:24,654 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 04:55:24,911 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:24,916 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2ab68bb5-b826-4da3-98c0-b13b9b9cadc2, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:678e2808-f01e-45ca-8f6c-3b0a4669caa8, CreationTimestamp2023-07-16T04:55:22.161425Z] moved to OPEN state
scm_1               | 2023-07-16 04:55:25,022 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:26,383 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:26,383 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 779c010a-321c-47c6-b40f-5b824cf0fe6a, Nodes: 96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:96fc65c5-22fc-48dd-984b-d88286c71717, CreationTimestamp2023-07-16T04:55:23.020303Z] moved to OPEN state
scm_1               | 2023-07-16 04:55:26,395 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:26,877 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:26,890 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 34f3b6b3-6b48-425a-9038-932902a73f74, Nodes: 4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:4059d340-ca55-4574-83cb-35af8dac7525, CreationTimestamp2023-07-16T04:55:23.562542Z] moved to OPEN state
scm_1               | 2023-07-16 04:55:26,891 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:31,768 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:31,772 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:31,772 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:31,775 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:31,884 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:31,884 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:32,175 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:32,175 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:33,040 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:33,041 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:35,348 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:55:35,349 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 0c624948-5297-45ad-b3be-b076e8f2ff81, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:678e2808-f01e-45ca-8f6c-3b0a4669caa8, CreationTimestamp2023-07-16T04:55:23.631022Z] moved to OPEN state
scm_1               | 2023-07-16 04:55:35,349 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:35,349 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:55:35,350 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:55:35,350 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-16 04:55:35,350 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-16 04:55:36,436 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 8776ef61-cd93-4ea2-af55-36c0689d72be, Nodes: 678e2808-f01e-45ca-8f6c-3b0a4669caa8{ip: 172.21.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4059d340-ca55-4574-83cb-35af8dac7525{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}96fc65c5-22fc-48dd-984b-d88286c71717{ip: 172.21.0.10, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:96fc65c5-22fc-48dd-984b-d88286c71717, CreationTimestamp2023-07-16T04:55:23.610874Z] moved to OPEN state
scm_1               | 2023-07-16 04:56:14,631 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-16 04:56:26,486 [IPC Server handler 38 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-16 04:57:18,512 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
scm_1               | 2023-07-16 04:57:29,502 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.7
Attaching to xcompat_old_client_1_0_0_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_recon_1, xcompat_old_client_1_1_0_1, xcompat_new_client_1, xcompat_datanode_1, xcompat_old_client_1_2_1_1, xcompat_datanode_2, xcompat_datanode_3, xcompat_scm_1, xcompat_s3g_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 04:57:54,854 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 25d68a9029e1/172.22.0.7
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.2.1
datanode_1          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_1          | STARTUP_MSG:   java = 11.0.13
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 04:57:54,931 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 04:57:57,171 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 04:57:57,726 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 04:57:58,596 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 04:57:58,606 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-16 04:58:00,003 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:25d68a9029e1 ip:172.22.0.7
datanode_1          | 2023-07-16 04:58:01,717 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_1          | 2023-07-16 04:58:02,983 [main] INFO reflections.Reflections: Reflections took 1092 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_1          | 2023-07-16 04:58:05,060 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 04:58:05,163 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-16 04:58:05,189 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 04:58:05,190 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 04:58:05,407 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:58:05,521 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:58:05,522 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-16 04:58:05,565 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-16 04:58:05,566 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-16 04:58:05,566 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-16 04:58:05,753 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 04:58:05,754 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-16 04:58:15,768 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 04:58:16,304 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 04:58:17,704 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-16 04:58:17,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-16 04:58:17,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-16 04:58:17,716 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 04:58:17,726 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:17,730 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 04:58:17,731 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:58:19,633 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-16 04:58:19,690 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:19,691 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:58:19,869 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:58:21,191 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:58:21,443 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 04:58:21,854 [main] INFO util.log: Logging initialized @34742ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 04:58:23,036 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-07-16 04:58:23,093 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 04:58:23,126 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 04:58:23,143 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 04:58:23,143 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 04:58:23,143 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 04:58:23,506 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-16 04:58:23,531 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_1          | 2023-07-16 04:58:23,856 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 04:58:23,880 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 04:58:23,882 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-16 04:58:24,171 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@629a9f26{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 04:58:24,245 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c40ffef{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 04:58:26,273 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@236ae13d{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-10676748816356727223/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 04:57:53,194 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 6efab504ad57/172.22.0.6
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.2.1
datanode_2          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_2          | STARTUP_MSG:   java = 11.0.13
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 04:57:53,277 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 04:57:55,581 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 04:57:56,291 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 04:57:57,617 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 04:57:57,617 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-16 04:57:58,724 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6efab504ad57 ip:172.22.0.6
datanode_2          | 2023-07-16 04:58:00,642 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_2          | 2023-07-16 04:58:01,907 [main] INFO reflections.Reflections: Reflections took 1096 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_2          | 2023-07-16 04:58:03,512 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 04:58:03,519 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-16 04:58:03,550 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 04:58:03,558 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 04:58:03,786 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:58:03,985 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:58:04,018 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-16 04:58:04,063 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-16 04:58:04,064 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-16 04:58:04,082 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-16 04:58:04,445 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 04:58:04,448 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-16 04:58:14,626 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 04:58:15,224 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 04:58:16,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-16 04:58:16,307 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-16 04:58:16,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-16 04:58:16,313 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 04:58:16,318 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:58:16,319 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 04:58:16,320 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 04:58:18,431 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-16 04:58:18,433 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:18,447 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:58:18,492 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:58:19,785 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:58:20,015 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 04:58:20,323 [main] INFO util.log: Logging initialized @35314ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 04:58:21,038 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-07-16 04:58:21,075 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 04:58:21,152 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 04:58:21,164 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 04:58:21,192 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 04:58:21,201 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 04:58:21,906 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-16 04:58:21,951 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_2          | 2023-07-16 04:58:22,320 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 04:58:22,320 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 04:58:22,346 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-07-16 04:58:22,443 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a1b8a46{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 04:58:22,444 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@40d52be7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 04:58:24,886 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@45a1d057{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-12863648692975576959/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 04:58:24,931 [main] INFO server.AbstractConnector: Started ServerConnector@322e49ee{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-16 04:58:24,932 [main] INFO server.Server: Started @39923ms
datanode_1          | 2023-07-16 04:58:26,362 [main] INFO server.AbstractConnector: Started ServerConnector@19489b27{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-16 04:58:26,364 [main] INFO server.Server: Started @39252ms
datanode_1          | 2023-07-16 04:58:26,383 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-16 04:58:26,383 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 04:58:26,386 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 04:58:26,404 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-16 04:58:26,864 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1cd4a594] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-16 04:58:27,439 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.9:9891
datanode_1          | 2023-07-16 04:58:27,775 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 04:58:30,336 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:58:31,338 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 04:58:32,361 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 04:58:32,365 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-16 04:58:32,742 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:32,844 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: c231805e-370f-4608-92fd-d33256063154: start RPC server
datanode_1          | 2023-07-16 04:58:32,854 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: c231805e-370f-4608-92fd-d33256063154: GrpcService started, listening on 9856
datanode_1          | 2023-07-16 04:58:32,867 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: c231805e-370f-4608-92fd-d33256063154: GrpcService started, listening on 9857
datanode_1          | 2023-07-16 04:58:32,876 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: c231805e-370f-4608-92fd-d33256063154: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 04:58:32,908 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c231805e-370f-4608-92fd-d33256063154 is started using port 9858 for RATIS
datanode_1          | 2023-07-16 04:58:32,908 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c231805e-370f-4608-92fd-d33256063154 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 04:58:32,909 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c231805e-370f-4608-92fd-d33256063154 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 04:58:32,909 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@af11559] INFO util.JvmPauseMonitor: JvmPauseMonitor-c231805e-370f-4608-92fd-d33256063154: Started
datanode_1          | 2023-07-16 04:58:34,932 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-16 04:58:36,347 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 25d68a9029e1/172.22.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.7:35094 remote=recon/172.22.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_2          | 2023-07-16 04:58:24,940 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 04:58:24,940 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-16 04:58:24,948 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 04:58:24,956 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-16 04:58:25,273 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@733195a7] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-16 04:58:25,644 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.9:9891
datanode_2          | 2023-07-16 04:58:25,969 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 04:58:28,708 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:58:29,709 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:58:30,710 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 04:58:32,350 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-16 04:58:32,360 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-16 04:58:32,784 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_2          | 2023-07-16 04:58:32,949 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start RPC server
datanode_2          | 2023-07-16 04:58:32,982 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 718f9ff6-0a54-459e-b595-cb309db1e2fa: GrpcService started, listening on 9856
datanode_2          | 2023-07-16 04:58:33,009 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 718f9ff6-0a54-459e-b595-cb309db1e2fa: GrpcService started, listening on 9857
datanode_2          | 2023-07-16 04:58:33,033 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 718f9ff6-0a54-459e-b595-cb309db1e2fa: GrpcService started, listening on 9858
datanode_2          | 2023-07-16 04:58:33,052 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 718f9ff6-0a54-459e-b595-cb309db1e2fa is started using port 9858 for RATIS
datanode_2          | 2023-07-16 04:58:33,053 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 718f9ff6-0a54-459e-b595-cb309db1e2fa is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-16 04:58:33,053 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 718f9ff6-0a54-459e-b595-cb309db1e2fa is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-16 04:58:33,054 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@790b1db2] INFO util.JvmPauseMonitor: JvmPauseMonitor-718f9ff6-0a54-459e-b595-cb309db1e2fa: Started
datanode_2          | 2023-07-16 04:58:35,292 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-07-16 04:58:35,726 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 6efab504ad57/172.22.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.6:39892 remote=recon/172.22.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.6:39892 remote=recon/172.22.0.9:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_2          | 2023-07-16 04:58:36,337 [Command processor thread] INFO server.RaftServer: 718f9ff6-0a54-459e-b595-cb309db1e2fa: addNew group-F01BA3D94F1C:[718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] returns group-F01BA3D94F1C:java.util.concurrent.CompletableFuture@1c9d3857[Not completed]
datanode_2          | 2023-07-16 04:58:36,380 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa: new RaftServerImpl for group-F01BA3D94F1C:[718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:58:36,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:58:36,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:58:36,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:58:36,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:36,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:58:36,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:58:36,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:58:36,401 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: ConfigurationManager, init=-1: [718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:58:36,406 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:58:36,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:58:36,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:58:36,418 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4dadd573-31b0-4101-9a17-f01ba3d94f1c does not exist. Creating ...
datanode_2          | 2023-07-16 04:58:36,432 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4dadd573-31b0-4101-9a17-f01ba3d94f1c/in_use.lock acquired by nodename 7@6efab504ad57
datanode_2          | 2023-07-16 04:58:36,466 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4dadd573-31b0-4101-9a17-f01ba3d94f1c has been successfully formatted.
datanode_2          | 2023-07-16 04:58:36,489 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-F01BA3D94F1C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:58:36,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:36,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:58:36,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:58:36,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:58:36,558 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:36,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:58:36,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:58:36,645 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4dadd573-31b0-4101-9a17-f01ba3d94f1c
datanode_2          | 2023-07-16 04:58:36,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:58:36,650 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:58:36,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:36,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:58:36,659 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:58:36,665 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:58:36,666 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:58:36,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:58:36,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:36,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:36,769 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:36,770 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:36,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:36,820 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:58:36,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:58:36,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:58:36,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:58:36,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:58:36,926 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: start as a follower, conf=-1: [718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:58:36,932 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:58:36,936 [pool-22-thread-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState
datanode_2          | 2023-07-16 04:58:36,961 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F01BA3D94F1C,id=718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_2          | 2023-07-16 04:58:37,015 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=4dadd573-31b0-4101-9a17-f01ba3d94f1c
datanode_2          | 2023-07-16 04:58:37,016 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=4dadd573-31b0-4101-9a17-f01ba3d94f1c.
datanode_2          | 2023-07-16 04:58:37,018 [Command processor thread] INFO server.RaftServer: 718f9ff6-0a54-459e-b595-cb309db1e2fa: addNew group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] returns group-8484F5BBEB1D:java.util.concurrent.CompletableFuture@28c8d339[Not completed]
datanode_2          | 2023-07-16 04:58:37,041 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa: new RaftServerImpl for group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:58:37,042 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:58:37,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:58:37,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:58:37,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:37,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:58:37,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:58:37,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:58:37,048 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:58:37,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:58:37,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:58:37,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:58:37,049 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d does not exist. Creating ...
datanode_2          | 2023-07-16 04:58:37,052 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/in_use.lock acquired by nodename 7@6efab504ad57
datanode_2          | 2023-07-16 04:58:37,077 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d has been successfully formatted.
datanode_2          | 2023-07-16 04:58:37,078 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8484F5BBEB1D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.7:35094 remote=recon/172.22.0.9:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-07-16 04:58:36,935 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-16 04:58:37,998 [Command processor thread] INFO server.RaftServer: c231805e-370f-4608-92fd-d33256063154: addNew group-905322695467:[c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] returns group-905322695467:java.util.concurrent.CompletableFuture@6d306c1a[Not completed]
datanode_1          | 2023-07-16 04:58:38,085 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154: new RaftServerImpl for group-905322695467:[c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:58:38,089 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:58:38,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:58:38,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:58:38,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:38,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:58:38,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:58:38,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:58:38,118 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: ConfigurationManager, init=-1: [c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:58:38,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:58:38,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:58:38,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:58:38,152 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b2537ab0-ba58-45e1-ab37-905322695467 does not exist. Creating ...
datanode_1          | 2023-07-16 04:58:38,173 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b2537ab0-ba58-45e1-ab37-905322695467/in_use.lock acquired by nodename 7@25d68a9029e1
datanode_1          | 2023-07-16 04:58:38,193 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b2537ab0-ba58-45e1-ab37-905322695467 has been successfully formatted.
datanode_1          | 2023-07-16 04:58:38,206 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-905322695467: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:58:38,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:38,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:58:38,294 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:58:38,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:38,338 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:38,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:58:38,406 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:58:38,437 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b2537ab0-ba58-45e1-ab37-905322695467
datanode_1          | 2023-07-16 04:58:38,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:58:38,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:38,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:38,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:58:38,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:58:38,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:58:38,470 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:58:38,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:58:38,546 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:38,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:38,587 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:38,672 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:38,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:38,688 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:58:38,689 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:58:38,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:58:38,697 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:58:38,699 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:58:38,996 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: start as a follower, conf=-1: [c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:58:39,035 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:58:39,038 [pool-22-thread-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState
datanode_1          | 2023-07-16 04:58:39,062 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-905322695467,id=c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:39,136 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b2537ab0-ba58-45e1-ab37-905322695467
datanode_1          | 2023-07-16 04:58:39,140 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b2537ab0-ba58-45e1-ab37-905322695467.
datanode_1          | 2023-07-16 04:58:39,141 [Command processor thread] INFO server.RaftServer: c231805e-370f-4608-92fd-d33256063154: addNew group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] returns group-8484F5BBEB1D:java.util.concurrent.CompletableFuture@4015830[Not completed]
datanode_1          | 2023-07-16 04:58:39,166 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154: new RaftServerImpl for group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:58:39,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:58:39,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:58:39,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:58:39,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:39,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:58:39,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:58:39,176 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:58:39,180 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:58:39,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:58:39,181 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:58:39,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:58:39,185 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d does not exist. Creating ...
datanode_1          | 2023-07-16 04:58:39,187 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/in_use.lock acquired by nodename 7@25d68a9029e1
datanode_1          | 2023-07-16 04:58:39,191 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d has been successfully formatted.
datanode_1          | 2023-07-16 04:58:39,196 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8484F5BBEB1D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:58:39,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:39,197 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:58:39,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:58:39,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:58:39,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:39,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:39,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:58:39,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:58:39,221 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_1          | 2023-07-16 04:58:39,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:58:39,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:39,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:39,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:58:39,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:58:39,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:58:39,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:58:39,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:58:39,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:39,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:39,238 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:39,242 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:39,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:39,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:58:39,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:58:39,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:58:39,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:58:39,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:58:39,252 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:58:39,263 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:58:39,264 [pool-22-thread-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState
datanode_1          | 2023-07-16 04:58:39,265 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8484F5BBEB1D,id=c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:39,285 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_1          | 2023-07-16 04:58:41,739 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d.
datanode_1          | 2023-07-16 04:58:41,743 [Command processor thread] INFO server.RaftServer: c231805e-370f-4608-92fd-d33256063154: addNew group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] returns group-8D29C90B7B96:java.util.concurrent.CompletableFuture@4674c905[Not completed]
datanode_1          | 2023-07-16 04:58:41,745 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154: new RaftServerImpl for group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 04:58:41,745 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 04:58:41,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 04:58:41,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 04:58:41,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:41,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 04:58:41,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 04:58:41,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:58:37,079 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-16 04:58:37,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:37,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:58:37,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:58:37,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:58:37,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:37,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:58:37,093 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:58:37,093 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_2          | 2023-07-16 04:58:37,093 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:58:37,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:58:37,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:37,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:58:37,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:58:37,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:58:37,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:58:37,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:58:37,098 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:37,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:37,103 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:37,117 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:37,117 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:37,118 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:58:37,118 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:58:37,118 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:58:37,118 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:58:37,119 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:58:37,121 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:58:37,124 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:58:37,124 [pool-22-thread-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState
datanode_2          | 2023-07-16 04:58:37,128 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8484F5BBEB1D,id=718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_2          | 2023-07-16 04:58:37,135 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_2          | 2023-07-16 04:58:41,312 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d.
datanode_2          | 2023-07-16 04:58:41,316 [Command processor thread] INFO server.RaftServer: 718f9ff6-0a54-459e-b595-cb309db1e2fa: addNew group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] returns group-8D29C90B7B96:java.util.concurrent.CompletableFuture@5c73c2c[Not completed]
datanode_2          | 2023-07-16 04:58:41,318 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa: new RaftServerImpl for group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 04:58:41,329 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 04:58:41,329 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 04:58:41,329 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 04:58:41,329 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:41,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 04:58:41,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 04:58:41,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 04:58:41,332 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 04:58:41,332 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 04:58:41,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 04:58:41,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 04:58:41,345 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 does not exist. Creating ...
datanode_2          | 2023-07-16 04:58:41,349 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/in_use.lock acquired by nodename 7@6efab504ad57
datanode_2          | 2023-07-16 04:58:41,354 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 has been successfully formatted.
datanode_2          | 2023-07-16 04:58:41,357 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8D29C90B7B96: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 04:58:41,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 04:58:41,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 04:58:41,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 04:58:41,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 04:58:41,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:41,376 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 04:58:41,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 04:58:41,391 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96
datanode_2          | 2023-07-16 04:58:41,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-07-16 04:58:41,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:58:41,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:41,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 04:58:41,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 04:58:41,400 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 04:58:41,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 04:58:41,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 04:58:41,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 04:58:41,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:41,460 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:41,462 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 04:58:41,471 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 04:58:41,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 04:58:41,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 04:58:41,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 04:58:41,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 04:58:41,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 04:58:41,498 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:58:41,498 [pool-22-thread-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 04:58:41,504 [pool-22-thread-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState
datanode_2          | 2023-07-16 04:58:41,509 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8D29C90B7B96,id=718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_1          | 2023-07-16 04:58:41,747 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 04:58:41,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 04:58:41,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 04:58:41,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 04:58:41,748 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 does not exist. Creating ...
datanode_1          | 2023-07-16 04:58:41,751 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/in_use.lock acquired by nodename 7@25d68a9029e1
datanode_1          | 2023-07-16 04:58:41,761 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 has been successfully formatted.
datanode_1          | 2023-07-16 04:58:41,766 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8D29C90B7B96: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 04:58:41,766 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:58:41,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 04:58:41,779 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 04:58:41,779 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 04:58:41,779 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:41,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:41,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 04:58:41,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 04:58:41,781 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96
datanode_1          | 2023-07-16 04:58:41,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-07-16 04:58:41,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:41,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:41,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 04:58:41,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 04:58:41,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 04:58:41,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 04:58:41,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 04:58:41,785 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 04:58:41,788 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:41,788 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:41,788 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 04:58:41,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 04:58:41,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 04:58:41,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 04:58:41,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 04:58:41,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 04:58:41,790 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 04:58:41,797 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:58:41,799 [pool-22-thread-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 04:58:41,799 [pool-22-thread-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState
datanode_1          | 2023-07-16 04:58:41,808 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8D29C90B7B96,id=c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:41,842 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96
datanode_1          | 2023-07-16 04:58:42,097 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96.
datanode_1          | 2023-07-16 04:58:42,295 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: receive requestVote(ELECTION, 718f9ff6-0a54-459e-b595-cb309db1e2fa, group-8484F5BBEB1D, 1, (t:0, i:0))
datanode_1          | 2023-07-16 04:58:42,297 [grpc-default-executor-1] INFO impl.VoteContext: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FOLLOWER: accept ELECTION from 718f9ff6-0a54-459e-b595-cb309db1e2fa: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-16 04:58:41,515 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96
datanode_2          | 2023-07-16 04:58:41,974 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState] INFO impl.FollowerState: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5038515088ns, electionTimeout:5007ms
datanode_2          | 2023-07-16 04:58:41,974 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState
datanode_2          | 2023-07-16 04:58:41,975 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:58:41,981 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 04:58:41,982 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1
datanode_2          | 2023-07-16 04:58:42,023 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:58:42,030 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-16 04:58:42,033 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1
datanode_2          | 2023-07-16 04:58:42,036 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-16 04:58:42,073 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96.
datanode_2          | 2023-07-16 04:58:42,079 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F01BA3D94F1C with new leaderId: 718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_2          | 2023-07-16 04:58:42,079 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: change Leader from null to 718f9ff6-0a54-459e-b595-cb309db1e2fa at term 1 for becomeLeader, leader elected after 5589ms
datanode_2          | 2023-07-16 04:58:42,092 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-16 04:58:42,127 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 04:58:42,165 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:58:42,172 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 04:58:42,187 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 04:58:42,197 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 04:58:42,191 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5066507518ns, electionTimeout:5056ms
datanode_2          | 2023-07-16 04:58:42,199 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState
datanode_2          | 2023-07-16 04:58:42,199 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:58:42,199 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 04:58:42,198 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 04:58:42,200 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2
datanode_2          | 2023-07-16 04:58:42,208 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:58:42,217 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 04:58:42,224 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 04:58:42,231 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderStateImpl
datanode_2          | 2023-07-16 04:58:42,322 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:58:42,458 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-07-16 04:58:42,464 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection:   Response 0: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-d276d553-d6a4-47fa-982e-159d66e9f3c3#0:FAIL-t1
datanode_2          | 2023-07-16 04:58:42,464 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection:   Response 1: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-c231805e-370f-4608-92fd-d33256063154#0:OK-t1
datanode_2          | 2023-07-16 04:58:42,465 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-07-16 04:58:42,466 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-07-16 04:58:42,466 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2
datanode_2          | 2023-07-16 04:58:42,467 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-LeaderElection2] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState
datanode_2          | 2023-07-16 04:58:42,508 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-LeaderElection1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C: set configuration 0: [718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-07-16 04:58:42,619 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-F01BA3D94F1C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4dadd573-31b0-4101-9a17-f01ba3d94f1c/current/log_inprogress_0
datanode_2          | 2023-07-16 04:58:46,666 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5166998321ns, electionTimeout:5155ms
datanode_2          | 2023-07-16 04:58:46,666 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState
datanode_2          | 2023-07-16 04:58:46,667 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 04:58:46,667 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 04:58:46,667 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3
datanode_2          | 2023-07-16 04:58:46,671 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:58:46,695 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-16 04:58:46,695 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.LeaderElection:   Response 0: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-c231805e-370f-4608-92fd-d33256063154#0:FAIL-t1
datanode_2          | 2023-07-16 04:58:46,695 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.LeaderElection: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3 ELECTION round 0: result REJECTED
datanode_2          | 2023-07-16 04:58:46,695 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-07-16 04:58:46,696 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3
datanode_2          | 2023-07-16 04:58:46,696 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-LeaderElection3] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState
datanode_2          | 2023-07-16 04:58:47,649 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: receive requestVote(ELECTION, d276d553-d6a4-47fa-982e-159d66e9f3c3, group-8484F5BBEB1D, 2, (t:0, i:0))
datanode_2          | 2023-07-16 04:58:47,651 [grpc-default-executor-1] INFO impl.VoteContext: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FOLLOWER: accept ELECTION from d276d553-d6a4-47fa-982e-159d66e9f3c3: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:58:47,651 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_2          | 2023-07-16 04:58:47,651 [grpc-default-executor-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState
datanode_2          | 2023-07-16 04:58:47,651 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-16 04:58:47,652 [grpc-default-executor-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-FollowerState
datanode_2          | 2023-07-16 04:58:47,661 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D replies to ELECTION vote request: d276d553-d6a4-47fa-982e-159d66e9f3c3<-718f9ff6-0a54-459e-b595-cb309db1e2fa#0:OK-t2. Peer's state: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D:t2, leader=null, voted=d276d553-d6a4-47fa-982e-159d66e9f3c3, raftlog=718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_2          | 2023-07-16 04:58:47,756 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8484F5BBEB1D with new leaderId: d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_2          | 2023-07-16 04:58:47,757 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: change Leader from null to d276d553-d6a4-47fa-982e-159d66e9f3c3 at term 2 for appendEntries, leader elected after 10677ms
datanode_2          | 2023-07-16 04:58:47,788 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-07-16 04:58:47,796 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:58:47,802 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8484F5BBEB1D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/current/log_inprogress_0
datanode_2          | 2023-07-16 04:58:51,843 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: receive requestVote(ELECTION, c231805e-370f-4608-92fd-d33256063154, group-8D29C90B7B96, 2, (t:0, i:0))
datanode_2          | 2023-07-16 04:58:51,844 [grpc-default-executor-1] INFO impl.VoteContext: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FOLLOWER: accept ELECTION from c231805e-370f-4608-92fd-d33256063154: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 04:58:51,844 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c231805e-370f-4608-92fd-d33256063154
datanode_2          | 2023-07-16 04:58:51,844 [grpc-default-executor-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: shutdown 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState
datanode_2          | 2023-07-16 04:58:51,844 [grpc-default-executor-1] INFO impl.RoleInfo: 718f9ff6-0a54-459e-b595-cb309db1e2fa: start 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState
datanode_2          | 2023-07-16 04:58:51,844 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-07-16 04:58:51,854 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96 replies to ELECTION vote request: c231805e-370f-4608-92fd-d33256063154<-718f9ff6-0a54-459e-b595-cb309db1e2fa#0:OK-t2. Peer's state: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96:t2, leader=null, voted=c231805e-370f-4608-92fd-d33256063154, raftlog=718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_2          | 2023-07-16 04:58:52,001 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8D29C90B7B96 with new leaderId: c231805e-370f-4608-92fd-d33256063154
datanode_2          | 2023-07-16 04:58:52,001 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: change Leader from null to c231805e-370f-4608-92fd-d33256063154 at term 2 for appendEntries, leader elected after 10635ms
datanode_2          | 2023-07-16 04:58:52,042 [grpc-default-executor-1] INFO server.RaftServer$Division: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-07-16 04:58:52,042 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 04:58:52,044 [718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 718f9ff6-0a54-459e-b595-cb309db1e2fa@group-8D29C90B7B96-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/current/log_inprogress_0
datanode_2          | 2023-07-16 04:58:54,817 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-07-16 04:59:05,407 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:58:42,306 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_1          | 2023-07-16 04:58:42,306 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState
datanode_1          | 2023-07-16 04:58:42,306 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState
datanode_1          | 2023-07-16 04:58:42,307 [c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-16 04:58:42,359 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D replies to ELECTION vote request: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-c231805e-370f-4608-92fd-d33256063154#0:OK-t1. Peer's state: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D:t1, leader=null, voted=718f9ff6-0a54-459e-b595-cb309db1e2fa, raftlog=c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:58:44,094 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState] INFO impl.FollowerState: c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5056889216ns, electionTimeout:5025ms
datanode_1          | 2023-07-16 04:58:44,095 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState
datanode_1          | 2023-07-16 04:58:44,095 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 04:58:44,102 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 04:58:44,106 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-FollowerState] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1
datanode_1          | 2023-07-16 04:58:44,116 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO impl.LeaderElection: c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:58:44,117 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO impl.LeaderElection: c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-16 04:58:44,118 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1
datanode_1          | 2023-07-16 04:58:44,118 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 04:58:44,119 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-905322695467 with new leaderId: c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:44,119 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: change Leader from null to c231805e-370f-4608-92fd-d33256063154 at term 1 for becomeLeader, leader elected after 5912ms
datanode_1          | 2023-07-16 04:58:44,119 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:58:44,155 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:58:44,164 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:44,164 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 04:58:44,170 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:58:44,175 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:58:44,175 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:58:44,206 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:44,214 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-16 04:58:44,217 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderStateImpl
datanode_1          | 2023-07-16 04:58:44,275 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:58:44,347 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-LeaderElection1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-905322695467: set configuration 0: [c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-16 04:58:44,435 [c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-905322695467-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b2537ab0-ba58-45e1-ab37-905322695467/current/log_inprogress_0
datanode_1          | 2023-07-16 04:58:46,684 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: receive requestVote(ELECTION, 718f9ff6-0a54-459e-b595-cb309db1e2fa, group-8D29C90B7B96, 1, (t:0, i:0))
datanode_1          | 2023-07-16 04:58:46,684 [grpc-default-executor-1] INFO impl.VoteContext: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FOLLOWER: reject ELECTION from 718f9ff6-0a54-459e-b595-cb309db1e2fa: our priority 1 > candidate's priority 0
datanode_1          | 2023-07-16 04:58:46,685 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_1          | 2023-07-16 04:58:46,685 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState
datanode_1          | 2023-07-16 04:58:46,685 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState
datanode_1          | 2023-07-16 04:58:46,685 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-16 04:58:46,691 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96 replies to ELECTION vote request: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-c231805e-370f-4608-92fd-d33256063154#0:FAIL-t1. Peer's state: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96:t1, leader=null, voted=null, raftlog=c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:58:47,663 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: receive requestVote(ELECTION, d276d553-d6a4-47fa-982e-159d66e9f3c3, group-8484F5BBEB1D, 2, (t:0, i:0))
datanode_1          | 2023-07-16 04:58:47,664 [grpc-default-executor-1] INFO impl.VoteContext: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FOLLOWER: accept ELECTION from d276d553-d6a4-47fa-982e-159d66e9f3c3: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-16 04:58:47,664 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_1          | 2023-07-16 04:58:47,664 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState
datanode_1          | 2023-07-16 04:58:47,664 [grpc-default-executor-1] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState
datanode_1          | 2023-07-16 04:58:47,664 [c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-07-16 04:58:47,670 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D replies to ELECTION vote request: d276d553-d6a4-47fa-982e-159d66e9f3c3<-c231805e-370f-4608-92fd-d33256063154#0:OK-t2. Peer's state: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D:t2, leader=null, voted=d276d553-d6a4-47fa-982e-159d66e9f3c3, raftlog=c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_1          | 2023-07-16 04:58:47,749 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8484F5BBEB1D with new leaderId: d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_1          | 2023-07-16 04:58:47,749 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: change Leader from null to d276d553-d6a4-47fa-982e-159d66e9f3c3 at term 2 for appendEntries, leader elected after 8544ms
datanode_1          | 2023-07-16 04:58:47,786 [grpc-default-executor-1] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-07-16 04:58:47,787 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:58:47,790 [c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8484F5BBEB1D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/current/log_inprogress_0
datanode_1          | 2023-07-16 04:58:51,789 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5104253784ns, electionTimeout:5099ms
datanode_1          | 2023-07-16 04:58:51,790 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState
datanode_1          | 2023-07-16 04:58:51,790 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-07-16 04:58:51,790 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 04:58:51,790 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-FollowerState] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2
datanode_1          | 2023-07-16 04:58:51,793 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.LeaderElection: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:1], old=null
datanode_1          | 2023-07-16 04:58:51,857 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.LeaderElection: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 04:58:51,857 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.LeaderElection:   Response 0: c231805e-370f-4608-92fd-d33256063154<-718f9ff6-0a54-459e-b595-cb309db1e2fa#0:OK-t2
datanode_1          | 2023-07-16 04:58:51,858 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.LeaderElection: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2 ELECTION round 0: result PASSED
datanode_1          | 2023-07-16 04:58:51,858 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: shutdown c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2
datanode_1          | 2023-07-16 04:58:51,858 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_1          | 2023-07-16 04:58:51,858 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8D29C90B7B96 with new leaderId: c231805e-370f-4608-92fd-d33256063154
datanode_1          | 2023-07-16 04:58:51,863 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: change Leader from null to c231805e-370f-4608-92fd-d33256063154 at term 2 for becomeLeader, leader elected after 10081ms
datanode_1          | 2023-07-16 04:58:51,864 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:58:51,865 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 04:58:51,873 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:51,875 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 04:58:51,875 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 04:58:51,876 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 04:58:51,876 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 04:58:51,883 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 04:58:51,883 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-16 04:58:51,904 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:58:51,905 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:51,906 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:58:51,917 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:58:51,918 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:58:51,918 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:58:51,936 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-07-16 04:58:51,936 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 04:58:51,936 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 04:58:51,937 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-07-16 04:58:51,937 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 04:58:51,937 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 04:58:51,940 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO impl.RoleInfo: c231805e-370f-4608-92fd-d33256063154: start c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderStateImpl
datanode_1          | 2023-07-16 04:58:51,945 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 04:58:51,964 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-LeaderElection2] INFO server.RaftServer$Division: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-07-16 04:58:51,968 [c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c231805e-370f-4608-92fd-d33256063154@group-8D29C90B7B96-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/current/log_inprogress_0
datanode_1          | 2023-07-16 04:58:54,942 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-07-16 04:59:47,279 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-16 04:57:54,524 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = c9a24f7e2cb5/172.22.0.5
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.2.1
datanode_3          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_3          | STARTUP_MSG:   java = 11.0.13
datanode_3          | ************************************************************/
datanode_3          | 2023-07-16 04:57:54,601 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 04:57:55,968 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 04:57:56,869 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 04:57:57,857 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-16 04:57:57,865 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-16 04:57:59,162 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c9a24f7e2cb5 ip:172.22.0.5
datanode_3          | 2023-07-16 04:58:00,939 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_3          | 2023-07-16 04:58:02,285 [main] INFO reflections.Reflections: Reflections took 1208 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_3          | 2023-07-16 04:58:04,249 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 04:58:04,395 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-16 04:58:04,474 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 04:58:04,493 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 04:58:04,733 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:58:04,875 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:58:04,879 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-16 04:58:04,904 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-16 04:58:04,904 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-16 04:58:04,912 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-16 04:58:05,085 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 04:58:05,092 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-16 04:58:15,355 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 04:58:16,019 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 04:58:17,877 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-16 04:58:17,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-16 04:58:17,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-16 04:58:17,927 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-16 04:58:17,933 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:17,937 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-16 04:58:17,943 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:58:19,314 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-16 04:58:19,326 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:19,344 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:58:19,537 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:58:20,703 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:58:20,936 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 04:58:21,214 [main] INFO util.log: Logging initialized @34226ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 04:58:22,493 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-07-16 04:58:22,529 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-16 04:58:22,550 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-16 04:58:22,593 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 04:58:22,593 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-16 04:58:22,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-07-16 04:58:23,048 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 04:58:23,054 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_3          | 2023-07-16 04:58:23,311 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 04:58:23,312 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 04:58:23,329 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-16 04:58:23,426 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2921199d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 04:58:23,428 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23ad71bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-16 04:58:26,281 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@51a16adf{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-4105115059945772211/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 04:58:26,314 [main] INFO server.AbstractConnector: Started ServerConnector@4aeb0e2b{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-16 04:58:26,316 [main] INFO server.Server: Started @39328ms
datanode_3          | 2023-07-16 04:58:26,343 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-16 04:58:26,343 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-16 04:58:26,345 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 04:58:26,357 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-16 04:58:26,580 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37a93e87] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-16 04:58:27,308 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.9:9891
datanode_3          | 2023-07-16 04:58:27,703 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 04:58:30,103 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:58:31,105 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 04:58:32,359 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-16 04:58:32,365 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-16 04:58:32,936 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:33,122 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: d276d553-d6a4-47fa-982e-159d66e9f3c3: start RPC server
datanode_3          | 2023-07-16 04:58:33,124 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: d276d553-d6a4-47fa-982e-159d66e9f3c3: GrpcService started, listening on 9856
datanode_3          | 2023-07-16 04:58:33,132 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: d276d553-d6a4-47fa-982e-159d66e9f3c3: GrpcService started, listening on 9857
datanode_3          | 2023-07-16 04:58:33,133 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: d276d553-d6a4-47fa-982e-159d66e9f3c3: GrpcService started, listening on 9858
datanode_3          | 2023-07-16 04:58:33,144 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d276d553-d6a4-47fa-982e-159d66e9f3c3 is started using port 9858 for RATIS
datanode_3          | 2023-07-16 04:58:33,144 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d276d553-d6a4-47fa-982e-159d66e9f3c3 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-16 04:58:33,149 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d276d553-d6a4-47fa-982e-159d66e9f3c3 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-16 04:58:33,144 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@36368c60] INFO util.JvmPauseMonitor: JvmPauseMonitor-d276d553-d6a4-47fa-982e-159d66e9f3c3: Started
datanode_3          | 2023-07-16 04:58:36,122 [EndpointStateMachine task thread for recon/172.22.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From c9a24f7e2cb5/172.22.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.5:38504 remote=recon/172.22.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.5:38504 remote=recon/172.22.0.9:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_3          | 2023-07-16 04:58:36,544 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-16 04:58:37,589 [Command processor thread] INFO server.RaftServer: d276d553-d6a4-47fa-982e-159d66e9f3c3: addNew group-DB0BB6A4A4E1:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] returns group-DB0BB6A4A4E1:java.util.concurrent.CompletableFuture@79aef96b[Not completed]
datanode_3          | 2023-07-16 04:58:37,637 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3: new RaftServerImpl for group-DB0BB6A4A4E1:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:58:37,641 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:58:37,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:58:37,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:58:37,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:37,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:58:37,652 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:58:37,652 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:58:37,667 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:58:37,672 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:58:37,682 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:58:37,684 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:58:37,686 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1 does not exist. Creating ...
datanode_3          | 2023-07-16 04:58:37,709 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1/in_use.lock acquired by nodename 7@c9a24f7e2cb5
datanode_3          | 2023-07-16 04:58:37,729 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1 has been successfully formatted.
datanode_3          | 2023-07-16 04:58:37,771 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-DB0BB6A4A4E1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:58:37,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:37,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:58:37,919 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:58:37,919 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:38,101 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:58:38,169 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:58:38,244 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1
datanode_3          | 2023-07-16 04:58:38,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:58:38,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:38,246 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,246 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:58:38,247 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:58:38,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:58:38,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:58:38,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:58:38,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:38,497 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:38,497 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:38,506 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:38,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:58:38,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:58:38,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:58:38,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:58:38,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:58:38,729 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-07-16 04:58:38,741 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:58:38,748 [pool-22-thread-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState
datanode_3          | 2023-07-16 04:58:38,774 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DB0BB6A4A4E1,id=d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:38,838 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1
datanode_3          | 2023-07-16 04:58:38,840 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1.
datanode_3          | 2023-07-16 04:58:38,845 [Command processor thread] INFO server.RaftServer: d276d553-d6a4-47fa-982e-159d66e9f3c3: addNew group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] returns group-8484F5BBEB1D:java.util.concurrent.CompletableFuture@3c7c9b76[Not completed]
datanode_3          | 2023-07-16 04:58:38,853 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3: new RaftServerImpl for group-8484F5BBEB1D:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:58:38,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:58:38,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:58:38,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:58:38,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:38,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:58:38,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:58:38,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:58:38,857 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:58:38,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:58:38,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:58:38,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:58:38,863 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d does not exist. Creating ...
datanode_3          | 2023-07-16 04:58:38,868 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/in_use.lock acquired by nodename 7@c9a24f7e2cb5
datanode_3          | 2023-07-16 04:58:38,871 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d has been successfully formatted.
datanode_3          | 2023-07-16 04:58:38,871 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8484F5BBEB1D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:58:38,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:38,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:58:38,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:58:38,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:38,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:58:38,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:58:38,881 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_3          | 2023-07-16 04:58:38,881 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:58:38,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:58:38,904 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:38,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:58:38,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:58:38,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:58:38,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:58:38,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:58:38,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:38,916 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:38,917 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:38,917 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:38,918 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:38,920 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:58:38,920 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:58:38,920 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:58:38,921 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:58:38,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:58:38,929 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:58:38,932 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:58:38,932 [pool-22-thread-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState
datanode_3          | 2023-07-16 04:58:38,935 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8484F5BBEB1D,id=d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:38,947 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d
datanode_3          | 2023-07-16 04:58:41,641 [grpc-default-executor-1] INFO server.RaftServer: d276d553-d6a4-47fa-982e-159d66e9f3c3: addNew group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1] returns group-8D29C90B7B96:java.util.concurrent.CompletableFuture@41c7bfe2[Not completed]
datanode_3          | 2023-07-16 04:58:41,646 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3: new RaftServerImpl for group-8D29C90B7B96:[d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 04:58:41,655 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 04:58:41,656 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 04:58:41,656 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 04:58:41,656 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:41,657 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 04:58:41,657 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 04:58:41,657 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:58:41,659 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: ConfigurationManager, init=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 04:58:41,664 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 04:58:41,664 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 04:58:41,664 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 04:58:41,665 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 does not exist. Creating ...
datanode_3          | 2023-07-16 04:58:41,667 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/in_use.lock acquired by nodename 7@c9a24f7e2cb5
datanode_3          | 2023-07-16 04:58:41,670 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96 has been successfully formatted.
datanode_3          | 2023-07-16 04:58:41,671 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:58:41,680 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8D29C90B7B96: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 04:58:41,695 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 04:58:41,695 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 04:58:41,697 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 04:58:41,724 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d.
datanode_3          | 2023-07-16 04:58:41,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:41,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:41,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 04:58:41,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 04:58:41,728 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96
datanode_3          | 2023-07-16 04:58:41,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-07-16 04:58:41,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:41,732 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:41,732 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 04:58:41,732 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 04:58:41,735 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 04:58:41,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 04:58:41,737 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 04:58:41,738 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 04:58:41,750 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:41,753 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:41,754 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 04:58:41,757 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 04:58:41,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 04:58:41,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 04:58:41,770 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 04:58:41,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 04:58:41,772 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 04:58:41,777 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: start as a follower, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:58:41,785 [pool-22-thread-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 04:58:41,786 [pool-22-thread-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState
datanode_3          | 2023-07-16 04:58:41,786 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8D29C90B7B96,id=d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:42,413 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: receive requestVote(ELECTION, 718f9ff6-0a54-459e-b595-cb309db1e2fa, group-8484F5BBEB1D, 1, (t:0, i:0))
datanode_3          | 2023-07-16 04:58:42,416 [grpc-default-executor-1] INFO impl.VoteContext: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FOLLOWER: reject ELECTION from 718f9ff6-0a54-459e-b595-cb309db1e2fa: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-16 04:58:42,420 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_3          | 2023-07-16 04:58:42,421 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState
datanode_3          | 2023-07-16 04:58:42,421 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-16 04:58:42,422 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState
datanode_3          | 2023-07-16 04:58:42,448 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D replies to ELECTION vote request: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-d276d553-d6a4-47fa-982e-159d66e9f3c3#0:FAIL-t1. Peer's state: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D:t1, leader=null, voted=null, raftlog=d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:58:43,907 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState] INFO impl.FollowerState: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5159543850ns, electionTimeout:5129ms
datanode_3          | 2023-07-16 04:58:43,907 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState
datanode_3          | 2023-07-16 04:58:43,908 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 04:58:43,910 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 04:58:43,911 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-FollowerState] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1
datanode_3          | 2023-07-16 04:58:43,916 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO impl.LeaderElection: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-07-16 04:58:43,917 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO impl.LeaderElection: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-16 04:58:43,918 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1
datanode_3          | 2023-07-16 04:58:43,918 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 04:58:43,918 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DB0BB6A4A4E1 with new leaderId: d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:43,919 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:58:43,935 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: change Leader from null to d276d553-d6a4-47fa-982e-159d66e9f3c3 at term 1 for becomeLeader, leader elected after 6137ms
datanode_3          | 2023-07-16 04:58:43,955 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:58:43,975 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:43,979 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 04:58:43,998 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:58:44,003 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:58:44,005 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:58:44,025 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:44,032 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 04:58:44,036 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderStateImpl
datanode_3          | 2023-07-16 04:58:44,072 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:58:44,103 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-LeaderElection1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:58:44,276 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-DB0BB6A4A4E1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1/current/log_inprogress_0
datanode_3          | 2023-07-16 04:58:46,678 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: receive requestVote(ELECTION, 718f9ff6-0a54-459e-b595-cb309db1e2fa, group-8D29C90B7B96, 1, (t:0, i:0))
datanode_3          | 2023-07-16 04:58:46,679 [grpc-default-executor-1] INFO impl.VoteContext: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FOLLOWER: accept ELECTION from 718f9ff6-0a54-459e-b595-cb309db1e2fa: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-16 04:58:46,679 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:718f9ff6-0a54-459e-b595-cb309db1e2fa
datanode_3          | 2023-07-16 04:58:46,679 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState
datanode_3          | 2023-07-16 04:58:46,679 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState
datanode_3          | 2023-07-16 04:58:46,679 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-16 04:58:46,699 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96 replies to ELECTION vote request: 718f9ff6-0a54-459e-b595-cb309db1e2fa<-d276d553-d6a4-47fa-982e-159d66e9f3c3#0:OK-t1. Peer's state: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96:t1, leader=null, voted=718f9ff6-0a54-459e-b595-cb309db1e2fa, raftlog=d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:58:47,600 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO impl.FollowerState: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5177860017ns, electionTimeout:5168ms
datanode_3          | 2023-07-16 04:58:47,600 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState
datanode_3          | 2023-07-16 04:58:47,601 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-16 04:58:47,601 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 04:58:47,601 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-FollowerState] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2
datanode_3          | 2023-07-16 04:58:47,605 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|priority:0], old=null
datanode_3          | 2023-07-16 04:58:47,676 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 04:58:47,676 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection:   Response 0: d276d553-d6a4-47fa-982e-159d66e9f3c3<-c231805e-370f-4608-92fd-d33256063154#0:OK-t2
datanode_3          | 2023-07-16 04:58:47,677 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.LeaderElection: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-07-16 04:58:47,677 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2
datanode_3          | 2023-07-16 04:58:47,678 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-07-16 04:58:47,678 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8484F5BBEB1D with new leaderId: d276d553-d6a4-47fa-982e-159d66e9f3c3
datanode_3          | 2023-07-16 04:58:47,678 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: change Leader from null to d276d553-d6a4-47fa-982e-159d66e9f3c3 at term 2 for becomeLeader, leader elected after 8805ms
datanode_3          | 2023-07-16 04:58:47,679 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 04:58:47,681 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:47,682 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 04:58:47,678 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:58:47,682 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 04:58:47,688 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 04:58:47,688 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 04:58:47,688 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 04:58:47,688 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 04:58:47,694 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:58:47,694 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:47,694 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:58:47,698 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:58:47,708 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:58:47,708 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:58:47,714 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 04:58:47,714 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 04:58:47,714 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 04:58:47,715 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 04:58:47,715 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 04:58:47,715 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 04:58:47,720 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderStateImpl
datanode_3          | 2023-07-16 04:58:47,721 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:58:47,728 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-LeaderElection2] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-07-16 04:58:47,728 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8484F5BBEB1D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/93dcac0d-4689-4e25-8773-8484f5bbeb1d/current/log_inprogress_0
datanode_3          | 2023-07-16 04:58:51,829 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: receive requestVote(ELECTION, c231805e-370f-4608-92fd-d33256063154, group-8D29C90B7B96, 2, (t:0, i:0))
datanode_3          | 2023-07-16 04:58:51,829 [grpc-default-executor-1] INFO impl.VoteContext: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FOLLOWER: accept ELECTION from c231805e-370f-4608-92fd-d33256063154: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-16 04:58:51,829 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c231805e-370f-4608-92fd-d33256063154
datanode_3          | 2023-07-16 04:58:51,829 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: shutdown d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState
datanode_3          | 2023-07-16 04:58:51,829 [grpc-default-executor-1] INFO impl.RoleInfo: d276d553-d6a4-47fa-982e-159d66e9f3c3: start d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState
datanode_3          | 2023-07-16 04:58:51,830 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState] INFO impl.FollowerState: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-07-16 04:58:51,832 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96 replies to ELECTION vote request: c231805e-370f-4608-92fd-d33256063154<-d276d553-d6a4-47fa-982e-159d66e9f3c3#0:OK-t2. Peer's state: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96:t2, leader=null, voted=c231805e-370f-4608-92fd-d33256063154, raftlog=d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLog:OPENED:c-1, conf=-1: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:58:52,009 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8D29C90B7B96 with new leaderId: c231805e-370f-4608-92fd-d33256063154
datanode_3          | 2023-07-16 04:58:52,026 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: change Leader from null to c231805e-370f-4608-92fd-d33256063154 at term 2 for appendEntries, leader elected after 10314ms
datanode_3          | 2023-07-16 04:58:52,052 [grpc-default-executor-1] INFO server.RaftServer$Division: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96: set configuration 0: [d276d553-d6a4-47fa-982e-159d66e9f3c3|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0, 718f9ff6-0a54-459e-b595-cb309db1e2fa|rpc:172.22.0.6:9856|admin:172.22.0.6:9857|client:172.22.0.6:9858|dataStream:|priority:0, c231805e-370f-4608-92fd-d33256063154|rpc:172.22.0.7:9856|admin:172.22.0.7:9857|client:172.22.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-07-16 04:58:52,053 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 04:58:52,059 [d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d276d553-d6a4-47fa-982e-159d66e9f3c3@group-8D29C90B7B96-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c419302c-e75b-41cd-838e-8d29c90b7b96/current/log_inprogress_0
datanode_3          | 2023-07-16 04:58:54,801 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-07-16 04:59:47,312 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 04:57:52,857 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = a6fdbe73effa/172.22.0.9
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.2.1
recon_1             | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
recon_1             | STARTUP_MSG:   java = 11.0.13
recon_1             | ************************************************************/
recon_1             | 2023-07-16 04:57:52,927 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-16 04:57:58,253 [main] INFO reflections.Reflections: Reflections took 379 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1             | 2023-07-16 04:58:01,394 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-16 04:58:03,147 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:58:12,386 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:58:14,473 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 04:58:14,866 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 04:58:14,873 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-16 04:58:21,712 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-07-16 04:58:22,162 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-16 04:58:22,272 [main] INFO util.log: Logging initialized @35245ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-16 04:58:23,183 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-07-16 04:58:23,239 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-16 04:58:23,321 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-16 04:58:23,323 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-16 04:58:23,343 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-16 04:58:23,343 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-16 04:58:24,947 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-16 04:58:26,429 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-16 04:58:26,476 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-16 04:58:26,512 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-16 04:58:26,837 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-16 04:58:26,838 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-16 04:58:29,234 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:58:29,669 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:58:29,764 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
recon_1             | 2023-07-16 04:58:29,768 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:57:55,373 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c35516c768de/172.22.0.10
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.2.1
recon_1             | 2023-07-16 04:58:29,969 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:58:30,249 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1             | 2023-07-16 04:58:30,344 [main] INFO reflections.Reflections: Reflections took 85 ms to scan 3 urls, producing 103 keys and 211 values 
recon_1             | 2023-07-16 04:58:30,440 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-16 04:58:30,487 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-16 04:58:30,501 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-16 04:58:30,507 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-16 04:58:30,583 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 04:58:30,675 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-16 04:58:30,739 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-16 04:58:30,857 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-16 04:58:30,857 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-16 04:58:30,996 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-16 04:58:31,026 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 04:58:31,026 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-16 04:58:31,591 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-16 04:58:31,604 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
recon_1             | 2023-07-16 04:58:31,695 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 04:58:31,696 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-16 04:58:31,702 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-16 04:58:31,733 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6da4feeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 04:58:31,744 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@498a612d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 04:58:37,833 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@811d8d6{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_2_1_jar-_-any-3972028949692669444/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/recon}
recon_1             | 2023-07-16 04:58:37,874 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1e98b788{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-16 04:58:37,874 [Listener at 0.0.0.0/9891] INFO server.Server: Started @50848ms
recon_1             | 2023-07-16 04:58:37,884 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-16 04:58:37,884 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 04:58:37,889 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 04:58:37,889 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-16 04:58:37,913 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-16 04:58:37,936 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 04:58:37,937 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-16 04:58:37,937 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 04:58:37,937 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-16 04:58:37,947 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:58:40,176 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-07-16 04:58:40,177 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 04:58:40,178 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=b2537ab0-ba58-45e1-ab37-905322695467 from SCM.
recon_1             | 2023-07-16 04:58:40,285 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b2537ab0-ba58-45e1-ab37-905322695467, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:34.982Z[UTC]].
recon_1             | 2023-07-16 04:58:40,308 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 81808.196us
recon_1             | 2023-07-16 04:58:40,314 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1 from SCM.
recon_1             | 2023-07-16 04:58:40,329 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1, Nodes: d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:34.599Z[UTC]].
recon_1             | 2023-07-16 04:58:40,329 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 14967.173us
recon_1             | 2023-07-16 04:58:40,329 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=4dadd573-31b0-4101-9a17-f01ba3d94f1c from SCM.
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:57:53,413 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = cf16597bc8c0/172.22.0.3
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:57:53,471 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:57:54,075 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:57:54,434 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 04:57:54,435 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 04:57:55,283 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-b8e1f442-c3ed-4101-a8e0-f5ca85ba6d3c; layoutVersion=2; scmId=809d6c29-45fd-4f0c-8d15-b0aef5b7d525
scm_1               | 2023-07-16 04:57:55,401 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at cf16597bc8c0/172.22.0.3
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 04:58:05,587 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = cf16597bc8c0/172.22.0.3
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-07-16 04:58:05,638 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 04:58:06,122 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 04:58:06,141 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 04:58:06,967 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:58:07,288 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm_1               | 2023-07-16 04:58:10,282 [main] INFO reflections.Reflections: Reflections took 1603 ms to scan 3 urls, producing 103 keys and 211 values 
scm_1               | 2023-07-16 04:58:15,297 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:58:16,334 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 04:58:17,719 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
scm_1               | 2023-07-16 04:58:17,732 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-16 04:58:18,309 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 04:58:18,428 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-16 04:58:18,434 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-16 04:58:18,439 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-16 04:58:18,451 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-16 04:58:18,628 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-16 04:58:18,723 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-16 04:58:18,762 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-16 04:58:19,016 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-16 04:58:19,085 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-16 04:58:19,088 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:58:19,497 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-16 04:58:19,750 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-16 04:58:19,866 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-16 04:58:19,868 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-16 04:58:19,939 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 0 containers.
scm_1               | 2023-07-16 04:58:20,025 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-16 04:58:20,078 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 04:58:20,081 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 04:58:26,477 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:58:27,024 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 04:58:27,393 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:58:27,439 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 04:58:27,394 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 04:58:27,548 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-16 04:58:27,971 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-16 04:58:28,009 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          0.1
scm_1               | Max Datanodes to Involve per Iteration(ratio)      0.2
scm_1               | Max Size to Move per Iteration                     32212254720B
scm_1               | 
scm_1               | 2023-07-16 04:58:28,010 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 04:58:28,011 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-16 04:58:28,016 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:58:28,383 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 04:58:28,488 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 04:58:28,496 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-16 04:58:29,870 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 04:58:29,892 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:58:29,922 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-16 04:58:30,132 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:58:30,133 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 04:58:30,134 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:58:30,134 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-16 04:58:30,619 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:58:30,619 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 04:58:30,630 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 04:58:30,643 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-16 04:58:31,192 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@342e690b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-16 04:58:31,342 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-16 04:58:31,344 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-16 04:58:31,541 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @34975ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-16 04:58:32,846 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-07-16 04:58:32,874 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 04:58:32,946 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 04:58:33,006 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 04:58:33,016 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 04:58:33,026 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 04:58:33,417 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-16 04:58:33,418 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
scm_1               | 2023-07-16 04:58:33,492 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/718f9ff6-0a54-459e-b595-cb309db1e2fa
scm_1               | 2023-07-16 04:58:33,532 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:58:33,657 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:58:33,657 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:58:33,673 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 04:58:33,698 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:58:33,724 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-16 04:58:33,724 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 04:58:33,731 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4dadd573-31b0-4101-9a17-f01ba3d94f1c to datanode:718f9ff6-0a54-459e-b595-cb309db1e2fa
scm_1               | 2023-07-16 04:58:33,731 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-07-16 04:58:33,783 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4dadd573-31b0-4101-9a17-f01ba3d94f1c, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:33.721Z[UTC]].
scm_1               | 2023-07-16 04:58:33,818 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6a15b73{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 04:58:33,834 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@41b64020{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 04:58:34,586 [IPC Server handler 14 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d276d553-d6a4-47fa-982e-159d66e9f3c3
scm_1               | 2023-07-16 04:58:34,593 [IPC Server handler 14 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:58:34,593 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:58:34,599 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1 to datanode:d276d553-d6a4-47fa-982e-159d66e9f3c3
scm_1               | 2023-07-16 04:58:34,617 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1, Nodes: d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:34.599Z[UTC]].
scm_1               | 2023-07-16 04:58:34,618 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:58:34,974 [IPC Server handler 78 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c231805e-370f-4608-92fd-d33256063154
scm_1               | 2023-07-16 04:58:34,980 [IPC Server handler 78 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 04:58:34,981 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:58:34,982 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b2537ab0-ba58-45e1-ab37-905322695467 to datanode:c231805e-370f-4608-92fd-d33256063154
scm_1               | 2023-07-16 04:58:34,988 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b2537ab0-ba58-45e1-ab37-905322695467, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:34.982Z[UTC]].
scm_1               | 2023-07-16 04:58:35,009 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-16 04:58:35,010 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:58:35,029 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-16 04:58:35,029 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 04:58:35,048 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-16 04:58:35,049 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 04:58:35,099 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d to datanode:c231805e-370f-4608-92fd-d33256063154
scm_1               | 2023-07-16 04:58:35,108 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d to datanode:d276d553-d6a4-47fa-982e-159d66e9f3c3
scm_1               | 2023-07-16 04:58:35,108 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d to datanode:718f9ff6-0a54-459e-b595-cb309db1e2fa
scm_1               | 2023-07-16 04:58:35,121 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 93dcac0d-4689-4e25-8773-8484f5bbeb1d, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:35.098Z[UTC]].
scm_1               | 2023-07-16 04:58:35,122 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 to datanode:718f9ff6-0a54-459e-b595-cb309db1e2fa
scm_1               | 2023-07-16 04:58:35,129 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 to datanode:d276d553-d6a4-47fa-982e-159d66e9f3c3
scm_1               | 2023-07-16 04:58:35,129 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 to datanode:c231805e-370f-4608-92fd-d33256063154
scm_1               | 2023-07-16 04:58:35,133 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c419302c-e75b-41cd-838e-8d29c90b7b96, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:35.122Z[UTC]].
scm_1               | 2023-07-16 04:58:35,139 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 contains same datanodes as previous pipelines: PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d nodeIds: 718f9ff6-0a54-459e-b595-cb309db1e2fa, d276d553-d6a4-47fa-982e-159d66e9f3c3, c231805e-370f-4608-92fd-d33256063154
scm_1               | 2023-07-16 04:58:36,103 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c628f6a{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_1_jar-_-any-13268290780250688662/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/scm}
scm_1               | 2023-07-16 04:58:36,207 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@78307a56{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-16 04:58:36,207 [Listener at 0.0.0.0/9860] INFO server.Server: Started @39641ms
scm_1               | 2023-07-16 04:58:36,228 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-16 04:58:36,228 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-16 04:58:36,234 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 04:58:41,622 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b2537ab0-ba58-45e1-ab37-905322695467, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c231805e-370f-4608-92fd-d33256063154, CreationTimestamp2023-07-16T04:58:34.982Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 04:58:41,636 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1, Nodes: d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d276d553-d6a4-47fa-982e-159d66e9f3c3, CreationTimestamp2023-07-16T04:58:34.599Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 04:58:41,638 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:41,638 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:41,684 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:41,776 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4dadd573-31b0-4101-9a17-f01ba3d94f1c, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:718f9ff6-0a54-459e-b595-cb309db1e2fa, CreationTimestamp2023-07-16T04:58:33.721Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 04:58:41,785 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:41,800 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:42,138 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:43,924 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:44,133 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:47,696 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 93dcac0d-4689-4e25-8773-8484f5bbeb1d, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d276d553-d6a4-47fa-982e-159d66e9f3c3, CreationTimestamp2023-07-16T04:58:35.098Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 04:58:47,696 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:47,699 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 04:58:47,699 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 04:58:47,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-16 04:58:47,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-16 04:58:47,701 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 04:58:47,701 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-16 04:58:47,704 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-16 04:58:47,705 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-16 04:58:51,878 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c419302c-e75b-41cd-838e-8d29c90b7b96, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c231805e-370f-4608-92fd-d33256063154, CreationTimestamp2023-07-16T04:58:35.122Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 04:58:52,736 [IPC Server handler 61 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-16 04:58:52,754 [IPC Server handler 61 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-16 04:58:52,754 [IPC Server handler 61 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-16 04:59:25,701 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.13
scm_1               | 2023-07-16 04:59:36,913 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.13
scm_1               | 2023-07-16 05:00:33,416 [IPC Server handler 16 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.13
scm_1               | 2023-07-16 05:00:44,354 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.13
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-07-16 04:57:55,408 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:58:05,416 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:58:05,776 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.10:9862
om_1                | 2023-07-16 04:58:05,776 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:58:05,777 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:58:05,887 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:58:10,379 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:12,382 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:14,385 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:16,387 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:18,389 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:20,392 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:22,394 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:24,409 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 04:58:26,425 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c35516c768de/172.22.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b8e1f442-c3ed-4101-a8e0-f5ca85ba6d3c;layoutVersion=0
om_1                | 2023-07-16 04:58:32,527 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at c35516c768de/172.22.0.10
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 04:58:36,642 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c35516c768de/172.22.0.10
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-07-16 04:58:36,680 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 04:58:42,311 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 04:58:42,570 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.10:9862
om_1                | 2023-07-16 04:58:42,570 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 04:58:42,571 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 04:58:42,587 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:58:42,647 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om_1                | 2023-07-16 04:58:43,262 [main] INFO reflections.Reflections: Reflections took 537 ms to scan 1 urls, producing 95 keys and 258 values [using 2 cores]
recon_1             | 2023-07-16 04:58:40,330 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4dadd573-31b0-4101-9a17-f01ba3d94f1c, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:33.721Z[UTC]].
recon_1             | 2023-07-16 04:58:40,330 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 593.103us
recon_1             | 2023-07-16 04:58:40,330 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 from SCM.
recon_1             | 2023-07-16 04:58:40,348 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c419302c-e75b-41cd-838e-8d29c90b7b96, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:35.122Z[UTC]].
recon_1             | 2023-07-16 04:58:40,348 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 7387.936us
recon_1             | 2023-07-16 04:58:40,349 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d from SCM.
recon_1             | 2023-07-16 04:58:40,351 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 93dcac0d-4689-4e25-8773-8484f5bbeb1d, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T04:58:35.098Z[UTC]].
recon_1             | 2023-07-16 04:58:40,351 [Listener at 0.0.0.0/9891] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 1139.606us
recon_1             | 2023-07-16 04:58:40,351 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 04:58:40,376 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-16 04:58:40,408 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-16 04:58:40,675 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-16 04:58:40,675 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-16 04:58:40,769 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-07-16 04:58:40,785 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-16 04:58:40,785 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-16 04:58:40,834 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 51822.05us
recon_1             | 2023-07-16 04:58:40,864 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 8556.141us
recon_1             | 2023-07-16 04:58:40,867 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 372.502us
recon_1             | 2023-07-16 04:58:40,878 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 4260.02us
recon_1             | 2023-07-16 04:58:40,887 [PipelineSyncTask] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 550.302us
recon_1             | 2023-07-16 04:58:40,962 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 258 milliseconds.
recon_1             | 2023-07-16 04:58:41,146 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 318 milliseconds to process 0 existing database records.
recon_1             | 2023-07-16 04:58:41,316 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 169 milliseconds for processing 0 containers.
recon_1             | 2023-07-16 04:58:41,568 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.6:39892: output error
recon_1             | 2023-07-16 04:58:41,569 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.7:35102: output error
recon_1             | 2023-07-16 04:58:41,569 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.5:38520: output error
recon_1             | 2023-07-16 04:58:41,581 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,583 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,583 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-16 04:57:53,300 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-16 04:57:53,301 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-16 04:57:53,446 [main] INFO util.log: Logging initialized @8766ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-16 04:57:54,780 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-07-16 04:57:55,022 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-16 04:57:55,128 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-16 04:57:55,134 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-16 04:57:55,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-16 04:57:55,164 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-16 04:57:55,419 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 1ba3da68c02a/172.22.0.2
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.2.1
s3g_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
s3g_1               | STARTUP_MSG:   java = 11.0.13
s3g_1               | ************************************************************/
s3g_1               | 2023-07-16 04:57:55,445 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-16 04:57:55,677 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-16 04:57:55,747 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-16 04:57:55,757 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
s3g_1               | 2023-07-16 04:57:56,028 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-16 04:57:56,029 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-16 04:57:56,042 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-07-16 04:57:56,233 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@407a7f2a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 04:57:56,287 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5f0fd5a0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 16, 2023 4:58:23 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-16 04:58:23,574 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@544e3679{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_1_jar-_-any-514476290576049145/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/s3gateway}
s3g_1               | 2023-07-16 04:58:23,760 [main] INFO server.AbstractConnector: Started ServerConnector@2a225dd7{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-16 04:58:23,773 [main] INFO server.Server: Started @39093ms
s3g_1               | 2023-07-16 04:58:23,800 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,584 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.7:35094: output error
recon_1             | 2023-07-16 04:58:41,584 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,593 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.6:39900: output error
recon_1             | 2023-07-16 04:58:41,604 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,604 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.5:38504: output error
recon_1             | 2023-07-16 04:58:41,607 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-07-16 04:58:41,739 [IPC Server handler 97 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d276d553-d6a4-47fa-982e-159d66e9f3c3
recon_1             | 2023-07-16 04:58:41,754 [IPC Server handler 97 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,772 [IPC Server handler 10 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c231805e-370f-4608-92fd-d33256063154
recon_1             | 2023-07-16 04:58:41,817 [IPC Server handler 10 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,772 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d276d553-d6a4-47fa-982e-159d66e9f3c3 to Node DB.
recon_1             | 2023-07-16 04:58:41,820 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1 reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,820 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: af2571b5-4258-4ae4-8cb3-db0bb6a4a4e1, Nodes: d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d276d553-d6a4-47fa-982e-159d66e9f3c3, CreationTimestamp2023-07-16T04:58:34.599Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 04:58:41,833 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 12389.16us
recon_1             | 2023-07-16 04:58:41,833 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,834 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,834 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c231805e-370f-4608-92fd-d33256063154 to Node DB.
recon_1             | 2023-07-16 04:58:41,844 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b2537ab0-ba58-45e1-ab37-905322695467 reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,845 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b2537ab0-ba58-45e1-ab37-905322695467, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c231805e-370f-4608-92fd-d33256063154, CreationTimestamp2023-07-16T04:58:34.982Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 04:58:41,845 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 404.502us
recon_1             | 2023-07-16 04:58:41,846 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:41,846 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:42,124 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/718f9ff6-0a54-459e-b595-cb309db1e2fa
recon_1             | 2023-07-16 04:58:42,125 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:42,126 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 718f9ff6-0a54-459e-b595-cb309db1e2fa to Node DB.
recon_1             | 2023-07-16 04:58:42,127 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=4dadd573-31b0-4101-9a17-f01ba3d94f1c reported by 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:42,133 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4dadd573-31b0-4101-9a17-f01ba3d94f1c, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:718f9ff6-0a54-459e-b595-cb309db1e2fa, CreationTimestamp2023-07-16T04:58:33.721Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 04:58:42,141 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 6218.332us
recon_1             | 2023-07-16 04:58:42,141 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:42,141 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:43,938 [IPC Server handler 11 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-16 04:58:43,943 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:43,944 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:44,135 [IPC Server handler 12 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-16 04:58:44,136 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:44,136 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:47,683 [IPC Server handler 94 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-16 04:58:47,684 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:47,684 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=93dcac0d-4689-4e25-8773-8484f5bbeb1d reported by d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:47,685 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 93dcac0d-4689-4e25-8773-8484f5bbeb1d, Nodes: c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d276d553-d6a4-47fa-982e-159d66e9f3c3, CreationTimestamp2023-07-16T04:58:35.098Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 04:58:47,686 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 869.004us
recon_1             | 2023-07-16 04:58:51,870 [IPC Server handler 11 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-16 04:58:51,871 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c419302c-e75b-41cd-838e-8d29c90b7b96 reported by c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 04:58:51,872 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c419302c-e75b-41cd-838e-8d29c90b7b96, Nodes: 718f9ff6-0a54-459e-b595-cb309db1e2fa{ip: 172.22.0.6, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d276d553-d6a4-47fa-982e-159d66e9f3c3{ip: 172.22.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c231805e-370f-4608-92fd-d33256063154{ip: 172.22.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c231805e-370f-4608-92fd-d33256063154, CreationTimestamp2023-07-16T04:58:35.122Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 04:58:51,872 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 545.703us
recon_1             | 2023-07-16 04:58:54,841 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 04:58:54,891 [IPC Server handler 12 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-16 04:58:54,911 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 21183.209us
recon_1             | 2023-07-16 04:58:54,922 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 04:59:05,432 [IPC Server handler 22 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-16 04:59:05,477 [EventQueue-ContainerReportForReconContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 375.002us
recon_1             | 2023-07-16 04:59:05,478 [EventQueue-ContainerReportForReconContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-16 04:59:37,939 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-16 04:59:37,940 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-16 04:59:38,638 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689483577940
recon_1             | 2023-07-16 04:59:38,665 [pool-16-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 04:59:38,679 [pool-16-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 04:59:38,806 [pool-16-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689483577940.
recon_1             | 2023-07-16 04:59:38,871 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-16 04:59:38,884 [pool-17-thread-1] INFO tasks.NSSummaryTask: Completed a reprocess run of NSSummaryTask
recon_1             | 2023-07-16 04:59:39,231 [pool-17-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-16 04:59:39,232 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:59:39,338 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-16 04:59:39,339 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.106 seconds to process 4 keys.
recon_1             | 2023-07-16 04:59:39,364 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-16 04:59:39,387 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-16 04:59:47,291 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-16 04:59:47,298 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@6a4d57b7, cost 441.602us
recon_1             | 2023-07-16 04:59:47,298 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
om_1                | 2023-07-16 04:58:43,292 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:58:44,808 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 04:58:44,979 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 04:58:44,980 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 04:58:45,242 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-16 04:58:45,286 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 04:58:45,287 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-16 04:58:45,310 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-16 04:58:45,320 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 04:58:45,349 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-16 04:58:45,358 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-16 04:58:45,385 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-16 04:58:45,479 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-07-16 04:58:45,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:58:45,481 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-07-16 04:58:45,482 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:58:45,482 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 04:58:45,483 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-16 04:58:45,485 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:58:45,488 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-16 04:58:45,489 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-16 04:58:45,701 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-16 04:58:45,704 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:58:45,704 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:58:45,717 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:58:45,725 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@628b819d[Not completed]
om_1                | 2023-07-16 04:58:45,725 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-16 04:58:45,759 [pool-23-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-16 04:58:45,765 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-16 04:58:45,766 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-16 04:58:45,767 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-16 04:58:45,769 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 04:58:45,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 04:58:45,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 04:58:45,769 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 04:58:45,770 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-16 04:58:45,781 [pool-23-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-16 04:58:45,781 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 04:58:45,786 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-16 04:58:45,787 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-16 04:58:45,789 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-16 04:58:45,803 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@c35516c768de
om_1                | 2023-07-16 04:58:45,809 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 04:58:45,886 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-16 04:58:45,894 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-16 04:58:45,899 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-16 04:58:45,922 [Listener at om/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om_1                | 2023-07-16 04:58:45,928 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-16 04:58:45,929 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 04:58:45,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:58:45,974 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-16 04:58:45,975 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-16 04:58:45,984 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-16 04:58:45,985 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:58:45,986 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-16 04:58:45,987 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 04:58:45,989 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-16 04:58:45,990 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-16 04:58:45,994 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-16 04:58:45,994 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 04:58:45,995 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 04:58:46,025 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-16 04:58:46,027 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-16 04:58:46,048 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:58:46,051 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 04:58:46,075 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 04:58:46,076 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-16 04:58:46,076 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-16 04:58:46,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-16 04:58:46,079 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-16 04:58:46,079 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-16 04:58:46,151 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-16 04:58:46,186 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 04:58:46,186 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-16 04:58:46,228 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.22.0.10:9862
om_1                | 2023-07-16 04:58:46,228 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-16 04:58:46,230 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-16 04:58:46,233 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-16 04:58:46,239 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:58:46,240 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-16 04:58:46,244 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-16 04:58:46,348 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-16 04:58:46,355 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-16 04:58:46,357 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$370/0x00000008404df040@110b7837] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-16 04:58:46,426 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-16 04:58:46,426 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 04:58:46,455 [Listener at om/9862] INFO util.log: Logging initialized @12992ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 04:58:46,640 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-07-16 04:58:46,650 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-16 04:58:46,655 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-16 04:58:46,657 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-16 04:58:46,657 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-16 04:58:46,657 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-16 04:58:46,753 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-16 04:58:46,755 [Listener at om/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
om_1                | 2023-07-16 04:58:46,802 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-16 04:58:46,802 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-16 04:58:46,804 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-07-16 04:58:46,816 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c951ada{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 04:58:46,817 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11f23038{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-16 04:58:47,170 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@ad3f70a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_1_jar-_-any-3493738851773511462/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/ozoneManager}
om_1                | 2023-07-16 04:58:47,186 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@2eb0cefe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-16 04:58:47,187 [Listener at om/9862] INFO server.Server: Started @13724ms
om_1                | 2023-07-16 04:58:47,191 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-16 04:58:47,191 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-16 04:58:47,193 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-16 04:58:47,193 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-16 04:58:47,194 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-16 04:58:47,214 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-07-16 04:58:47,242 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@45bbc52f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-16 04:58:51,265 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5026627599ns, electionTimeout:5014ms
om_1                | 2023-07-16 04:58:51,269 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 04:58:51,270 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-16 04:58:51,273 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-16 04:58:51,275 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:58:51,294 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-07-16 04:58:51,295 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-16 04:58:51,297 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 04:58:51,297 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-16 04:58:51,304 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5410ms
om_1                | 2023-07-16 04:58:51,322 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-16 04:58:51,330 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 04:58:51,335 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 04:58:51,352 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-07-16 04:58:51,352 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-16 04:58:51,353 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-16 04:58:51,359 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 04:58:51,368 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-16 04:58:51,370 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-16 04:58:51,440 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-16 04:58:51,502 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-07-16 04:58:51,766 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-16 04:58:52,099 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | ]
om_1                | 2023-07-16 04:58:52,604 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-16 04:59:38,355 [qtp2081751971-42] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-16 04:59:38,403 [qtp2081751971-42] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483578362 in 41 milliseconds
om_1                | 2023-07-16 04:59:38,499 [qtp2081751971-42] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 94 milliseconds
om_1                | 2023-07-16 04:59:38,500 [qtp2081751971-42] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483578362
Attaching to xcompat_om_1, xcompat_old_client_1_2_1_1, xcompat_old_client_1_1_0_1, xcompat_new_client_1, xcompat_scm_1, xcompat_recon_1, xcompat_s3g_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_0_0_1, xcompat_old_client_1_3_0_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 05:01:16,390 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 18990627588b/172.23.0.4
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.3.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_1          | STARTUP_MSG:   java = 11.0.14.1
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 05:01:16,440 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 05:01:16,759 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 05:01:17,445 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 05:01:18,687 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 05:01:18,701 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-16 05:01:19,367 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:18990627588b ip:172.23.0.4
datanode_1          | 2023-07-16 05:01:21,237 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1          | 2023-07-16 05:01:22,502 [main] INFO reflections.Reflections: Reflections took 1029 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_1          | 2023-07-16 05:01:23,780 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-16 05:01:24,718 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 05:01:24,817 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-16 05:01:24,819 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 05:01:24,824 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 05:01:24,908 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 05:01:25,180 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 05:01:25,181 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-16 05:01:25,196 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-16 05:01:25,197 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-16 05:01:25,198 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-16 05:01:25,548 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 05:01:25,549 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-16 05:01:34,050 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-16 05:01:34,643 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 05:01:35,554 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 05:01:36,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 05:01:36,125 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-16 05:01:36,132 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 05:01:36,137 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-16 05:01:36,139 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-16 05:01:36,140 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-16 05:01:36,155 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 05:01:36,160 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:01:36,161 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 05:01:36,164 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 05:01:36,261 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 05:01:36,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-16 05:01:36,314 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-16 05:01:38,998 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-16 05:01:39,029 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-16 05:01:39,029 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-16 05:01:39,030 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:01:39,030 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:01:39,069 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:01:39,329 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-16 05:01:40,541 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 05:01:40,724 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 05:01:41,029 [main] INFO util.log: Logging initialized @34848ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 05:01:42,658 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-16 05:01:42,688 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 05:01:42,763 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 05:01:42,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 05:01:42,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 05:01:42,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 05:01:43,417 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-16 05:01:43,423 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1          | 2023-07-16 05:01:43,655 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 05:01:43,668 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 05:01:43,670 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-16 05:01:43,838 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3434a4f0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 05:01:43,856 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 05:01:47,134 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5efcab{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-5782867749693648250/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-16 05:01:47,246 [main] INFO server.AbstractConnector: Started ServerConnector@3fdcde7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-16 05:01:47,247 [main] INFO server.Server: Started @41065ms
datanode_1          | 2023-07-16 05:01:47,320 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-16 05:01:47,320 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 05:01:47,333 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 05:01:47,388 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-16 05:01:47,609 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@32ac8df2] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-07-16 05:01:48,196 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_1          | 2023-07-16 05:01:48,524 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 05:01:50,900 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:50,908 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:51,903 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:51,912 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:52,913 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:53,924 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:54,926 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:55,927 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:01:56,950 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 18990627588b/172.23.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.4:33942 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.4:33942 remote=recon/172.23.0.6:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 05:01:16,507 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 707448f11eb8/172.23.0.9
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.3.0
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-07-16 05:02:00,961 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 18990627588b/172.23.0.4 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.4:40566 remote=scm/172.23.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.4:40566 remote=scm/172.23.0.7:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-07-16 05:02:03,074 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-6563edce-42f8-435a-a703-8231d2ab61a4/container.db to cache
datanode_1          | 2023-07-16 05:02:03,074 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-6563edce-42f8-435a-a703-8231d2ab61a4/container.db for volume DS-6563edce-42f8-435a-a703-8231d2ab61a4
datanode_1          | 2023-07-16 05:02:03,075 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 05:02:03,078 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-07-16 05:02:03,342 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_1          | 2023-07-16 05:02:03,403 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.RaftServer: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start RPC server
datanode_1          | 2023-07-16 05:02:03,441 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 05:02:03,450 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: GrpcService started, listening on 9856
datanode_1          | 2023-07-16 05:02:03,463 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: GrpcService started, listening on 9857
datanode_1          | 2023-07-16 05:02:03,485 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1 is started using port 9858 for RATIS
datanode_1          | 2023-07-16 05:02:03,485 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: Started
datanode_1          | 2023-07-16 05:02:03,486 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 05:02:03,486 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 05:02:03,653 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_2          | STARTUP_MSG:   java = 11.0.14.1
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 05:01:16,591 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 05:01:16,855 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 05:01:17,415 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 05:01:18,755 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 05:01:18,755 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-16 05:01:19,439 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:707448f11eb8 ip:172.23.0.9
datanode_2          | 2023-07-16 05:01:21,459 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2          | 2023-07-16 05:01:22,597 [main] INFO reflections.Reflections: Reflections took 876 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_2          | 2023-07-16 05:01:23,781 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-16 05:01:24,665 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 05:01:24,728 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-16 05:01:24,763 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 05:01:24,769 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 05:01:24,873 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 05:01:24,987 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 05:01:25,033 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-16 05:01:25,048 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-07-16 05:01:25,060 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-16 05:01:25,064 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-16 05:01:25,461 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 05:01:25,461 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-16 05:01:34,334 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-16 05:01:34,616 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 05:01:35,179 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 05:01:36,066 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 05:01:36,082 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-16 05:01:36,103 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 05:01:36,116 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-16 05:01:36,120 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-16 05:01:36,121 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-16 05:01:36,123 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 05:01:36,137 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:01:36,144 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 05:01:36,162 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:01:36,292 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:01:36,335 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-16 05:01:36,336 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-16 05:01:39,046 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-16 05:01:39,085 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-16 05:01:39,097 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-16 05:01:39,098 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:01:39,100 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 05:01:39,103 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:01:39,394 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-16 05:01:40,559 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 05:01:40,742 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 05:01:41,072 [main] INFO util.log: Logging initialized @35255ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 05:01:42,514 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-16 05:01:42,735 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 05:01:42,768 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 05:01:42,813 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 05:01:42,820 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.util.concurrent.TimeoutException
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	... 1 more
datanode_1          | 2023-07-16 05:02:06,781 [Command processor thread] INFO server.RaftServer: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: addNew group-2CD52AD987F2:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:1|startupRole:FOLLOWER] returns group-2CD52AD987F2:java.util.concurrent.CompletableFuture@68cc40cf[Not completed]
datanode_1          | 2023-07-16 05:02:06,804 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: new RaftServerImpl for group-2CD52AD987F2:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 05:02:06,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 05:02:06,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 05:02:06,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 05:02:06,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:06,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:02:06,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:02:06,815 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: ConfigurationManager, init=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 05:02:06,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:02:06,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:02:06,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 05:02:06,841 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:06,847 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 05:02:06,848 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 05:02:06,947 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:02:06,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 05:02:06,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 05:02:06,953 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 05:02:06,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 05:02:06,955 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6bf2b11e-235c-438f-a2ae-2cd52ad987f2 does not exist. Creating ...
datanode_1          | 2023-07-16 05:02:06,960 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6bf2b11e-235c-438f-a2ae-2cd52ad987f2/in_use.lock acquired by nodename 7@18990627588b
datanode_1          | 2023-07-16 05:02:06,971 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6bf2b11e-235c-438f-a2ae-2cd52ad987f2 has been successfully formatted.
datanode_1          | 2023-07-16 05:02:06,988 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2CD52AD987F2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 05:02:06,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:02:07,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:02:07,013 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:02:07,014 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 05:02:07,017 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 05:02:07,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,065 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:02:07,069 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 05:02:07,100 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6bf2b11e-235c-438f-a2ae-2cd52ad987f2
datanode_1          | 2023-07-16 05:02:07,119 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 05:02:07,120 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:02:07,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 05:02:07,126 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 05:02:07,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-07-16 05:01:16,911 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = c8bf479bdd0a/172.23.0.5
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.3.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_3          | STARTUP_MSG:   java = 11.0.14.1
datanode_3          | ************************************************************/
datanode_3          | 2023-07-16 05:01:16,948 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 05:01:17,288 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 05:01:18,190 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 05:01:19,324 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-07-16 05:01:19,324 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-16 05:01:19,879 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c8bf479bdd0a ip:172.23.0.5
datanode_3          | 2023-07-16 05:01:21,786 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3          | 2023-07-16 05:01:23,001 [main] INFO reflections.Reflections: Reflections took 1041 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_3          | 2023-07-16 05:01:24,115 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-16 05:01:25,273 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 05:01:25,409 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-07-16 05:01:25,426 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 05:01:25,428 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 05:01:25,659 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 05:01:25,882 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 05:01:25,883 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-07-16 05:01:25,951 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-07-16 05:01:25,952 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-16 05:01:25,952 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-16 05:01:26,175 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-07-16 05:01:26,182 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-16 05:01:35,011 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-16 05:01:35,681 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-07-16 05:01:36,147 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 05:01:37,081 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-16 05:01:37,118 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-07-16 05:01:37,127 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-16 05:01:37,132 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-16 05:01:37,133 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-07-16 05:01:37,133 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-16 05:01:37,135 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-07-16 05:01:37,181 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:01:37,185 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-07-16 05:01:37,215 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:01:37,383 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 05:01:37,514 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-16 05:01:37,517 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-07-16 05:01:40,252 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-16 05:01:40,275 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-16 05:01:40,320 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-07-16 05:01:40,320 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:01:40,324 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:01:40,341 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 05:01:40,431 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-16 05:01:41,322 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-07-16 05:01:41,543 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 05:01:42,101 [main] INFO util.log: Logging initialized @35872ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 05:01:43,221 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-07-16 05:01:43,244 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-07-16 05:01:43,310 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-07-16 05:01:43,322 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 05:01:43,323 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 05:02:07,129 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 05:02:07,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 05:02:07,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,150 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:07,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:07,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:07,184 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:07,185 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:07,186 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: start as a follower, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:07,190 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 05:02:07,191 [pool-22-thread-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState
datanode_1          | 2023-07-16 05:02:07,202 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2CD52AD987F2,id=99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_1          | 2023-07-16 05:02:07,202 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:07,202 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:07,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:07,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 05:02:07,211 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 05:02:07,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 05:02:07,297 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6bf2b11e-235c-438f-a2ae-2cd52ad987f2
datanode_1          | 2023-07-16 05:02:07,311 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6bf2b11e-235c-438f-a2ae-2cd52ad987f2.
datanode_1          | 2023-07-16 05:02:07,312 [Command processor thread] INFO server.RaftServer: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: addNew group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-2E7C004CACCE:java.util.concurrent.CompletableFuture@67fef043[Not completed]
datanode_1          | 2023-07-16 05:02:07,344 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: new RaftServerImpl for group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 05:02:07,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 05:02:07,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 05:02:07,345 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 05:02:07,345 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:07,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:02:07,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:02:07,346 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: ConfigurationManager, init=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 05:02:07,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:02:07,353 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:02:07,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 05:02:07,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:07,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 05:02:07,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 05:02:07,364 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:02:07,376 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 05:02:07,376 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 05:02:07,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 05:02:07,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:01:43,337 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-07-16 05:01:43,897 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 05:01:43,903 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3          | 2023-07-16 05:01:44,151 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 05:01:44,151 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 05:01:44,198 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-07-16 05:01:44,474 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@71560f51{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 05:01:44,493 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@67d4c48d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-07-16 05:01:46,995 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6df4af5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-17469322295056465389/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 05:01:47,193 [main] INFO server.AbstractConnector: Started ServerConnector@7a65a360{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-07-16 05:01:47,197 [main] INFO server.Server: Started @40968ms
datanode_3          | 2023-07-16 05:01:47,268 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-16 05:01:47,268 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-16 05:01:47,272 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 05:01:47,341 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-07-16 05:01:47,454 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@478ddd13] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-07-16 05:01:47,984 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_3          | 2023-07-16 05:01:48,233 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 05:01:50,867 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:50,870 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:51,871 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:51,876 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:52,877 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:53,878 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:54,879 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:55,881 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:01:56,907 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From c8bf479bdd0a/172.23.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:46906 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:46906 remote=recon/172.23.0.6:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 2023-07-16 05:01:42,822 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 05:01:43,296 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-16 05:01:43,313 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2          | 2023-07-16 05:01:43,585 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 05:01:43,592 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 05:01:43,609 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-07-16 05:01:43,706 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e002356{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 05:01:43,724 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f3fc42f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 05:01:46,743 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34045582{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-3603068013575151156/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 05:01:46,895 [main] INFO server.AbstractConnector: Started ServerConnector@4017fe2c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-16 05:01:46,896 [main] INFO server.Server: Started @41116ms
datanode_2          | 2023-07-16 05:01:46,914 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 05:01:46,914 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-16 05:01:46,918 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 05:01:47,016 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-16 05:01:47,237 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@69dc9123] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-07-16 05:01:47,845 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_2          | 2023-07-16 05:01:48,263 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 05:01:50,680 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:50,712 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:51,684 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:51,714 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:52,685 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:53,686 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:54,687 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:55,689 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.7:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:01:56,778 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 707448f11eb8/172.23.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:39924 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:39924 remote=recon/172.23.0.6:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-07-16 05:02:00,893 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From c8bf479bdd0a/172.23.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:56872 remote=scm/172.23.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:56872 remote=scm/172.23.0.7:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-07-16 05:02:03,053 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-580eddb2-3627-4f78-be9b-3eedccf4bede/container.db to cache
datanode_3          | 2023-07-16 05:02:03,053 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-580eddb2-3627-4f78-be9b-3eedccf4bede/container.db for volume DS-580eddb2-3627-4f78-be9b-3eedccf4bede
datanode_3          | 2023-07-16 05:02:03,055 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-07-16 05:02:03,072 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-07-16 05:02:03,338 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:03,485 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.RaftServer: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start RPC server
datanode_3          | 2023-07-16 05:02:03,493 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: GrpcService started, listening on 9858
datanode_3          | 2023-07-16 05:02:03,497 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: GrpcService started, listening on 9856
datanode_3          | 2023-07-16 05:02:03,501 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: GrpcService started, listening on 9857
datanode_3          | 2023-07-16 05:02:03,524 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c94c9266-9edb-44c8-a5eb-4c0716fa66dd is started using port 9858 for RATIS
datanode_3          | 2023-07-16 05:02:03,524 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c94c9266-9edb-44c8-a5eb-4c0716fa66dd is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-07-16 05:02:03,524 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c94c9266-9edb-44c8-a5eb-4c0716fa66dd is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-07-16 05:02:03,525 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c94c9266-9edb-44c8-a5eb-4c0716fa66dd: Started
datanode_3          | 2023-07-16 05:02:08,843 [Command processor thread] INFO server.RaftServer: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: addNew group-9EB3489B6695:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-9EB3489B6695:java.util.concurrent.CompletableFuture@268e8028[Not completed]
datanode_3          | 2023-07-16 05:02:08,915 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: new RaftServerImpl for group-9EB3489B6695:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 05:02:08,940 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 05:02:08,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 05:02:08,944 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 05:02:08,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:08,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:02:08,955 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 05:02:08,995 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: ConfigurationManager, init=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 05:02:09,008 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 05:02:09,030 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:02:09,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:02:09,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:09,122 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:02:09,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 05:02:09,218 [grpc-default-executor-0] INFO server.RaftServer: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: addNew group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER] returns group-2E7C004CACCE:java.util.concurrent.CompletableFuture@3556c4a5[Not completed]
datanode_3          | 2023-07-16 05:02:09,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:02:09,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,394 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: new RaftServerImpl for group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 05:02:09,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 05:02:09,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 05:02:09,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 05:02:09,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:09,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:02:09,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 05:02:09,397 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: ConfigurationManager, init=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 05:02:09,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 05:02:09,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:02:09,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:02:09,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:09,400 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:02:09,400 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 05:02:09,404 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:02:09,405 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,406 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,406 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,414 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/abad7756-298a-4535-9029-9eb3489b6695 does not exist. Creating ...
datanode_3          | 2023-07-16 05:02:09,442 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/abad7756-298a-4535-9029-9eb3489b6695/in_use.lock acquired by nodename 7@c8bf479bdd0a
datanode_3          | 2023-07-16 05:02:09,462 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/abad7756-298a-4535-9029-9eb3489b6695 has been successfully formatted.
datanode_1          | 2023-07-16 05:02:07,380 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce does not exist. Creating ...
datanode_1          | 2023-07-16 05:02:07,390 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/in_use.lock acquired by nodename 7@18990627588b
datanode_1          | 2023-07-16 05:02:07,400 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce has been successfully formatted.
datanode_1          | 2023-07-16 05:02:07,404 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2E7C004CACCE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 05:02:07,404 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:02:07,407 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:02:07,431 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:02:07,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 05:02:07,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 05:02:07,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:02:07,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 05:02:07,463 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce
datanode_1          | 2023-07-16 05:02:07,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 05:02:07,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 05:02:07,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 05:02:07,466 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:07,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:07,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:07,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:07,468 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:07,468 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:07,468 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: start as a follower, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:07,469 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 05:02:07,470 [pool-22-thread-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState
datanode_1          | 2023-07-16 05:02:07,471 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2E7C004CACCE,id=99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_1          | 2023-07-16 05:02:07,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:07,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 05:02:07,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 05:02:07,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 05:02:07,475 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:07,476 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=025125ff-099f-4966-8b13-2e7c004cacce
datanode_1          | 2023-07-16 05:02:07,497 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:10,947 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce.
datanode_1          | 2023-07-16 05:02:10,948 [Command processor thread] INFO server.RaftServer: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: addNew group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER] returns group-3D3F70EF2B3F:java.util.concurrent.CompletableFuture@72524683[Not completed]
datanode_3          | 2023-07-16 05:02:09,475 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9EB3489B6695: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:02:09,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 05:02:09,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:02:09,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:02:09,553 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 05:02:09,554 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 05:02:09,569 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,586 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 05:02:09,588 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 05:02:09,606 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/abad7756-298a-4535-9029-9eb3489b6695
datanode_3          | 2023-07-16 05:02:09,607 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 05:02:09,607 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:09,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,615 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 05:02:09,616 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 05:02:09,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:02:09,618 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 05:02:09,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 05:02:09,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:09,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:09,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:09,665 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:09,665 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:09,670 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: start as a follower, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:09,670 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:02:09,672 [pool-22-thread-1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState
datanode_3          | 2023-07-16 05:02:09,701 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:09,708 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:09,712 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9EB3489B6695,id=c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:09,714 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:09,715 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:02:09,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:02:09,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:02:09,772 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce does not exist. Creating ...
datanode_3          | 2023-07-16 05:02:09,773 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=abad7756-298a-4535-9029-9eb3489b6695
datanode_3          | 2023-07-16 05:02:09,785 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=abad7756-298a-4535-9029-9eb3489b6695.
datanode_3          | 2023-07-16 05:02:09,786 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/in_use.lock acquired by nodename 7@c8bf479bdd0a
datanode_3          | 2023-07-16 05:02:09,788 [Command processor thread] INFO server.RaftServer: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: addNew group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER] returns group-3D3F70EF2B3F:java.util.concurrent.CompletableFuture@11b3f247[Not completed]
datanode_3          | 2023-07-16 05:02:09,788 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce has been successfully formatted.
datanode_3          | 2023-07-16 05:02:09,789 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2E7C004CACCE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:02:09,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 05:02:09,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:02:09,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:02:09,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 05:02:09,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 05:02:09,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 05:02:09,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 05:02:09,797 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce
datanode_3          | 2023-07-16 05:02:09,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 05:02:09,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:09,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 05:02:09,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 05:02:09,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:02:09,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 05:02:09,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 05:02:09,809 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,810 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:09,810 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:09,810 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:09,823 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:09,825 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:09,825 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: start as a follower, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:09,826 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:02:09,826 [pool-22-thread-1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState
datanode_3          | 2023-07-16 05:02:09,827 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2E7C004CACCE,id=c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:09,838 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:09,851 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:09,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:09,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:02:09,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:02:09,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:02:09,877 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: new RaftServerImpl for group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-07-16 05:02:09,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 05:02:09,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 05:02:09,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 05:02:09,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:09,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:02:09,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 05:02:09,882 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: ConfigurationManager, init=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 05:02:09,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 05:02:09,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:02:09,887 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:02:09,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:02:09,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:02:09,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 05:02:09,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:02:09,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:02:09,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:02:09,915 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f does not exist. Creating ...
datanode_3          | 2023-07-16 05:02:09,922 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/in_use.lock acquired by nodename 7@c8bf479bdd0a
datanode_3          | 2023-07-16 05:02:09,924 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f has been successfully formatted.
datanode_3          | 2023-07-16 05:02:09,925 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3D3F70EF2B3F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:02:09,925 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 05:02:09,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:02:09,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:02:09,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 05:02:09,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 05:02:09,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 05:02:09,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 05:02:09,961 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 05:02:09,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:02:09,963 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 05:02:09,963 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 05:02:09,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 05:02:10,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:10,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:02:10,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:10,044 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:10,044 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:02:10,045 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: start as a follower, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:10,045 [pool-22-thread-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:02:10,045 [pool-22-thread-1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:10,072 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3D3F70EF2B3F,id=c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:10,074 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 05:02:10,075 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:02:10,075 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:02:10,075 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:02:10,076 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:10,085 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:10,084 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f
datanode_3          | 2023-07-16 05:02:11,310 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f.
datanode_3          | 2023-07-16 05:02:12,699 [grpc-default-executor-0] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: receive requestVote(ELECTION, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1, group-2E7C004CACCE, 1, (t:0, i:0))
datanode_3          | 2023-07-16 05:02:12,706 [grpc-default-executor-0] INFO impl.VoteContext: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FOLLOWER: reject ELECTION from 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: our priority 1 > candidate's priority 0
datanode_3          | 2023-07-16 05:02:12,709 [grpc-default-executor-0] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_3          | 2023-07-16 05:02:12,710 [grpc-default-executor-0] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState
datanode_3          | 2023-07-16 05:02:12,710 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState was interrupted
datanode_3          | 2023-07-16 05:02:12,712 [grpc-default-executor-0] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState
datanode_3          | 2023-07-16 05:02:12,716 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:12,720 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:12,734 [grpc-default-executor-0] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE replies to ELECTION vote request: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1<-c94c9266-9edb-44c8-a5eb-4c0716fa66dd#0:FAIL-t1. Peer's state: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE:t1, leader=null, voted=null, raftlog=Memoized:c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:14,755 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5083173457ns, electionTimeout:5045ms
datanode_3          | 2023-07-16 05:02:14,756 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState
datanode_3          | 2023-07-16 05:02:14,756 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 05:02:14,759 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 05:02:14,759 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1
datanode_3          | 2023-07-16 05:02:14,764 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:14,765 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-16 05:02:14,766 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1
datanode_3          | 2023-07-16 05:02:14,766 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 05:02:14,766 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9EB3489B6695 with new leaderId: c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:14,768 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: change Leader from null to c94c9266-9edb-44c8-a5eb-4c0716fa66dd at term 1 for becomeLeader, leader elected after 5675ms
datanode_3          | 2023-07-16 05:02:14,784 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 05:02:14,796 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:14,801 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 05:02:14,805 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 05:02:14,810 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 05:02:14,810 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 05:02:14,818 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:14,824 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 05:02:14,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderStateImpl
datanode_3          | 2023-07-16 05:02:14,870 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 05:02:14,921 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-LeaderElection1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695: set configuration 0: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:14,981 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-9EB3489B6695-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/abad7756-298a-4535-9029-9eb3489b6695/current/log_inprogress_0
datanode_3          | 2023-07-16 05:02:15,182 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137143999ns, electionTimeout:5096ms
datanode_3          | 2023-07-16 05:02:15,183 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:15,183 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 05:02:15,184 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 05:02:15,184 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2
datanode_3          | 2023-07-16 05:02:15,191 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:15,197 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 795320bb-f526-452d-9845-732289f5fb28
datanode_3          | 2023-07-16 05:02:15,198 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:15,202 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:15,211 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_3          | 2023-07-16 05:02:15,240 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 05:02:15,240 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection:   Response 0: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:FAIL-t1
datanode_3          | 2023-07-16 05:02:15,241 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-16 05:02:15,241 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-07-16 05:02:15,241 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2
datanode_3          | 2023-07-16 05:02:15,241 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection2] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:15,261 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:15,263 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:17,737 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5025944640ns, electionTimeout:5017ms
datanode_3          | 2023-07-16 05:02:17,738 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState
datanode_3          | 2023-07-16 05:02:17,738 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-16 05:02:17,738 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 05:02:17,738 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3
datanode_1          | 2023-07-16 05:02:10,961 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: new RaftServerImpl for group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 05:02:10,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 05:02:10,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 05:02:10,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 05:02:10,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:10,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:02:10,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:02:10,963 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: ConfigurationManager, init=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 05:02:10,963 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:02:10,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:02:10,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 05:02:10,965 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:02:10,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 05:02:10,974 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 05:02:10,984 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:02:10,984 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 05:02:10,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 05:02:10,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 05:02:10,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 05:02:10,986 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f does not exist. Creating ...
datanode_1          | 2023-07-16 05:02:10,990 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/in_use.lock acquired by nodename 7@18990627588b
datanode_1          | 2023-07-16 05:02:10,993 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f has been successfully formatted.
datanode_1          | 2023-07-16 05:02:10,994 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3D3F70EF2B3F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 05:02:11,010 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:02:11,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:02:11,015 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:02:11,016 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 05:02:11,016 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 05:02:11,016 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:11,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:02:11,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 05:02:11,022 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f
datanode_1          | 2023-07-16 05:02:11,022 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-07-16 05:02:11,022 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:02:11,022 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:11,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 05:02:11,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-07-16 05:02:11,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-07-16 05:02:11,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 05:02:11,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 05:02:11,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 05:02:11,038 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:11,039 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:02:11,039 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:11,056 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:11,056 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:02:11,057 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: start as a follower, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-07-16 05:02:00,705 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 707448f11eb8/172.23.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:42790 remote=scm/172.23.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:42790 remote=scm/172.23.0.7:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-07-16 05:02:03,034 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-899d59b3-c49e-4077-811c-e6d0135406b0/container.db to cache
datanode_2          | 2023-07-16 05:02:03,035 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-e68c2822-703e-4616-bc56-61db335b5854/DS-899d59b3-c49e-4077-811c-e6d0135406b0/container.db for volume DS-899d59b3-c49e-4077-811c-e6d0135406b0
datanode_2          | 2023-07-16 05:02:03,044 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-16 05:02:03,054 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-07-16 05:02:03,349 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:03,491 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.RaftServer: 795320bb-f526-452d-9845-732289f5fb28: start RPC server
datanode_2          | 2023-07-16 05:02:03,527 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 795320bb-f526-452d-9845-732289f5fb28: GrpcService started, listening on 9858
datanode_2          | 2023-07-16 05:02:03,532 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 795320bb-f526-452d-9845-732289f5fb28: GrpcService started, listening on 9856
datanode_2          | 2023-07-16 05:02:03,549 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO server.GrpcService: 795320bb-f526-452d-9845-732289f5fb28: GrpcService started, listening on 9857
datanode_2          | 2023-07-16 05:02:03,589 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 795320bb-f526-452d-9845-732289f5fb28 is started using port 9858 for RATIS
datanode_2          | 2023-07-16 05:02:03,590 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 795320bb-f526-452d-9845-732289f5fb28 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-07-16 05:02:03,590 [EndpointStateMachine task thread for scm/172.23.0.7:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 795320bb-f526-452d-9845-732289f5fb28 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-07-16 05:02:03,592 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-795320bb-f526-452d-9845-732289f5fb28: Started
datanode_2          | 2023-07-16 05:02:08,433 [Command processor thread] INFO server.RaftServer: 795320bb-f526-452d-9845-732289f5fb28: addNew group-4CE0C81B386D:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-4CE0C81B386D:java.util.concurrent.CompletableFuture@bf919c9[Not completed]
datanode_3          | 2023-07-16 05:02:17,744 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:17,745 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:17,750 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:17,776 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 05:02:17,777 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.LeaderElection:   Response 0: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:OK-t2
datanode_3          | 2023-07-16 05:02:17,778 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-07-16 05:02:17,779 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3
datanode_3          | 2023-07-16 05:02:17,779 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-07-16 05:02:17,781 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2E7C004CACCE with new leaderId: c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_3          | 2023-07-16 05:02:17,784 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: change Leader from null to c94c9266-9edb-44c8-a5eb-4c0716fa66dd at term 2 for becomeLeader, leader elected after 8381ms
datanode_3          | 2023-07-16 05:02:17,785 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 05:02:17,785 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:17,786 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 05:02:17,786 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 05:02:17,796 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 05:02:17,796 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 05:02:17,800 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:02:17,800 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 05:02:17,829 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 05:02:17,829 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:02:17,829 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 05:02:17,832 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 05:02:17,833 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:02:17,833 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:02:17,833 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 05:02:17,833 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:02:17,837 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:02:17,838 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 05:02:17,838 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-07-16 05:02:17,839 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderStateImpl
datanode_3          | 2023-07-16 05:02:17,840 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 05:02:17,843 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/current/log_inprogress_0
datanode_3          | 2023-07-16 05:02:17,859 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE-LeaderElection3] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-2E7C004CACCE: set configuration 0: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:20,310 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068745116ns, electionTimeout:5047ms
datanode_3          | 2023-07-16 05:02:20,310 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:20,311 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-07-16 05:02:20,311 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-07-16 05:02:20,311 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4
datanode_3          | 2023-07-16 05:02:20,320 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:20,321 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:20,323 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:20,372 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 05:02:20,377 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.LeaderElection:   Response 0: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t2
datanode_3          | 2023-07-16 05:02:20,377 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.LeaderElection:   Response 1: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:FAIL-t2
datanode_3          | 2023-07-16 05:02:20,378 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.LeaderElection: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4 ELECTION round 0: result REJECTED
datanode_3          | 2023-07-16 05:02:20,378 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-07-16 05:02:20,379 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4
datanode_3          | 2023-07-16 05:02:20,379 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-LeaderElection4] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:20,380 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:20,385 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:20,414 [grpc-default-executor-0] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: receive requestVote(ELECTION, 795320bb-f526-452d-9845-732289f5fb28, group-3D3F70EF2B3F, 2, (t:0, i:0))
datanode_3          | 2023-07-16 05:02:20,415 [grpc-default-executor-0] INFO impl.VoteContext: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FOLLOWER: reject ELECTION from 795320bb-f526-452d-9845-732289f5fb28: already has voted for c94c9266-9edb-44c8-a5eb-4c0716fa66dd at current term 2
datanode_3          | 2023-07-16 05:02:20,415 [grpc-default-executor-0] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F replies to ELECTION vote request: 795320bb-f526-452d-9845-732289f5fb28<-c94c9266-9edb-44c8-a5eb-4c0716fa66dd#0:FAIL-t2. Peer's state: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F:t2, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:25,536 [grpc-default-executor-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: receive requestVote(ELECTION, 795320bb-f526-452d-9845-732289f5fb28, group-3D3F70EF2B3F, 3, (t:0, i:0))
datanode_3          | 2023-07-16 05:02:25,536 [grpc-default-executor-1] INFO impl.VoteContext: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FOLLOWER: accept ELECTION from 795320bb-f526-452d-9845-732289f5fb28: our priority 0 <= candidate's priority 1
datanode_3          | 2023-07-16 05:02:25,537 [grpc-default-executor-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:795320bb-f526-452d-9845-732289f5fb28
datanode_3          | 2023-07-16 05:02:25,537 [grpc-default-executor-1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: shutdown c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:25,537 [grpc-default-executor-1] INFO impl.RoleInfo: c94c9266-9edb-44c8-a5eb-4c0716fa66dd: start c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState
datanode_3          | 2023-07-16 05:02:25,537 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState was interrupted
datanode_3          | 2023-07-16 05:02:25,546 [grpc-default-executor-1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F replies to ELECTION vote request: 795320bb-f526-452d-9845-732289f5fb28<-c94c9266-9edb-44c8-a5eb-4c0716fa66dd#0:OK-t3. Peer's state: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F:t3, leader=null, voted=795320bb-f526-452d-9845-732289f5fb28, raftlog=Memoized:c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:25,563 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:02:25,568 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:02:25,767 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3D3F70EF2B3F with new leaderId: 795320bb-f526-452d-9845-732289f5fb28
datanode_3          | 2023-07-16 05:02:25,776 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd-server-thread1] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: change Leader from null to 795320bb-f526-452d-9845-732289f5fb28 at term 3 for appendEntries, leader elected after 15879ms
datanode_3          | 2023-07-16 05:02:25,791 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd-server-thread2] INFO server.RaftServer$Division: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F: set configuration 0: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:02:25,792 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd-server-thread2] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 05:02:25,794 [c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c94c9266-9edb-44c8-a5eb-4c0716fa66dd@group-3D3F70EF2B3F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/current/log_inprogress_0
datanode_2          | 2023-07-16 05:02:08,477 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28: new RaftServerImpl for group-4CE0C81B386D:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 05:02:08,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:02:08,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 05:02:08,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 05:02:08,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:08,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 05:02:08,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 05:02:08,491 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: ConfigurationManager, init=-1: peers:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 05:02:08,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:02:08,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 05:02:08,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 05:02:08,553 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:08,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 05:02:08,562 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 05:02:08,749 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:02:08,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 05:02:08,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:02:08,764 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 05:02:08,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 05:02:08,770 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d1126c79-63f2-4929-8ac7-4ce0c81b386d does not exist. Creating ...
datanode_2          | 2023-07-16 05:02:08,800 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1126c79-63f2-4929-8ac7-4ce0c81b386d/in_use.lock acquired by nodename 7@707448f11eb8
datanode_2          | 2023-07-16 05:02:08,823 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d1126c79-63f2-4929-8ac7-4ce0c81b386d has been successfully formatted.
datanode_2          | 2023-07-16 05:02:08,873 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-4CE0C81B386D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 05:02:08,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 05:02:08,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:02:08,956 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:02:08,959 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:02:08,965 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:02:08,973 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:08,998 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 05:02:09,010 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:02:09,022 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1126c79-63f2-4929-8ac7-4ce0c81b386d
datanode_2          | 2023-07-16 05:02:09,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 05:02:09,035 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:09,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:09,050 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 05:02:09,052 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 05:02:09,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 05:02:09,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 05:02:09,057 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 05:02:09,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:09,105 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:09,124 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:09,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:09,153 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:09,157 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:09,168 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: start as a follower, conf=-1: peers:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:09,168 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 05:02:09,176 [pool-22-thread-1] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState
datanode_2          | 2023-07-16 05:02:09,199 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4CE0C81B386D,id=795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:09,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:09,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 05:02:09,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 05:02:09,211 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 05:02:09,218 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:09,218 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:09,305 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d1126c79-63f2-4929-8ac7-4ce0c81b386d
datanode_2          | 2023-07-16 05:02:09,306 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d1126c79-63f2-4929-8ac7-4ce0c81b386d.
datanode_2          | 2023-07-16 05:02:09,307 [Command processor thread] INFO server.RaftServer: 795320bb-f526-452d-9845-732289f5fb28: addNew group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-2E7C004CACCE:java.util.concurrent.CompletableFuture@1e11f7fa[Not completed]
datanode_2          | 2023-07-16 05:02:09,311 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28: new RaftServerImpl for group-2E7C004CACCE:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 05:02:09,325 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:02:09,326 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 05:02:09,326 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 05:02:09,326 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:09,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 05:02:09,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 05:02:09,334 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: ConfigurationManager, init=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 05:02:09,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:02:09,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 05:02:09,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 05:02:09,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:09,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 05:02:09,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 05:02:09,338 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:02:09,339 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 05:02:09,339 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:02:09,339 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 05:02:09,340 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 05:02:09,340 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce does not exist. Creating ...
datanode_2          | 2023-07-16 05:02:09,350 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/in_use.lock acquired by nodename 7@707448f11eb8
datanode_2          | 2023-07-16 05:02:09,361 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce has been successfully formatted.
datanode_2          | 2023-07-16 05:02:09,364 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2E7C004CACCE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 05:02:09,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 05:02:09,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:02:09,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:02:09,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:02:09,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:02:09,370 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:09,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 05:02:09,377 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:02:09,377 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce
datanode_2          | 2023-07-16 05:02:09,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 05:02:09,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:09,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:09,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 05:02:09,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 05:02:09,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 05:02:09,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 05:02:09,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 05:02:09,382 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:09,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:09,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:09,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:09,389 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:09,391 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:09,392 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: start as a follower, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:09,392 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 05:02:09,393 [pool-22-thread-1] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState
datanode_2          | 2023-07-16 05:02:09,408 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2E7C004CACCE,id=795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:09,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:09,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 05:02:09,411 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 05:02:09,411 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 05:02:09,412 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=025125ff-099f-4966-8b13-2e7c004cacce
datanode_2          | 2023-07-16 05:02:09,410 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:09,424 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:10,754 [grpc-default-executor-0] INFO server.RaftServer: 795320bb-f526-452d-9845-732289f5fb28: addNew group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER] returns group-3D3F70EF2B3F:java.util.concurrent.CompletableFuture@3224dd10[Not completed]
datanode_2          | 2023-07-16 05:02:10,756 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28: new RaftServerImpl for group-3D3F70EF2B3F:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 05:02:10,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:02:10,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-07-16 05:02:10,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 05:02:10,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:10,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 05:02:10,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 05:02:10,776 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: ConfigurationManager, init=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-07-16 05:02:10,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:02:10,777 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:02:11,057 [pool-22-thread-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 05:02:11,057 [pool-22-thread-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:11,058 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3D3F70EF2B3F,id=99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_1          | 2023-07-16 05:02:11,058 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-07-16 05:02:11,058 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-07-16 05:02:11,058 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-07-16 05:02:11,058 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-07-16 05:02:11,059 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:11,077 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:11,077 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f
datanode_1          | 2023-07-16 05:02:11,318 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f.
datanode_1          | 2023-07-16 05:02:12,234 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5042809168ns, electionTimeout:5030ms
datanode_1          | 2023-07-16 05:02:12,234 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState
datanode_1          | 2023-07-16 05:02:12,235 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 05:02:12,238 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 05:02:12,238 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-FollowerState] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1
datanode_1          | 2023-07-16 05:02:12,247 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO impl.LeaderElection: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:12,248 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO impl.LeaderElection: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-07-16 05:02:12,248 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1
datanode_1          | 2023-07-16 05:02:12,249 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 05:02:12,249 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2CD52AD987F2 with new leaderId: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_1          | 2023-07-16 05:02:12,249 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: change Leader from null to 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1 at term 1 for becomeLeader, leader elected after 5408ms
datanode_1          | 2023-07-16 05:02:12,273 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 05:02:12,279 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:02:12,280 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 05:02:12,292 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-07-16 05:02:12,292 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 05:02:12,293 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-07-16 05:02:12,304 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:02:12,305 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-07-16 05:02:12,308 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderStateImpl
datanode_1          | 2023-07-16 05:02:12,327 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 05:02:12,364 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-LeaderElection1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2: set configuration 0: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:12,443 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2CD52AD987F2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6bf2b11e-235c-438f-a2ae-2cd52ad987f2/current/log_inprogress_0
datanode_1          | 2023-07-16 05:02:12,616 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5145841204ns, electionTimeout:5112ms
datanode_1          | 2023-07-16 05:02:12,617 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState
datanode_1          | 2023-07-16 05:02:12,618 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-07-16 05:02:12,618 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-07-16 05:02:12,618 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2
datanode_1          | 2023-07-16 05:02:12,622 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.LeaderElection: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:12,639 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:12,639 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:12,644 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_1          | 2023-07-16 05:02:12,645 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 795320bb-f526-452d-9845-732289f5fb28
datanode_1          | 2023-07-16 05:02:12,746 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.LeaderElection: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-07-16 05:02:12,747 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.LeaderElection:   Response 0: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1<-795320bb-f526-452d-9845-732289f5fb28#0:OK-t1
datanode_1          | 2023-07-16 05:02:12,747 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.LeaderElection:   Response 1: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1<-c94c9266-9edb-44c8-a5eb-4c0716fa66dd#0:FAIL-t1
datanode_1          | 2023-07-16 05:02:12,748 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.LeaderElection: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-07-16 05:02:12,748 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-07-16 05:02:12,748 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2
datanode_1          | 2023-07-16 05:02:12,748 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-LeaderElection2] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState
datanode_1          | 2023-07-16 05:02:12,764 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:12,770 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:15,247 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-3D3F70EF2B3F, 1, (t:0, i:0))
datanode_1          | 2023-07-16 05:02:15,249 [grpc-default-executor-0] INFO impl.VoteContext: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FOLLOWER: accept ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: our priority 0 <= candidate's priority 0
datanode_1          | 2023-07-16 05:02:15,249 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_1          | 2023-07-16 05:02:15,249 [grpc-default-executor-0] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:15,249 [grpc-default-executor-0] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:15,250 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState was interrupted
datanode_1          | 2023-07-16 05:02:15,261 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:15,262 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:15,267 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t1. Peer's state: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F:t1, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:17,757 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-2E7C004CACCE, 2, (t:0, i:0))
datanode_1          | 2023-07-16 05:02:17,758 [grpc-default-executor-0] INFO impl.VoteContext: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FOLLOWER: accept ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 05:02:10,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 05:02:17,759 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_1          | 2023-07-16 05:02:17,760 [grpc-default-executor-0] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState
datanode_1          | 2023-07-16 05:02:17,760 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState was interrupted
datanode_1          | 2023-07-16 05:02:17,760 [grpc-default-executor-0] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState
datanode_1          | 2023-07-16 05:02:17,771 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:17,772 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:17,773 [grpc-default-executor-0] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t2. Peer's state: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE:t2, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:17,918 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2E7C004CACCE with new leaderId: c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_1          | 2023-07-16 05:02:17,920 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: change Leader from null to c94c9266-9edb-44c8-a5eb-4c0716fa66dd at term 2 for appendEntries, leader elected after 10564ms
datanode_1          | 2023-07-16 05:02:17,986 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE: set configuration 0: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:17,988 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 05:02:18,004 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-2E7C004CACCE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/current/log_inprogress_0
datanode_1          | 2023-07-16 05:02:20,277 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:20,278 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:20,328 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-3D3F70EF2B3F, 2, (t:0, i:0))
datanode_1          | 2023-07-16 05:02:20,328 [grpc-default-executor-1] INFO impl.VoteContext: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FOLLOWER: accept ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: our priority 0 <= candidate's priority 0
datanode_1          | 2023-07-16 05:02:20,328 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_1          | 2023-07-16 05:02:20,329 [grpc-default-executor-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:20,329 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState was interrupted
datanode_1          | 2023-07-16 05:02:20,330 [grpc-default-executor-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:20,332 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:20,332 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:20,344 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t2. Peer's state: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F:t2, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:20,381 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: receive requestVote(ELECTION, 795320bb-f526-452d-9845-732289f5fb28, group-3D3F70EF2B3F, 2, (t:0, i:0))
datanode_1          | 2023-07-16 05:02:20,383 [grpc-default-executor-1] INFO impl.VoteContext: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FOLLOWER: reject ELECTION from 795320bb-f526-452d-9845-732289f5fb28: already has voted for c94c9266-9edb-44c8-a5eb-4c0716fa66dd at current term 2
datanode_1          | 2023-07-16 05:02:20,385 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F replies to ELECTION vote request: 795320bb-f526-452d-9845-732289f5fb28<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:FAIL-t2. Peer's state: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F:t2, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:25,430 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:25,431 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:25,527 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: receive requestVote(ELECTION, 795320bb-f526-452d-9845-732289f5fb28, group-3D3F70EF2B3F, 3, (t:0, i:0))
datanode_1          | 2023-07-16 05:02:25,527 [grpc-default-executor-1] INFO impl.VoteContext: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FOLLOWER: accept ELECTION from 795320bb-f526-452d-9845-732289f5fb28: our priority 0 <= candidate's priority 1
datanode_1          | 2023-07-16 05:02:25,527 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:795320bb-f526-452d-9845-732289f5fb28
datanode_1          | 2023-07-16 05:02:25,528 [grpc-default-executor-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: shutdown 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:25,528 [grpc-default-executor-1] INFO impl.RoleInfo: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: start 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState
datanode_1          | 2023-07-16 05:02:25,528 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState was interrupted
datanode_1          | 2023-07-16 05:02:25,529 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:02:25,529 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:02:25,530 [grpc-default-executor-1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F replies to ELECTION vote request: 795320bb-f526-452d-9845-732289f5fb28<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t3. Peer's state: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F:t3, leader=null, voted=795320bb-f526-452d-9845-732289f5fb28, raftlog=Memoized:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:25,724 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3D3F70EF2B3F with new leaderId: 795320bb-f526-452d-9845-732289f5fb28
datanode_1          | 2023-07-16 05:02:25,724 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: change Leader from null to 795320bb-f526-452d-9845-732289f5fb28 at term 3 for appendEntries, leader elected after 14759ms
datanode_1          | 2023-07-16 05:02:25,725 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO server.RaftServer$Division: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F: set configuration 0: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:02:25,726 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1-server-thread1] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-07-16 05:02:25,728 [99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1@group-3D3F70EF2B3F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/current/log_inprogress_0
datanode_2          | 2023-07-16 05:02:10,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:02:10,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 05:02:10,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 05:02:10,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:02:10,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 05:02:10,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:02:10,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 05:02:10,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 05:02:10,786 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f does not exist. Creating ...
datanode_2          | 2023-07-16 05:02:10,796 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/in_use.lock acquired by nodename 7@707448f11eb8
datanode_2          | 2023-07-16 05:02:10,800 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f has been successfully formatted.
datanode_2          | 2023-07-16 05:02:10,815 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3D3F70EF2B3F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 05:02:10,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-07-16 05:02:10,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:02:10,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:02:10,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:02:10,820 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:02:10,820 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:10,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 05:02:10,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:02:10,833 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f
datanode_2          | 2023-07-16 05:02:10,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 05:02:10,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:10,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:10,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 05:02:10,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 05:02:10,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 05:02:10,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 05:02:10,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 05:02:10,837 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 05:02:10,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:10,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:02:10,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:10,863 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:10,863 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:02:10,870 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: start as a follower, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:10,870 [pool-22-thread-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 05:02:10,871 [pool-22-thread-1] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:10,880 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3D3F70EF2B3F,id=795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:10,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 05:02:10,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 05:02:10,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 05:02:10,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 05:02:10,894 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:10,913 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:11,204 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce.
datanode_2          | 2023-07-16 05:02:12,666 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: receive requestVote(ELECTION, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1, group-2E7C004CACCE, 1, (t:0, i:0))
datanode_2          | 2023-07-16 05:02:12,668 [grpc-default-executor-0] INFO impl.VoteContext: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FOLLOWER: accept ELECTION from 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1: our priority 0 <= candidate's priority 0
datanode_2          | 2023-07-16 05:02:12,669 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_2          | 2023-07-16 05:02:12,669 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState
datanode_2          | 2023-07-16 05:02:12,670 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState was interrupted
datanode_2          | 2023-07-16 05:02:12,671 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState
datanode_2          | 2023-07-16 05:02:12,672 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:12,672 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:12,703 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE replies to ELECTION vote request: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1<-795320bb-f526-452d-9845-732289f5fb28#0:OK-t1. Peer's state: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE:t1, leader=null, voted=99ae7825-7730-4aa4-9d8a-2fe3b1c925a1, raftlog=Memoized:795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:14,278 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5102285712ns, electionTimeout:5058ms
datanode_2          | 2023-07-16 05:02:14,279 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState
datanode_2          | 2023-07-16 05:02:14,279 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-07-16 05:02:14,282 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 05:02:14,282 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1
datanode_2          | 2023-07-16 05:02:14,297 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:14,298 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-07-16 05:02:14,299 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1
datanode_2          | 2023-07-16 05:02:14,299 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-07-16 05:02:14,299 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4CE0C81B386D with new leaderId: 795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:14,301 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: change Leader from null to 795320bb-f526-452d-9845-732289f5fb28 at term 1 for becomeLeader, leader elected after 5746ms
datanode_2          | 2023-07-16 05:02:14,314 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 05:02:14,326 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:14,331 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 05:02:14,338 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 05:02:14,339 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 05:02:14,339 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:02:14,353 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:14,358 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 05:02:14,363 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderStateImpl
datanode_2          | 2023-07-16 05:02:14,385 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 05:02:14,402 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-LeaderElection1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D: set configuration 0: peers:[795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:14,457 [795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-4CE0C81B386D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1126c79-63f2-4929-8ac7-4ce0c81b386d/current/log_inprogress_0
datanode_2          | 2023-07-16 05:02:15,230 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-3D3F70EF2B3F, 1, (t:0, i:0))
datanode_2          | 2023-07-16 05:02:15,231 [grpc-default-executor-0] INFO impl.VoteContext: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FOLLOWER: reject ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: our priority 1 > candidate's priority 0
datanode_2          | 2023-07-16 05:02:15,231 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_2          | 2023-07-16 05:02:15,231 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:15,232 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState was interrupted
datanode_2          | 2023-07-16 05:02:15,232 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:15,235 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:15,235 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:15,236 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:FAIL-t1. Peer's state: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F:t1, leader=null, voted=null, raftlog=Memoized:795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:17,757 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-2E7C004CACCE, 2, (t:0, i:0))
datanode_2          | 2023-07-16 05:02:17,758 [grpc-default-executor-0] INFO impl.VoteContext: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FOLLOWER: accept ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 05:02:17,758 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_2          | 2023-07-16 05:02:17,759 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState
datanode_2          | 2023-07-16 05:02:17,759 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState was interrupted
datanode_2          | 2023-07-16 05:02:17,759 [grpc-default-executor-0] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState
datanode_2          | 2023-07-16 05:02:17,763 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:17,764 [grpc-default-executor-0] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:OK-t2. Peer's state: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE:t2, leader=null, voted=c94c9266-9edb-44c8-a5eb-4c0716fa66dd, raftlog=Memoized:795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:17,768 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:17,919 [795320bb-f526-452d-9845-732289f5fb28-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2E7C004CACCE with new leaderId: c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_2          | 2023-07-16 05:02:17,920 [795320bb-f526-452d-9845-732289f5fb28-server-thread1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: change Leader from null to c94c9266-9edb-44c8-a5eb-4c0716fa66dd at term 2 for appendEntries, leader elected after 8583ms
datanode_2          | 2023-07-16 05:02:17,925 [795320bb-f526-452d-9845-732289f5fb28-server-thread1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE: set configuration 0: peers:[99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER, c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:17,926 [795320bb-f526-452d-9845-732289f5fb28-server-thread1] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 05:02:17,932 [795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-2E7C004CACCE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/025125ff-099f-4966-8b13-2e7c004cacce/current/log_inprogress_0
datanode_2          | 2023-07-16 05:02:20,269 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5037388891ns, electionTimeout:5034ms
datanode_2          | 2023-07-16 05:02:20,270 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:20,270 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-07-16 05:02:20,271 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 05:02:20,271 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2
datanode_2          | 2023-07-16 05:02:20,278 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:20,284 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:20,285 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:20,285 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for c94c9266-9edb-44c8-a5eb-4c0716fa66dd
datanode_2          | 2023-07-16 05:02:20,289 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
datanode_2          | 2023-07-16 05:02:20,360 [grpc-default-executor-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: receive requestVote(ELECTION, c94c9266-9edb-44c8-a5eb-4c0716fa66dd, group-3D3F70EF2B3F, 2, (t:0, i:0))
datanode_2          | 2023-07-16 05:02:20,360 [grpc-default-executor-1] INFO impl.VoteContext: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-CANDIDATE: reject ELECTION from c94c9266-9edb-44c8-a5eb-4c0716fa66dd: already has voted for 795320bb-f526-452d-9845-732289f5fb28 at current term 2
datanode_2          | 2023-07-16 05:02:20,362 [grpc-default-executor-1] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F replies to ELECTION vote request: c94c9266-9edb-44c8-a5eb-4c0716fa66dd<-795320bb-f526-452d-9845-732289f5fb28#0:FAIL-t2. Peer's state: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F:t2, leader=null, voted=795320bb-f526-452d-9845-732289f5fb28, raftlog=Memoized:795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:20,427 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-07-16 05:02:20,427 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection:   Response 0: 795320bb-f526-452d-9845-732289f5fb28<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:FAIL-t2
datanode_2          | 2023-07-16 05:02:20,427 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection:   Response 1: 795320bb-f526-452d-9845-732289f5fb28<-c94c9266-9edb-44c8-a5eb-4c0716fa66dd#0:FAIL-t2
datanode_2          | 2023-07-16 05:02:20,427 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-07-16 05:02:20,428 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_2          | 2023-07-16 05:02:20,428 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2
datanode_2          | 2023-07-16 05:02:20,428 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection2] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:20,444 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:20,444 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:25,487 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.FollowerState: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058803879ns, electionTimeout:5042ms
datanode_2          | 2023-07-16 05:02:25,487 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState
datanode_2          | 2023-07-16 05:02:25,487 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-07-16 05:02:25,488 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-07-16 05:02:25,488 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-FollowerState] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 05:01:15,802 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 10045b0d2744/172.23.0.11
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-07-16 05:01:15,907 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 05:01:24,797 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-16 05:01:27,461 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 05:01:28,201 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.11:9862
om_1                | 2023-07-16 05:01:28,201 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 05:01:28,201 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 05:01:28,280 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:01:29,005 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863]
om_1                | 2023-07-16 05:01:32,782 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:34,784 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:36,786 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:38,787 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:40,789 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:42,791 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:44,798 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:46,799 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:48,802 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:50,804 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:52,807 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:54,810 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 10045b0d2744/172.23.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:01:59,020 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e404bc97-9b63-495d-8ac7-9b52cab4f8fa is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-07-16 05:02:01,035 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e404bc97-9b63-495d-8ac7-9b52cab4f8fa is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-e68c2822-703e-4616-bc56-61db335b5854;layoutVersion=3
om_1                | 2023-07-16 05:02:03,276 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
datanode_2          | 2023-07-16 05:02:25,491 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:02:25,519 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:02:25,519 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:02:25,538 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-16 05:02:25,541 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.LeaderElection:   Response 0: 795320bb-f526-452d-9845-732289f5fb28<-99ae7825-7730-4aa4-9d8a-2fe3b1c925a1#0:OK-t3
datanode_2          | 2023-07-16 05:02:25,541 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.LeaderElection: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-07-16 05:02:25,544 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: shutdown 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3
datanode_2          | 2023-07-16 05:02:25,544 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_2          | 2023-07-16 05:02:25,544 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3D3F70EF2B3F with new leaderId: 795320bb-f526-452d-9845-732289f5fb28
datanode_2          | 2023-07-16 05:02:25,549 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: change Leader from null to 795320bb-f526-452d-9845-732289f5fb28 at term 3 for becomeLeader, leader elected after 14761ms
datanode_2          | 2023-07-16 05:02:25,550 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 05:02:25,552 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:25,555 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 05:02:25,558 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 05:02:25,559 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 05:02:25,559 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:02:25,559 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:02:25,565 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 05:02:25,602 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-16 05:02:25,602 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:02:25,603 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-16 05:02:25,606 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-16 05:02:25,607 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:02:25,607 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:02:25,608 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:02:25,608 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:02:25,613 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-16 05:02:25,614 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:02:25,614 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-16 05:02:25,614 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-16 05:02:25,614 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:02:25,615 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:02:25,615 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:02:25,615 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:02:25,616 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO impl.RoleInfo: 795320bb-f526-452d-9845-732289f5fb28: start 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderStateImpl
datanode_2          | 2023-07-16 05:02:25,617 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 05:02:25,620 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f/current/log_inprogress_0
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 05:01:11,751 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 4f8f0c821630/172.23.0.6
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.3.0
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 10045b0d2744/172.23.0.11
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 05:02:05,827 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 10045b0d2744/172.23.0.11
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1             | STARTUP_MSG:   java = 11.0.14.1
recon_1             | ************************************************************/
recon_1             | 2023-07-16 05:01:11,835 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-07-16 05:01:16,429 [main] INFO reflections.Reflections: Reflections took 619 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1             | 2023-07-16 05:01:21,405 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-07-16 05:01:24,093 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 05:01:32,571 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 05:01:34,898 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-16 05:01:34,916 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-16 05:01:34,986 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 05:01:35,199 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-07-16 05:01:35,267 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-07-16 05:01:39,015 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 2023-07-16 05:01:42,691 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-07-16 05:02:05,850 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-07-16 05:02:06,943 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-16 05:02:07,875 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-07-16 05:02:08,085 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.11:9862
om_1                | 2023-07-16 05:02:08,085 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 05:02:08,085 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 05:02:08,121 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:02:08,147 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1                | 2023-07-16 05:02:09,127 [main] INFO reflections.Reflections: Reflections took 902 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1                | 2023-07-16 05:02:09,153 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:02:09,767 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863]
om_1                | 2023-07-16 05:02:09,921 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9863]
om_1                | 2023-07-16 05:02:11,550 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:02:11,711 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 05:02:11,712 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-07-16 05:02:11,993 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-07-16 05:02:12,050 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-07-16 05:02:12,088 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 05:02:12,088 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-07-16 05:02:12,105 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-07-16 05:02:12,121 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-07-16 05:02:12,152 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-07-16 05:02:12,163 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-07-16 05:02:12,203 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-07-16 05:02:12,378 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-16 05:02:12,404 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-16 05:02:12,407 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-07-16 05:02:12,407 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-16 05:02:12,407 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-07-16 05:02:12,408 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 05:02:12,410 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-07-16 05:02:12,412 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:02:12,417 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-16 05:02:12,427 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-07-16 05:02:12,476 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-16 05:02:12,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-16 05:02:12,491 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-16 05:02:12,971 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-07-16 05:02:12,974 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-07-16 05:02:12,981 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-07-16 05:02:12,982 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 05:02:12,982 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 05:02:13,007 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 05:02:13,032 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@d325518[Not completed]
om_1                | 2023-07-16 05:02:13,033 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-07-16 05:02:13,112 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-07-16 05:02:13,119 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-07-16 05:02:13,142 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-16 05:02:13,143 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-16 05:02:13,144 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-16 05:02:13,144 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-07-16 05:02:13,156 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 05:02:13,156 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 05:02:13,190 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-16 05:02:13,197 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 05:02:13,221 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-16 05:02:13,222 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-16 05:02:13,278 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-07-16 05:02:13,290 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-07-16 05:02:13,304 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-16 05:02:13,575 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-07-16 05:02:13,582 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-07-16 05:02:13,583 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-07-16 05:02:13,584 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-16 05:02:13,584 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-16 05:02:13,851 [main] INFO reflections.Reflections: Reflections took 640 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om_1                | 2023-07-16 05:02:13,992 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 05:02:14,006 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 05:02:14,134 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-07-16 05:02:14,147 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 05:02:14,148 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-07-16 05:02:14,189 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.23.0.11:9862
om_1                | 2023-07-16 05:02:14,190 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-07-16 05:02:14,192 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-07-16 05:02:14,198 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@10045b0d2744
om_1                | 2023-07-16 05:02:14,212 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-07-16 05:02:14,217 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-16 05:02:14,230 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-16 05:02:14,230 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:02:14,231 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-16 05:02:14,233 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-16 05:02:14,235 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 05:02:14,276 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-16 05:02:14,284 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-07-16 05:02:14,291 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-16 05:02:14,292 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 05:02:14,293 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-07-16 05:02:14,294 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 05:02:14,295 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-16 05:02:14,295 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-16 05:02:14,296 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-16 05:02:14,297 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 05:02:14,297 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 05:02:14,326 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-16 05:02:14,327 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-16 05:02:14,327 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-16 05:02:14,328 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-16 05:02:14,340 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 05:02:14,340 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 05:02:14,352 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 05:02:14,352 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-07-16 05:02:14,356 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 05:02:14,367 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 05:02:14,368 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-16 05:02:14,380 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-16 05:02:14,383 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 2023-07-16 05:01:42,744 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-07-16 05:01:42,846 [main] INFO util.log: Logging initialized @38877ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-07-16 05:01:43,431 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-07-16 05:01:43,499 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-07-16 05:01:43,584 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-07-16 05:01:43,595 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-07-16 05:01:43,616 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-07-16 05:01:43,617 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-07-16 05:01:45,171 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-07-16 05:01:47,469 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-07-16 05:01:47,487 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-07-16 05:01:47,505 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-07-16 05:01:47,669 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-07-16 05:01:47,676 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-07-16 05:01:49,883 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:01:50,248 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:01:50,425 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1             | 2023-07-16 05:01:50,436 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-07-16 05:01:50,615 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:01:50,888 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1             | 2023-07-16 05:01:50,990 [main] INFO reflections.Reflections: Reflections took 97 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1             | 2023-07-16 05:01:51,186 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-16 05:01:51,263 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-07-16 05:01:51,285 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-16 05:01:51,303 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-07-16 05:01:51,445 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-07-16 05:01:51,528 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 05:01:51,725 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-07-16 05:01:51,804 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-16 05:01:52,109 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-07-16 05:01:52,112 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-16 05:01:52,293 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-07-16 05:01:52,359 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 05:01:52,359 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-16 05:01:52,804 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-07-16 05:01:52,816 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1             | 2023-07-16 05:01:52,868 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 05:01:52,868 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-07-16 05:01:52,870 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-07-16 05:01:52,904 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 05:01:52,912 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 05:01:57,546 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@17554316{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-6925355615762560942/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1             | 2023-07-16 05:01:57,564 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-07-16 05:01:57,565 [Listener at 0.0.0.0/9891] INFO server.Server: Started @53596ms
recon_1             | 2023-07-16 05:01:57,568 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-16 05:01:57,568 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 05:01:57,574 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 05:01:57,575 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-07-16 05:01:57,602 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-16 05:01:57,609 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 05:01:57,609 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-07-16 05:01:57,610 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-16 05:02:25,635 [795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F-LeaderElection3] INFO server.RaftServer$Division: 795320bb-f526-452d-9845-732289f5fb28@group-3D3F70EF2B3F: set configuration 0: peers:[c94c9266-9edb-44c8-a5eb-4c0716fa66dd|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 795320bb-f526-452d-9845-732289f5fb28|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1|rpc:172.23.0.4:9856|admin:172.23.0.4:9857|client:172.23.0.4:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 05:02:14,383 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-16 05:02:14,386 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-16 05:02:14,387 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-16 05:02:14,401 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-07-16 05:02:14,489 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-16 05:02:14,495 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-16 05:02:14,500 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-16 05:02:14,555 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-07-16 05:02:14,555 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 05:02:14,577 [Listener at om/9862] INFO util.log: Logging initialized @10671ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 05:02:14,682 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-07-16 05:02:14,688 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-07-16 05:02:14,695 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-07-16 05:02:14,696 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-07-16 05:02:14,697 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-07-16 05:02:14,697 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-07-16 05:02:14,737 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-07-16 05:02:14,738 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1                | 2023-07-16 05:02:14,795 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-07-16 05:02:14,796 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-07-16 05:02:14,799 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-07-16 05:02:14,828 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c8efde4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 05:02:14,833 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b4814d0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-07-16 05:02:15,319 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@22da200e{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-9175405658858708974/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om_1                | 2023-07-16 05:02:15,327 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7282af25{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-07-16 05:02:15,327 [Listener at om/9862] INFO server.Server: Started @11422ms
om_1                | 2023-07-16 05:02:15,329 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-07-16 05:02:15,329 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-07-16 05:02:15,331 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-07-16 05:02:15,332 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-07-16 05:02:15,333 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-07-16 05:02:15,365 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-07-16 05:02:15,377 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1182413a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-07-16 05:02:19,528 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171988331ns, electionTimeout:5154ms
om_1                | 2023-07-16 05:02:19,530 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 05:02:19,530 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-16 05:02:19,534 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-07-16 05:02:19,535 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 05:02:19,541 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 05:02:19,542 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-07-16 05:02:19,543 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-07-16 05:02:19,543 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-07-16 05:02:19,544 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 6266ms
om_1                | 2023-07-16 05:02:19,551 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-07-16 05:02:19,559 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 05:02:19,560 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 05:02:19,569 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 2023-07-16 05:01:57,610 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-07-16 05:01:57,615 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 05:02:01,056 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e404bc97-9b63-495d-8ac7-9b52cab4f8fa is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-07-16 05:02:03,858 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-07-16 05:02:03,859 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 05:02:03,859 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-07-16 05:02:03,860 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-07-16 05:02:03,863 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-07-16 05:02:03,865 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-07-16 05:02:04,144 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-16 05:02:04,144 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-16 05:02:04,173 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-07-16 05:02:04,174 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-07-16 05:02:04,283 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 05:02:04,294 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6bf2b11e-235c-438f-a2ae-2cd52ad987f2 from SCM.
recon_1             | 2023-07-16 05:02:04,395 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6bf2b11e-235c-438f-a2ae-2cd52ad987f2, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:03.996Z[UTC]].
recon_1             | 2023-07-16 05:02:04,430 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 272 milliseconds.
recon_1             | 2023-07-16 05:02:04,645 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.4:33942: output error
recon_1             | 2023-07-16 05:02:04,656 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.9:39924: output error
recon_1             | 2023-07-16 05:02:04,655 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.5:46906: output error
recon_1             | 2023-07-16 05:02:04,647 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.9:39932: output error
recon_1             | 2023-07-16 05:02:04,647 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.4:33956: output error
recon_1             | 2023-07-16 05:02:04,645 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.5:46922: output error
recon_1             | 2023-07-16 05:02:04,663 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-16 05:02:04,667 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-16 05:02:19,569 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-07-16 05:02:19,571 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-07-16 05:02:19,581 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 05:02:19,582 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-07-16 05:02:19,588 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-07-16 05:02:19,628 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-16 05:02:19,704 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 05:02:19,791 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-07-16 05:02:19,971 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-07-16 05:02:23,454 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-07-16 05:02:23,525 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-07-16 05:02:48,457 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:02:48,457 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:02:48,457 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:02:58,060 [qtp1578712821-47] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-07-16 05:02:58,114 [qtp1578712821-47] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483778068 in 46 milliseconds
om_1                | 2023-07-16 05:02:58,220 [qtp1578712821-47] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 104 milliseconds
om_1                | 2023-07-16 05:02:58,220 [qtp1578712821-47] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689483778068
om_1                | 2023-07-16 05:03:44,205 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:03:44,206 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:03:44,206 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:03:44,206 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-07-16 05:03:44,206 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-16 05:02:04,668 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-16 05:02:04,669 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-16 05:02:04,670 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-07-16 05:01:16,373 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-07-16 05:01:16,374 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-07-16 05:01:16,609 [main] INFO util.log: Logging initialized @9983ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-07-16 05:01:17,981 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-07-16 05:01:18,396 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-07-16 05:01:18,486 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-07-16 05:01:18,533 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-07-16 05:01:18,539 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-07-16 05:01:18,539 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-07-16 05:01:19,163 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 32e2e3d44873/172.23.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1               | STARTUP_MSG:   java = 11.0.14.1
s3g_1               | ************************************************************/
s3g_1               | 2023-07-16 05:01:19,232 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-07-16 05:01:19,357 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-07-16 05:01:19,846 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-07-16 05:01:21,319 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-16 05:01:21,321 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-07-16 05:01:21,534 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-16 05:01:21,589 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1               | 2023-07-16 05:01:21,859 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-07-16 05:01:21,862 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-07-16 05:01:21,866 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-07-16 05:01:22,055 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@68d279ec{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 05:01:22,078 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3e74829{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jul 16, 2023 5:01:48 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-07-16 05:01:48,962 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@328e4ec2{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-13621446218925523856/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-07-16 05:02:04,672 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
s3g_1               | 2023-07-16 05:01:49,011 [main] INFO server.AbstractConnector: Started ServerConnector@49c66ade{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-07-16 05:01:49,016 [main] INFO server.Server: Started @42387ms
s3g_1               | 2023-07-16 05:01:49,024 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-07-16 05:01:49,024 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-07-16 05:01:49,029 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-07-16 05:02:05,380 [IPC Server handler 1 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/795320bb-f526-452d-9845-732289f5fb28
recon_1             | 2023-07-16 05:02:05,386 [IPC Server handler 1 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:05,394 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 795320bb-f526-452d-9845-732289f5fb28 to Node DB.
recon_1             | 2023-07-16 05:02:05,633 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c94c9266-9edb-44c8-a5eb-4c0716fa66dd
recon_1             | 2023-07-16 05:02:05,633 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:05,635 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c94c9266-9edb-44c8-a5eb-4c0716fa66dd to Node DB.
recon_1             | 2023-07-16 05:02:05,673 [IPC Server handler 0 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
recon_1             | 2023-07-16 05:02:05,674 [IPC Server handler 0 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:05,680 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1 to Node DB.
recon_1             | 2023-07-16 05:02:07,037 [IPC Server handler 79 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-16 05:02:07,038 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6bf2b11e-235c-438f-a2ae-2cd52ad987f2 reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:07,038 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6bf2b11e-235c-438f-a2ae-2cd52ad987f2, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1, CreationTimestamp2023-07-16T05:02:03.996Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 05:02:07,333 [IPC Server handler 1 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-16 05:02:07,470 [IPC Server handler 8 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-07-16 05:02:07,475 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=025125ff-099f-4966-8b13-2e7c004cacce. Trying to get from SCM.
recon_1             | 2023-07-16 05:02:07,486 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 025125ff-099f-4966-8b13-2e7c004cacce, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.656Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 05:02:07,492 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 025125ff-099f-4966-8b13-2e7c004cacce, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.656Z[UTC]].
recon_1             | 2023-07-16 05:02:07,494 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:07,602 [IPC Server handler 9 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-16 05:02:08,880 [IPC Server handler 18 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-07-16 05:02:08,881 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d1126c79-63f2-4929-8ac7-4ce0c81b386d. Trying to get from SCM.
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 05:01:15,726 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 1e2d3aae2e4d/172.23.0.7
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-07-16 05:01:15,828 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 05:01:16,466 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:01:16,689 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 05:01:16,741 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 05:01:17,592 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 05:01:18,827 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 05:01:18,857 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:01:18,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 05:01:18,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:01:18,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-16 05:01:18,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-16 05:01:18,871 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-16 05:01:18,883 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:01:18,911 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-16 05:01:18,933 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 05:01:19,036 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-16 05:01:19,110 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-16 05:01:19,128 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-16 05:01:20,858 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-16 05:01:20,897 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-16 05:01:20,915 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-16 05:01:20,915 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 05:01:20,938 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:01:20,972 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 05:01:21,124 [main] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: addNew group-61DB335B5854:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|priority:0|startupRole:FOLLOWER] returns group-61DB335B5854:java.util.concurrent.CompletableFuture@6e16b8b5[Not completed]
scm_1               | 2023-07-16 05:01:21,376 [pool-2-thread-1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: new RaftServerImpl for group-61DB335B5854:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-07-16 05:01:21,409 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-16 05:01:21,415 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-16 05:01:21,415 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-16 05:01:21,415 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 05:01:21,416 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:01:21,418 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 05:01:21,509 [pool-2-thread-1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: ConfigurationManager, init=-1: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 05:01:21,532 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 05:01:21,585 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 05:01:21,592 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 05:01:21,755 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-16 05:01:21,770 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-16 05:01:21,793 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 05:01:22,008 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 05:01:23,489 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 05:01:23,490 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 05:01:23,558 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:01:23,562 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-16 05:01:23,571 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 05:01:23,574 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854 does not exist. Creating ...
scm_1               | 2023-07-16 05:01:23,676 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/in_use.lock acquired by nodename 13@1e2d3aae2e4d
scm_1               | 2023-07-16 05:01:23,772 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854 has been successfully formatted.
scm_1               | 2023-07-16 05:01:23,854 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 05:01:23,998 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-16 05:01:23,999 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:01:24,015 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-16 05:01:24,062 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-16 05:01:24,090 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 05:01:24,142 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 05:01:24,156 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-16 05:01:24,188 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854
scm_1               | 2023-07-16 05:01:24,192 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:01:24,192 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:01:24,197 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 05:01:24,198 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 05:01:24,210 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-16 05:01:24,222 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 05:01:24,236 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-16 05:01:24,238 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-16 05:01:24,315 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-16 05:01:24,320 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 05:01:24,329 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 05:01:24,344 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-16 05:01:24,546 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:01:24,571 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:01:24,577 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: start as a follower, conf=-1: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:24,583 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-07-16 05:01:24,585 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState
recon_1             | 2023-07-16 05:02:08,885 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d1126c79-63f2-4929-8ac7-4ce0c81b386d, Nodes: 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.387Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 05:02:08,886 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d1126c79-63f2-4929-8ac7-4ce0c81b386d, Nodes: 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.387Z[UTC]].
recon_1             | 2023-07-16 05:02:08,887 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d1126c79-63f2-4929-8ac7-4ce0c81b386d reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:08,887 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d1126c79-63f2-4929-8ac7-4ce0c81b386d, Nodes: 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:795320bb-f526-452d-9845-732289f5fb28, CreationTimestamp2023-07-16T05:02:05.387Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 05:02:09,366 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,479 [IPC Server handler 9 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-07-16 05:02:09,484 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=abad7756-298a-4535-9029-9eb3489b6695. Trying to get from SCM.
recon_1             | 2023-07-16 05:02:09,490 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: abad7756-298a-4535-9029-9eb3489b6695, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.625Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 05:02:09,491 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: abad7756-298a-4535-9029-9eb3489b6695, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.625Z[UTC]].
recon_1             | 2023-07-16 05:02:09,492 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=abad7756-298a-4535-9029-9eb3489b6695 reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,492 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: abad7756-298a-4535-9029-9eb3489b6695, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c94c9266-9edb-44c8-a5eb-4c0716fa66dd, CreationTimestamp2023-07-16T05:02:05.625Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 05:02:09,493 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,841 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f. Trying to get from SCM.
recon_1             | 2023-07-16 05:02:09,896 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.702Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 05:02:09,897 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.702Z[UTC]].
recon_1             | 2023-07-16 05:02:09,898 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,900 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,961 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:09,962 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:10,817 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:10,817 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:11,007 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:11,009 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:12,252 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:12,252 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:14,303 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:14,303 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:14,773 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:14,773 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:17,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:17,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=025125ff-099f-4966-8b13-2e7c004cacce reported by c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:17,790 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 025125ff-099f-4966-8b13-2e7c004cacce, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c94c9266-9edb-44c8-a5eb-4c0716fa66dd, CreationTimestamp2023-07-16T05:02:05.656Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 05:02:25,586 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f reported by 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:02:25,587 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:795320bb-f526-452d-9845-732289f5fb28, CreationTimestamp2023-07-16T05:02:05.702Z[UTC]] moved to OPEN state
recon_1             | 2023-07-16 05:02:26,105 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 05:02:26,181 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 05:02:36,833 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-16 05:02:36,862 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 05:02:36,876 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-16 05:02:36,886 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-07-16 05:02:57,615 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-07-16 05:02:57,616 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-07-16 05:02:58,356 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689483777616
recon_1             | 2023-07-16 05:02:58,370 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 05:02:58,371 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-07-16 05:02:58,502 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689483777616.
recon_1             | 2023-07-16 05:02:58,543 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-07-16 05:02:58,555 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-07-16 05:02:58,580 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-07-16 05:02:59,008 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-07-16 05:02:59,011 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-07-16 05:02:59,011 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-16 05:02:59,020 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-07-16 05:02:59,159 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-07-16 05:02:59,164 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.152 seconds to process 4 keys.
recon_1             | 2023-07-16 05:02:59,221 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-07-16 05:02:59,290 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-07-16 05:02:59,313 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | 2023-07-16 05:01:24,621 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:01:24,640 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61DB335B5854,id=e404bc97-9b63-495d-8ac7-9b52cab4f8fa
scm_1               | 2023-07-16 05:01:24,643 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-16 05:01:24,641 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 05:01:24,656 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-16 05:01:24,657 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-16 05:01:24,658 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-16 05:01:24,696 [main] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start RPC server
scm_1               | 2023-07-16 05:01:24,946 [main] INFO server.GrpcService: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: GrpcService started, listening on 9894
scm_1               | 2023-07-16 05:01:24,956 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e404bc97-9b63-495d-8ac7-9b52cab4f8fa: Started
scm_1               | 2023-07-16 05:01:29,780 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.FollowerState: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5195205244ns, electionTimeout:5096ms
scm_1               | 2023-07-16 05:01:29,787 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState
scm_1               | 2023-07-16 05:01:29,787 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-16 05:01:29,796 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-07-16 05:01:29,819 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1
scm_1               | 2023-07-16 05:01:29,882 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.LeaderElection: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:29,883 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.LeaderElection: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-16 05:01:29,884 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1
scm_1               | 2023-07-16 05:01:29,885 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-16 05:01:29,885 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: change Leader from null to e404bc97-9b63-495d-8ac7-9b52cab4f8fa at term 1 for becomeLeader, leader elected after 8130ms
scm_1               | 2023-07-16 05:01:29,953 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-16 05:01:30,018 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:01:30,019 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:01:30,046 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-16 05:01:30,073 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-16 05:01:30,087 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-16 05:01:30,281 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:01:30,283 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 05:01:30,297 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderStateImpl
scm_1               | 2023-07-16 05:01:30,406 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-07-16 05:01:30,871 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: set configuration 0: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:31,094 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/log_inprogress_0
scm_1               | 2023-07-16 05:01:32,976 [main] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: close
scm_1               | 2023-07-16 05:01:32,981 [main] INFO server.GrpcService: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown server GrpcServerProtocolService now
scm_1               | 2023-07-16 05:01:32,983 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: shutdown
scm_1               | 2023-07-16 05:01:32,983 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-61DB335B5854,id=e404bc97-9b63-495d-8ac7-9b52cab4f8fa
scm_1               | 2023-07-16 05:01:32,984 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderStateImpl
scm_1               | 2023-07-16 05:01:33,058 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO impl.PendingRequests: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-16 05:01:33,096 [main] INFO server.GrpcService: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-07-16 05:01:33,147 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO impl.StateMachineUpdater: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-07-16 05:01:33,161 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO impl.StateMachineUpdater: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-07-16 05:01:33,180 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO impl.StateMachineUpdater: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-07-16 05:01:33,264 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: closes. applyIndex: 0
scm_1               | 2023-07-16 05:01:33,304 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1               | 2023-07-16 05:01:33,340 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker close()
scm_1               | 2023-07-16 05:01:33,396 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e404bc97-9b63-495d-8ac7-9b52cab4f8fa: Stopped
scm_1               | 2023-07-16 05:01:33,412 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:01:33,420 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-e68c2822-703e-4616-bc56-61db335b5854; layoutVersion=4; scmId=e404bc97-9b63-495d-8ac7-9b52cab4f8fa
scm_1               | 2023-07-16 05:01:33,503 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 1e2d3aae2e4d/172.23.0.7
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-07-16 05:01:43,863 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 1e2d3aae2e4d/172.23.0.7
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-07-16 05:01:43,903 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 05:01:44,302 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:01:44,656 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-07-16 05:01:44,762 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 05:01:48,654 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:01:49,476 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:01:50,221 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm_1               | 2023-07-16 05:01:50,223 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-16 05:01:50,375 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 05:01:50,416 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e404bc97-9b63-495d-8ac7-9b52cab4f8fa
scm_1               | 2023-07-16 05:01:50,555 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 05:01:50,699 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 05:01:50,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:01:50,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-07-16 05:01:50,710 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:01:50,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-07-16 05:01:50,713 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-16 05:01:50,714 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-07-16 05:01:50,719 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:01:50,720 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-07-16 05:01:50,721 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 05:01:50,734 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-16 05:01:50,740 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-16 05:01:50,740 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-07-16 05:01:51,339 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-16 05:01:51,360 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-07-16 05:01:51,361 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-07-16 05:01:51,361 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 05:01:51,362 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:01:51,392 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 05:01:51,422 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: found a subdirectory /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854
scm_1               | 2023-07-16 05:01:51,443 [main] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: addNew group-61DB335B5854:[] returns group-61DB335B5854:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm_1               | 2023-07-16 05:01:51,510 [pool-16-thread-1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: new RaftServerImpl for group-61DB335B5854:[] with SCMStateMachine:uninitialized
scm_1               | 2023-07-16 05:01:51,515 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-07-16 05:01:51,518 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-16 05:01:51,519 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-16 05:01:51,520 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-07-16 05:01:51,524 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:01:51,525 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 05:01:51,561 [pool-16-thread-1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 05:01:51,562 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-07-16 05:01:51,577 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 05:01:51,580 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 05:01:51,674 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-07-16 05:01:51,678 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-07-16 05:01:51,678 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 05:01:52,114 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 05:01:52,116 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 05:01:52,119 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:01:52,122 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-16 05:01:52,123 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 05:01:52,126 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-07-16 05:01:52,127 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-16 05:01:52,128 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-07-16 05:01:52,165 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1               | 2023-07-16 05:01:52,640 [main] INFO reflections.Reflections: Reflections took 341 ms to scan 3 urls, producing 112 keys and 252 values 
scm_1               | 2023-07-16 05:01:52,778 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-07-16 05:01:52,780 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-07-16 05:01:52,788 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-16 05:01:52,789 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-07-16 05:01:52,865 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-16 05:01:52,887 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-07-16 05:01:52,896 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-16 05:01:52,901 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-07-16 05:01:52,952 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-07-16 05:01:52,952 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-07-16 05:01:52,968 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-07-16 05:01:52,971 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-07-16 05:01:52,983 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-07-16 05:01:52,983 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-07-16 05:01:52,998 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-07-16 05:01:53,012 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-07-16 05:01:53,101 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-07-16 05:01:53,133 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-07-16 05:01:53,202 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-07-16 05:01:53,248 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-07-16 05:01:53,252 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-16 05:01:53,264 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-07-16 05:01:53,276 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:01:53,278 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 05:01:55,222 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 05:01:55,278 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 05:01:55,408 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 05:01:55,604 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 05:01:55,638 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 05:01:55,649 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 05:01:55,781 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 05:01:55,814 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-07-16 05:01:55,826 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-07-16 05:01:56,286 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-07-16 05:01:56,312 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        true
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-07-16 05:01:56,312 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 05:01:56,312 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-07-16 05:01:56,338 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 05:01:56,342 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-07-16 05:01:56,365 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/in_use.lock acquired by nodename 7@1e2d3aae2e4d
scm_1               | 2023-07-16 05:01:56,387 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=e404bc97-9b63-495d-8ac7-9b52cab4f8fa} from /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/raft-meta
scm_1               | 2023-07-16 05:01:56,498 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: set configuration 0: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:56,507 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 05:01:56,531 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-16 05:01:56,539 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:01:56,544 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-16 05:01:56,553 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-16 05:01:56,579 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 05:01:56,610 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 05:01:56,614 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-16 05:01:56,642 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854
scm_1               | 2023-07-16 05:01:56,643 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:01:56,655 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:01:56,660 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-07-16 05:01:56,676 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 05:01:56,679 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-16 05:01:56,685 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 05:01:56,700 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-16 05:01:56,701 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-07-16 05:01:56,754 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-07-16 05:01:56,767 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 05:01:56,767 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 05:01:56,769 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-07-16 05:01:56,963 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: set configuration 0: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:56,965 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/log_inprogress_0
scm_1               | 2023-07-16 05:01:56,980 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1               | 2023-07-16 05:01:56,984 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:01:57,379 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: start as a follower, conf=0: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:01:57,380 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-07-16 05:01:57,386 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState
scm_1               | 2023-07-16 05:01:57,416 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61DB335B5854,id=e404bc97-9b63-495d-8ac7-9b52cab4f8fa
scm_1               | 2023-07-16 05:01:57,420 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:01:57,420 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 05:01:57,434 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-16 05:01:57,464 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-16 05:01:57,468 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-07-16 05:01:57,469 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-07-16 05:01:57,478 [Listener at 0.0.0.0/9860] INFO server.RaftServer: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start RPC server
scm_1               | 2023-07-16 05:01:57,659 [Listener at 0.0.0.0/9860] INFO server.GrpcService: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: GrpcService started, listening on 9894
scm_1               | 2023-07-16 05:01:57,689 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e404bc97-9b63-495d-8ac7-9b52cab4f8fa: Started
scm_1               | 2023-07-16 05:01:57,695 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-07-16 05:01:57,706 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-07-16 05:01:57,860 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 05:01:57,880 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 05:01:57,880 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-16 05:01:58,257 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-07-16 05:01:58,258 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 05:01:58,259 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-07-16 05:01:58,321 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 05:01:58,327 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-07-16 05:01:58,329 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 05:01:58,340 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-07-16 05:01:58,501 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@561f9d92] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-07-16 05:01:58,595 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-07-16 05:01:58,595 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-07-16 05:01:58,681 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @23488ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-07-16 05:01:59,160 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-16 05:01:59,170 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 05:01:59,184 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 05:01:59,185 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 05:01:59,186 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 05:01:59,189 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 05:01:59,259 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-07-16 05:01:59,261 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1               | 2023-07-16 05:01:59,378 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-07-16 05:01:59,380 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 05:01:59,383 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-07-16 05:01:59,411 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63124022{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 05:01:59,412 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@eaf8427{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 05:01:59,993 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7606bd03{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-11507120263029182264/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm_1               | 2023-07-16 05:02:00,016 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@701c223a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-16 05:02:00,017 [Listener at 0.0.0.0/9860] INFO server.Server: Started @24824ms
scm_1               | 2023-07-16 05:02:00,026 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-07-16 05:02:00,026 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-16 05:02:00,028 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 05:02:02,579 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.FollowerState: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5193451418ns, electionTimeout:5157ms
scm_1               | 2023-07-16 05:02:02,580 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState
scm_1               | 2023-07-16 05:02:02,581 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-16 05:02:02,586 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-07-16 05:02:02,586 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-FollowerState] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1
scm_1               | 2023-07-16 05:02:02,602 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.LeaderElection: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:02:02,603 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.LeaderElection: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-16 05:02:02,604 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: shutdown e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1
scm_1               | 2023-07-16 05:02:02,604 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-16 05:02:02,605 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-16 05:02:02,605 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-07-16 05:02:02,610 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: change Leader from null to e404bc97-9b63-495d-8ac7-9b52cab4f8fa at term 2 for becomeLeader, leader elected after 10949ms
scm_1               | 2023-07-16 05:02:02,622 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-07-16 05:02:02,627 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:02:02,629 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:02:02,637 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-16 05:02:02,637 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-16 05:02:02,638 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-07-16 05:02:02,643 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:02:02,647 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 05:02:02,655 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO impl.RoleInfo: e404bc97-9b63-495d-8ac7-9b52cab4f8fa: start e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderStateImpl
scm_1               | 2023-07-16 05:02:02,663 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-16 05:02:02,672 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/log_inprogress_0 to /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/log_0-0
scm_1               | 2023-07-16 05:02:02,678 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-LeaderElection1] INFO server.RaftServer$Division: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854: set configuration 1: peers:[e404bc97-9b63-495d-8ac7-9b52cab4f8fa|rpc:1e2d3aae2e4d:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:02:02,690 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/e68c2822-703e-4616-bc56-61db335b5854/current/log_inprogress_1
scm_1               | 2023-07-16 05:02:02,698 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-07-16 05:02:02,699 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-16 05:02:02,702 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:02,703 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-16 05:02:02,704 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-07-16 05:02:02,704 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-07-16 05:02:02,711 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 05:02:02,711 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-07-16 05:02:02,854 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.5:56872: output error
scm_1               | 2023-07-16 05:02:02,855 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-16 05:02:02,907 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.4:40566: output error
scm_1               | 2023-07-16 05:02:02,909 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-16 05:02:02,891 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.9:42790: output error
scm_1               | 2023-07-16 05:02:02,909 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-07-16 05:02:03,897 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
scm_1               | 2023-07-16 05:02:03,899 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 05:02:03,956 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 05:02:03,962 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 05:02:03,996 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 05:02:04,010 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-07-16 05:02:04,013 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6bf2b11e-235c-438f-a2ae-2cd52ad987f2 to datanode:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
scm_1               | 2023-07-16 05:02:04,116 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6bf2b11e-235c-438f-a2ae-2cd52ad987f2, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:03.996Z[UTC]].
scm_1               | 2023-07-16 05:02:04,118 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:05,372 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/795320bb-f526-452d-9845-732289f5fb28
scm_1               | 2023-07-16 05:02:05,374 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 05:02:05,381 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 05:02:05,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-07-16 05:02:05,387 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d1126c79-63f2-4929-8ac7-4ce0c81b386d to datanode:795320bb-f526-452d-9845-732289f5fb28
scm_1               | 2023-07-16 05:02:05,399 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d1126c79-63f2-4929-8ac7-4ce0c81b386d, Nodes: 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.387Z[UTC]].
scm_1               | 2023-07-16 05:02:05,399 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:05,622 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c94c9266-9edb-44c8-a5eb-4c0716fa66dd
scm_1               | 2023-07-16 05:02:05,623 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 05:02:05,624 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 05:02:05,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=abad7756-298a-4535-9029-9eb3489b6695 to datanode:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
scm_1               | 2023-07-16 05:02:05,625 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-07-16 05:02:05,625 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 05:02:05,626 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-07-16 05:02:05,626 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 05:02:05,638 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-07-16 05:02:05,638 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-07-16 05:02:05,640 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: abad7756-298a-4535-9029-9eb3489b6695, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.625Z[UTC]].
scm_1               | 2023-07-16 05:02:05,641 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:05,656 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=025125ff-099f-4966-8b13-2e7c004cacce to datanode:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
scm_1               | 2023-07-16 05:02:05,662 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=025125ff-099f-4966-8b13-2e7c004cacce to datanode:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
scm_1               | 2023-07-16 05:02:05,669 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=025125ff-099f-4966-8b13-2e7c004cacce to datanode:795320bb-f526-452d-9845-732289f5fb28
scm_1               | 2023-07-16 05:02:05,676 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 025125ff-099f-4966-8b13-2e7c004cacce, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.656Z[UTC]].
scm_1               | 2023-07-16 05:02:05,700 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:05,702 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f to datanode:c94c9266-9edb-44c8-a5eb-4c0716fa66dd
scm_1               | 2023-07-16 05:02:05,706 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f to datanode:795320bb-f526-452d-9845-732289f5fb28
scm_1               | 2023-07-16 05:02:05,706 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f to datanode:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
scm_1               | 2023-07-16 05:02:05,710 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:02:05.702Z[UTC]].
scm_1               | 2023-07-16 05:02:05,710 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:05,716 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f contains same datanodes as previous pipelines: PipelineID=025125ff-099f-4966-8b13-2e7c004cacce nodeIds: c94c9266-9edb-44c8-a5eb-4c0716fa66dd, 795320bb-f526-452d-9845-732289f5fb28, 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1
scm_1               | 2023-07-16 05:02:07,028 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6bf2b11e-235c-438f-a2ae-2cd52ad987f2, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:99ae7825-7730-4aa4-9d8a-2fe3b1c925a1, CreationTimestamp2023-07-16T05:02:03.996Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 05:02:07,038 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:07,046 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:07,432 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:08,946 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d1126c79-63f2-4929-8ac7-4ce0c81b386d, Nodes: 795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:795320bb-f526-452d-9845-732289f5fb28, CreationTimestamp2023-07-16T05:02:05.387Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 05:02:08,953 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:08,954 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:09,378 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:09,536 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: abad7756-298a-4535-9029-9eb3489b6695, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c94c9266-9edb-44c8-a5eb-4c0716fa66dd, CreationTimestamp2023-07-16T05:02:05.625Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 05:02:09,540 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:09,544 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:09,822 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:09,941 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:10,830 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:11,005 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:12,257 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:14,309 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:14,776 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:17,792 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 025125ff-099f-4966-8b13-2e7c004cacce, Nodes: 99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:c94c9266-9edb-44c8-a5eb-4c0716fa66dd, CreationTimestamp2023-07-16T05:02:05.656Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 05:02:17,792 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:17,797 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:02:17,801 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:02:17,801 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 05:02:17,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-16 05:02:17,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-16 05:02:17,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-07-16 05:02:17,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-16 05:02:17,803 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-16 05:02:17,804 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-16 05:02:17,809 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-16 05:02:17,816 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-16 05:02:23,673 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-16 05:02:23,697 [e404bc97-9b63-495d-8ac7-9b52cab4f8fa@group-61DB335B5854-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-16 05:02:23,702 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-16 05:02:25,583 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: bc7a6fc6-fb92-471a-94bf-3d3f70ef2b3f, Nodes: c94c9266-9edb-44c8-a5eb-4c0716fa66dd{ip: 172.23.0.5, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}795320bb-f526-452d-9845-732289f5fb28{ip: 172.23.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}99ae7825-7730-4aa4-9d8a-2fe3b1c925a1{ip: 172.23.0.4, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:795320bb-f526-452d-9845-732289f5fb28, CreationTimestamp2023-07-16T05:02:05.702Z[UTC]] moved to OPEN state
scm_1               | 2023-07-16 05:02:57,312 [IPC Server handler 14 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.13
scm_1               | 2023-07-16 05:03:09,820 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.13
scm_1               | 2023-07-16 05:04:08,520 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.13
scm_1               | 2023-07-16 05:04:19,972 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.13
Attaching to xcompat_old_client_1_2_1_1, xcompat_datanode_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_datanode_3, xcompat_datanode_5, xcompat_datanode_4, xcompat_datanode_2, xcompat_old_client_1_1_0_1, xcompat_old_client_1_0_0_1, xcompat_recon_1, xcompat_s3g_1, xcompat_new_client_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-07-16 05:05:04,454 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = e93585490b94/172.24.0.11
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-07-16 05:05:04,612 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 05:05:05,026 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-07-16 05:05:06,206 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-07-16 05:05:07,901 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-07-16 05:05:07,904 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-07-16 05:05:09,478 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e93585490b94 ip:172.24.0.11
datanode_1          | 2023-07-16 05:05:11,590 [main] INFO reflections.Reflections: Reflections took 1353 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_1          | 2023-07-16 05:05:16,778 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-07-16 05:05:17,844 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-07-16 05:05:20,414 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 05:05:20,583 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-07-16 05:05:20,587 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-07-16 05:05:20,619 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-07-16 05:05:20,974 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-07-16 05:05:20,999 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 05:05:21,042 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-07-16 05:05:21,125 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-07-16 05:05:05,546 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 4caf8da3e4ee/172.24.0.12
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-07-16 05:05:05,599 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-07-16 05:05:06,215 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-07-16 05:05:07,256 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-07-16 05:05:09,025 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-07-16 05:05:09,025 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-07-16 05:05:10,405 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4caf8da3e4ee ip:172.24.0.12
datanode_2          | 2023-07-16 05:05:12,643 [main] INFO reflections.Reflections: Reflections took 1742 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_2          | 2023-07-16 05:05:18,165 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-16 05:05:18,667 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-07-16 05:05:21,311 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-07-16 05:05:21,568 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-07-16 05:05:21,600 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-07-16 05:05:21,638 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-07-16 05:05:21,903 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-07-16 05:05:21,921 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 05:05:21,950 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-07-16 05:05:21,968 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-07-16 05:05:21,132 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-07-16 05:05:21,148 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-07-16 05:05:21,393 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-07-16 05:05:21,394 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-07-16 05:05:38,821 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-07-16 05:05:39,813 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-07-16 05:05:41,335 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-07-16 05:05:42,704 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 05:05:42,728 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-07-16 05:05:42,753 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 05:05:42,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-07-16 05:05:42,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-07-16 05:05:42,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-07-16 05:05:42,756 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-07-16 05:05:42,761 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:05:42,770 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-07-16 05:05:42,779 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 05:05:43,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 05:05:43,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-07-16 05:05:43,337 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-07-16 05:05:46,814 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-07-16 05:05:46,859 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-07-16 05:05:46,888 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-07-16 05:05:46,889 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:05:46,889 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:05:46,919 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:05:47,296 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-07-16 05:05:48,286 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-07-16 05:05:49,690 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-07-16 05:05:49,901 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-07-16 05:05:50,418 [main] INFO util.log: Logging initialized @62837ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 05:05:51,307 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-16 05:05:51,375 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-07-16 05:05:51,396 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 05:05:51,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-07-16 05:05:51,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 05:05:51,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 05:05:51,882 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-07-16 05:05:51,938 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-07-16 05:05:51,945 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-07-16 05:05:52,293 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 05:05:52,293 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-07-16 05:05:52,372 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-07-16 05:05:52,612 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@74ed7111{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-07-16 05:05:52,741 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@66e62e19{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-07-16 05:05:54,064 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@56fc2cea{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12117795721804454427/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-07-16 05:05:54,175 [main] INFO server.AbstractConnector: Started ServerConnector@f09c282{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-07-16 05:05:54,175 [main] INFO server.Server: Started @66595ms
datanode_1          | 2023-07-16 05:05:54,194 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-07-16 05:05:54,194 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-07-16 05:05:54,216 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-07-16 05:05:54,517 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-07-16 05:05:54,841 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-07-16 05:05:54,868 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-07-16 05:05:57,813 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-07-16 05:05:57,814 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-07-16 05:05:57,861 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-07-16 05:05:57,930 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_4          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4          | 2023-07-16 05:05:05,065 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4          | /************************************************************
datanode_4          | STARTUP_MSG: Starting HddsDatanodeService
datanode_4          | STARTUP_MSG:   host = 37e9a0ff2c7a/172.24.0.10
datanode_4          | STARTUP_MSG:   args = []
datanode_4          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-07-16 05:05:58,131 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-07-16 05:05:59,161 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.5:9891
datanode_1          | 2023-07-16 05:05:59,652 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-07-16 05:06:01,978 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:01,980 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:02,981 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:02,981 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:03,982 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:03,983 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:04,984 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:04,985 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:05,986 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:05,988 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:06,990 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:07,992 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:08,993 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:09,994 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-07-16 05:06:11,061 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From e93585490b94/172.24.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:60004 remote=recon/172.24.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:60004 remote=recon/172.24.0.5:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 2023-07-16 05:05:21,986 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-07-16 05:05:21,989 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-07-16 05:05:22,405 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-07-16 05:05:22,420 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-07-16 05:05:39,462 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-07-16 05:05:40,815 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-07-16 05:05:41,517 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 05:05:42,877 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 05:05:42,961 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-07-16 05:05:42,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-07-16 05:05:42,971 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-07-16 05:05:42,984 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-07-16 05:05:42,984 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-07-16 05:05:43,058 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-07-16 05:05:43,065 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:05:43,121 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-07-16 05:05:43,127 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:05:43,460 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:05:43,578 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-07-16 05:05:43,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-07-16 05:05:47,557 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-07-16 05:05:47,681 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-07-16 05:05:47,692 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-07-16 05:05:47,700 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:05:47,701 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-07-16 05:05:47,806 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:05:48,438 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-07-16 05:05:48,973 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-07-16 05:05:50,778 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 05:05:50,964 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 05:05:51,357 [main] INFO util.log: Logging initialized @63417ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 05:05:52,208 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-16 05:05:52,317 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-07-16 05:05:52,519 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 05:05:52,630 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-07-16 05:05:52,631 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 05:05:52,631 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 05:05:53,386 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-07-16 05:05:53,393 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-07-16 05:05:53,413 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-07-16 05:05:53,741 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-07-16 05:05:53,741 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-07-16 05:05:53,743 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-07-16 05:05:53,915 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3314f179{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-07-16 05:05:53,965 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fafa9bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-07-16 05:05:54,822 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2ffaa711{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12763670793594587174/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-07-16 05:05:54,921 [main] INFO server.AbstractConnector: Started ServerConnector@43414b88{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-07-16 05:05:54,928 [main] INFO server.Server: Started @66988ms
datanode_2          | 2023-07-16 05:05:54,945 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 05:05:54,945 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-07-16 05:05:54,960 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-07-16 05:05:55,639 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-16 05:05:56,167 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-07-16 05:05:56,175 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-07-16 05:05:58,389 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-07-16 05:05:58,389 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-07-16 05:05:58,392 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-07-16 05:05:58,429 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 05:06:15,001 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From e93585490b94/172.24.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:57398 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:57398 remote=scm/172.24.0.4:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 05:06:18,015 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-eafa12ad-5c10-4717-bf0c-9e6a150e1f8c/container.db to cache
datanode_1          | 2023-07-16 05:06:18,015 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-eafa12ad-5c10-4717-bf0c-9e6a150e1f8c/container.db for volume DS-eafa12ad-5c10-4717-bf0c-9e6a150e1f8c
datanode_1          | 2023-07-16 05:06:18,100 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 05:06:18,117 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-07-16 05:06:18,691 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-07-16 05:06:18,692 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 301767aa-b1e5-4681-b115-67455213d3e0
datanode_1          | 2023-07-16 05:06:18,820 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 301767aa-b1e5-4681-b115-67455213d3e0: start RPC server
datanode_1          | 2023-07-16 05:06:18,862 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 301767aa-b1e5-4681-b115-67455213d3e0: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 05:06:18,879 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 301767aa-b1e5-4681-b115-67455213d3e0: GrpcService started, listening on 9856
datanode_1          | 2023-07-16 05:06:18,882 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 301767aa-b1e5-4681-b115-67455213d3e0: GrpcService started, listening on 9857
datanode_1          | 2023-07-16 05:06:18,909 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 301767aa-b1e5-4681-b115-67455213d3e0 is started using port 9858 for RATIS
datanode_1          | 2023-07-16 05:06:18,909 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 301767aa-b1e5-4681-b115-67455213d3e0 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 05:06:18,909 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 301767aa-b1e5-4681-b115-67455213d3e0 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 05:06:18,912 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-301767aa-b1e5-4681-b115-67455213d3e0: Started
datanode_1          | 2023-07-16 05:06:19,050 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 05:06:24,061 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 301767aa-b1e5-4681-b115-67455213d3e0: addNew group-A5D1EDC638C5:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-A5D1EDC638C5:java.util.concurrent.CompletableFuture@6adcbfd9[Not completed]
datanode_4          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_4          | STARTUP_MSG:   java = 11.0.19
datanode_4          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | ************************************************************/
datanode_4          | 2023-07-16 05:05:05,160 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-07-16 05:05:05,544 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 2023-07-16 05:05:06,956 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-07-16 05:05:08,316 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-07-16 05:05:08,316 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4          | 2023-07-16 05:05:10,053 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:37e9a0ff2c7a ip:172.24.0.10
datanode_4          | 2023-07-16 05:05:12,163 [main] INFO reflections.Reflections: Reflections took 1567 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_4          | 2023-07-16 05:05:17,608 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-07-16 05:05:18,168 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4          | 2023-07-16 05:05:20,757 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4          | 2023-07-16 05:05:20,909 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4          | 2023-07-16 05:05:20,919 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4          | 2023-07-16 05:05:20,937 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4          | 2023-07-16 05:05:21,525 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4          | 2023-07-16 05:05:21,549 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-07-16 05:05:21,586 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-07-16 05:05:21,615 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-07-16 05:05:21,616 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4          | 2023-07-16 05:05:21,617 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_4          | 2023-07-16 05:05:21,965 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4          | 2023-07-16 05:05:21,968 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4          | 2023-07-16 05:05:39,793 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-07-16 05:05:41,334 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-07-16 05:05:41,910 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-07-16 05:05:43,835 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-07-16 05:05:43,881 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4          | 2023-07-16 05:05:43,882 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-07-16 05:05:43,882 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4          | 2023-07-16 05:05:43,882 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 2023-07-16 05:05:43,882 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4          | 2023-07-16 05:05:43,883 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 2023-07-16 05:05:43,923 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:05:43,961 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4          | 2023-07-16 05:05:43,973 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-16 05:05:44,216 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-07-16 05:05:44,270 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4          | 2023-07-16 05:05:44,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4          | 2023-07-16 05:05:48,490 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_4          | 2023-07-16 05:05:48,532 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4          | 2023-07-16 05:05:48,533 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4          | 2023-07-16 05:05:48,536 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-16 05:05:48,540 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-16 05:05:48,639 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-16 05:05:49,316 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4          | 2023-07-16 05:05:49,718 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_4          | 2023-07-16 05:05:51,530 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4          | 2023-07-16 05:05:51,647 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4          | 2023-07-16 05:05:51,887 [main] INFO util.log: Logging initialized @64345ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4          | 2023-07-16 05:05:53,398 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4          | 2023-07-16 05:05:53,462 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 2023-07-16 05:05:53,530 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-07-16 05:05:53,550 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4          | 2023-07-16 05:05:53,558 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 2023-07-16 05:05:53,558 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-07-16 05:05:53,994 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_4          | 2023-07-16 05:05:54,011 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4          | 2023-07-16 05:05:54,078 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_4          | 2023-07-16 05:05:54,348 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-07-16 05:05:54,358 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-07-16 05:05:54,367 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_4          | 2023-07-16 05:05:54,539 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ad6448e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4          | 2023-07-16 05:05:54,547 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1abd1a28{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-07-16 05:05:56,177 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@687d31a9{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-10550609678375870884/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4          | 2023-07-16 05:05:56,302 [main] INFO server.AbstractConnector: Started ServerConnector@1529d534{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4          | 2023-07-16 05:05:56,302 [main] INFO server.Server: Started @68760ms
datanode_4          | 2023-07-16 05:05:56,324 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-07-16 05:05:56,324 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-07-16 05:05:56,380 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4          | 2023-07-16 05:05:56,827 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-07-16 05:05:57,084 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_4          | 2023-07-16 05:05:57,088 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_4          | 2023-07-16 05:05:59,592 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 2023-07-16 05:05:59,592 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_4          | 2023-07-16 05:05:59,632 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_4          | 2023-07-16 05:05:59,718 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-07-16 05:05:58,445 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-07-16 05:05:59,407 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.5:9891
datanode_2          | 2023-07-16 05:06:00,187 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-07-16 05:06:02,090 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:02,107 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:03,094 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:03,108 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:04,095 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:04,109 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:05,097 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:05,120 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:06,098 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:06,154 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:07,155 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:08,157 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:09,164 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:10,165 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:11,175 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 4caf8da3e4ee/172.24.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:55146 remote=recon/172.24.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | 2023-07-16 05:05:59,702 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4          | 2023-07-16 05:06:00,724 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.5:9891
datanode_4          | 2023-07-16 05:06:01,234 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4          | 2023-07-16 05:06:03,265 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:03,269 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:04,266 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:04,270 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:05,267 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:05,270 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:06,268 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:06,270 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:07,269 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:08,270 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:09,271 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:10,272 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:11,324 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 37e9a0ff2c7a/172.24.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:55568 remote=recon/172.24.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:55568 remote=recon/172.24.0.5:9891]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:55146 remote=recon/172.24.0.5:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 2023-07-16 05:06:24,239 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0: new RaftServerImpl for group-A5D1EDC638C5:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 05:06:24,279 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-07-16 05:06:24,282 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-07-16 05:06:24,282 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 05:06:24,283 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:06:24,283 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:06:24,284 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:06:24,332 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: ConfigurationManager, init=-1: peers:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-07-16 05:06:24,336 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-07-16 05:06:24,362 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:06:24,367 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-07-16 05:06:24,443 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:06:24,465 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-07-16 05:06:24,492 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 05:06:24,492 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 05:06:24,643 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-07-16 05:06:24,837 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 05:06:24,851 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:06:24,874 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 05:06:24,879 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 05:06:24,882 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 05:06:24,885 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 05:06:24,886 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f02ad586-a085-4098-91a1-a5d1edc638c5 does not exist. Creating ...
datanode_1          | 2023-07-16 05:06:24,938 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f02ad586-a085-4098-91a1-a5d1edc638c5/in_use.lock acquired by nodename 7@e93585490b94
datanode_1          | 2023-07-16 05:06:24,995 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f02ad586-a085-4098-91a1-a5d1edc638c5 has been successfully formatted.
datanode_1          | 2023-07-16 05:06:25,074 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO ratis.ContainerStateMachine: group-A5D1EDC638C5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-07-16 05:06:25,136 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:06:25,204 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:06:25,204 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:06:25,216 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 05:06:25,221 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-07-16 05:06:25,235 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:06:25,327 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:06:25,327 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-07-16 05:06:25,328 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:06:25,408 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f02ad586-a085-4098-91a1-a5d1edc638c5
datanode_1          | 2023-07-16 05:06:25,409 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 2023-07-16 05:06:25,419 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:06:25,420 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:06:25,420 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 05:06:25,421 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-07-16 05:05:05,156 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | /************************************************************
datanode_3          | 2023-07-16 05:05:02,603 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | STARTUP_MSG: Starting HddsDatanodeService
datanode_5          | STARTUP_MSG:   host = 4fd9c9bae09d/172.24.0.15
datanode_5          | STARTUP_MSG:   args = []
datanode_3          | /************************************************************
datanode_4          | 2023-07-16 05:06:15,290 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 37e9a0ff2c7a/172.24.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:55052 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 05:06:25,421 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 952facba62e3/172.24.0.8
datanode_2          | 2023-07-16 05:06:15,180 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_5          | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 05:06:25,422 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-07-16 05:06:25,463 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 05:06:25,564 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_2          | java.net.SocketTimeoutException: Call From 4caf8da3e4ee/172.24.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:37996 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 2023-07-16 05:06:25,581 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:06:25,680 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:06:25,681 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 2023-07-16 05:06:25,699 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | ************************************************************/
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-07-16 05:05:05,539 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | ************************************************************/
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c3b9c2e51263/172.24.0.13
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-07-16 05:06:25,783 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:06:25,783 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
datanode_1          | 2023-07-16 05:06:25,794 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: start as a follower, conf=-1: peers:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 2023-07-16 05:05:02,725 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-07-16 05:05:05,274 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | 2023-07-16 05:05:05,655 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 2023-07-16 05:06:25,794 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | 2023-07-16 05:05:03,458 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 2023-07-16 05:05:07,132 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 2023-07-16 05:06:25,807 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState
om_1                | ************************************************************/
om_1                | 2023-07-16 05:05:05,653 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-07-16 05:05:04,707 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-07-16 05:05:06,675 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-07-16 05:05:08,793 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-16 05:05:04,214 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 2023-07-16 05:06:25,850 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:06:25,862 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:06:25,863 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A5D1EDC638C5,id=301767aa-b1e5-4681-b115-67455213d3e0
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
scm_1               | 2023-07-16 05:05:02,820 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
datanode_5          | 2023-07-16 05:05:08,793 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5          | 2023-07-16 05:05:10,498 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4fd9c9bae09d ip:172.24.0.15
datanode_1          | 2023-07-16 05:06:25,865 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 05:05:18,361 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-07-16 05:05:22,967 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1               | /************************************************************
datanode_5          | 2023-07-16 05:05:12,694 [main] INFO reflections.Reflections: Reflections took 1746 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_5          | 2023-07-16 05:05:18,028 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-07-16 05:06:25,865 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om_1                | 2023-07-16 05:05:24,154 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.13:9862
om_1                | 2023-07-16 05:05:24,216 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
datanode_3          | 2023-07-16 05:05:06,675 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-07-16 05:05:08,093 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:952facba62e3 ip:172.24.0.8
datanode_3          | 2023-07-16 05:05:10,292 [main] INFO reflections.Reflections: Reflections took 1656 ms to scan 2 urls, producing 107 keys and 232 values 
datanode_5          | 2023-07-16 05:05:18,591 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5          | 2023-07-16 05:05:20,574 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-07-16 05:06:25,865 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om_1                | 2023-07-16 05:05:24,216 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
datanode_3          | 2023-07-16 05:05:15,453 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 5a10f63682a1/172.24.0.4
datanode_5          | 2023-07-16 05:05:20,784 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-07-16 05:05:20,822 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 2023-07-16 05:06:25,883 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om_1                | 2023-07-16 05:05:24,490 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:05:26,692 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
datanode_3          | 2023-07-16 05:05:15,931 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-07-16 05:05:18,352 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-07-16 05:05:18,666 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-07-16 05:05:20,835 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5          | 2023-07-16 05:05:21,179 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 2023-07-16 05:06:26,020 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:55052 remote=scm/172.24.0.4:9861]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 2023-07-16 05:05:21,226 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-16 05:05:21,286 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 2023-07-16 05:06:26,028 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5.
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 2023-07-16 05:05:18,723 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-07-16 05:05:18,729 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-07-16 05:05:19,012 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-07-16 05:05:19,110 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-16 05:05:21,303 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5          | 2023-07-16 05:05:21,303 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | 2023-07-16 05:06:26,028 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 301767aa-b1e5-4681-b115-67455213d3e0: addNew group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-C9411BC9FD15:java.util.concurrent.CompletableFuture@636bd241[Not completed]
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 2023-07-16 05:05:21,303 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-07-16 05:05:21,612 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:37996 remote=scm/172.24.0.4:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 2023-07-16 05:06:26,032 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0: new RaftServerImpl for group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-07-16 05:05:21,641 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5          | 2023-07-16 05:05:37,738 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 05:05:19,135 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-07-16 05:06:18,250 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-a9813a87-bb62-436a-b327-121845f2e5bc/container.db to cache
om_1                | 2023-07-16 05:05:32,269 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-07-16 05:05:34,272 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-07-16 05:05:36,274 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
datanode_5          | 2023-07-16 05:05:38,924 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-16 05:05:39,805 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-16 05:06:18,251 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-a9813a87-bb62-436a-b327-121845f2e5bc/container.db for volume DS-a9813a87-bb62-436a-b327-121845f2e5bc
om_1                | 2023-07-16 05:05:38,276 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-07-16 05:05:40,278 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-07-16 05:05:42,280 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
datanode_5          | 2023-07-16 05:05:41,582 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-07-16 05:05:41,584 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-07-16 05:06:18,274 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om_1                | 2023-07-16 05:05:44,283 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-07-16 05:05:46,285 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-07-16 05:05:48,288 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
datanode_5          | 2023-07-16 05:05:41,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-07-16 05:05:41,625 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-16 05:06:18,297 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om_1                | 2023-07-16 05:05:50,289 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-07-16 05:05:52,291 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-07-16 05:05:54,293 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
datanode_5          | 2023-07-16 05:05:41,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
s3g_1               | 2023-07-16 05:05:04,242 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-07-16 05:06:18,762 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om_1                | 2023-07-16 05:05:56,294 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-07-16 05:05:58,296 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-07-16 05:06:00,300 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
datanode_5          | 2023-07-16 05:05:41,632 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
s3g_1               | 2023-07-16 05:05:04,841 [main] INFO util.log: Logging initialized @19312ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 05:05:19,173 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-07-16 05:06:18,768 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_4          | 2023-07-16 05:06:18,907 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start RPC server
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-07-16 05:05:41,655 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
s3g_1               | 2023-07-16 05:05:06,460 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-07-16 05:06:26,043 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-16 05:06:18,927 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: GrpcService started, listening on 9858
om_1                | 2023-07-16 05:06:02,305 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
datanode_3          | 2023-07-16 05:05:19,178 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-07-16 05:05:19,185 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-07-16 05:05:19,678 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5          | 2023-07-16 05:05:41,658 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
s3g_1               | 2023-07-16 05:05:06,716 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
datanode_1          | 2023-07-16 05:06:26,050 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-07-16 05:06:26,050 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-07-16 05:06:04,321 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
datanode_3          | 2023-07-16 05:05:19,684 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-07-16 05:05:37,346 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-07-16 05:05:38,863 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-07-16 05:05:41,721 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
s3g_1               | 2023-07-16 05:05:06,793 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-07-16 05:06:26,051 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-07-16 05:06:18,938 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: GrpcService started, listening on 9856
om_1                | 2023-07-16 05:06:06,323 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 18.
datanode_5          | 2023-07-16 05:05:41,721 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
s3g_1               | 2023-07-16 05:05:06,818 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
datanode_1          | 2023-07-16 05:06:26,051 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-07-16 05:06:18,940 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: GrpcService started, listening on 9857
datanode_4          | 2023-07-16 05:06:18,947 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 131a1d93-dd5a-481f-884e-0521d7ee6f91 is started using port 9858 for RATIS
datanode_4          | 2023-07-16 05:06:18,956 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 131a1d93-dd5a-481f-884e-0521d7ee6f91 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-07-16 05:06:18,956 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 131a1d93-dd5a-481f-884e-0521d7ee6f91 is started using port 9856 for RATIS_SERVER
datanode_4          | 2023-07-16 05:06:18,965 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-131a1d93-dd5a-481f-884e-0521d7ee6f91: Started
datanode_5          | 2023-07-16 05:05:41,803 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
s3g_1               | 2023-07-16 05:05:06,832 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-07-16 05:06:26,051 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-07-16 05:06:26,051 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-07-16 05:06:26,058 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-07-16 05:06:19,083 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
s3g_1               | 2023-07-16 05:05:06,832 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:05:39,275 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-07-16 05:05:40,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-07-16 05:06:31,731 [grpc-default-executor-2] WARN server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Failed requestVote 333b8eb3-808f-40e6-a1dc-b9ce55f887b5->131a1d93-dd5a-481f-884e-0521d7ee6f91#0
datanode_5          | 2023-07-16 05:05:41,807 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5          | 2023-07-16 05:05:41,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-07-16 05:05:44,837 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-07-16 05:05:44,860 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
s3g_1               | 2023-07-16 05:05:07,388 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir9832503391071686209
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
datanode_5          | 2023-07-16 05:05:44,883 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-07-16 05:05:44,898 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-07-16 05:05:44,924 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-16 05:05:44,949 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
s3g_1               | 2023-07-16 05:05:09,630 [main] INFO s3.Gateway: STARTUP_MSG: 
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
om_1                | 2023-07-16 05:06:08,331 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c3b9c2e51263/172.24.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 19.
datanode_5          | 2023-07-16 05:05:45,393 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
scm_1               | STARTUP_MSG:   args = [--init]
recon_1             | 2023-07-16 05:05:02,873 [main] INFO recon.ReconServer: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
datanode_3          | 2023-07-16 05:05:41,056 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
s3g_1               | STARTUP_MSG:   host = f43566a4e1fd/172.24.0.3
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
om_1                | 2023-07-16 05:06:14,288 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:bf1223b9-84ee-4a14-b347-7ad6126fbc27 is not the leader. Could not determine the leader node.
datanode_5          | 2023-07-16 05:05:46,138 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | /************************************************************
datanode_1          | 2023-07-16 05:06:26,066 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-07-16 05:06:26,078 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 does not exist. Creating ...
datanode_3          | 2023-07-16 05:05:41,082 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1               | STARTUP_MSG:   args = []
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_5          | 2023-07-16 05:05:47,242 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG: Starting ReconServer
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 2023-07-16 05:06:26,087 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/in_use.lock acquired by nodename 7@e93585490b94
datanode_1          | 2023-07-16 05:06:26,097 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 has been successfully formatted.
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_5          | 2023-07-16 05:05:47,333 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-07-16 05:06:18,199 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-b6cf0c60-d55d-4c94-9478-e482cafb72eb/container.db to cache
datanode_3          | 2023-07-16 05:05:41,191 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-07-16 05:05:41,191 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
datanode_5          | 2023-07-16 05:05:47,831 [main] INFO util.log: Logging initialized @59753ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-07-16 05:06:18,199 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-b6cf0c60-d55d-4c94-9478-e482cafb72eb/container.db for volume DS-b6cf0c60-d55d-4c94-9478-e482cafb72eb
datanode_2          | 2023-07-16 05:06:18,236 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-07-16 05:06:18,267 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-07-16 05:05:41,191 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-07-16 05:05:41,192 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
datanode_5          | 2023-07-16 05:05:48,773 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5          | 2023-07-16 05:05:48,871 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5          | 2023-07-16 05:05:48,964 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5          | 2023-07-16 05:05:48,989 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 05:05:41,193 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:05:41,195 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
s3g_1               | STARTUP_MSG:   java = 11.0.19
datanode_4          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_5          | 2023-07-16 05:05:49,004 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5          | 2023-07-16 05:05:49,009 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5          | 2023-07-16 05:05:49,415 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_5          | 2023-07-16 05:05:49,462 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 05:05:41,195 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:05:41,389 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 05:05:41,428 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-07-16 05:05:41,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir9832503391071686209, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_5          | 2023-07-16 05:05:49,469 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-07-16 05:06:18,719 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-07-16 05:06:18,719 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:18,869 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start RPC server
recon_1             | STARTUP_MSG:   host = bfb16f0e3daa/172.24.0.5
datanode_3          | 2023-07-16 05:05:45,106 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-07-16 05:05:45,146 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-07-16 05:05:45,171 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
s3g_1               | ************************************************************/
datanode_2          | 2023-07-16 05:06:18,892 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: GrpcService started, listening on 9858
datanode_1          | 2023-07-16 05:06:26,105 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO ratis.ContainerStateMachine: group-C9411BC9FD15: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-07-16 05:05:49,786 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5          | 2023-07-16 05:05:49,832 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5          | 2023-07-16 05:05:49,837 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-16 05:05:45,171 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:05:45,192 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:05:45,219 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:06:18,895 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: GrpcService started, listening on 9856
s3g_1               | 2023-07-16 05:05:09,749 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-07-16 05:06:26,105 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:06:26,123 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:06:26,123 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:06:26,123 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-07-16 05:06:26,123 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 05:05:46,206 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-07-16 05:05:47,001 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-07-16 05:05:49,031 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-07-16 05:06:18,898 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: GrpcService started, listening on 9857
s3g_1               | 2023-07-16 05:05:09,902 [main] INFO s3.Gateway: Starting Ozone S3 gateway
datanode_1          | 2023-07-16 05:06:26,124 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
s3g_1               | 2023-07-16 05:05:10,950 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   java = 11.0.19
datanode_5          | 2023-07-16 05:05:50,105 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1abd1a28{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-07-16 05:05:49,196 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-07-16 05:05:49,571 [main] INFO util.log: Logging initialized @63707ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-07-16 05:05:51,035 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-07-16 05:06:18,915 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 is started using port 9858 for RATIS
datanode_1          | 2023-07-16 05:06:26,165 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:06:26,167 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
s3g_1               | 2023-07-16 05:05:12,893 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-07-16 05:05:12,893 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-07-16 05:05:13,476 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-07-16 05:05:13,478 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-07-16 05:05:51,122 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_2          | 2023-07-16 05:06:18,915 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-07-16 05:06:26,167 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1               | 2023-07-16 05:05:13,473 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | 2023-07-16 05:05:50,137 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@53fd59d4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5          | 2023-07-16 05:05:51,363 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3abb6aa{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17624438706830209491/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-07-16 05:05:51,176 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_2          | 2023-07-16 05:06:18,915 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-07-16 05:06:26,167 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15
s3g_1               | 2023-07-16 05:05:14,093 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
recon_1             | STARTUP_MSG:   java = 11.0.19
datanode_3          | 2023-07-16 05:05:51,249 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-07-16 05:05:51,292 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
datanode_2          | 2023-07-16 05:06:18,916 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Started
datanode_1          | 2023-07-16 05:06:26,167 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
s3g_1               | 2023-07-16 05:05:14,101 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3          | 2023-07-16 05:05:51,293 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 05:06:19,041 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-07-16 05:06:26,168 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
s3g_1               | 2023-07-16 05:05:14,161 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-07-16 05:05:14,201 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3113a37{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-07-16 05:05:14,202 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52eacb4b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-07-16 05:05:47,893 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.115s with 0.354s GC time.
datanode_3          | 2023-07-16 05:05:51,642 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | 2023-07-16 05:06:20,838 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1          | 2023-07-16 05:06:26,169 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=354ms
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_5          | 2023-07-16 05:05:51,429 [main] INFO server.AbstractConnector: Started ServerConnector@52a605c3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_5          | 2023-07-16 05:05:51,430 [main] INFO server.Server: Started @63352ms
datanode_3          | 2023-07-16 05:05:51,731 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-07-16 05:05:51,733 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | ************************************************************/
datanode_1          | 2023-07-16 05:06:26,172 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-07-16 05:06:26,173 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
datanode_3          | 2023-07-16 05:05:51,956 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-07-16 05:06:26,173 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
s3g_1               | 2023-07-16 05:05:53,430 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@77f03916{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir9832503391071686209/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-2308754993392988060/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
recon_1             | 2023-07-16 05:05:03,053 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-07-16 05:06:31,741 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Failed requestVote f02a32cc-4f60-4ab6-a6b0-1a6b831becc3->131a1d93-dd5a-481f-884e-0521d7ee6f91#0
scm_1               | 2023-07-16 05:05:03,022 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 05:05:04,102 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
datanode_3          | 2023-07-16 05:05:51,956 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1          | 2023-07-16 05:06:26,176 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
s3g_1               | 2023-07-16 05:05:53,500 [main] INFO server.AbstractConnector: Started ServerConnector@2c282004{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
recon_1             | 2023-07-16 05:05:10,866 [main] INFO reflections.Reflections: Reflections took 618 ms to scan 1 urls, producing 20 keys and 75 values 
datanode_4          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-8C72950665A4 not found.
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_3          | 2023-07-16 05:05:51,977 [main] INFO server.session: node0 Scavenging every 660000ms
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
datanode_1          | 2023-07-16 05:06:26,178 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
s3g_1               | 2023-07-16 05:05:53,500 [main] INFO server.Server: Started @67971ms
recon_1             | 2023-07-16 05:05:19,016 [main] INFO reflections.Reflections: Reflections took 671 ms to scan 3 urls, producing 132 keys and 288 values 
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_3          | 2023-07-16 05:05:52,059 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3314f179{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 05:05:06,628 [main] INFO reflections.Reflections: Reflections took 1840 ms to scan 3 urls, producing 132 keys and 288 values 
scm_1               | 2023-07-16 05:05:08,086 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_1          | 2023-07-16 05:06:26,180 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
s3g_1               | 2023-07-16 05:05:53,512 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-07-16 05:05:20,531 [main] INFO recon.ReconServer: Initializing Recon server...
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_5          | 2023-07-16 05:05:51,432 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-07-16 05:05:52,085 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fafa9bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
datanode_1          | 2023-07-16 05:06:26,217 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1               | 2023-07-16 05:05:53,512 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 05:05:25,805 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-07-16 05:05:37,442 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
datanode_3          | 2023-07-16 05:05:53,502 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2ffaa711{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1669038908899224652/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1          | 2023-07-16 05:06:26,933 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-301767aa-b1e5-4681-b115-67455213d3e0: Detected pause in JVM or host machine approximately 0.433s with 0.704s GC time.
s3g_1               | 2023-07-16 05:05:53,527 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
datanode_4          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_4          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_3          | 2023-07-16 05:05:53,621 [main] INFO server.AbstractConnector: Started ServerConnector@43414b88{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
om_1                | 2023-07-16 05:06:16,303 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:bf1223b9-84ee-4a14-b347-7ad6126fbc27 is not the leader. Could not determine the leader node.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=90ms
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
datanode_5          | 2023-07-16 05:05:51,432 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_3          | 2023-07-16 05:05:53,642 [main] INFO server.Server: Started @67814ms
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=614ms
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_3          | 2023-07-16 05:05:53,664 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
datanode_1          | 2023-07-16 05:06:26,947 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_3          | 2023-07-16 05:05:53,664 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-07-16 05:05:53,676 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-07-16 05:05:54,192 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
recon_1             | 2023-07-16 05:05:41,149 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-07-16 05:05:41,150 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 2023-07-16 05:05:41,730 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
datanode_5          | 2023-07-16 05:05:51,460 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5          | 2023-07-16 05:05:51,809 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
datanode_1          | 2023-07-16 05:06:26,947 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 2023-07-16 05:05:42,065 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
datanode_3          | 2023-07-16 05:05:54,557 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-07-16 05:05:54,587 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-07-16 05:05:57,215 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
datanode_1          | 2023-07-16 05:06:26,947 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | 2023-07-16 05:06:32,161 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Failed requestVote 333b8eb3-808f-40e6-a1dc-b9ce55f887b5->131a1d93-dd5a-481f-884e-0521d7ee6f91#0
datanode_4          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
datanode_1          | 2023-07-16 05:06:26,950 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:05:57,215 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-07-16 05:05:57,219 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-07-16 05:05:57,242 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
scm_1               | 2023-07-16 05:05:08,212 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
datanode_1          | 2023-07-16 05:06:26,950 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_5          | 2023-07-16 05:05:51,996 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_5          | 2023-07-16 05:05:52,011 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_5          | 2023-07-16 05:05:54,772 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
scm_1               | 2023-07-16 05:05:10,714 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 05:05:12,011 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-07-16 05:06:26,955 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:06:26,955 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-07-16 05:06:26,955 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState
scm_1               | 2023-07-16 05:05:12,074 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:05:12,081 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1             | 2023-07-16 05:05:42,091 [main] INFO recon.ReconServer: Creating Recon Schema.
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_1          | 2023-07-16 05:06:26,956 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C9411BC9FD15,id=301767aa-b1e5-4681-b115-67455213d3e0
datanode_1          | 2023-07-16 05:06:26,956 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-07-16 05:05:54,777 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
scm_1               | 2023-07-16 05:05:12,089 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:05:12,096 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1             | 2023-07-16 05:05:48,756 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
datanode_1          | 2023-07-16 05:06:26,956 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:05:57,662 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5          | 2023-07-16 05:05:54,788 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 05:05:12,103 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 2023-07-16 05:05:48,927 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 2023-07-16 05:06:26,956 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:05:58,684 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.5:9891
datanode_5          | 2023-07-16 05:05:54,791 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
scm_1               | 2023-07-16 05:05:12,165 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 2023-07-16 05:05:49,116 [main] INFO util.log: Logging initialized @63199ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 2023-07-16 05:06:26,956 [301767aa-b1e5-4681-b115-67455213d3e0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:05:59,238 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-07-16 05:06:01,772 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-16 05:05:54,828 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 2023-07-16 05:05:50,933 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_3          | 2023-07-16 05:06:01,787 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:12,221 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:05:56,164 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.5:9891
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_1          | 2023-07-16 05:06:26,960 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:06:02,780 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:12,244 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_5          | 2023-07-16 05:05:56,710 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863 after 21 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 21.
recon_1             | 2023-07-16 05:05:51,027 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
datanode_2          | 	... 1 more
datanode_4          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_1          | 2023-07-16 05:06:26,997 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:06:02,790 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:06:03,781 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-16 05:05:58,594 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-46f5dd7f-dd64-4591-bc47-757bf1eef498;layoutVersion=6
recon_1             | 2023-07-16 05:05:51,090 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-07-16 05:06:24,106 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: addNew group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-44F4E9684BB5:java.util.concurrent.CompletableFuture@5f50f3c8[Not completed]
datanode_4          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_1          | 2023-07-16 05:06:26,997 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15
datanode_3          | 2023-07-16 05:06:03,791 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:12,257 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
datanode_5          | 2023-07-16 05:05:58,623 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-16 05:05:59,595 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1             | 2023-07-16 05:05:51,172 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
datanode_2          | 2023-07-16 05:06:24,247 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: new RaftServerImpl for group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_1          | 2023-07-16 05:06:29,834 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-301767aa-b1e5-4681-b115-67455213d3e0: Detected pause in JVM or host machine approximately 0.397s with 0.671s GC time.
datanode_3          | 2023-07-16 05:06:04,782 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:12,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-07-16 05:05:12,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-07-16 05:06:19,414 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
recon_1             | 2023-07-16 05:05:51,176 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-07-16 05:06:24,258 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:06:24,259 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=671ms
datanode_3          | 2023-07-16 05:06:04,797 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:12,502 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-07-16 05:05:59,624 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | /************************************************************
datanode_2          | 2023-07-16 05:06:24,260 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 2023-07-16 05:05:51,177 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_1          | 2023-07-16 05:06:30,943 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO impl.FollowerState: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5147666036ns, electionTimeout:5079ms
datanode_3          | 2023-07-16 05:06:05,784 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:16,034 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-07-16 05:06:00,597 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at c3b9c2e51263/172.24.0.13
datanode_2          | 2023-07-16 05:06:24,266 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1             | 2023-07-16 05:05:51,834 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_1          | 2023-07-16 05:06:30,944 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: shutdown 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState
datanode_3          | 2023-07-16 05:06:05,798 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:16,050 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5          | 2023-07-16 05:06:00,625 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | 2023-07-16 05:05:51,877 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_1          | 2023-07-16 05:06:30,944 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 05:06:06,799 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:16,084 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-07-16 05:06:01,599 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:24,266 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-07-16 05:05:54,241 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_1          | 2023-07-16 05:06:30,966 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 05:06:07,802 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:16,085 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_5          | 2023-07-16 05:06:01,626 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:24,268 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 05:06:29,029 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
recon_1             | 2023-07-16 05:05:54,284 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1          | 2023-07-16 05:06:30,969 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-FollowerState] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1
datanode_3          | 2023-07-16 05:06:08,808 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm_1               | 2023-07-16 05:05:16,088 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-16 05:06:02,601 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:24,335 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | /************************************************************
recon_1             | 2023-07-16 05:05:54,311 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
datanode_4          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_1          | 2023-07-16 05:06:31,019 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:09,809 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:06:10,839 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5          | 2023-07-16 05:06:02,636 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:24,341 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | STARTUP_MSG: Starting OzoneManager
recon_1             | 2023-07-16 05:05:54,590 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 2023-07-16 05:06:31,041 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-07-16 05:05:16,092 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
datanode_2          | 2023-07-16 05:06:24,368 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-07-16 05:06:03,257 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_5          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1          | 2023-07-16 05:06:31,050 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | java.net.SocketTimeoutException: Call From 952facba62e3/172.24.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:51630 remote=recon/172.24.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1             | 2023-07-16 05:05:54,611 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
datanode_2          | 2023-07-16 05:06:24,371 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om_1                | STARTUP_MSG:   host = c3b9c2e51263/172.24.0.13
datanode_1          | 2023-07-16 05:06:31,051 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm_1               | 2023-07-16 05:05:16,215 [main] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: addNew group-757BF1EEF498:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER] returns group-757BF1EEF498:java.util.concurrent.CompletableFuture@6573d2f7[Not completed]
recon_1             | 2023-07-16 05:06:01,642 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:06:02,446 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om_1                | STARTUP_MSG:   args = []
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 2023-07-16 05:06:31,051 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: shutdown 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm_1               | 2023-07-16 05:05:16,399 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27: new RaftServerImpl for group-757BF1EEF498:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
recon_1             | 2023-07-16 05:06:02,915 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
datanode_2          | 2023-07-16 05:06:24,464 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | 2023-07-16 05:06:31,070 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm_1               | 2023-07-16 05:05:16,435 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1             | 2023-07-16 05:06:02,932 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
datanode_2          | 2023-07-16 05:06:24,491 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
datanode_1          | 2023-07-16 05:06:31,073 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A5D1EDC638C5 with new leaderId: 301767aa-b1e5-4681-b115-67455213d3e0
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm_1               | 2023-07-16 05:05:16,436 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 2023-07-16 05:06:03,368 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-16 05:06:24,519 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:50Z
datanode_1          | 2023-07-16 05:06:31,074 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: change Leader from null to 301767aa-b1e5-4681-b115-67455213d3e0 at term 1 for becomeLeader, leader elected after 6629ms
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
scm_1               | 2023-07-16 05:05:16,439 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 2023-07-16 05:06:03,740 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-16 05:06:24,523 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 05:06:31,166 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-07-16 05:06:31,215 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:06:31,219 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-07-16 05:06:31,229 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm_1               | 2023-07-16 05:05:16,452 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_2          | 2023-07-16 05:06:24,676 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 05:06:24,856 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-16 05:06:32,343 [grpc-default-executor-0] INFO server.RaftServer: 131a1d93-dd5a-481f-884e-0521d7ee6f91: addNew group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-44F4E9684BB5:java.util.concurrent.CompletableFuture@664458a7[Not completed]
datanode_4          | 2023-07-16 05:06:32,711 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91: new RaftServerImpl for group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-16 05:06:32,717 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-16 05:06:32,725 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_1          | 2023-07-16 05:06:31,230 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-07-16 05:06:31,231 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.util.concurrent.TimeoutException
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm_1               | 2023-07-16 05:05:16,456 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:05:16,460 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_4          | 2023-07-16 05:06:32,725 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | 2023-07-16 05:05:16,522 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: ConfigurationManager, init=-1: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 05:05:16,533 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1             | 2023-07-16 05:06:03,817 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-07-16 05:06:03,959 [main] INFO node.SCMNodeManager: Entering startup safe mode.
datanode_5          | 	... 1 more
datanode_2          | 2023-07-16 05:06:24,867 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | ************************************************************/
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
scm_1               | 2023-07-16 05:05:16,606 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 05:05:16,607 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 05:05:17,110 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_5          | 2023-07-16 05:06:03,603 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-16 05:06:03,637 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 05:06:29,090 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-07-16 05:06:04,605 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-07-16 05:06:04,638 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:32,726 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-07-16 05:06:32,728 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 2023-07-16 05:06:24,879 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-07-16 05:06:31,276 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:32,729 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 05:06:35,853 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
recon_1             | 2023-07-16 05:06:03,975 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-07-16 05:06:04,013 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
datanode_2          | 2023-07-16 05:06:24,905 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:06:24,906 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-07-16 05:06:37,145 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm_1               | 2023-07-16 05:05:17,263 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 05:05:17,346 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
datanode_2          | 2023-07-16 05:06:24,907 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 05:06:24,915 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 does not exist. Creating ...
om_1                | 2023-07-16 05:06:37,622 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.13:9862
datanode_4          | 2023-07-16 05:06:32,814 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-16 05:06:32,817 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-16 05:06:32,864 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-07-16 05:06:24,937 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/in_use.lock acquired by nodename 7@4caf8da3e4ee
datanode_2          | 2023-07-16 05:06:24,994 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 has been successfully formatted.
datanode_1          | 2023-07-16 05:06:31,281 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-07-16 05:06:05,259 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om_1                | 2023-07-16 05:06:37,627 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-07-16 05:06:37,627 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-07-16 05:06:37,927 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:06:05,357 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
datanode_4          | 2023-07-16 05:06:32,876 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-16 05:06:33,002 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm_1               | 2023-07-16 05:05:17,394 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 2023-07-16 05:06:25,131 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO ratis.ContainerStateMachine: group-44F4E9684BB5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-07-16 05:06:25,156 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 05:05:17,652 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-07-16 05:05:17,793 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1             | 2023-07-16 05:06:05,531 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-07-16 05:06:05,604 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 2023-07-16 05:06:25,253 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:06:25,259 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:05:19,554 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
datanode_4          | 2023-07-16 05:06:33,027 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:51630 remote=recon/172.24.0.5:9891]
datanode_1          | 2023-07-16 05:06:31,316 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderStateImpl
datanode_1          | 2023-07-16 05:06:31,503 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 2023-07-16 05:06:25,280 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:06:25,281 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-07-16 05:05:19,609 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 05:05:19,695 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 2023-07-16 05:06:31,716 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-LeaderElection1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5: set configuration 0: peers:[301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | 2023-07-16 05:06:25,287 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:25,396 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 05:05:19,704 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:05:19,741 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 2023-07-16 05:06:05,608 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 2023-07-16 05:06:32,036 [301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-A5D1EDC638C5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f02ad586-a085-4098-91a1-a5d1edc638c5/current/log_inprogress_0
datanode_1          | 2023-07-16 05:06:32,165 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO impl.FollowerState: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5210100116ns, electionTimeout:5168ms
datanode_1          | 2023-07-16 05:06:32,171 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: shutdown 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState
datanode_1          | 2023-07-16 05:06:32,171 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-16 05:05:19,742 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-07-16 05:06:33,087 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-07-16 05:06:33,091 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-07-16 05:06:33,219 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 2023-07-16 05:06:32,171 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-07-16 05:06:38,298 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-07-16 05:06:38,976 [main] INFO reflections.Reflections: Reflections took 535 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-07-16 05:06:39,023 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
scm_1               | 2023-07-16 05:05:19,757 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498 does not exist. Creating ...
datanode_4          | 2023-07-16 05:06:33,327 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-07-16 05:06:33,348 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 2023-07-16 05:06:32,171 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2
om_1                | 2023-07-16 05:06:39,209 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:06:40,150 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
om_1                | 2023-07-16 05:06:40,368 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9863]
scm_1               | 2023-07-16 05:05:19,814 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/in_use.lock acquired by nodename 13@5a10f63682a1
datanode_4          | 2023-07-16 05:06:33,410 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-07-16 05:06:33,413 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:05:19,959 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498 has been successfully formatted.
scm_1               | 2023-07-16 05:05:20,141 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 05:05:20,383 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-07-16 05:06:32,183 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
recon_1             | 2023-07-16 05:06:05,655 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-07-16 05:06:05,872 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-07-16 05:06:05,878 [main] INFO recon.ReconServer: Recon server initialized successfully!
datanode_1          | 2023-07-16 05:06:32,201 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1             | 2023-07-16 05:06:05,878 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-07-16 05:06:06,188 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-07-16 05:06:33,443 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-07-16 05:06:33,455 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-07-16 05:06:25,408 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:06:25,409 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:25,466 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5
om_1                | 2023-07-16 05:06:42,613 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
datanode_1          | 2023-07-16 05:06:32,201 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:06:32,231 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_4          | 2023-07-16 05:06:33,457 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 does not exist. Creating ...
datanode_4          | 2023-07-16 05:06:33,527 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/in_use.lock acquired by nodename 7@37e9a0ff2c7a
datanode_5          | Caused by: java.util.concurrent.TimeoutException
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om_1                | 2023-07-16 05:06:42,800 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-07-16 05:06:44,319 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-07-16 05:06:46,660 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-07-16 05:06:46,801 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-07-16 05:06:46,831 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-16 05:06:25,467 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om_1                | 2023-07-16 05:06:46,942 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
recon_1             | 2023-07-16 05:06:06,206 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-07-16 05:06:06,206 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-07-16 05:06:07,332 [main] INFO http.HttpServer2: Jetty bound to port 9888
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 2023-07-16 05:06:32,248 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 2023-07-16 05:06:25,474 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om_1                | 2023-07-16 05:06:46,946 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
recon_1             | 2023-07-16 05:06:07,360 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-07-16 05:06:07,461 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-07-16 05:06:07,463 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 2023-07-16 05:06:33,585 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 has been successfully formatted.
datanode_4          | 2023-07-16 05:06:33,813 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO ratis.ContainerStateMachine: group-44F4E9684BB5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-16 05:06:33,821 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-16 05:06:33,900 [grpc-default-executor-2] INFO server.RaftServer: 131a1d93-dd5a-481f-884e-0521d7ee6f91: addNew group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-8C72950665A4:java.util.concurrent.CompletableFuture@4819221e[Not completed]
datanode_4          | 2023-07-16 05:06:33,951 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-16 05:06:33,971 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:33,985 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-16 05:06:33,996 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-07-16 05:06:34,044 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om_1                | 2023-07-16 05:06:47,323 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
datanode_5          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 2023-07-16 05:06:25,484 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1             | 2023-07-16 05:06:07,467 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-07-16 05:05:20,404 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:05:20,429 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-07-16 05:05:20,436 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-16 05:06:47,505 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 2023-07-16 05:06:25,489 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1             | 2023-07-16 05:06:07,496 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3e7ddb7e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-07-16 05:06:07,498 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@66b3eab0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-07-16 05:06:13,446 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5ecd2979{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-3613739503568187963/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-07-16 05:06:13,467 [main] INFO server.AbstractConnector: Started ServerConnector@4df13b7e{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
om_1                | 2023-07-16 05:06:47,514 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 2023-07-16 05:06:25,489 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 05:06:25,495 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-07-16 05:06:47,583 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-07-16 05:05:20,442 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode_2          | 2023-07-16 05:06:25,504 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-07-16 05:06:25,505 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 05:06:47,594 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-07-16 05:05:20,563 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-07-16 05:05:20,636 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 2023-07-16 05:06:25,583 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-07-16 05:06:33,229 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
datanode_5          | 	... 1 more
datanode_4          | 2023-07-16 05:06:34,185 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-07-16 05:06:34,186 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-07-16 05:06:34,186 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:34,290 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5
datanode_2          | 2023-07-16 05:06:25,592 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:06:47,649 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
datanode_5          | 2023-07-16 05:06:05,606 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.5:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-07-16 05:06:34,292 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-07-16 05:06:34,301 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:34,310 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-07-16 05:06:34,320 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-07-16 05:06:25,651 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:25,658 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-07-16 05:06:33,456 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
datanode_5          | 2023-07-16 05:06:05,644 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 05:06:47,688 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_2          | 2023-07-16 05:06:25,663 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-16 05:06:34,322 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-07-16 05:06:34,343 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-16 05:06:06,645 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 05:06:47,863 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-07-16 05:06:25,719 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:25,719 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:25,729 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:07,646 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om_1                | 2023-07-16 05:06:47,887 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1             | 2023-07-16 05:06:13,469 [main] INFO server.Server: Started @87551ms
recon_1             | 2023-07-16 05:06:13,481 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-07-16 05:06:25,734 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-07-16 05:06:25,736 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
scm_1               | 2023-07-16 05:05:20,638 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-07-16 05:06:33,462 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection:   Response 0: 301767aa-b1e5-4681-b115-67455213d3e0<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t0
datanode_1          | 2023-07-16 05:06:33,467 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
recon_1             | 2023-07-16 05:06:13,481 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-07-16 05:06:13,485 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-07-16 05:06:13,485 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
datanode_5          | 2023-07-16 05:06:08,647 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:25,757 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-44F4E9684BB5,id=333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:25,770 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-07-16 05:06:25,780 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:06:33,467 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-07-16 05:06:33,497 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:09,648 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-07-16 05:06:25,780 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 05:06:33,522 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-07-16 05:06:33,529 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-16 05:06:10,649 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.4:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-07-16 05:06:14,823 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 952facba62e3/172.24.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:48606 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 2023-07-16 05:06:33,753 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
datanode_1          | 2023-07-16 05:06:33,754 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2: ELECTION PASSED received 1 response(s) and 1 exception(s):
datanode_5          | 2023-07-16 05:06:10,651 [EndpointStateMachine task thread for recon/172.24.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 2023-07-16 05:06:33,754 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection:   Response 0: 301767aa-b1e5-4681-b115-67455213d3e0<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t1
om_1                | 2023-07-16 05:06:47,890 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
datanode_5          | java.net.SocketTimeoutException: Call From 4fd9c9bae09d/172.24.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:53560 remote=recon/172.24.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 2023-07-16 05:06:33,754 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
om_1                | 2023-07-16 05:06:47,892 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1             | 2023-07-16 05:06:13,512 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-07-16 05:06:13,523 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-07-16 05:06:13,524 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm_1               | 2023-07-16 05:05:20,715 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498
scm_1               | 2023-07-16 05:05:20,716 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
datanode_1          | 2023-07-16 05:06:33,755 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.LeaderElection: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2 ELECTION round 0: result PASSED
datanode_1          | 2023-07-16 05:06:33,755 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: shutdown 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2
recon_1             | 2023-07-16 05:06:13,524 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-07-16 05:06:13,524 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
datanode_4          | 2023-07-16 05:06:34,343 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 2023-07-16 05:06:33,757 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-07-16 05:06:33,758 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 301767aa-b1e5-4681-b115-67455213d3e0
recon_1             | 2023-07-16 05:06:13,526 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
datanode_4          | 2023-07-16 05:06:34,347 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-07-16 05:06:34,457 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 2023-07-16 05:06:33,758 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: change Leader from null to 301767aa-b1e5-4681-b115-67455213d3e0 at term 1 for becomeLeader, leader elected after 7707ms
datanode_1          | 2023-07-16 05:06:33,759 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1             | 2023-07-16 05:06:16,631 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:bf1223b9-84ee-4a14-b347-7ad6126fbc27 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om_1                | 2023-07-16 05:06:47,892 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-07-16 05:06:47,892 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
datanode_4          | 2023-07-16 05:06:34,473 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:34,621 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om_1                | 2023-07-16 05:06:47,893 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-07-16 05:06:47,893 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
datanode_4          | 2023-07-16 05:06:34,623 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-07-16 05:06:34,627 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om_1                | 2023-07-16 05:06:47,898 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:06:47,900 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
datanode_2          | 2023-07-16 05:06:25,784 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 2023-07-16 05:06:34,684 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-16 05:06:34,684 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-16 05:06:34,699 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
scm_1               | 2023-07-16 05:05:20,717 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:05:20,733 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 2023-07-16 05:06:34,703 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-07-16 05:06:34,710 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState
datanode_4          | 2023-07-16 05:06:34,726 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
scm_1               | 2023-07-16 05:05:20,740 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 05:05:20,741 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 2023-07-16 05:06:34,729 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-16 05:06:34,768 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-44F4E9684BB5,id=131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_4          | 2023-07-16 05:06:34,780 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm_1               | 2023-07-16 05:05:20,757 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 05:05:20,760 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 05:06:47,904 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 2023-07-16 05:06:33,762 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:34,783 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-07-16 05:06:34,786 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-07-16 05:06:34,790 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm_1               | 2023-07-16 05:05:20,761 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-07-16 05:06:47,933 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 2023-07-16 05:06:33,764 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-07-16 05:06:34,919 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91: new RaftServerImpl for group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 05:06:25,785 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 05:06:25,786 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
scm_1               | 2023-07-16 05:05:20,943 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-07-16 05:06:47,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 2023-07-16 05:06:34,924 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:06:25,879 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5
datanode_2          | 2023-07-16 05:06:30,795 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO impl.FollowerState: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058529180ns, electionTimeout:5013ms
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm_1               | 2023-07-16 05:05:20,974 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:06:47,943 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 2023-07-16 05:06:33,764 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 2023-07-16 05:06:30,795 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState
datanode_2          | 2023-07-16 05:06:30,796 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-07-16 05:06:48,992 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 2023-07-16 05:06:33,765 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-07-16 05:06:34,925 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 2023-07-16 05:06:30,806 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-07-16 05:06:30,806 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1
om_1                | 2023-07-16 05:06:48,997 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | 2023-07-16 05:06:33,765 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
datanode_2          | 2023-07-16 05:06:30,823 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:30,848 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 05:06:48,997 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:48606 remote=scm/172.24.0.4:9861]
datanode_1          | 2023-07-16 05:06:33,771 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 2023-07-16 05:06:30,863 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:06:30,861 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 2023-07-16 05:06:33,778 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 2023-07-16 05:06:30,861 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_2          | 2023-07-16 05:06:32,050 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 2023-07-16 05:06:33,966 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 2023-07-16 05:06:32,052 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
datanode_2          | 2023-07-16 05:06:32,053 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t0
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 2023-07-16 05:06:33,976 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.4:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-07-16 05:06:21,473 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 3 pipelines from SCM.
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 2023-07-16 05:06:32,057 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
datanode_2          | 2023-07-16 05:06:32,061 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 PRE_VOTE round 0: result PASSED
om_1                | 2023-07-16 05:06:48,998 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1             | 2023-07-16 05:06:21,477 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-07-16 05:06:21,496 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 from SCM.
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | 2023-07-16 05:06:32,076 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:32,129 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 05:06:48,998 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1             | 2023-07-16 05:06:21,968 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d from SCM.
recon_1             | 2023-07-16 05:06:21,972 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f from SCM.
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:53560 remote=recon/172.24.0.5:9891]
datanode_2          | 2023-07-16 05:06:32,129 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:06:32,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
om_1                | 2023-07-16 05:06:49,001 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
recon_1             | 2023-07-16 05:06:21,977 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-07-16 05:06:21,977 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 2023-07-16 05:06:32,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1: ELECTION PASSED received 1 response(s) and 1 exception(s):
scm_1               | 2023-07-16 05:05:21,225 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-07-16 05:06:49,013 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@319fe034[Not completed]
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1             | 2023-07-16 05:06:21,983 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-07-16 05:06:33,979 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 05:06:34,009 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm_1               | 2023-07-16 05:05:21,231 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 05:05:21,240 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-07-16 05:06:49,013 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
recon_1             | 2023-07-16 05:06:21,984 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
scm_1               | 2023-07-16 05:05:21,326 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:05:21,374 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-07-16 05:06:49,020 [main] INFO om.OzoneManager: Creating RPC Server
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
recon_1             | 2023-07-16 05:06:22,229 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:51630 / 172.24.0.8:51630
recon_1             | 2023-07-16 05:06:22,237 [IPC Server handler 99 on default port 9891] INFO ipc.Server: IPC Server handler 99 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:60004 / 172.24.0.11:60004
datanode_1          | 2023-07-16 05:06:34,019 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm_1               | 2023-07-16 05:05:21,378 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: start as a follower, conf=-1: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:05:21,420 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
recon_1             | 2023-07-16 05:06:23,362 [IPC Server handler 96 on default port 9891] WARN ipc.Server: IPC Server handler 96 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:55568 / 172.24.0.10:55568: output error
datanode_1          | 2023-07-16 05:06:34,024 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 05:06:34,025 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm_1               | 2023-07-16 05:05:21,469 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState
scm_1               | 2023-07-16 05:05:21,513 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 05:06:49,134 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
datanode_3          | 2023-07-16 05:06:17,968 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-92c4f268-78e2-4938-a295-6b59f42ce175/container.db to cache
datanode_4          | 2023-07-16 05:06:34,932 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 2023-07-16 05:06:23,369 [IPC Server handler 95 on default port 9891] WARN ipc.Server: IPC Server handler 95 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:39772 / 172.24.0.15:39772: output error
recon_1             | 2023-07-16 05:06:23,375 [IPC Server handler 85 on default port 9891] WARN ipc.Server: IPC Server handler 85 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:48532 / 172.24.0.12:48532: output error
recon_1             | 2023-07-16 05:06:23,375 [IPC Server handler 86 on default port 9891] WARN ipc.Server: IPC Server handler 86 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:52790 / 172.24.0.10:52790: output error
datanode_1          | 2023-07-16 05:06:34,025 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-07-16 05:06:32,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t1
om_1                | 2023-07-16 05:06:49,191 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-16 05:06:34,934 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:06:17,973 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-92c4f268-78e2-4938-a295-6b59f42ce175/container.db for volume DS-92c4f268-78e2-4938-a295-6b59f42ce175
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm_1               | 2023-07-16 05:05:21,524 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 2023-07-16 05:06:23,375 [IPC Server handler 88 on default port 9891] WARN ipc.Server: IPC Server handler 88 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:37234 / 172.24.0.8:37234: output error
datanode_2          | 2023-07-16 05:06:32,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-44F4E9684BB5 not found.
om_1                | 2023-07-16 05:06:49,195 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-16 05:06:34,935 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:06:18,018 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm_1               | 2023-07-16 05:05:21,527 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-757BF1EEF498,id=bf1223b9-84ee-4a14-b347-7ad6126fbc27
recon_1             | 2023-07-16 05:06:23,375 [IPC Server handler 89 on default port 9891] WARN ipc.Server: IPC Server handler 89 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:52786 / 172.24.0.10:52786: output error
datanode_4          | 2023-07-16 05:06:34,935 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-07-16 05:06:34,936 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: ConfigurationManager, init=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-16 05:06:34,936 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-16 05:06:34,939 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-07-16 05:06:34,943 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-16 05:06:34,945 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-07-16 05:06:34,955 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 05:05:21,534 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-07-16 05:06:32,203 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1 ELECTION round 0: result PASSED
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om_1                | 2023-07-16 05:06:49,195 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-07-16 05:06:34,026 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 2023-07-16 05:06:23,380 [IPC Server handler 97 on default port 9891] WARN ipc.Server: IPC Server handler 97 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:39782 / 172.24.0.15:39782: output error
scm_1               | 2023-07-16 05:05:21,550 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
datanode_3          | 2023-07-16 05:06:18,044 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4          | 2023-07-16 05:06:34,955 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-07-16 05:06:32,203 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om_1                | 2023-07-16 05:06:49,195 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
datanode_1          | 2023-07-16 05:06:34,027 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 2023-07-16 05:06:23,380 [IPC Server handler 94 on default port 9891] WARN ipc.Server: IPC Server handler 94 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:48518 / 172.24.0.12:48518: output error
scm_1               | 2023-07-16 05:05:21,553 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
datanode_3          | 2023-07-16 05:06:18,435 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4          | 2023-07-16 05:06:34,962 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 05:06:32,215 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om_1                | 2023-07-16 05:06:49,195 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-07-16 05:06:34,040 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1             | 2023-07-16 05:06:23,381 [IPC Server handler 90 on default port 9891] WARN ipc.Server: IPC Server handler 90 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:37232 / 172.24.0.8:37232: output error
scm_1               | 2023-07-16 05:05:21,553 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:06:18,435 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_4          | 2023-07-16 05:06:34,964 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 05:06:32,216 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om_1                | 2023-07-16 05:06:49,195 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-07-16 05:06:34,041 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-16 05:06:23,390 [IPC Server handler 96 on default port 9891] INFO ipc.Server: IPC Server handler 96 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
datanode_3          | 2023-07-16 05:06:18,575 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start RPC server
datanode_4          | 2023-07-16 05:06:34,970 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:06:32,228 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 1 for becomeLeader, leader elected after 7758ms
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
om_1                | 2023-07-16 05:06:49,225 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 05:05:21,621 [main] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start RPC server
scm_1               | 2023-07-16 05:05:23,081 [main] INFO server.GrpcService: bf1223b9-84ee-4a14-b347-7ad6126fbc27: GrpcService started, listening on 9894
scm_1               | 2023-07-16 05:05:23,159 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-bf1223b9-84ee-4a14-b347-7ad6126fbc27: Started
scm_1               | 2023-07-16 05:05:26,703 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.FollowerState: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5234929161ns, electionTimeout:5177ms
scm_1               | 2023-07-16 05:05:26,705 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState
datanode_2          | 2023-07-16 05:06:32,355 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-07-16 05:06:32,418 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:06:32,433 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm_1               | 2023-07-16 05:05:26,705 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-07-16 05:05:26,712 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-16 05:05:26,712 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1
scm_1               | 2023-07-16 05:05:26,724 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:05:26,725 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-16 05:06:32,480 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 05:06:32,489 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 05:06:32,497 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:06:32,584 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3          | 2023-07-16 05:06:18,600 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: GrpcService started, listening on 9858
om_1                | 2023-07-16 05:06:49,226 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 05:06:49,254 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3          | 2023-07-16 05:06:18,603 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: GrpcService started, listening on 9856
datanode_2          | 2023-07-16 05:06:32,623 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_4          | 2023-07-16 05:06:34,970 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-07-16 05:06:34,982 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-07-16 05:06:34,982 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:06:32,802 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om_1                | 2023-07-16 05:06:49,255 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:06:18,606 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: GrpcService started, listening on 9857
datanode_4          | 2023-07-16 05:06:34,996 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-07-16 05:06:35,031 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-07-16 05:06:35,039 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:32,802 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:32,803 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om_1                | 2023-07-16 05:06:49,313 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 2023-07-16 05:06:18,669 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 is started using port 9858 for RATIS
scm_1               | 2023-07-16 05:05:26,739 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:05:26,740 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-16 05:05:26,744 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1
datanode_2          | 2023-07-16 05:06:32,808 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:06:32,820 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om_1                | 2023-07-16 05:06:49,340 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-07-16 05:06:15,667 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | 2023-07-16 05:06:18,674 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-07-16 05:06:35,067 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 1 for appendEntries, leader elected after 2043ms
datanode_4          | 2023-07-16 05:06:35,074 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 does not exist. Creating ...
datanode_4          | 2023-07-16 05:06:35,092 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/in_use.lock acquired by nodename 7@37e9a0ff2c7a
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_2          | 2023-07-16 05:06:32,848 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:06:32,848 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | java.net.SocketTimeoutException: Call From 4fd9c9bae09d/172.24.0.15 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:57376 remote=scm/172.24.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 2023-07-16 05:06:18,674 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 is started using port 9856 for RATIS_SERVER
scm_1               | 2023-07-16 05:05:26,745 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-16 05:05:26,745 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: change Leader from null to bf1223b9-84ee-4a14-b347-7ad6126fbc27 at term 1 for becomeLeader, leader elected after 9635ms
scm_1               | 2023-07-16 05:05:26,793 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
om_1                | 2023-07-16 05:06:49,361 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 2023-07-16 05:06:18,674 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: Started
datanode_4          | 2023-07-16 05:06:35,107 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 has been successfully formatted.
scm_1               | 2023-07-16 05:05:26,820 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:05:26,821 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 2023-07-16 05:06:18,792 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-16 05:06:35,115 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO ratis.ContainerStateMachine: group-8C72950665A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-16 05:06:35,116 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-16 05:06:35,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_3          | 2023-07-16 05:06:22,285 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_4          | 2023-07-16 05:06:35,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:35,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-16 05:06:35,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_4          | 2023-07-16 05:06:35,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-07-16 05:06:34,042 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-07-16 05:06:34,044 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 90 on default port 9891] INFO ipc.Server: IPC Server handler 90 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_4          | 2023-07-16 05:06:35,182 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-07-16 05:06:34,044 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm_1               | 2023-07-16 05:05:26,827 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-07-16 05:06:49,362 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_4          | 2023-07-16 05:06:35,183 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-07-16 05:06:35,183 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:05:26,864 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 05:06:32,849 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Failed requestVote 301767aa-b1e5-4681-b115-67455213d3e0->333b8eb3-808f-40e6-a1dc-b9ce55f887b5#0
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
om_1                | 2023-07-16 05:06:49,629 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm_1               | 2023-07-16 05:05:26,865 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om_1                | 2023-07-16 05:06:49,805 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_1          | 2023-07-16 05:06:34,046 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-07-16 05:06:34,047 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-07-16 05:06:34,047 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_1          | 2023-07-16 05:06:34,047 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:06:34,047 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-07-16 05:06:34,063 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderStateImpl
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_1          | 2023-07-16 05:06:34,065 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-07-16 05:06:49,816 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_1          | 2023-07-16 05:06:34,071 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0
om_1                | 2023-07-16 05:06:49,818 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-07-16 05:06:49,818 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:05:26,929 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:05:27,011 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 05:05:27,122 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderStateImpl
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
om_1                | 2023-07-16 05:06:49,822 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_1          | 2023-07-16 05:06:34,117 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderElection2] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_1          | 2023-07-16 05:06:35,020 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15.
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm_1               | 2023-07-16 05:05:27,638 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_1          | 2023-07-16 05:06:35,765 [grpc-default-executor-1] WARN server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: The server role is not yet initialized.
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1               | 2023-07-16 05:05:28,536 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: set configuration 0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:05:28,803 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/log_inprogress_0
scm_1               | 2023-07-16 05:05:29,183 [main] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: close
datanode_1          | 2023-07-16 05:06:35,775 [grpc-default-executor-1] INFO leader.FollowerInfo: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
om_1                | 2023-07-16 05:06:49,823 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-07-16 05:06:50,980 [main] INFO reflections.Reflections: Reflections took 1545 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 2023-07-16 05:06:35,184 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4
datanode_4          | 2023-07-16 05:06:35,187 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-07-16 05:06:35,191 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-07-16 05:06:43,065 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderStateImpl] INFO impl.TransferLeadership: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: start transferring leadership to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
om_1                | 2023-07-16 05:06:51,778 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 05:06:51,803 [main] INFO ipc.Server: Listener at om:9862
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-07-16 05:06:23,650 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: addNew group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-8C72950665A4:java.util.concurrent.CompletableFuture@442716a3[Not completed]
datanode_3          | 2023-07-16 05:06:23,797 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: new RaftServerImpl for group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-07-16 05:06:43,065 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderStateImpl] INFO impl.TransferLeadership: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: sendStartLeaderElection to follower 333b8eb3-808f-40e6-a1dc-b9ce55f887b5, lastEntry=(t:1, i:0)
datanode_2          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_2          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_3          | 2023-07-16 05:06:23,808 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-07-16 05:06:23,809 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-07-16 05:06:23,813 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 05:06:23,814 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-07-16 05:06:43,068 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderStateImpl] INFO impl.TransferLeadership: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: SUCCESS sent StartLeaderElection to transferee 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 immediately as it already has up-to-date log
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_3          | 2023-07-16 05:06:23,819 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:06:23,819 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 2023-07-16 05:06:35,191 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_1          | 2023-07-16 05:06:43,245 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO impl.TransferLeadership: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: Received startLeaderElection reply from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: success? true
om_1                | 2023-07-16 05:06:51,807 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-07-16 05:06:54,808 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-07-16 05:05:29,197 [main] INFO server.GrpcService: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown server GrpcServerProtocolService now
scm_1               | 2023-07-16 05:05:29,197 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: shutdown
scm_1               | 2023-07-16 05:05:29,199 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-757BF1EEF498,id=bf1223b9-84ee-4a14-b347-7ad6126fbc27
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1          | 2023-07-16 05:06:44,162 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: change Leader from 301767aa-b1e5-4681-b115-67455213d3e0 to null at term 2 for updateCurrentTerm
om_1                | 2023-07-16 05:06:54,838 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-07-16 05:06:54,838 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm_1               | 2023-07-16 05:05:29,199 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderStateImpl
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 2023-07-16 05:06:35,191 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-07-16 05:06:35,192 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-07-16 05:06:55,198 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.13:9862
om_1                | 2023-07-16 05:06:55,202 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
scm_1               | 2023-07-16 05:05:29,304 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO impl.PendingRequests: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-07-16 05:05:29,392 [main] INFO server.GrpcService: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown server GrpcServerProtocolService successfully
datanode_4          | 2023-07-16 05:06:35,192 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-07-16 05:06:35,193 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | 2023-07-16 05:06:55,210 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm_1               | 2023-07-16 05:05:29,443 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO impl.StateMachineUpdater: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-07-16 05:05:29,461 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO impl.StateMachineUpdater: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater: Took a snapshot at index 0
datanode_4          | 2023-07-16 05:06:35,193 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 94 on default port 9891] INFO ipc.Server: IPC Server handler 94 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
datanode_1          | 2023-07-16 05:06:44,164 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: changes role from    LEADER to FOLLOWER at term 2 for appendEntries
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
om_1                | 2023-07-16 05:06:55,234 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@c3b9c2e51263
scm_1               | 2023-07-16 05:05:29,475 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO impl.StateMachineUpdater: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-07-16 05:05:29,569 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: closes. applyIndex: 0
scm_1               | 2023-07-16 05:05:29,826 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker close()
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_1          | 2023-07-16 05:06:44,164 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: shutdown 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-LeaderStateImpl
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_5          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
om_1                | 2023-07-16 05:06:55,260 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
scm_1               | 2023-07-16 05:05:29,832 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-bf1223b9-84ee-4a14-b347-7ad6126fbc27: Stopped
scm_1               | 2023-07-16 05:05:29,832 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:05:29,844 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-46f5dd7f-dd64-4591-bc47-757bf1eef498; layoutVersion=7; scmId=bf1223b9-84ee-4a14-b347-7ad6126fbc27
scm_1               | 2023-07-16 05:05:30,207 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
datanode_1          | 2023-07-16 05:06:44,164 [grpc-default-executor-2] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: receive requestVote(ELECTION, 333b8eb3-808f-40e6-a1dc-b9ce55f887b5, group-C9411BC9FD15, 2, (t:1, i:0))
datanode_1          | 2023-07-16 05:06:44,165 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO impl.PendingRequests: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-PendingRequests: sendNotLeaderResponses
datanode_1          | 2023-07-16 05:06:44,167 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO impl.RoleInfo: 301767aa-b1e5-4681-b115-67455213d3e0: start 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FollowerState
datanode_1          | 2023-07-16 05:06:44,165 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1          | 2023-07-16 05:06:44,165 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->11d6e79b-1aa6-4571-843b-2e54970fb159-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->11d6e79b-1aa6-4571-843b-2e54970fb159-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1          | 2023-07-16 05:06:44,204 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 2023-07-16 05:06:35,199 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-07-16 05:06:35,207 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:35,800 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-131a1d93-dd5a-481f-884e-0521d7ee6f91: Detected pause in JVM or host machine approximately 0.261s with 0.543s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=56ms
datanode_2          | 2023-07-16 05:06:32,851 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-07-16 05:06:32,852 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:06:32,852 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:06:32,857 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om_1                | 2023-07-16 05:06:55,266 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-07-16 05:06:55,290 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-07-16 05:06:55,292 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:06:55,297 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:57376 remote=scm/172.24.0.4:9861]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=487ms
om_1                | 2023-07-16 05:06:55,300 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-16 05:06:55,318 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 05:06:55,340 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-07-16 05:06:55,342 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 5a10f63682a1/172.24.0.4
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | 2023-07-16 05:06:35,837 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-07-16 05:06:35,838 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-07-16 05:06:55,343 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-07-16 05:06:55,362 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-07-16 05:06:55,363 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-07-16 05:06:55,363 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 2023-07-16 05:06:35,838 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-07-16 05:06:35,842 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-07-16 05:06:44,204 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 2 for appendEntries, leader elected after 41ms
datanode_1          | 2023-07-16 05:06:44,251 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:06:44,253 [grpc-default-executor-2] INFO impl.VoteContext: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-FOLLOWER: reject ELECTION from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: this server is a follower and still has a valid leader 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_3          | 2023-07-16 05:06:23,881 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: ConfigurationManager, init=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om_1                | 2023-07-16 05:06:55,368 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-07-16 05:06:55,368 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-07-16 05:06:55,373 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-07-16 05:06:55,375 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:06:23,889 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1               | 2023-07-16 05:05:49,745 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
om_1                | 2023-07-16 05:06:55,381 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-07-16 05:06:55,381 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-07-16 05:06:44,261 [grpc-default-executor-2] INFO server.RaftServer$Division: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15 replies to ELECTION vote request: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-301767aa-b1e5-4681-b115-67455213d3e0#0:FAIL-t2. Peer's state: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15:t2, leader=333b8eb3-808f-40e6-a1dc-b9ce55f887b5, voted=null, raftlog=Memoized:301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLog:OPENED:c0, conf=1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:06:44,269 [301767aa-b1e5-4681-b115-67455213d3e0-server-thread1] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
datanode_4          | 2023-07-16 05:06:35,851 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-16 05:06:35,890 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: start as a follower, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-07-16 05:06:55,411 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | STARTUP_MSG:   host = 5a10f63682a1/172.24.0.4
scm_1               | STARTUP_MSG:   args = []
datanode_1          | 2023-07-16 05:06:44,382 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0 to /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_0-0
datanode_4          | 2023-07-16 05:06:35,892 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-07-16 05:06:35,892 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState
om_1                | 2023-07-16 05:06:55,412 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
datanode_1          | 2023-07-16 05:06:44,270 [grpc-default-executor-4] INFO server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_4          | 2023-07-16 05:06:35,905 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C72950665A4,id=131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_4          | 2023-07-16 05:06:35,906 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 05:06:55,476 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/7a19afa71401e356cd86a99648e94278ea2850c8 ; compiled by 'runner' on 2023-07-16T03:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-07-16 05:06:44,270 [grpc-default-executor-3] INFO server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5-AppendLogResponseHandler: follower responses appendEntries COMPLETED
om_1                | 2023-07-16 05:06:55,478 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-07-16 05:06:35,906 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:06:23,905 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:06:23,920 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:06:24,047 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om_1                | 2023-07-16 05:06:55,479 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
datanode_4          | 2023-07-16 05:06:35,907 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:06:24,145 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-16 05:06:24,174 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:06:24,180 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 05:05:49,831 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-07-16 05:05:51,006 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:06:32,858 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-16 05:06:55,497 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-07-16 05:06:35,907 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-16 05:06:35,910 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-16 05:06:35,946 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-07-16 05:06:36,231 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Failed APPEND_ENTRIES request 333b8eb3-808f-40e6-a1dc-b9ce55f887b5->131a1d93-dd5a-481f-884e-0521d7ee6f91#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_1          | 2023-07-16 05:06:44,447 [301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_1
scm_1               | 2023-07-16 05:05:52,838 [main] INFO reflections.Reflections: Reflections took 1220 ms to scan 3 urls, producing 132 keys and 288 values 
datanode_2          | 2023-07-16 05:06:32,871 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderStateImpl
om_1                | 2023-07-16 05:06:55,497 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | address: "172.24.0.12:9856"
datanode_4          | dataStreamAddress: "172.24.0.12:9858"
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-07-16 05:06:44,434 [grpc-default-executor-4] INFO leader.FollowerInfo: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->333b8eb3-808f-40e6-a1dc-b9ce55f887b5: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
scm_1               | 2023-07-16 05:05:54,451 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_2          | 2023-07-16 05:06:33,055 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 05:06:33,310 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-07-16 05:06:44,974 [grpc-default-executor-0] INFO server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->11d6e79b-1aa6-4571-843b-2e54970fb159-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm_1               | 2023-07-16 05:05:54,537 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-07-16 05:06:01,117 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-07-16 05:06:01,790 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_1          | 2023-07-16 05:06:44,974 [grpc-default-executor-0] INFO leader.FollowerInfo: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->11d6e79b-1aa6-4571-843b-2e54970fb159: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_1          | 2023-07-16 05:06:44,977 [grpc-default-executor-0] INFO server.GrpcLogAppender: 301767aa-b1e5-4681-b115-67455213d3e0@group-C9411BC9FD15->11d6e79b-1aa6-4571-843b-2e54970fb159-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm_1               | 2023-07-16 05:06:02,927 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-07-16 05:06:02,930 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-07-16 05:06:03,282 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-07-16 05:06:24,357 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 05:06:24,522 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-07-16 05:06:24,544 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-07-16 05:07:19,054 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:03,643 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:bf1223b9-84ee-4a14-b347-7ad6126fbc27
scm_1               | 2023-07-16 05:06:03,810 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-07-16 05:06:03,830 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-07-16 05:06:24,548 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:06:24,551 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 05:06:24,555 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-07-16 05:08:19,054 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:03,832 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
datanode_2          | 2023-07-16 05:06:33,710 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Failed requestVote 301767aa-b1e5-4681-b115-67455213d3e0->333b8eb3-808f-40e6-a1dc-b9ce55f887b5#0
datanode_2          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: group-C9411BC9FD15 not found.
datanode_3          | 2023-07-16 05:06:24,558 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:06:24,561 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 does not exist. Creating ...
datanode_3          | 2023-07-16 05:06:24,614 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/in_use.lock acquired by nodename 7@952facba62e3
datanode_1          | 2023-07-16 05:09:19,055 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:03,836 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_3          | 2023-07-16 05:06:24,663 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 has been successfully formatted.
datanode_3          | 2023-07-16 05:06:24,722 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO ratis.ContainerStateMachine: group-8C72950665A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:06:24,779 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-07-16 05:10:19,056 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:03,837 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-07-16 05:06:03,838 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-07-16 05:06:55,503 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:18,398 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-d3d852aa-161f-4153-b509-ada62c2fc95d/container.db to cache
datanode_5          | 2023-07-16 05:06:18,398 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-46f5dd7f-dd64-4591-bc47-757bf1eef498/DS-d3d852aa-161f-4153-b509-ada62c2fc95d/container.db for volume DS-d3d852aa-161f-4153-b509-ada62c2fc95d
datanode_5          | 2023-07-16 05:06:18,410 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-07-16 05:11:19,056 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | clientAddress: "172.24.0.12:9858"
scm_1               | 2023-07-16 05:06:03,838 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-07-16 05:06:03,839 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om_1                | 2023-07-16 05:06:55,509 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-16 05:06:18,428 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-07-16 05:06:03,841 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:03,846 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-07-16 05:06:55,514 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-07-16 05:06:55,522 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-07-16 05:06:55,523 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-07-16 05:06:55,524 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-07-16 05:12:19,057 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | adminAddress: "172.24.0.12:9857"
datanode_4          | startupRole: FOLLOWER
scm_1               | 2023-07-16 05:06:03,847 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-07-16 05:06:03,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-07-16 05:06:55,525 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-07-16 05:06:18,847 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5          | 2023-07-16 05:06:18,852 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_1          | 2023-07-16 05:13:19,057 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | ,id: "f02a32cc-4f60-4ab6-a6b0-1a6b831becc3"
datanode_3          | 2023-07-16 05:06:24,893 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-07-16 05:06:03,874 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-07-16 05:06:03,877 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-07-16 05:06:55,526 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-07-16 05:06:55,526 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-07-16 05:06:55,535 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-07-16 05:06:55,559 [main] INFO server.RaftServer: om1: start RPC server
datanode_1          | 2023-07-16 05:14:19,058 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | address: "172.24.0.8:9856"
datanode_3          | 2023-07-16 05:06:24,901 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:04,252 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-07-16 05:06:04,256 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-07-16 05:06:55,688 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-07-16 05:06:55,704 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-07-16 05:06:55,712 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-07-16 05:06:55,854 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm_1               | 2023-07-16 05:06:04,263 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1          | 2023-07-16 05:15:19,058 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-07-16 05:06:55,854 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-07-16 05:06:55,971 [main] INFO util.log: Logging initialized @35858ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-07-16 05:06:56,485 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-16 05:06:04,264 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_5          | 2023-07-16 05:06:18,989 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.RaftServer: 11d6e79b-1aa6-4571-843b-2e54970fb159: start RPC server
datanode_5          | 2023-07-16 05:06:19,004 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 11d6e79b-1aa6-4571-843b-2e54970fb159: GrpcService started, listening on 9858
datanode_5          | 2023-07-16 05:06:19,007 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 11d6e79b-1aa6-4571-843b-2e54970fb159: GrpcService started, listening on 9856
datanode_5          | 2023-07-16 05:06:19,009 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO server.GrpcService: 11d6e79b-1aa6-4571-843b-2e54970fb159: GrpcService started, listening on 9857
om_1                | 2023-07-16 05:06:56,493 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm_1               | 2023-07-16 05:06:04,264 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:06:04,269 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_5          | 2023-07-16 05:06:19,022 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 11d6e79b-1aa6-4571-843b-2e54970fb159 is started using port 9858 for RATIS
datanode_5          | 2023-07-16 05:06:19,023 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 11d6e79b-1aa6-4571-843b-2e54970fb159 is started using port 9857 for RATIS_ADMIN
datanode_5          | 2023-07-16 05:06:19,023 [EndpointStateMachine task thread for scm/172.24.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 11d6e79b-1aa6-4571-843b-2e54970fb159 is started using port 9856 for RATIS_SERVER
datanode_5          | 2023-07-16 05:06:19,024 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-11d6e79b-1aa6-4571-843b-2e54970fb159: Started
om_1                | 2023-07-16 05:06:56,510 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-07-16 05:06:04,286 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: found a subdirectory /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498
scm_1               | 2023-07-16 05:06:04,294 [main] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: addNew group-757BF1EEF498:[] returns group-757BF1EEF498:java.util.concurrent.CompletableFuture@2d913116[Not completed]
datanode_2          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_5          | 2023-07-16 05:06:19,082 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:06:21,264 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_5          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_4          | dataStreamAddress: "172.24.0.8:9858"
om_1                | 2023-07-16 05:06:56,516 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm_1               | 2023-07-16 05:06:04,360 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27: new RaftServerImpl for group-757BF1EEF498:[] with SCMStateMachine:uninitialized
datanode_2          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 2023-07-16 05:06:24,905 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 05:06:24,911 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-07-16 05:06:56,518 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-07-16 05:06:04,377 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 2023-07-16 05:06:24,954 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | clientAddress: "172.24.0.8:9858"
om_1                | 2023-07-16 05:06:56,518 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-07-16 05:06:04,377 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 85 on default port 9891] INFO ipc.Server: IPC Server handler 85 on default port 9891 caught an exception
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 2023-07-16 05:06:24,987 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | adminAddress: "172.24.0.8:9857"
om_1                | 2023-07-16 05:06:56,698 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm_1               | 2023-07-16 05:06:04,377 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
recon_1             | java.nio.channels.ClosedChannelException
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 2023-07-16 05:06:25,013 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | startupRole: FOLLOWER
om_1                | 2023-07-16 05:06:56,726 [main] INFO http.HttpServer2: Jetty bound to port 9874
scm_1               | 2023-07-16 05:06:04,378 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 2023-07-16 05:06:25,016 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | ,id: "131a1d93-dd5a-481f-884e-0521d7ee6f91"
scm_1               | 2023-07-16 05:06:04,384 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om_1                | 2023-07-16 05:06:56,730 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-07-16 05:06:25,059 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4
datanode_4          | address: "172.24.0.10:9856"
scm_1               | 2023-07-16 05:06:04,384 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
om_1                | 2023-07-16 05:06:56,888 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-07-16 05:06:25,072 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | priority: 1
scm_1               | 2023-07-16 05:06:04,408 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
om_1                | 2023-07-16 05:06:56,888 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-07-16 05:06:25,075 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | dataStreamAddress: "172.24.0.10:9858"
scm_1               | 2023-07-16 05:06:04,408 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
om_1                | 2023-07-16 05:06:56,897 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-07-16 05:06:25,096 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | clientAddress: "172.24.0.10:9858"
scm_1               | 2023-07-16 05:06:04,428 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_5          | Caused by: java.util.concurrent.TimeoutException
om_1                | 2023-07-16 05:06:56,943 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@d964a3d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-07-16 05:06:56,949 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1172a648{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | adminAddress: "172.24.0.10:9857"
scm_1               | 2023-07-16 05:06:04,443 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-07-16 05:06:33,991 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: new RaftServerImpl for group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om_1                | 2023-07-16 05:06:57,398 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7859c88f{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-8244566005141347860/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
datanode_3          | 2023-07-16 05:06:25,142 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | startupRole: FOLLOWER
scm_1               | 2023-07-16 05:06:04,508 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2          | 2023-07-16 05:06:33,995 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 2023-07-16 05:06:25,142 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | , old:)
scm_1               | 2023-07-16 05:06:04,523 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-07-16 05:06:33,995 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 2023-07-16 05:06:25,164 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: The server role is not yet initialized.
scm_1               | 2023-07-16 05:06:04,533 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
datanode_2          | 2023-07-16 05:06:33,996 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om_1                | 2023-07-16 05:06:57,439 [main] INFO server.AbstractConnector: Started ServerConnector@556746e2{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
datanode_5          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 2023-07-16 05:06:25,180 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:121)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitServerRequestAsync$11(RaftServerImpl.java:825)
datanode_2          | 2023-07-16 05:06:33,997 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om_1                | 2023-07-16 05:06:57,440 [main] INFO server.Server: Started @37327ms
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 2023-07-16 05:06:25,185 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm_1               | 2023-07-16 05:06:04,535 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-07-16 05:06:34,000 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om_1                | 2023-07-16 05:06:57,452 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 2023-07-16 05:06:25,282 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1               | 2023-07-16 05:06:04,629 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-07-16 05:06:34,000 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 2023-07-16 05:06:57,452 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 2023-07-16 05:06:25,284 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | 2023-07-16 05:06:34,000 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 2023-07-16 05:06:57,454 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
datanode_5          | 	... 1 more
datanode_3          | 2023-07-16 05:06:25,406 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:06:25,420 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | Caused by: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: The server role is not yet initialized.
datanode_2          | 2023-07-16 05:06:34,001 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_5          | 2023-07-16 05:06:23,266 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | 2023-07-16 05:06:25,424 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1               | 2023-07-16 05:06:05,129 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerImpl.preAppendEntriesAsync(RaftServerImpl.java:1451)
datanode_2          | 2023-07-16 05:06:34,004 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 86 on default port 9891] INFO ipc.Server: IPC Server handler 86 on default port 9891 caught an exception
om_1                | 2023-07-16 05:06:57,456 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 2023-07-16 05:06:25,477 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:06:25,477 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:06:05,154 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:06:34,004 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-07-16 05:06:57,547 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 2023-07-16 05:06:25,509 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: start as a follower, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:25,526 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerImpl.appendEntriesAsync(RaftServerImpl.java:1393)
datanode_2          | 2023-07-16 05:06:34,006 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om_1                | 2023-07-16 05:06:57,837 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-07-16 05:06:58,438 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm_1               | 2023-07-16 05:06:05,155 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 05:06:05,155 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:06:05,155 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 2023-07-16 05:06:34,006 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-16 05:06:05,156 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 05:06:05,174 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-07-16 05:06:05,180 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
om_1                | 2023-07-16 05:07:00,687 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5174072028ns, electionTimeout:5162ms
datanode_2          | 2023-07-16 05:06:34,007 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
om_1                | 2023-07-16 05:07:00,689 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
datanode_2          | 2023-07-16 05:06:34,010 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-07-16 05:07:00,690 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_2          | 2023-07-16 05:06:34,010 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-07-16 05:07:00,693 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_2          | 2023-07-16 05:06:34,039 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_3          | 2023-07-16 05:06:25,528 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState
datanode_3          | 2023-07-16 05:06:25,552 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C72950665A4,id=f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_3          | 2023-07-16 05:06:25,559 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-07-16 05:07:00,693 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
datanode_2          | 2023-07-16 05:06:34,056 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_4          | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$25(RaftServerProxy.java:637)
datanode_4          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
om_1                | 2023-07-16 05:07:00,697 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:34,068 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:06:25,559 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:06:25,587 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_4          | 	... 5 more
datanode_4          | 2023-07-16 05:06:37,306 [grpc-default-executor-1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: receive requestVote(PRE_VOTE, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, group-8C72950665A4, 0, (t:0, i:0))
om_1                | 2023-07-16 05:07:00,698 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-07-16 05:07:00,701 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 2023-07-16 05:06:34,046 [grpc-default-executor-1] INFO server.RaftServer: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: addNew group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-C9411BC9FD15:java.util.concurrent.CompletableFuture@7bad73ed[Not completed]
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3          | 2023-07-16 05:06:25,587 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:06:25,591 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-07-16 05:06:34,074 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-07-16 05:06:05,180 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
datanode_4          | 2023-07-16 05:06:37,324 [grpc-default-executor-1] INFO impl.VoteContext: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FOLLOWER: accept PRE_VOTE from f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: our priority 0 <= candidate's priority 1
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_3          | 2023-07-16 05:06:25,604 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.util.concurrent.TimeoutException
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 88 on default port 9891] INFO ipc.Server: IPC Server handler 88 on default port 9891 caught an exception
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1               | 2023-07-16 05:06:05,358 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-07-16 05:06:34,075 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-07-16 05:06:34,075 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_4          | 2023-07-16 05:06:37,338 [grpc-default-executor-1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4 replies to PRE_VOTE vote request: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-131a1d93-dd5a-481f-884e-0521d7ee6f91#0:OK-t0. Peer's state: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4:t0, leader=null, voted=, raftlog=Memoized:131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:05,484 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
datanode_2          | 2023-07-16 05:06:34,076 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 does not exist. Creating ...
om_1                | 2023-07-16 05:07:00,701 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-07-16 05:06:25,695 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_4          | 2023-07-16 05:06:37,358 [grpc-default-executor-0] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: receive requestVote(ELECTION, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, group-8C72950665A4, 1, (t:0, i:0))
scm_1               | 2023-07-16 05:06:05,487 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
datanode_2          | 2023-07-16 05:06:34,080 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/in_use.lock acquired by nodename 7@4caf8da3e4ee
om_1                | 2023-07-16 05:07:00,701 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_3          | 2023-07-16 05:06:29,841 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: new RaftServerImpl for group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm_1               | 2023-07-16 05:06:05,580 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-07-16 05:06:05,583 [main] INFO ha.SequenceIdGenerator: upgrade rootCertificateId to 1
om_1                | 2023-07-16 05:07:00,702 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 	... 1 more
datanode_4          | 2023-07-16 05:06:37,364 [grpc-default-executor-0] INFO impl.VoteContext: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FOLLOWER: accept ELECTION from f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: our priority 0 <= candidate's priority 1
datanode_4          | 2023-07-16 05:06:37,364 [grpc-default-executor-0] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_3          | 2023-07-16 05:06:29,843 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1               | 2023-07-16 05:06:05,589 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
datanode_2          | 2023-07-16 05:06:34,084 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 has been successfully formatted.
om_1                | 2023-07-16 05:07:00,710 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 11389ms
datanode_5          | 2023-07-16 05:06:30,166 [grpc-default-executor-1] INFO server.RaftServer: 11d6e79b-1aa6-4571-843b-2e54970fb159: addNew group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-C9411BC9FD15:java.util.concurrent.CompletableFuture@46d28a5f[Not completed]
datanode_4          | 2023-07-16 05:06:37,365 [grpc-default-executor-0] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: shutdown 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState
datanode_4          | 2023-07-16 05:06:37,365 [grpc-default-executor-0] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState
datanode_3          | 2023-07-16 05:06:29,844 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_2          | 2023-07-16 05:06:34,101 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO ratis.ContainerStateMachine: group-C9411BC9FD15: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
om_1                | 2023-07-16 05:07:00,719 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-07-16 05:06:30,166 [grpc-default-executor-0] INFO server.RaftServer: 11d6e79b-1aa6-4571-843b-2e54970fb159: addNew group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-8C72950665A4:java.util.concurrent.CompletableFuture@4c35cb2[Not completed]
om_1                | 2023-07-16 05:07:00,725 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-07-16 05:07:00,725 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:06:05,830 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-07-16 05:06:05,887 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
datanode_2          | 2023-07-16 05:06:34,102 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-07-16 05:06:37,365 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState] INFO impl.FollowerState: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-FollowerState was interrupted
datanode_5          | 2023-07-16 05:06:30,331 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159: new RaftServerImpl for group-C9411BC9FD15:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om_1                | 2023-07-16 05:07:00,731 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_3          | 2023-07-16 05:06:29,845 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-07-16 05:06:05,904 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
datanode_2          | 2023-07-16 05:06:34,108 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-16 05:06:37,387 [grpc-default-executor-0] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4 replies to ELECTION vote request: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-131a1d93-dd5a-481f-884e-0521d7ee6f91#0:OK-t1. Peer's state: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4:t1, leader=null, voted=f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, raftlog=Memoized:131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:30,348 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-07-16 05:07:00,732 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_3          | 2023-07-16 05:06:29,846 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm_1               | 2023-07-16 05:06:05,933 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
datanode_2          | 2023-07-16 05:06:34,109 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:37,526 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:30,351 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-07-16 05:07:00,732 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_3          | 2023-07-16 05:06:29,847 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:06:05,980 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
datanode_2          | 2023-07-16 05:06:34,109 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-16 05:06:37,552 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-16 05:06:30,352 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-07-16 05:07:00,741 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_3          | 2023-07-16 05:06:29,847 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 05:06:05,980 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
datanode_2          | 2023-07-16 05:06:34,110 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:06:34,111 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:30,352 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
om_1                | 2023-07-16 05:07:00,742 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_3          | 2023-07-16 05:06:29,848 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-07-16 05:06:06,012 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
datanode_4          | 2023-07-16 05:06:37,797 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0
datanode_4          | 2023-07-16 05:06:37,963 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C72950665A4 with new leaderId: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_5          | 2023-07-16 05:06:30,354 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-07-16 05:07:00,745 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_3          | 2023-07-16 05:06:29,849 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1               | 2023-07-16 05:06:06,012 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
datanode_4          | 2023-07-16 05:06:37,964 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: change Leader from null to f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 at term 1 for appendEntries, leader elected after 3018ms
datanode_4          | 2023-07-16 05:06:38,002 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4: set configuration 0: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:30,354 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-07-16 05:07:00,767 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:29,850 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-07-16 05:06:38,002 [131a1d93-dd5a-481f-884e-0521d7ee6f91-server-thread2] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-07-16 05:06:38,004 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-8C72950665A4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/current/log_inprogress_0
datanode_5          | 2023-07-16 05:06:30,388 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-07-16 05:07:00,805 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-07-16 05:06:06,020 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
datanode_3          | 2023-07-16 05:06:29,852 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:06:29,853 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:06:29,854 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-07-16 05:06:30,397 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-07-16 05:07:00,902 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-16 05:06:06,029 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
datanode_2          | 2023-07-16 05:06:34,118 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 05:06:34,118 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-07-16 05:06:29,854 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-07-16 05:06:30,422 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-07-16 05:07:01,034 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-16 05:06:06,042 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
datanode_3          | 2023-07-16 05:06:29,855 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-16 05:06:30,427 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | [id: "om1"
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 05:06:06,052 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
datanode_3          | 2023-07-16 05:06:29,855 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-07-16 05:06:30,596 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-07-16 05:06:30,618 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 89 on default port 9891] INFO ipc.Server: IPC Server handler 89 on default port 9891 caught an exception
scm_1               | 2023-07-16 05:06:06,199 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
datanode_3          | 2023-07-16 05:06:29,858 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-16 05:06:30,677 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-07-16 05:06:06,207 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-16 05:06:06,265 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
datanode_2          | 2023-07-16 05:06:34,121 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:34,121 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15
om_1                | 2023-07-16 05:07:02,919 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:voljre2l for user:hadoop
datanode_3          | 2023-07-16 05:06:29,867 [grpc-default-executor-0] INFO server.RaftServer: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: addNew group-44F4E9684BB5:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-44F4E9684BB5:java.util.concurrent.CompletableFuture@9060b62[Not completed]
datanode_3          | 2023-07-16 05:06:29,892 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 05:06:06,416 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
datanode_2          | 2023-07-16 05:06:34,122 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-07-16 05:06:34,122 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om_1                | 2023-07-16 05:07:10,094 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: default of layout LEGACY in volume: voljre2l
om_1                | 2023-07-16 05:07:14,529 [qtp268350620-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-07-16 05:07:15,125 [qtp268350620-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1689484035092 in 32 milliseconds
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-07-16 05:06:06,438 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
om_1                | 2023-07-16 05:07:15,153 [qtp268350620-54] INFO db.RDBCheckpointUtils: Waited for 26 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1689484035092 availability.
datanode_5          | 2023-07-16 05:06:30,688 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-16 05:06:30,841 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-07-16 05:06:06,444 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
datanode_4          | 2023-07-16 05:06:39,927 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-16 05:06:39,927 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-07-16 05:07:15,215 [qtp268350620-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 59 milliseconds
datanode_3          | 2023-07-16 05:06:29,892 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:06:29,892 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-16 05:06:06,472 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
datanode_4          | 2023-07-16 05:06:43,187 [grpc-default-executor-1] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: shutdown 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState
datanode_2          | 2023-07-16 05:06:34,123 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:34,125 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-07-16 05:06:29,893 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:06:29,893 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 05:06:06,483 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
datanode_4          | 2023-07-16 05:06:43,187 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState] INFO impl.FollowerState: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-FollowerState was interrupted
datanode_2          | 2023-07-16 05:06:34,125 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-07-16 05:06:34,125 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:06:29,893 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 does not exist. Creating ...
datanode_5          | 2023-07-16 05:06:30,925 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm_1               | 2023-07-16 05:06:06,501 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
datanode_4          | 2023-07-16 05:06:43,191 [grpc-default-executor-1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_4          | 2023-07-16 05:06:43,195 [grpc-default-executor-1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: change Leader from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 to null at term 1 for ELECTION
datanode_2          | 2023-07-16 05:06:34,126 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-07-16 05:06:29,898 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/in_use.lock acquired by nodename 7@952facba62e3
datanode_5          | 2023-07-16 05:06:30,946 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-07-16 05:06:06,752 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
datanode_4          | 2023-07-16 05:06:43,200 [grpc-default-executor-1] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1
datanode_4          | 2023-07-16 05:06:43,271 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:34,132 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-07-16 05:06:29,907 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5 has been successfully formatted.
datanode_5          | 2023-07-16 05:06:30,947 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-07-16 05:06:09,143 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om_1                | 2023-07-16 05:07:15,216 [qtp268350620-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
datanode_4          | 2023-07-16 05:06:43,315 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:34,133 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-07-16 05:06:29,911 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO ratis.ContainerStateMachine: group-44F4E9684BB5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:06:29,911 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-07-16 05:06:09,422 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-07-16 05:07:15,217 [qtp268350620-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1689484035092
datanode_4          | 2023-07-16 05:06:43,322 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-16 05:06:43,359 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:06:29,912 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:06:29,942 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-16 05:06:09,661 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
om_1                | 2023-07-16 05:07:17,882 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ratis of layout LEGACY in volume: voljre2l
datanode_4          | 2023-07-16 05:06:43,373 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_4          | 2023-07-16 05:06:44,948 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Completed APPEND_ENTRIES, lastRequest: null
datanode_5          | 2023-07-16 05:06:30,947 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-07-16 05:06:30,948 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_3          | 2023-07-16 05:06:29,942 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-07-16 05:07:24,505 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ecbucket of layout LEGACY in volume: voljre2l
scm_1               | 2023-07-16 05:06:09,681 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-07-16 05:06:09,872 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-07-16 05:06:09,881 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-07-16 05:06:34,190 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:34,238 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0
datanode_2          | 2023-07-16 05:06:34,360 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 05:06:09,887 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-07-16 05:06:09,892 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-07-16 05:06:09,983 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
datanode_3          | 2023-07-16 05:06:29,952 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:06:34,360 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:34,360 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-07-16 05:06:31,023 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-07-16 05:06:10,065 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-07-16 05:06:44,985 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_4          |   requestorId: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_5          | 2023-07-16 05:06:31,041 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159: new RaftServerImpl for group-8C72950665A4:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm_1               | 2023-07-16 05:06:10,065 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
datanode_3          | 2023-07-16 05:06:29,956 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          |   replyId: "131a1d93-dd5a-481f-884e-0521d7ee6f91"
datanode_2          | 2023-07-16 05:06:34,360 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:34,360 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-16 05:06:31,041 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1               | 2023-07-16 05:06:10,070 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
datanode_3          | 2023-07-16 05:06:29,959 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          |   raftGroupId {
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_5          | 2023-07-16 05:06:31,041 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-07-16 05:06:10,311 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
datanode_3          | 2023-07-16 05:06:29,959 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          |     id: "\002L\277\207\376\030O\354\243\254D\364\351hK\265"
datanode_2          | 2023-07-16 05:06:34,507 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-16 05:06:10,315 [main] INFO server.StorageContainerManager: 
datanode_3          | 2023-07-16 05:06:29,959 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          |   }
datanode_2          | 2023-07-16 05:06:34,509 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_5          | 2023-07-16 05:06:31,041 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | Container Balancer status:
datanode_4          |   callId: 7
datanode_3          | 2023-07-16 05:06:29,959 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_5          | 2023-07-16 05:06:31,042 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:06:34,510 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState
datanode_4          |   success: true
datanode_3          | 2023-07-16 05:06:29,959 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm_1               | Key                            Value
datanode_5          | 2023-07-16 05:06:31,042 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_2          | 2023-07-16 05:06:34,635 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C9411BC9FD15,id=333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-07-16 05:06:31,042 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | 2023-07-16 05:06:34,637 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | Running                        false
datanode_4          | }
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:31,042 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: ConfigurationManager, init=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_5          | 2023-07-16 05:06:31,049 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-07-16 05:06:34,639 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 05:06:34,640 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1             | 2023-07-16 05:06:23,404 [IPC Server handler 97 on default port 9891] INFO ipc.Server: IPC Server handler 97 on default port 9891 caught an exception
datanode_4          | term: 1
datanode_4          | nextIndex: 1
datanode_4          | matchIndex: 18446744073709551615
scm_1               | Container Balancer Configuration values:
datanode_2          | 2023-07-16 05:06:34,641 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1             | java.nio.channels.ClosedChannelException
datanode_5          | 2023-07-16 05:06:31,049 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | isHearbeat: true
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm_1               | Key                                                Value
datanode_2          | 2023-07-16 05:06:34,649 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_4          | 
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 05:06:34,702 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_4          | 2023-07-16 05:06:45,039 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 05:06:29,960 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_4          | 2023-07-16 05:06:45,044 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 131a1d93-dd5a-481f-884e-0521d7ee6f91<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t2
datanode_3          | 2023-07-16 05:06:29,961 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 05:06:35,229 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 301767aa-b1e5-4681-b115-67455213d3e0
scm_1               | Max Size to Move per Iteration                     500GB
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_4          | 2023-07-16 05:06:45,049 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1 ELECTION round 0: result PASSED
datanode_3          | 2023-07-16 05:06:29,961 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:35,229 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: change Leader from null to 301767aa-b1e5-4681-b115-67455213d3e0 at term 1 for appendEntries, leader elected after 1223ms
scm_1               | Max Size Entering Target per Iteration             26GB
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_4          | 2023-07-16 05:06:45,057 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: shutdown 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1
datanode_3          | 2023-07-16 05:06:29,975 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:35,654 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Failed APPEND_ENTRIES request 301767aa-b1e5-4681-b115-67455213d3e0->333b8eb3-808f-40e6-a1dc-b9ce55f887b5#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_2          | address: "172.24.0.12:9856"
datanode_5          | 2023-07-16 05:06:31,050 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_4          | 2023-07-16 05:06:45,061 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-07-16 05:06:30,082 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | priority: 1
scm_1               | Max Size Leaving Source per Iteration              26GB
datanode_5          | 2023-07-16 05:06:31,070 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_4          | 2023-07-16 05:06:45,065 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:30,089 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | dataStreamAddress: "172.24.0.12:9858"
scm_1               | 
datanode_5          | 2023-07-16 05:06:31,070 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_4          | 2023-07-16 05:06:45,100 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: change Leader from null to 131a1d93-dd5a-481f-884e-0521d7ee6f91 at term 2 for becomeLeader, leader elected after 1870ms
datanode_3          | 2023-07-16 05:06:30,089 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-07-16 05:06:30,090 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:06:10,317 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
datanode_5          | 2023-07-16 05:06:31,070 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_4          | 2023-07-16 05:06:45,128 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Completed APPEND_ENTRIES, lastRequest: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5->131a1d93-dd5a-481f-884e-0521d7ee6f91#5-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_4          | address: "172.24.0.12:9856"
datanode_2          | clientAddress: "172.24.0.12:9858"
scm_1               | 2023-07-16 05:06:10,328 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
datanode_5          | 2023-07-16 05:06:31,071 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_3          | 2023-07-16 05:06:30,091 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | adminAddress: "172.24.0.12:9857"
scm_1               | 2023-07-16 05:06:10,348 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-07-16 05:06:10,372 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/in_use.lock acquired by nodename 7@5a10f63682a1
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_4          | dataStreamAddress: "172.24.0.12:9858"
datanode_3          | 2023-07-16 05:06:30,092 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:30,092 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-16 05:06:31,071 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-07-16 05:06:10,390 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=bf1223b9-84ee-4a14-b347-7ad6126fbc27} from /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/raft-meta
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_5          | 2023-07-16 05:06:31,071 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-16 05:06:31,096 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: receive requestVote(PRE_VOTE, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, group-8C72950665A4, 0, (t:0, i:0))
datanode_2          | startupRole: FOLLOWER
datanode_3          | 2023-07-16 05:06:30,092 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState
datanode_3          | 2023-07-16 05:06:30,096 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-44F4E9684BB5,id=f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_4          | clientAddress: "172.24.0.12:9858"
datanode_4          | adminAddress: "172.24.0.12:9857"
scm_1               | 2023-07-16 05:06:10,489 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: set configuration 0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:10,496 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-16 05:06:31,113 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 11d6e79b-1aa6-4571-843b-2e54970fb159: Failed requestVote f02a32cc-4f60-4ab6-a6b0-1a6b831becc3->11d6e79b-1aa6-4571-843b-2e54970fb159#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4 is not in [RUNNING]: current state is NEW
datanode_3          | 2023-07-16 05:06:30,096 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | startupRole: FOLLOWER
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | ,id: "11d6e79b-1aa6-4571-843b-2e54970fb159"
scm_1               | 2023-07-16 05:06:10,517 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-07-16 05:06:31,152 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 does not exist. Creating ...
datanode_3          | 2023-07-16 05:06:30,097 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | ,id: "f02a32cc-4f60-4ab6-a6b0-1a6b831becc3"
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2          | address: "172.24.0.15:9856"
scm_1               | 2023-07-16 05:06:10,518 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:06:31,203 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/in_use.lock acquired by nodename 6@4fd9c9bae09d
datanode_4          | address: "172.24.0.8:9856"
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_2          | dataStreamAddress: "172.24.0.15:9858"
scm_1               | 2023-07-16 05:06:10,521 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-07-16 05:06:31,248 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15 has been successfully formatted.
datanode_3          | 2023-07-16 05:06:30,098 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | dataStreamAddress: "172.24.0.8:9858"
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 05:06:23,408 [IPC Server handler 95 on default port 9891] INFO ipc.Server: IPC Server handler 95 on default port 9891 caught an exception
scm_1               | 2023-07-16 05:06:10,523 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-07-16 05:06:31,447 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO ratis.ContainerStateMachine: group-C9411BC9FD15: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:06:30,098 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | clientAddress: "172.24.0.8:9858"
recon_1             | java.nio.channels.ClosedChannelException
datanode_2          | clientAddress: "172.24.0.15:9858"
scm_1               | 2023-07-16 05:06:10,529 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode_5          | 2023-07-16 05:06:31,481 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 05:06:30,101 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | adminAddress: "172.24.0.8:9857"
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2          | adminAddress: "172.24.0.15:9857"
scm_1               | 2023-07-16 05:06:10,549 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-07-16 05:06:31,562 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:06:30,110 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | startupRole: FOLLOWER
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-07-16 05:06:10,550 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-07-16 05:06:31,568 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:06:31,572 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-07-16 05:06:31,587 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-07-16 05:06:31,608 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:31,643 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | startupRole: FOLLOWER
datanode_4          | ,id: "131a1d93-dd5a-481f-884e-0521d7ee6f91"
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_3          | 2023-07-16 05:06:30,758 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.FollowerState: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5230754032ns, electionTimeout:5170ms
datanode_3          | 2023-07-16 05:06:30,759 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState
datanode_3          | 2023-07-16 05:06:30,760 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-07-16 05:06:31,643 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-07-16 05:06:31,643 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:10,552 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-16 05:06:10,566 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498
datanode_3          | 2023-07-16 05:06:30,765 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 05:06:30,765 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1
datanode_5          | 2023-07-16 05:06:31,690 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15
datanode_3          | 2023-07-16 05:06:30,769 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:10,568 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
datanode_4          | address: "172.24.0.10:9856"
datanode_4          | priority: 1
datanode_2          | ,id: "301767aa-b1e5-4681-b115-67455213d3e0"
datanode_5          | 2023-07-16 05:06:31,694 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-07-16 05:06:31,694 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:06:30,820 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:06:10,570 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
datanode_4          | dataStreamAddress: "172.24.0.10:9858"
datanode_4          | clientAddress: "172.24.0.10:9858"
datanode_4          | adminAddress: "172.24.0.10:9857"
datanode_4          | startupRole: FOLLOWER
datanode_4          | , old:)
datanode_4          | 2023-07-16 05:06:45,129 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 131a1d93-dd5a-481f-884e-0521d7ee6f91: Completed APPEND_ENTRIES, lastReply: null
datanode_4          | 2023-07-16 05:06:45,145 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-07-16 05:06:45,227 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-07-16 05:06:31,700 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:31,711 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | address: "172.24.0.11:9856"
datanode_2          | dataStreamAddress: "172.24.0.11:9858"
datanode_5          | 2023-07-16 05:06:31,722 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-16 05:06:31,729 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-16 05:06:31,732 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | clientAddress: "172.24.0.11:9858"
datanode_2          | adminAddress: "172.24.0.11:9857"
datanode_2          | startupRole: FOLLOWER
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_4          | 2023-07-16 05:06:45,228 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-07-16 05:06:45,260 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | , old:)
datanode_2          | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: The server role is not yet initialized.
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_4          | 2023-07-16 05:06:45,262 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-07-16 05:06:45,262 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:121)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitServerRequestAsync$11(RaftServerImpl.java:825)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_4          | 2023-07-16 05:06:45,312 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 2023-07-16 05:06:10,571 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode_2          | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_4          | 2023-07-16 05:06:45,334 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4          | 2023-07-16 05:06:45,429 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-07-16 05:06:30,821 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:06:30,866 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_5          | 2023-07-16 05:06:31,733 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:30,867 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:31,059 [grpc-default-executor-0] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: receive requestVote(PRE_VOTE, 333b8eb3-808f-40e6-a1dc-b9ce55f887b5, group-44F4E9684BB5, 0, (t:0, i:0))
datanode_5          | 2023-07-16 05:06:31,790 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-07-16 05:06:10,573 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-07-16 05:06:10,573 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-07-16 05:06:31,091 [grpc-default-executor-0] INFO impl.VoteContext: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FOLLOWER: accept PRE_VOTE from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: our priority 0 <= candidate's priority 0
datanode_3          | 2023-07-16 05:06:31,144 [grpc-default-executor-0] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5 replies to PRE_VOTE vote request: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t0. Peer's state: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5:t0, leader=null, voted=, raftlog=Memoized:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-07-16 05:06:10,576 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-07-16 05:06:10,576 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-07-16 05:06:10,576 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-07-16 05:06:45,429 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | Caused by: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: The server role is not yet initialized.
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerImpl.preAppendEntriesAsync(RaftServerImpl.java:1451)
datanode_5          | 2023-07-16 05:06:31,795 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-07-16 05:06:10,614 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
datanode_4          | 2023-07-16 05:06:45,429 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-07-16 05:06:31,439 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4 is not in [RUNNING]: current state is NEW
datanode_3          | 2023-07-16 05:06:32,019 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-8C72950665A4 not found.
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 05:06:10,616 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:10,644 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-07-16 05:06:32,025 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
datanode_3          | 2023-07-16 05:06:32,028 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4 is not in [RUNNING]: current state is NEW
datanode_5          | 2023-07-16 05:06:31,884 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1             | 2023-07-16 05:06:23,424 [IPC Server handler 87 on default port 9891] WARN ipc.Server: IPC Server handler 87 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:42248 / 172.24.0.11:42248: output error
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerImpl.appendEntriesAsync(RaftServerImpl.java:1393)
datanode_2          | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$25(RaftServerProxy.java:637)
datanode_5          | 2023-07-16 05:06:31,886 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 2023-07-16 05:06:23,424 [IPC Server handler 87 on default port 9891] INFO ipc.Server: IPC Server handler 87 on default port 9891 caught an exception
datanode_2          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
datanode_3          | 2023-07-16 05:06:32,030 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 131a1d93-dd5a-481f-884e-0521d7ee6f91: group-8C72950665A4 not found.
datanode_5          | 2023-07-16 05:06:31,886 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1             | java.nio.channels.ClosedChannelException
datanode_2          | 	... 5 more
scm_1               | 2023-07-16 05:06:10,648 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 05:06:10,649 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
datanode_3          | 2023-07-16 05:06:32,031 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1 PRE_VOTE round 0: result REJECTED
datanode_2          | 2023-07-16 05:06:36,023 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5.
datanode_5          | 2023-07-16 05:06:31,912 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-07-16 05:06:10,789 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: set configuration 0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:10,791 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/log_inprogress_0
scm_1               | 2023-07-16 05:06:10,819 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:06:32,038 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_3          | 2023-07-16 05:06:32,047 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1
datanode_5          | 2023-07-16 05:06:31,912 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-07-16 05:06:11,215 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: start as a follower, conf=0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:32,071 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection1] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState
datanode_2          | 2023-07-16 05:06:36,024 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: addNew group-59BA0C080380:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-59BA0C080380:java.util.concurrent.CompletableFuture@946a4a4[Not completed]
datanode_5          | 2023-07-16 05:06:31,919 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_3          | 2023-07-16 05:06:32,132 [grpc-default-executor-0] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: receive requestVote(ELECTION, 333b8eb3-808f-40e6-a1dc-b9ce55f887b5, group-44F4E9684BB5, 1, (t:0, i:0))
datanode_3          | 2023-07-16 05:06:32,135 [grpc-default-executor-0] INFO impl.VoteContext: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FOLLOWER: accept ELECTION from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: our priority 0 <= candidate's priority 0
datanode_5          | 2023-07-16 05:06:31,929 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-07-16 05:06:31,934 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState
datanode_4          | 2023-07-16 05:06:45,471 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_4          | 2023-07-16 05:06:45,477 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-07-16 05:06:32,135 [grpc-default-executor-0] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_3          | 2023-07-16 05:06:32,135 [grpc-default-executor-0] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState
datanode_5          | 2023-07-16 05:06:31,953 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C9411BC9FD15,id=11d6e79b-1aa6-4571-843b-2e54970fb159
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-16 05:06:11,219 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from      null to FOLLOWER at term 1 for startAsFollower
datanode_3          | 2023-07-16 05:06:32,136 [grpc-default-executor-0] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState
datanode_2          | 2023-07-16 05:06:36,038 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: new RaftServerImpl for group-59BA0C080380:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-16 05:06:45,480 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-07-16 05:06:31,955 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-07-16 05:06:11,220 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState
datanode_3          | 2023-07-16 05:06:32,135 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState] INFO impl.FollowerState: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState was interrupted
datanode_2          | 2023-07-16 05:06:36,039 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-07-16 05:06:45,483 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-07-16 05:06:31,963 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_5          | 2023-07-16 05:06:31,968 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_3          | 2023-07-16 05:06:32,152 [grpc-default-executor-0] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5 replies to ELECTION vote request: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t1. Peer's state: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5:t1, leader=null, voted=333b8eb3-808f-40e6-a1dc-b9ce55f887b5, raftlog=Memoized:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:36,039 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-16 05:06:45,484 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_4          | 2023-07-16 05:06:45,486 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-16 05:06:31,971 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_3          | 2023-07-16 05:06:33,646 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:36,040 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-07-16 05:06:36,041 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:06:36,041 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-07-16 05:06:31,971 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_3          | 2023-07-16 05:06:33,654 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 1 for appendEntries, leader elected after 3793ms
datanode_4          | 2023-07-16 05:06:45,488 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-07-16 05:06:45,540 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-07-16 05:06:31,973 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:33,902 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:32,071 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 does not exist. Creating ...
datanode_5          | 2023-07-16 05:06:32,077 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/in_use.lock acquired by nodename 6@4fd9c9bae09d
datanode_5          | 2023-07-16 05:06:32,097 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4 has been successfully formatted.
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO ratis.ContainerStateMachine: group-8C72950665A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_3          | 2023-07-16 05:06:33,954 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-07-16 05:06:45,542 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:45,542 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-07-16 05:06:11,237 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:06:11,237 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 05:06:11,244 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-757BF1EEF498,id=bf1223b9-84ee-4a14-b347-7ad6126fbc27
datanode_3          | 2023-07-16 05:06:34,729 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0
datanode_4          | 2023-07-16 05:06:45,544 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:11,260 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-07-16 05:06:11,261 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-07-16 05:06:11,262 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
datanode_3          | 2023-07-16 05:06:36,007 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4.
datanode_3          | 2023-07-16 05:06:36,008 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: addNew group-518A42FD211D:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-518A42FD211D:java.util.concurrent.CompletableFuture@5088fce2[Not completed]
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-07-16 05:06:11,263 [bf1223b9-84ee-4a14-b347-7ad6126fbc27-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 05:06:36,041 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-07-16 05:06:36,046 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: ConfigurationManager, init=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-07-16 05:06:32,137 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3          | 2023-07-16 05:06:36,019 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: new RaftServerImpl for group-518A42FD211D:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-07-16 05:06:32,149 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:32,156 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-07-16 05:06:32,156 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:06:36,047 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-07-16 05:06:36,020 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1             | 2023-07-16 05:06:23,436 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:53560 / 172.24.0.15:53560: output error
recon_1             | 2023-07-16 05:06:23,436 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
datanode_5          | 2023-07-16 05:06:32,156 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:36,048 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:06:36,020 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | java.nio.channels.ClosedChannelException
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-16 05:06:32,159 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4
datanode_5          | 2023-07-16 05:06:32,159 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 05:06:36,022 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_4          | 2023-07-16 05:06:45,547 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-07-16 05:06:32,159 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:06:36,052 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-07-16 05:06:36,027 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_2          | 2023-07-16 05:06:36,052 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-07-16 05:06:36,054 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 05:06:11,286 [main] INFO server.RaftServer: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start RPC server
datanode_2          | 2023-07-16 05:06:36,054 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-07-16 05:06:11,589 [main] INFO server.GrpcService: bf1223b9-84ee-4a14-b347-7ad6126fbc27: GrpcService started, listening on 9894
datanode_5          | 2023-07-16 05:06:32,159 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_3          | 2023-07-16 05:06:36,027 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:06:36,028 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-07-16 05:06:11,608 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-bf1223b9-84ee-4a14-b347-7ad6126fbc27: Started
datanode_2          | 2023-07-16 05:06:36,055 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-07-16 05:06:32,165 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_3          | 2023-07-16 05:06:36,028 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: ConfigurationManager, init=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-07-16 05:06:36,028 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1               | 2023-07-16 05:06:11,666 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
datanode_2          | 2023-07-16 05:06:36,055 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-07-16 05:06:11,670 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
datanode_5          | 2023-07-16 05:06:32,167 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-16 05:06:32,167 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-16 05:06:32,168 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-07-16 05:06:32,175 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-07-16 05:06:11,685 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
datanode_5          | 2023-07-16 05:06:32,181 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-07-16 05:06:45,587 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderStateImpl
datanode_4          | 2023-07-16 05:06:45,614 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_4          | 2023-07-16 05:06:45,623 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0 to /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_0-0
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-07-16 05:06:11,693 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
datanode_5          | 2023-07-16 05:06:32,193 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:06:32,984 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-11d6e79b-1aa6-4571-843b-2e54970fb159: Detected pause in JVM or host machine approximately 0.384s with 0.786s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=154ms
datanode_5          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=632ms
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-07-16 05:06:11,693 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-07-16 05:06:11,817 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5          | 2023-07-16 05:06:33,132 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: receive requestVote(PRE_VOTE, 301767aa-b1e5-4681-b115-67455213d3e0, group-C9411BC9FD15, 0, (t:0, i:0))
datanode_5          | 2023-07-16 05:06:33,146 [grpc-default-executor-1] INFO impl.VoteContext: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FOLLOWER: accept PRE_VOTE from 301767aa-b1e5-4681-b115-67455213d3e0: our priority 0 <= candidate's priority 0
datanode_5          | 2023-07-16 05:06:33,162 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-07-16 05:06:33,162 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-07-16 05:06:11,852 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-07-16 05:06:11,852 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-07-16 05:06:13,286 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
datanode_4          | 2023-07-16 05:06:45,653 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_1
datanode_4          | 2023-07-16 05:06:45,680 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5-LeaderElection1] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:53,060 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91: new RaftServerImpl for group-1D9760A6FAE4:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-07-16 05:06:53,061 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1               | 2023-07-16 05:06:13,296 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-07-16 05:06:36,063 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:06:36,077 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-07-16 05:06:53,061 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-07-16 05:06:53,062 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-07-16 05:06:36,040 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 05:06:13,302 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
datanode_2          | 2023-07-16 05:06:36,079 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-07-16 05:06:36,079 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-07-16 05:06:33,162 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-07-16 05:06:33,163 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:06:36,040 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-07-16 05:06:13,491 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
datanode_2          | 2023-07-16 05:06:36,079 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-07-16 05:06:13,492 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
datanode_2          | 2023-07-16 05:06:36,082 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-07-16 05:06:13,492 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | 2023-07-16 05:06:33,163 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3          | 2023-07-16 05:06:36,041 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:06:36,041 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-07-16 05:06:13,493 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
datanode_2          | 2023-07-16 05:06:36,082 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1fa08159-4631-48c1-a393-59ba0c080380 does not exist. Creating ...
datanode_5          | 2023-07-16 05:06:33,179 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: start as a follower, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3          | 2023-07-16 05:06:36,043 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:06:36,044 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-07-16 05:06:13,610 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
datanode_2          | 2023-07-16 05:06:36,092 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1fa08159-4631-48c1-a393-59ba0c080380/in_use.lock acquired by nodename 7@4caf8da3e4ee
datanode_5          | 2023-07-16 05:06:33,180 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:06:36,048 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 05:06:36,071 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm_1               | 2023-07-16 05:06:13,610 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-07-16 05:06:36,104 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1fa08159-4631-48c1-a393-59ba0c080380 has been successfully formatted.
datanode_2          | 2023-07-16 05:06:36,133 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO ratis.ContainerStateMachine: group-59BA0C080380: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-07-16 05:06:53,064 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm_1               | 2023-07-16 05:06:13,758 [main] INFO util.log: Logging initialized @41358ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5          | 2023-07-16 05:06:33,181 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState
datanode_2          | 2023-07-16 05:06:36,134 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_3          | 2023-07-16 05:06:36,072 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:06:36,072 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-07-16 05:06:33,203 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C72950665A4,id=11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_2          | 2023-07-16 05:06:36,134 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:06:36,136 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:53,064 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-07-16 05:06:36,073 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-07-16 05:06:36,073 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:06:36,075 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-16 05:06:33,206 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 05:06:23,438 [IPC Server handler 98 on default port 9891] WARN ipc.Server: IPC Server handler 98 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:55146 / 172.24.0.12:55146: output error
datanode_4          | 2023-07-16 05:06:53,065 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-07-16 05:06:36,076 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ad8a4b11-a3a4-4244-9a6b-518a42fd211d does not exist. Creating ...
datanode_5          | 2023-07-16 05:06:33,206 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-07-16 05:06:33,207 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-07-16 05:06:53,066 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: ConfigurationManager, init=-1: peers:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-07-16 05:06:53,068 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-07-16 05:06:53,070 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-07-16 05:06:14,178 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-07-16 05:06:14,202 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-07-16 05:06:14,229 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-07-16 05:06:53,071 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | 2023-07-16 05:06:23,439 [IPC Server handler 98 on default port 9891] INFO ipc.Server: IPC Server handler 98 on default port 9891 caught an exception
datanode_5          | 2023-07-16 05:06:33,208 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-16 05:06:53,072 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-07-16 05:06:36,080 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ad8a4b11-a3a4-4244-9a6b-518a42fd211d/in_use.lock acquired by nodename 7@952facba62e3
datanode_5          | 2023-07-16 05:06:33,214 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:06:14,234 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-07-16 05:06:14,236 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-07-16 05:06:53,075 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3          | 2023-07-16 05:06:36,088 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ad8a4b11-a3a4-4244-9a6b-518a42fd211d has been successfully formatted.
datanode_5          | 2023-07-16 05:06:33,234 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-07-16 05:06:14,236 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-07-16 05:06:36,150 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-07-16 05:06:53,075 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3          | 2023-07-16 05:06:36,109 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO ratis.ContainerStateMachine: group-518A42FD211D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:06:36,113 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-16 05:06:33,244 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15 replies to PRE_VOTE vote request: 301767aa-b1e5-4681-b115-67455213d3e0<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t0. Peer's state: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15:t0, leader=null, voted=, raftlog=Memoized:11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:33,581 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: receive requestVote(ELECTION, 301767aa-b1e5-4681-b115-67455213d3e0, group-C9411BC9FD15, 1, (t:0, i:0))
datanode_5          | 2023-07-16 05:06:33,582 [grpc-default-executor-1] INFO impl.VoteContext: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FOLLOWER: accept ELECTION from 301767aa-b1e5-4681-b115-67455213d3e0: our priority 0 <= candidate's priority 0
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
datanode_4          | 2023-07-16 05:06:53,075 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 05:06:36,113 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-07-16 05:06:36,153 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-07-16 05:06:14,373 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
datanode_4          | 2023-07-16 05:06:53,076 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-07-16 05:06:36,114 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:36,159 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-07-16 05:06:14,375 [main] INFO http.HttpServer2: Jetty bound to port 9876
datanode_4          | 2023-07-16 05:06:53,078 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 131a1d93-dd5a-481f-884e-0521d7ee6f91: addNew group-1D9760A6FAE4:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-1D9760A6FAE4:java.util.concurrent.CompletableFuture@41ab33a8[Not completed]
datanode_3          | 2023-07-16 05:06:36,114 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:06:36,161 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-07-16 05:06:36,168 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_3          | 2023-07-16 05:06:36,115 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:06:36,168 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:36,173 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1fa08159-4631-48c1-a393-59ba0c080380
datanode_2          | 2023-07-16 05:06:36,178 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_3          | 2023-07-16 05:06:36,130 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:33,583 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:301767aa-b1e5-4681-b115-67455213d3e0
datanode_2          | 2023-07-16 05:06:36,179 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm_1               | 2023-07-16 05:06:14,377 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_4          | 2023-07-16 05:06:53,087 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_2          | 2023-07-16 05:06:36,179 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:36,199 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-07-16 05:06:53,087 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-07-16 05:06:36,134 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-07-16 05:06:36,135 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-07-16 05:06:36,199 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-07-16 05:06:14,452 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-07-16 05:06:53,101 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_3          | 2023-07-16 05:06:36,136 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:36,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-07-16 05:06:36,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-07-16 05:06:53,102 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:36,136 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ad8a4b11-a3a4-4244-9a6b-518a42fd211d
datanode_5          | 2023-07-16 05:06:33,583 [grpc-default-executor-1] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: shutdown 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState
scm_1               | 2023-07-16 05:06:14,453 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-07-16 05:06:14,457 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_4          | 2023-07-16 05:06:53,102 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_3          | 2023-07-16 05:06:36,138 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-07-16 05:06:36,138 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:53,102 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-07-16 05:06:36,139 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:33,584 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState] INFO impl.FollowerState: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState was interrupted
datanode_4          | 2023-07-16 05:06:53,102 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1855ec52-aa87-492c-936f-1d9760a6fae4 does not exist. Creating ...
datanode_3          | 2023-07-16 05:06:36,139 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-07-16 05:06:33,586 [grpc-default-executor-1] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState
datanode_4          | 2023-07-16 05:06:53,115 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1855ec52-aa87-492c-936f-1d9760a6fae4/in_use.lock acquired by nodename 7@37e9a0ff2c7a
datanode_3          | 2023-07-16 05:06:36,139 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-07-16 05:06:33,595 [grpc-default-executor-1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15 replies to ELECTION vote request: 301767aa-b1e5-4681-b115-67455213d3e0<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t1. Peer's state: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15:t1, leader=null, voted=301767aa-b1e5-4681-b115-67455213d3e0, raftlog=Memoized:11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:53,127 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1855ec52-aa87-492c-936f-1d9760a6fae4 has been successfully formatted.
datanode_3          | 2023-07-16 05:06:36,139 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-07-16 05:06:36,140 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-07-16 05:06:14,498 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@d3e9629{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-07-16 05:06:14,500 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6da53709{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-07-16 05:06:14,767 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@e7c3ba2{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-4454032961360131851/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
datanode_4          | 2023-07-16 05:06:53,149 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO ratis.ContainerStateMachine: group-1D9760A6FAE4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-07-16 05:06:36,140 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-07-16 05:06:34,479 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 301767aa-b1e5-4681-b115-67455213d3e0
scm_1               | 2023-07-16 05:06:14,795 [main] INFO server.AbstractConnector: Started ServerConnector@3c380db5{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-07-16 05:06:14,795 [main] INFO server.Server: Started @42394ms
scm_1               | 2023-07-16 05:06:14,804 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-07-16 05:06:53,149 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-07-16 05:06:36,167 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1               | 2023-07-16 05:06:14,804 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-07-16 05:06:14,809 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-07-16 05:06:16,247 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.FollowerState: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5026622336ns, electionTimeout:5004ms
datanode_4          | 2023-07-16 05:06:53,149 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-07-16 05:06:36,171 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-07-16 05:06:16,279 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState
scm_1               | 2023-07-16 05:06:16,283 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-07-16 05:06:16,337 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-07-16 05:06:53,160 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:06:36,265 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:36,202 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-07-16 05:06:36,211 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:36,224 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:53,161 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-07-16 05:06:36,266 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:36,327 [grpc-default-executor-1] WARN server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-44F4E9684BB5: The server role is not yet initialized.
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_4          | 2023-07-16 05:06:53,161 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-07-16 05:06:36,266 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 05:06:36,343 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-07-16 05:06:16,344 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-FollowerState] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1
scm_1               | 2023-07-16 05:06:16,362 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:53,161 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-07-16 05:06:36,267 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:36,343 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:36,344 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 05:06:16,363 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-07-16 05:06:16,496 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:34,479 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: change Leader from null to 301767aa-b1e5-4681-b115-67455213d3e0 at term 1 for appendEntries, leader elected after 3971ms
datanode_5          | 2023-07-16 05:06:34,698 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-16 05:06:23,445 [IPC Server handler 93 on default port 9891] WARN ipc.Server: IPC Server handler 93 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:42254 / 172.24.0.11:42254: output error
recon_1             | 2023-07-16 05:06:23,445 [IPC Server handler 93 on default port 9891] INFO ipc.Server: IPC Server handler 93 on default port 9891 caught an exception
scm_1               | 2023-07-16 05:06:16,502 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.LeaderElection: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-07-16 05:06:16,502 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: shutdown bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1
datanode_5          | 2023-07-16 05:06:34,801 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 05:06:36,267 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:36,346 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-07-16 05:06:36,352 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-07-16 05:06:35,129 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0
datanode_5          | 2023-07-16 05:06:37,276 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: receive requestVote(PRE_VOTE, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, group-8C72950665A4, 0, (t:0, i:0))
datanode_5          | 2023-07-16 05:06:37,277 [grpc-default-executor-0] INFO impl.VoteContext: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FOLLOWER: accept PRE_VOTE from f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: our priority 0 <= candidate's priority 1
scm_1               | 2023-07-16 05:06:16,503 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-07-16 05:06:16,505 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-07-16 05:06:16,511 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
recon_1             | java.nio.channels.ClosedChannelException
datanode_5          | 2023-07-16 05:06:37,278 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4 replies to PRE_VOTE vote request: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t0. Peer's state: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4:t0, leader=null, voted=, raftlog=Memoized:11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:37,321 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: receive requestVote(ELECTION, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, group-8C72950665A4, 1, (t:0, i:0))
datanode_5          | 2023-07-16 05:06:37,321 [grpc-default-executor-0] INFO impl.VoteContext: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FOLLOWER: accept ELECTION from f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 05:06:36,354 [grpc-default-executor-1] INFO leader.FollowerInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_2          | 2023-07-16 05:06:36,362 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: start as a follower, conf=-1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_5          | 2023-07-16 05:06:37,322 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_5          | 2023-07-16 05:06:37,322 [grpc-default-executor-0] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: shutdown 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState
datanode_5          | 2023-07-16 05:06:37,322 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState] INFO impl.FollowerState: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState was interrupted
datanode_2          | 2023-07-16 05:06:36,362 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-07-16 05:06:16,529 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: change Leader from null to bf1223b9-84ee-4a14-b347-7ad6126fbc27 at term 2 for becomeLeader, leader elected after 12008ms
scm_1               | 2023-07-16 05:06:16,575 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-07-16 05:06:37,326 [grpc-default-executor-0] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-FollowerState
datanode_5          | 2023-07-16 05:06:37,333 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4 replies to ELECTION vote request: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t1. Peer's state: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4:t1, leader=null, voted=f02a32cc-4f60-4ab6-a6b0-1a6b831becc3, raftlog=Memoized:11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:36,363 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState
datanode_2          | 2023-07-16 05:06:36,365 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-59BA0C080380,id=333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:36,366 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-07-16 05:06:16,610 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
datanode_2          | 2023-07-16 05:06:36,366 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-07-16 05:06:36,366 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-07-16 05:06:37,950 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C72950665A4 with new leaderId: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_5          | 2023-07-16 05:06:37,951 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: change Leader from null to f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 at term 1 for appendEntries, leader elected after 6900ms
scm_1               | 2023-07-16 05:06:16,616 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-07-16 05:06:16,649 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-07-16 05:06:16,650 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-07-16 05:06:16,655 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_3          | 2023-07-16 05:06:36,270 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: start as a follower, conf=-1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:36,270 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:06:36,270 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState
datanode_2          | 2023-07-16 05:06:36,366 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-07-16 05:06:53,163 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-07-16 05:06:53,167 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-07-16 05:06:16,698 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-07-16 05:06:16,722 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 05:06:36,271 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-518A42FD211D,id=f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_3          | 2023-07-16 05:06:36,271 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-07-16 05:06:53,172 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-07-16 05:06:53,172 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1855ec52-aa87-492c-936f-1d9760a6fae4
scm_1               | 2023-07-16 05:06:16,728 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO impl.RoleInfo: bf1223b9-84ee-4a14-b347-7ad6126fbc27: start bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderStateImpl
scm_1               | 2023-07-16 05:06:16,743 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-16 05:06:16,753 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/log_inprogress_0 to /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/log_0-0
datanode_2          | 2023-07-16 05:06:36,373 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-07-16 05:06:36,271 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-07-16 05:06:36,271 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-07-16 05:06:36,272 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-07-16 05:06:36,374 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_5          | 2023-07-16 05:06:38,019 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4: set configuration 0: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:38,021 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-16 05:06:38,024 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-8C72950665A4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/current/log_inprogress_0
datanode_2          | 2023-07-16 05:06:36,376 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=1fa08159-4631-48c1-a393-59ba0c080380
scm_1               | 2023-07-16 05:06:16,813 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-LeaderElection1] INFO server.RaftServer$Division: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498: set configuration 1: peers:[bf1223b9-84ee-4a14-b347-7ad6126fbc27|rpc:5a10f63682a1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:16,825 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/46f5dd7f-dd64-4591-bc47-757bf1eef498/current/log_inprogress_1
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: receive requestVote(ELECTION, 333b8eb3-808f-40e6-a1dc-b9ce55f887b5, group-C9411BC9FD15, 2, (t:1, i:0))
scm_1               | 2023-07-16 05:06:16,855 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
datanode_3          | 2023-07-16 05:06:36,272 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-07-16 05:06:53,174 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1             | 2023-07-16 05:06:24,889 [IPC Server handler 36 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_2          | 2023-07-16 05:06:36,377 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=1fa08159-4631-48c1-a393-59ba0c080380.
scm_1               | 2023-07-16 05:06:16,862 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-07-16 05:06:16,883 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_4          | 2023-07-16 05:06:53,174 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1             | 2023-07-16 05:06:25,101 [IPC Server handler 36 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f02a32cc-4f60-4ab6-a6b0-1a6b831becc3{ip: 172.24.0.8, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2          | 2023-07-16 05:06:37,823 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:36,273 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:06:36,288 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d
datanode_4          | 2023-07-16 05:06:53,174 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1             | 2023-07-16 05:06:25,166 [IPC Server handler 2 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:37,829 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-07-16 05:06:37,832 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0
datanode_2          | 2023-07-16 05:06:39,775 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-07-16 05:06:16,884 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-07-16 05:06:16,885 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
recon_1             | 2023-07-16 05:06:25,166 [IPC Server handler 99 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/301767aa-b1e5-4681-b115-67455213d3e0
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO impl.VoteContext: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FOLLOWER: accept ELECTION from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: our priority 0 <= candidate's priority 1
datanode_2          | 2023-07-16 05:06:39,776 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-07-16 05:06:41,567 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO impl.FollowerState: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5204772309ns, electionTimeout:5192ms
scm_1               | 2023-07-16 05:06:16,885 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
datanode_4          | 2023-07-16 05:06:53,175 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1             | 2023-07-16 05:06:25,229 [IPC Server handler 99 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 301767aa-b1e5-4681-b115-67455213d3e0{ip: 172.24.0.11, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: change Leader from 301767aa-b1e5-4681-b115-67455213d3e0 to null at term 2 for updateCurrentTerm
datanode_2          | 2023-07-16 05:06:41,570 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState
datanode_3          | 2023-07-16 05:06:36,291 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d.
scm_1               | 2023-07-16 05:06:16,892 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
recon_1             | 2023-07-16 05:06:25,231 [IPC Server handler 2 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 333b8eb3-808f-40e6-a1dc-b9ce55f887b5{ip: 172.24.0.12, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-07-16 05:06:25,380 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5. Trying to get from SCM.
recon_1             | 2023-07-16 05:06:25,410 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 to Node DB.
scm_1               | 2023-07-16 05:06:16,975 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-07-16 05:06:17,278 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:48606 / 172.24.0.8:48606: output error
scm_1               | 2023-07-16 05:06:17,282 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
datanode_3          | 2023-07-16 05:06:37,233 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.FollowerState: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5161640588ns, electionTimeout:5092ms
datanode_3          | 2023-07-16 05:06:37,233 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:333b8eb3-808f-40e6-a1dc-b9ce55f887b5
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3          | 2023-07-16 05:06:37,234 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: shutdown 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState
datanode_5          | 2023-07-16 05:06:43,421 [grpc-default-executor-0] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_2          | 2023-07-16 05:06:41,570 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 05:06:37,234 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-07-16 05:06:37,234 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2
datanode_4          | 2023-07-16 05:06:53,175 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1             | 2023-07-16 05:06:25,427 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 301767aa-b1e5-4681-b115-67455213d3e0 to Node DB.
recon_1             | 2023-07-16 05:06:25,427 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 to Node DB.
datanode_2          | 2023-07-16 05:06:41,570 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-07-16 05:06:53,176 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-07-16 05:06:53,177 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_3          | 2023-07-16 05:06:37,245 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:41,571 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-FollowerState] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 2023-07-16 05:06:25,476 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: f02ad586-a085-4098-91a1-a5d1edc638c5, Nodes: 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:301767aa-b1e5-4681-b115-67455213d3e0, CreationTimestamp2023-07-16T05:06:21.479Z[UTC]] to Recon pipeline metadata.
datanode_3          | 2023-07-16 05:06:37,297 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-07-16 05:06:37,297 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection:   Response 0: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t0
datanode_3          | 2023-07-16 05:06:37,297 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-07-16 05:06:37,310 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:41,577 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_5          | 2023-07-16 05:06:43,422 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState] INFO impl.FollowerState: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-FollowerState was interrupted
recon_1             | 2023-07-16 05:06:25,513 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5 reported by 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_2          | 2023-07-16 05:06:41,579 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_5          | 2023-07-16 05:06:43,475 [grpc-default-executor-0] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15 replies to ELECTION vote request: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t2. Peer's state: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15:t2, leader=null, voted=333b8eb3-808f-40e6-a1dc-b9ce55f887b5, raftlog=Memoized:11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLog:OPENED:c0, conf=0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:43,980 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_4          | 2023-07-16 05:06:53,177 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 2023-07-16 05:06:25,536 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5. Trying to get from SCM.
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_5          | 2023-07-16 05:06:43,984 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread3] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 2 for appendEntries, leader elected after 558ms
datanode_5          | 2023-07-16 05:06:44,019 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:06:44,022 [11d6e79b-1aa6-4571-843b-2e54970fb159-server-thread2] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
recon_1             | 2023-07-16 05:06:25,618 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 024cbf87-fe18-4fec-a3ac-44f4e9684bb5, Nodes: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.334Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-07-16 05:06:25,620 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 reported by 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)
recon_1             | 2023-07-16 05:06:25,627 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
datanode_2          | 2023-07-16 05:06:41,587 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:53,178 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_5          | 2023-07-16 05:06:44,024 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0 to /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_0-0
datanode_3          | 2023-07-16 05:06:37,371 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-07-16 05:06:41,587 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2 ELECTION round 0: result PASSED (term=1)
recon_1             | 2023-07-16 05:06:26,187 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15. Trying to get from SCM.
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_5          | 2023-07-16 05:06:44,054 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_1
datanode_5          | 2023-07-16 05:06:44,184 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 11d6e79b-1aa6-4571-843b-2e54970fb159: Completed APPEND_ENTRIES, lastRequest: null
datanode_5          | 2023-07-16 05:06:44,963 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 11d6e79b-1aa6-4571-843b-2e54970fb159: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_5          |   requestorId: "301767aa-b1e5-4681-b115-67455213d3e0"
datanode_5          |   replyId: "11d6e79b-1aa6-4571-843b-2e54970fb159"
datanode_4          | 2023-07-16 05:06:53,183 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2          | 2023-07-16 05:06:41,587 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2
recon_1             | 2023-07-16 05:06:26,205 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 20604c91-9021-41ef-9746-c9411bc9fd15, Nodes: 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.540Z[UTC]] to Recon pipeline metadata.
datanode_3          | 2023-07-16 05:06:37,372 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection:   Response 0: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t1
datanode_5          |   raftGroupId {
datanode_4          | 2023-07-16 05:06:53,717 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-131a1d93-dd5a-481f-884e-0521d7ee6f91: Detected pause in JVM or host machine approximately 0.369s with 0.528s GC time.
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-07-16 05:06:26,218 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11)
datanode_3          | 2023-07-16 05:06:37,372 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2 ELECTION round 0: result PASSED
datanode_5          |     id: " `L\221\220!A\357\227F\311A\033\311\375\025"
datanode_5          |   }
scm_1               | 2023-07-16 05:06:17,304 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:37996 / 172.24.0.12:37996: output error
scm_1               | 2023-07-16 05:06:17,308 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
datanode_2          | 2023-07-16 05:06:41,587 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          |   callId: 5
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=528ms
datanode_3          | 2023-07-16 05:06:37,372 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2
datanode_3          | 2023-07-16 05:06:37,373 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 05:06:37,373 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C72950665A4 with new leaderId: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_3          | 2023-07-16 05:06:37,373 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: change Leader from null to f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 at term 1 for becomeLeader, leader elected after 13334ms
datanode_2          | 2023-07-16 05:06:41,587 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-59BA0C080380 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:41,588 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 1 for becomeLeader, leader elected after 5534ms
recon_1             | 2023-07-16 05:06:29,944 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
recon_1             | 2023-07-16 05:06:29,945 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
recon_1             | 2023-07-16 05:06:31,078 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11)
datanode_3          | 2023-07-16 05:06:37,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 05:06:37,499 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1             | 2023-07-16 05:06:31,494 [IPC Server handler 81 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/11d6e79b-1aa6-4571-843b-2e54970fb159
recon_1             | 2023-07-16 05:06:31,495 [IPC Server handler 81 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 11d6e79b-1aa6-4571-843b-2e54970fb159{ip: 172.24.0.15, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_4          | 2023-07-16 05:06:53,814 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:41,588 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 2023-07-16 05:06:31,496 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 11d6e79b-1aa6-4571-843b-2e54970fb159 to Node DB.
datanode_4          | 2023-07-16 05:06:53,814 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-07-16 05:06:41,589 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_5          |   success: true
datanode_5          | }
recon_1             | 2023-07-16 05:06:31,498 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)
datanode_4          | 2023-07-16 05:06:53,814 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-07-16 05:06:41,589 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 05:06:41,589 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-07-16 05:06:41,589 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | term: 1
datanode_5          | nextIndex: 1
recon_1             | 2023-07-16 05:06:31,498 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)
datanode_4          | 2023-07-16 05:06:53,814 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-07-16 05:06:37,500 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 05:06:37,506 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 05:06:37,509 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | matchIndex: 18446744073709551615
datanode_5          | isHearbeat: true
recon_1             | 2023-07-16 05:06:32,145 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)
datanode_4          | 2023-07-16 05:06:53,818 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_5          | 
datanode_5          | 2023-07-16 05:06:44,961 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 11d6e79b-1aa6-4571-843b-2e54970fb159: Completed APPEND_ENTRIES, lastRequest: 301767aa-b1e5-4681-b115-67455213d3e0->11d6e79b-1aa6-4571-843b-2e54970fb159#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
recon_1             | 2023-07-16 05:06:32,146 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)
datanode_4          | 2023-07-16 05:06:53,844 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: start as a follower, conf=-1: peers:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:37,509 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:06:41,589 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:06:41,590 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:53,846 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-07-16 05:06:37,557 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | address: "172.24.0.12:9856"
recon_1             | 2023-07-16 05:06:32,254 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 reported by 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)
datanode_2          | 2023-07-16 05:06:41,590 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 05:06:41,590 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderStateImpl
datanode_3          | 2023-07-16 05:06:37,578 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | priority: 1
recon_1             | 2023-07-16 05:06:33,866 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11)
recon_1             | 2023-07-16 05:06:33,945 [IPC Server handler 80 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/131a1d93-dd5a-481f-884e-0521d7ee6f91
recon_1             | 2023-07-16 05:06:33,946 [IPC Server handler 80 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 131a1d93-dd5a-481f-884e-0521d7ee6f91{ip: 172.24.0.10, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_3          | 2023-07-16 05:06:37,672 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | dataStreamAddress: "172.24.0.12:9858"
recon_1             | 2023-07-16 05:06:33,946 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 reported by 131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10)
datanode_2          | 2023-07-16 05:06:41,590 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_4          | 2023-07-16 05:06:53,846 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState
datanode_3          | 2023-07-16 05:06:37,676 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | clientAddress: "172.24.0.12:9858"
recon_1             | 2023-07-16 05:06:33,947 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 131a1d93-dd5a-481f-884e-0521d7ee6f91 to Node DB.
datanode_2          | 2023-07-16 05:06:41,606 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1fa08159-4631-48c1-a393-59ba0c080380/current/log_inprogress_0
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_4          | 2023-07-16 05:06:53,855 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D9760A6FAE4,id=131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:37,676 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | adminAddress: "172.24.0.12:9857"
recon_1             | 2023-07-16 05:06:34,104 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 reported by 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)
datanode_2          | 2023-07-16 05:06:41,688 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380-LeaderElection2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-59BA0C080380: set configuration 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_3          | 2023-07-16 05:06:37,686 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | startupRole: FOLLOWER
recon_1             | 2023-07-16 05:06:35,145 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by 131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10)
datanode_2          | 2023-07-16 05:06:43,075 [grpc-default-executor-1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_4          | 2023-07-16 05:06:53,856 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-07-16 05:06:37,692 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | ,id: "11d6e79b-1aa6-4571-843b-2e54970fb159"
recon_1             | 2023-07-16 05:06:36,098 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
datanode_2          | 2023-07-16 05:06:43,075 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState] INFO impl.FollowerState: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-FollowerState was interrupted
scm_1               | 2023-07-16 05:06:17,308 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:57376 / 172.24.0.15:57376: output error
scm_1               | 2023-07-16 05:06:17,316 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:55052 / 172.24.0.10:55052: output error
datanode_3          | 2023-07-16 05:06:37,693 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | address: "172.24.0.15:9856"
recon_1             | 2023-07-16 05:06:36,098 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
datanode_2          | 2023-07-16 05:06:43,078 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderStateImpl] INFO impl.TransferLeadership: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: start transferring leadership to 131a1d93-dd5a-481f-884e-0521d7ee6f91
scm_1               | 2023-07-16 05:06:17,333 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
datanode_3          | 2023-07-16 05:06:37,697 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | dataStreamAddress: "172.24.0.15:9858"
recon_1             | 2023-07-16 05:06:36,141 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=1fa08159-4631-48c1-a393-59ba0c080380. Trying to get from SCM.
datanode_2          | 2023-07-16 05:06:43,120 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderStateImpl] INFO impl.TransferLeadership: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: sendStartLeaderElection to follower 131a1d93-dd5a-481f-884e-0521d7ee6f91, lastEntry=(t:1, i:0)
datanode_4          | 2023-07-16 05:06:53,856 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3          | 2023-07-16 05:06:37,703 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | clientAddress: "172.24.0.15:9858"
recon_1             | 2023-07-16 05:06:36,147 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 1fa08159-4631-48c1-a393-59ba0c080380, Nodes: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.606Z[UTC]] to Recon pipeline metadata.
datanode_2          | 2023-07-16 05:06:43,154 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderStateImpl] INFO impl.TransferLeadership: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: SUCCESS sent StartLeaderElection to transferee 131a1d93-dd5a-481f-884e-0521d7ee6f91 immediately as it already has up-to-date log
datanode_4          | 2023-07-16 05:06:53,856 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3          | 2023-07-16 05:06:37,703 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | adminAddress: "172.24.0.15:9857"
recon_1             | 2023-07-16 05:06:36,148 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=1fa08159-4631-48c1-a393-59ba0c080380 reported by 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)
datanode_2          | 2023-07-16 05:06:43,168 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-07-16 05:06:43,169 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: change Leader from 301767aa-b1e5-4681-b115-67455213d3e0 to null at term 1 for ELECTION
datanode_4          | 2023-07-16 05:06:53,856 [131a1d93-dd5a-481f-884e-0521d7ee6f91-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-07-16 05:06:37,704 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | startupRole: FOLLOWER
recon_1             | 2023-07-16 05:06:37,413 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 reported by f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)
datanode_2          | 2023-07-16 05:06:43,171 [grpc-default-executor-1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_5          | ,id: "301767aa-b1e5-4681-b115-67455213d3e0"
recon_1             | 2023-07-16 05:06:52,402 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f reported by 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)
datanode_2          | 2023-07-16 05:06:43,255 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for 0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_4          | 2023-07-16 05:06:53,861 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | address: "172.24.0.11:9856"
datanode_2          | 2023-07-16 05:06:43,284 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 11d6e79b-1aa6-4571-843b-2e54970fb159
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_4          | 2023-07-16 05:06:53,863 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-07-16 05:06:37,720 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1             | 2023-07-16 05:06:53,146 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=1855ec52-aa87-492c-936f-1d9760a6fae4. Trying to get from SCM.
datanode_5          | dataStreamAddress: "172.24.0.11:9858"
datanode_2          | 2023-07-16 05:06:43,304 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_4          | 2023-07-16 05:06:53,889 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=1855ec52-aa87-492c-936f-1d9760a6fae4
datanode_3          | 2023-07-16 05:06:37,728 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-07-16 05:06:53,160 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 1855ec52-aa87-492c-936f-1d9760a6fae4, Nodes: 131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:131a1d93-dd5a-481f-884e-0521d7ee6f91, CreationTimestamp2023-07-16T05:06:21.563Z[UTC]] to Recon pipeline metadata.
datanode_5          | clientAddress: "172.24.0.11:9858"
datanode_2          | 2023-07-16 05:06:43,329 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
datanode_4          | 2023-07-16 05:06:53,894 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=1855ec52-aa87-492c-936f-1d9760a6fae4.
datanode_3          | 2023-07-16 05:06:37,729 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1             | 2023-07-16 05:07:13,525 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
datanode_5          | adminAddress: "172.24.0.11:9857"
datanode_2          | 2023-07-16 05:06:43,276 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread1] INFO impl.TransferLeadership: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: Received startLeaderElection reply from 131a1d93-dd5a-481f-884e-0521d7ee6f91: success? true
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_4          | 2023-07-16 05:06:58,975 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO impl.FollowerState: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5129014494ns, electionTimeout:5110ms
datanode_4          | 2023-07-16 05:06:58,978 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: shutdown 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState
recon_1             | 2023-07-16 05:07:13,527 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
datanode_5          | startupRole: FOLLOWER
datanode_2          | 2023-07-16 05:06:43,304 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 301767aa-b1e5-4681-b115-67455213d3e0
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_4          | 2023-07-16 05:06:58,978 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-07-16 05:06:37,730 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1             | 2023-07-16 05:07:15,286 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1689484033527
datanode_5          | , old:)
datanode_2          | 2023-07-16 05:06:43,543 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
datanode_4          | 2023-07-16 05:06:58,979 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-07-16 05:06:58,979 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-FollowerState] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2
recon_1             | 2023-07-16 05:07:15,332 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
datanode_3          | 2023-07-16 05:06:37,731 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-07-16 05:06:44,975 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 11d6e79b-1aa6-4571-843b-2e54970fb159: Completed APPEND_ENTRIES, lastReply: null
datanode_2          | 2023-07-16 05:06:43,543 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.LeaderElection:   Response 0: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5<-11d6e79b-1aa6-4571-843b-2e54970fb159#0:OK-t2
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_4          | 2023-07-16 05:06:58,984 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:58,986 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_4          | 2023-07-16 05:06:58,988 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-07-16 05:06:58,988 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.LeaderElection: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-07-16 05:06:52,313 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 11d6e79b-1aa6-4571-843b-2e54970fb159: addNew group-FC70F0AEDA8F:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-FC70F0AEDA8F:java.util.concurrent.CompletableFuture@4898b4c1[Not completed]
datanode_2          | 2023-07-16 05:06:43,543 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.LeaderElection: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3 ELECTION round 0: result PASSED
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_4          | 2023-07-16 05:06:58,988 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: shutdown 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2
recon_1             | 2023-07-16 05:07:16,070 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689484033527.
datanode_5          | 2023-07-16 05:06:52,322 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159: new RaftServerImpl for group-FC70F0AEDA8F:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-07-16 05:06:43,544 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_4          | 2023-07-16 05:06:58,988 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 05:06:37,732 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 2023-07-16 05:07:16,364 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
datanode_5          | 2023-07-16 05:06:52,323 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-07-16 05:06:43,544 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_4          | 2023-07-16 05:06:58,988 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D9760A6FAE4 with new leaderId: 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:37,732 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-07-16 05:06:52,323 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_2          | 2023-07-16 05:06:43,544 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C9411BC9FD15 with new leaderId: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:43,545 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: change Leader from null to 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 at term 2 for becomeLeader, leader elected after 375ms
datanode_3          | 2023-07-16 05:06:37,733 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
recon_1             | 2023-07-16 05:07:17,018 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
datanode_5          | 2023-07-16 05:06:52,323 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_2          | 2023-07-16 05:06:43,552 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-07-16 05:06:58,991 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: change Leader from null to 131a1d93-dd5a-481f-884e-0521d7ee6f91 at term 1 for becomeLeader, leader elected after 5916ms
datanode_3          | 2023-07-16 05:06:37,738 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 2023-07-16 05:07:17,028 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
datanode_5          | 2023-07-16 05:06:52,324 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm_1               | 2023-07-16 05:06:17,315 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:57398 / 172.24.0.11:57398: output error
datanode_2          | 2023-07-16 05:06:43,553 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-07-16 05:06:58,991 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 05:06:37,739 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 2023-07-16 05:07:17,031 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
datanode_5          | 2023-07-16 05:06:52,325 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-07-16 05:06:17,334 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
datanode_2          | 2023-07-16 05:06:43,553 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-07-16 05:06:58,992 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:06:37,752 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderStateImpl
recon_1             | 2023-07-16 05:07:17,032 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
datanode_5          | 2023-07-16 05:06:52,326 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | java.nio.channels.ClosedChannelException
datanode_2          | 2023-07-16 05:06:43,556 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4          | 2023-07-16 05:06:58,992 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-07-16 05:06:37,755 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 2023-07-16 05:07:17,032 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
datanode_5          | 2023-07-16 05:06:52,327 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: ConfigurationManager, init=-1: peers:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2          | 2023-07-16 05:06:43,558 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-07-16 05:06:58,994 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 05:06:37,769 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5ce31707-4feb-46a0-9df8-8c72950665a4/current/log_inprogress_0
recon_1             | 2023-07-16 05:07:17,033 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
datanode_5          | 2023-07-16 05:06:52,327 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_2          | 2023-07-16 05:06:43,558 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-07-16 05:06:58,994 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-07-16 05:06:37,835 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4-LeaderElection2] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-8C72950665A4: set configuration 0: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-16 05:07:17,053 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_5          | 2023-07-16 05:06:52,332 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-07-16 05:06:41,303 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO impl.FollowerState: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5032731115ns, electionTimeout:5030ms
recon_1             | 2023-07-16 05:07:17,053 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.021 seconds to process 0 keys.
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_4          | 2023-07-16 05:06:58,994 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-07-16 05:06:58,995 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:06:43,558 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:06:41,303 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState
recon_1             | 2023-07-16 05:07:17,097 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
datanode_5          | 2023-07-16 05:06:52,332 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-07-16 05:06:58,995 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-07-16 05:06:43,559 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-07-16 05:06:41,303 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1             | 2023-07-16 05:07:17,101 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
datanode_5          | 2023-07-16 05:06:52,333 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
datanode_3          | 2023-07-16 05:06:41,304 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1             | 2023-07-16 05:07:35,523 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_4.xcompat_default.
datanode_4          | 2023-07-16 05:06:58,995 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO impl.RoleInfo: 131a1d93-dd5a-481f-884e-0521d7ee6f91: start 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderStateImpl
datanode_5          | 2023-07-16 05:06:52,333 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-07-16 05:06:41,304 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-FollowerState] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3
recon_1             | 2023-07-16 05:07:35,763 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-07-16 05:07:47,689 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
datanode_5          | 2023-07-16 05:06:52,335 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-07-16 05:06:41,362 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-16 05:07:47,727 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
datanode_4          | 2023-07-16 05:06:58,997 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-16 05:06:52,336 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-07-16 05:06:41,363 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-07-16 05:06:43,560 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-16 05:06:43,571 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:06:52,336 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
datanode_3          | 2023-07-16 05:06:41,375 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:41,376 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.LeaderElection: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-07-16 05:06:52,350 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          | 2023-07-16 05:06:41,376 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3
datanode_2          | 2023-07-16 05:06:43,572 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_4          | 2023-07-16 05:06:59,001 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1855ec52-aa87-492c-936f-1d9760a6fae4/current/log_inprogress_0
datanode_4          | 2023-07-16 05:06:59,017 [131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4-LeaderElection2] INFO server.RaftServer$Division: 131a1d93-dd5a-481f-884e-0521d7ee6f91@group-1D9760A6FAE4: set configuration 0: peers:[131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-16 05:07:52,138 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_3          | 2023-07-16 05:06:41,378 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-07-16 05:06:41,378 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-518A42FD211D with new leaderId: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_4          | 2023-07-16 05:07:19,084 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-16 05:08:19,085 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-16 05:09:19,086 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 05:06:41,387 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: change Leader from null to f02a32cc-4f60-4ab6-a6b0-1a6b831becc3 at term 1 for becomeLeader, leader elected after 5337ms
datanode_4          | 2023-07-16 05:10:19,086 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:06:52,350 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-07-16 05:06:52,350 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-07-16 05:06:41,388 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
recon_1             | 2023-07-16 05:07:52,139 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-07-16 05:06:52,350 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-07-16 05:06:43,573 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:06:43,609 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-16 05:06:43,611 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1             | 2023-07-16 05:07:52,184 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-07-16 05:07:52,184 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-07-16 05:07:52,221 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 2023-07-16 05:07:52,222 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-07-16 05:07:52,234 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 89 milliseconds to process 0 existing database records.
datanode_5          | 2023-07-16 05:06:52,352 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-07-16 05:06:52,352 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-07-16 05:06:52,352 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f does not exist. Creating ...
datanode_2          | 2023-07-16 05:06:43,613 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-07-16 05:06:43,614 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-07-16 05:06:43,615 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-07-16 05:06:43,616 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-07-16 05:06:41,393 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderStateImpl
datanode_3          | 2023-07-16 05:06:41,400 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-07-16 05:06:52,358 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f/in_use.lock acquired by nodename 6@4fd9c9bae09d
recon_1             | 2023-07-16 05:07:52,241 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_5          | 2023-07-16 05:06:52,368 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f has been successfully formatted.
datanode_2          | 2023-07-16 05:06:43,648 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-07-16 05:06:43,649 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-07-16 05:06:43,649 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-07-16 05:06:43,649 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-07-16 05:06:43,649 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-07-16 05:06:43,658 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-07-16 05:06:43,666 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_4          | 2023-07-16 05:11:19,087 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-16 05:12:19,091 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:06:52,371 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO ratis.ContainerStateMachine: group-FC70F0AEDA8F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-07-16 05:06:52,372 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-07-16 05:06:52,375 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-07-16 05:13:19,091 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-07-16 05:06:17,333 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
datanode_5          | 2023-07-16 05:06:52,376 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-07-16 05:06:52,377 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-07-16 05:06:43,667 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
recon_1             | 2023-07-16 05:07:52,250 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
recon_1             | 2023-07-16 05:07:52,274 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 83 milliseconds.
recon_1             | 2023-07-16 05:07:52,302 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 69 milliseconds for processing 2 containers.
recon_1             | 2023-07-16 05:07:59,990 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_5.xcompat_default.
datanode_5          | 2023-07-16 05:06:52,378 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-07-16 05:06:43,668 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 2023-07-16 05:08:00,012 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 not found. Cannot add container #3
datanode_3          | 2023-07-16 05:06:41,402 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ad8a4b11-a3a4-4244-9a6b-518a42fd211d/current/log_inprogress_0
datanode_3          | 2023-07-16 05:06:41,448 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D-LeaderElection3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-518A42FD211D: set configuration 0: peers:[f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:44,834 [grpc-default-executor-3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: receive requestVote(ELECTION, 131a1d93-dd5a-481f-884e-0521d7ee6f91, group-44F4E9684BB5, 2, (t:1, i:0))
datanode_5          | 2023-07-16 05:06:52,379 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-07-16 05:06:43,668 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-07-16 05:06:43,675 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderStateImpl
datanode_2          | 2023-07-16 05:06:43,688 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_4          | 2023-07-16 05:14:19,092 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-07-16 05:15:19,093 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3          | 2023-07-16 05:06:44,834 [grpc-default-executor-3] INFO impl.VoteContext: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FOLLOWER: accept ELECTION from 131a1d93-dd5a-481f-884e-0521d7ee6f91: our priority 0 <= candidate's priority 1
datanode_5          | 2023-07-16 05:06:52,382 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1             | 2023-07-16 05:08:00,017 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
datanode_5          | 2023-07-16 05:06:52,384 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1             | 2023-07-16 05:08:00,171 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-07-16 05:08:00,179 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 not found. Cannot add container #3
recon_1             | 2023-07-16 05:08:00,180 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
recon_1             | 2023-07-16 05:08:00,196 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_4.xcompat_default.
recon_1             | 2023-07-16 05:08:00,220 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 not found. Cannot add container #3
datanode_3          | 2023-07-16 05:06:44,835 [grpc-default-executor-3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: change Leader from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 to null at term 2 for updateCurrentTerm
datanode_3          | 2023-07-16 05:06:44,835 [grpc-default-executor-3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:44,835 [grpc-default-executor-3] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: shutdown f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState
recon_1             | 2023-07-16 05:08:00,223 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
datanode_3          | 2023-07-16 05:06:44,835 [grpc-default-executor-3] INFO impl.RoleInfo: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: start f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState
datanode_3          | 2023-07-16 05:06:44,835 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState] INFO impl.FollowerState: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-FollowerState was interrupted
datanode_3          | 2023-07-16 05:06:44,842 [grpc-default-executor-3] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5 replies to ELECTION vote request: 131a1d93-dd5a-481f-884e-0521d7ee6f91<-f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#0:OK-t2. Peer's state: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5:t2, leader=null, voted=131a1d93-dd5a-481f-884e-0521d7ee6f91, raftlog=Memoized:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLog:OPENED:c0, conf=0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-07-16 05:08:00,231 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-07-16 05:08:00,241 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 not found. Cannot add container #3
recon_1             | 2023-07-16 05:08:00,242 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
recon_1             | 2023-07-16 05:08:00,286 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-07-16 05:08:00,305 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 not found. Cannot add container #3
recon_1             | 2023-07-16 05:08:00,306 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
datanode_3          | 2023-07-16 05:06:44,916 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: Completed APPEND_ENTRIES, lastRequest: null
datanode_2          | 2023-07-16 05:06:43,707 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_0 to /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_0-0
datanode_2          | 2023-07-16 05:06:43,737 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/20604c91-9021-41ef-9746-c9411bc9fd15/current/log_inprogress_1
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
datanode_5          | 2023-07-16 05:06:52,385 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-07-16 05:06:45,500 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_3          |   requestorId: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_3          |   replyId: "f02a32cc-4f60-4ab6-a6b0-1a6b831becc3"
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
datanode_5          | 2023-07-16 05:06:52,390 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f
datanode_2          | 2023-07-16 05:06:43,817 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15-LeaderElection3] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-C9411BC9FD15: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER, 11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 301767aa-b1e5-4681-b115-67455213d3e0|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-07-16 05:06:44,181 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Completed APPEND_ENTRIES, lastRequest: 301767aa-b1e5-4681-b115-67455213d3e0->333b8eb3-808f-40e6-a1dc-b9ce55f887b5#4-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_2          | address: "172.24.0.12:9856"
datanode_2          | priority: 1
datanode_5          | 2023-07-16 05:06:52,391 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
datanode_3          |   raftGroupId {
datanode_3          |     id: "\002L\277\207\376\030O\354\243\254D\364\351hK\265"
recon_1             | 2023-07-16 05:08:52,285 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
datanode_5          | 2023-07-16 05:06:52,391 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-07-16 05:06:52,392 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-07-16 05:06:52,395 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | dataStreamAddress: "172.24.0.12:9858"
datanode_3          |   }
recon_1             | 2023-07-16 05:08:52,304 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
datanode_5          | 2023-07-16 05:06:52,395 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
datanode_2          | clientAddress: "172.24.0.12:9858"
datanode_3          |   callId: 7
recon_1             | 2023-07-16 05:08:52,304 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 18
datanode_5          | 2023-07-16 05:06:52,397 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-07-16 05:06:52,397 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | adminAddress: "172.24.0.12:9857"
datanode_3          |   success: true
recon_1             | 2023-07-16 05:09:45,622 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #4 got from xcompat_datanode_3.xcompat_default.
datanode_5          | 2023-07-16 05:06:52,399 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2          | startupRole: FOLLOWER
datanode_3          | }
datanode_3          | term: 1
datanode_3          | nextIndex: 1
datanode_5          | 2023-07-16 05:06:52,413 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2          | ,id: "11d6e79b-1aa6-4571-843b-2e54970fb159"
datanode_3          | matchIndex: 18446744073709551615
datanode_3          | isHearbeat: true
datanode_3          | 
datanode_5          | 2023-07-16 05:06:52,415 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
datanode_2          | address: "172.24.0.15:9856"
datanode_3          | 2023-07-16 05:06:45,545 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: Completed APPEND_ENTRIES, lastRequest: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
recon_1             | 2023-07-16 05:09:45,647 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #4 to Recon.
datanode_5          | 2023-07-16 05:06:52,864 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
datanode_2          | dataStreamAddress: "172.24.0.15:9858"
datanode_3          | address: "172.24.0.12:9856"
recon_1             | 2023-07-16 05:09:52,325 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
datanode_5          | 2023-07-16 05:06:52,864 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-07-16 05:06:19,679 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_2          | clientAddress: "172.24.0.15:9858"
datanode_3          | dataStreamAddress: "172.24.0.12:9858"
recon_1             | 2023-07-16 05:09:52,326 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 21
datanode_5          | 2023-07-16 05:06:52,865 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1               | 2023-07-16 05:06:19,717 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 11d6e79b-1aa6-4571-843b-2e54970fb159{ip: 172.24.0.15, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2          | adminAddress: "172.24.0.15:9857"
datanode_3          | clientAddress: "172.24.0.12:9858"
recon_1             | 2023-07-16 05:10:52,327 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
datanode_5          | 2023-07-16 05:06:52,867 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-11d6e79b-1aa6-4571-843b-2e54970fb159: Detected pause in JVM or host machine approximately 0.340s with 0.431s GC time.
scm_1               | 2023-07-16 05:06:19,768 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_2          | startupRole: FOLLOWER
datanode_2          | ,id: "301767aa-b1e5-4681-b115-67455213d3e0"
datanode_2          | address: "172.24.0.11:9856"
datanode_2          | dataStreamAddress: "172.24.0.11:9858"
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=431ms
datanode_2          | clientAddress: "172.24.0.11:9858"
recon_1             | 2023-07-16 05:10:52,327 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
datanode_5          | 2023-07-16 05:06:52,901 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:06:19,850 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f to datanode:11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_2          | adminAddress: "172.24.0.11:9857"
recon_1             | 2023-07-16 05:11:52,328 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
datanode_5          | 2023-07-16 05:06:52,901 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-07-16 05:06:19,872 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-07-16 05:06:19,877 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
datanode_2          | startupRole: FOLLOWER
recon_1             | 2023-07-16 05:11:52,328 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
datanode_5          | 2023-07-16 05:06:52,906 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: start as a follower, conf=-1: peers:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | adminAddress: "172.24.0.12:9857"
scm_1               | 2023-07-16 05:06:19,885 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
datanode_2          | , old:)
recon_1             | 2023-07-16 05:12:52,307 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
datanode_3          | startupRole: FOLLOWER
scm_1               | 2023-07-16 05:06:20,269 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_2          | 2023-07-16 05:06:44,182 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Completed APPEND_ENTRIES, lastReply: null
recon_1             | 2023-07-16 05:12:52,325 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
datanode_5          | 2023-07-16 05:06:52,906 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | ,id: "f02a32cc-4f60-4ab6-a6b0-1a6b831becc3"
scm_1               | 2023-07-16 05:06:20,273 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 131a1d93-dd5a-481f-884e-0521d7ee6f91{ip: 172.24.0.10, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2          | 2023-07-16 05:06:44,219 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Completed APPEND_ENTRIES, lastRequest: null
recon_1             | 2023-07-16 05:12:52,327 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=b4a536fb-1835-4523-a02b-76c2dca715c4 from SCM.
datanode_5          | 2023-07-16 05:06:52,907 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState
datanode_3          | address: "172.24.0.8:9856"
datanode_5          | 2023-07-16 05:06:52,909 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FC70F0AEDA8F,id=11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_3          | dataStreamAddress: "172.24.0.8:9858"
recon_1             | 2023-07-16 05:12:52,328 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 05:06:20,276 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_2          | 2023-07-16 05:06:44,241 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_5          | 2023-07-16 05:06:52,917 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | clientAddress: "172.24.0.8:9858"
recon_1             | 2023-07-16 05:12:52,328 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-16 05:06:20,280 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
datanode_2          |   requestorId: "301767aa-b1e5-4681-b115-67455213d3e0"
datanode_5          | 2023-07-16 05:06:52,918 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-07-16 05:06:52,920 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1             | 2023-07-16 05:12:52,334 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 27 milliseconds for processing 3 containers.
recon_1             | 2023-07-16 05:12:52,340 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 48 milliseconds.
datanode_2          |   replyId: "333b8eb3-808f-40e6-a1dc-b9ce55f887b5"
datanode_3          | adminAddress: "172.24.0.8:9857"
datanode_5          | 2023-07-16 05:06:52,920 [11d6e79b-1aa6-4571-843b-2e54970fb159-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1             | 2023-07-16 05:13:52,330 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 05:06:20,552 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_2          |   raftGroupId {
datanode_3          | startupRole: FOLLOWER
datanode_5          | 2023-07-16 05:06:52,929 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1             | 2023-07-16 05:13:52,330 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-07-16 05:06:20,556 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f02a32cc-4f60-4ab6-a6b0-1a6b831becc3{ip: 172.24.0.8, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2          |     id: " `L\221\220!A\357\227F\311A\033\311\375\025"
datanode_3          | ,id: "131a1d93-dd5a-481f-884e-0521d7ee6f91"
datanode_5          | 2023-07-16 05:06:52,930 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 2023-07-16 05:14:52,330 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-07-16 05:06:20,557 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_2          |   }
datanode_3          | address: "172.24.0.10:9856"
datanode_5          | 2023-07-16 05:06:52,937 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f
datanode_5          | 2023-07-16 05:06:52,947 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f.
scm_1               | 2023-07-16 05:06:20,572 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
datanode_2          |   callId: 7
recon_1             | 2023-07-16 05:14:52,330 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
datanode_5          | 2023-07-16 05:06:58,069 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO impl.FollowerState: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5162281738ns, electionTimeout:5134ms
scm_1               | 2023-07-16 05:06:20,572 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 05:06:20,572 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
datanode_3          | priority: 1
datanode_5          | 2023-07-16 05:06:58,070 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: shutdown 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState
datanode_5          | 2023-07-16 05:06:58,070 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          |   success: true
datanode_3          | dataStreamAddress: "172.24.0.10:9858"
datanode_5          | 2023-07-16 05:06:58,078 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-07-16 05:06:20,574 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
datanode_2          | }
datanode_3          | clientAddress: "172.24.0.10:9858"
datanode_5          | 2023-07-16 05:06:58,079 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-FollowerState] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1
scm_1               | 2023-07-16 05:06:20,574 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_2          | term: 1
datanode_2          | nextIndex: 1
datanode_5          | 2023-07-16 05:06:58,083 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.LeaderElection: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:20,856 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3          | adminAddress: "172.24.0.10:9857"
datanode_2          | matchIndex: 18446744073709551615
datanode_5          | 2023-07-16 05:06:58,085 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.LeaderElection: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-07-16 05:06:20,953 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f, Nodes: 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:19.847817Z[UTC]]
datanode_3          | startupRole: FOLLOWER
datanode_2          | isHearbeat: true
datanode_5          | 2023-07-16 05:06:58,091 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.LeaderElection: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:20,982 [IPC Server handler 91 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/301767aa-b1e5-4681-b115-67455213d3e0
datanode_3          | , old:)
datanode_2          | 
datanode_5          | 2023-07-16 05:06:58,091 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.LeaderElection: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-07-16 05:06:20,999 [IPC Server handler 91 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 301767aa-b1e5-4681-b115-67455213d3e0{ip: 172.24.0.11, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-07-16 05:06:21,015 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_2          | 2023-07-16 05:06:44,871 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: receive requestVote(ELECTION, 131a1d93-dd5a-481f-884e-0521d7ee6f91, group-44F4E9684BB5, 2, (t:1, i:0))
datanode_5          | 2023-07-16 05:06:58,092 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: shutdown 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1
scm_1               | 2023-07-16 05:06:21,047 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 to datanode:11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_3          | 2023-07-16 05:06:45,558 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: Completed APPEND_ENTRIES, lastReply: null
datanode_2          | 2023-07-16 05:06:44,883 [grpc-default-executor-1] INFO impl.VoteContext: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LEADER: accept ELECTION from 131a1d93-dd5a-481f-884e-0521d7ee6f91: our priority 0 <= candidate's priority 1
datanode_5          | 2023-07-16 05:06:58,092 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-07-16 05:06:21,049 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 to datanode:131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_3          | 2023-07-16 05:06:45,793 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_2          | 2023-07-16 05:06:44,883 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: change Leader from 333b8eb3-808f-40e6-a1dc-b9ce55f887b5 to null at term 2 for updateCurrentTerm
datanode_5          | 2023-07-16 05:06:58,093 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FC70F0AEDA8F with new leaderId: 11d6e79b-1aa6-4571-843b-2e54970fb159
scm_1               | 2023-07-16 05:06:21,065 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4 to datanode:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_3          | 2023-07-16 05:06:45,794 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread2] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: change Leader from null to 131a1d93-dd5a-481f-884e-0521d7ee6f91 at term 2 for appendEntries, leader elected after 958ms
datanode_2          | 2023-07-16 05:06:44,883 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: changes role from    LEADER to FOLLOWER at term 2 for candidate:131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_5          | 2023-07-16 05:06:58,093 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: change Leader from null to 11d6e79b-1aa6-4571-843b-2e54970fb159 at term 1 for becomeLeader, leader elected after 5759ms
scm_1               | 2023-07-16 05:06:21,160 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3          | 2023-07-16 05:06:45,887 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO server.RaftServer$Division: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-07-16 05:06:45,889 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-server-thread1] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_2          | 2023-07-16 05:06:44,889 [grpc-default-executor-1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: shutdown 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-LeaderStateImpl
scm_1               | 2023-07-16 05:06:21,164 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5ce31707-4feb-46a0-9df8-8c72950665a4, Nodes: 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10)f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.047233Z[UTC]]
datanode_3          | 2023-07-16 05:06:45,892 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0 to /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_0-0
datanode_5          | 2023-07-16 05:06:58,117 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-07-16 05:06:58,177 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1               | 2023-07-16 05:06:21,173 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d to datanode:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_5          | 2023-07-16 05:06:58,182 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-07-16 05:06:44,899 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3          | 2023-07-16 05:06:45,904 [f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_1
scm_1               | 2023-07-16 05:06:21,213 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_5          | 2023-07-16 05:06:58,204 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-07-16 05:07:18,792 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,260 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 333b8eb3-808f-40e6-a1dc-b9ce55f887b5{ip: 172.24.0.12, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_5          | 2023-07-16 05:06:58,204 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-07-16 05:06:44,926 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3          | 2023-07-16 05:08:18,794 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,241 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_5          | 2023-07-16 05:06:58,206 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-07-16 05:06:44,937 [grpc-default-executor-1] INFO impl.PendingRequests: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-PendingRequests: sendNotLeaderResponses
datanode_3          | 2023-07-16 05:09:18,797 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,276 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_5          | 2023-07-16 05:06:58,228 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-07-16 05:06:44,979 [grpc-default-executor-1] INFO impl.RoleInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5: start 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-FollowerState
datanode_5          | 2023-07-16 05:06:58,236 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-07-16 05:06:21,283 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: ad8a4b11-a3a4-4244-9a6b-518a42fd211d, Nodes: f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.173197Z[UTC]]
datanode_2          | 2023-07-16 05:06:44,993 [grpc-default-executor-2] INFO server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3          | 2023-07-16 05:10:18,804 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:06:58,245 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO impl.RoleInfo: 11d6e79b-1aa6-4571-843b-2e54970fb159: start 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderStateImpl
scm_1               | 2023-07-16 05:06:21,334 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 to datanode:333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_2          | 2023-07-16 05:06:44,993 [grpc-default-executor-2] INFO leader.FollowerInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_3          | 2023-07-16 05:11:18,804 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,341 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 to datanode:f02a32cc-4f60-4ab6-a6b0-1a6b831becc3
datanode_5          | 2023-07-16 05:06:58,259 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-07-16 05:12:18,805 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,005 [grpc-default-executor-1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5 replies to ELECTION vote request: 131a1d93-dd5a-481f-884e-0521d7ee6f91<-333b8eb3-808f-40e6-a1dc-b9ce55f887b5#0:OK-t2. Peer's state: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5:t2, leader=null, voted=131a1d93-dd5a-481f-884e-0521d7ee6f91, raftlog=Memoized:333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLog:OPENED:c0, conf=0: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:21,350 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5 to datanode:131a1d93-dd5a-481f-884e-0521d7ee6f91
scm_1               | 2023-07-16 05:06:21,398 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_5          | 2023-07-16 05:06:58,273 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f/current/log_inprogress_0
datanode_3          | 2023-07-16 05:13:18,805 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,131 [grpc-default-executor-1] INFO server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->131a1d93-dd5a-481f-884e-0521d7ee6f91-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm_1               | 2023-07-16 05:06:21,468 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 024cbf87-fe18-4fec-a3ac-44f4e9684bb5, Nodes: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.334538Z[UTC]]
datanode_5          | 2023-07-16 05:06:58,311 [11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F-LeaderElection1] INFO server.RaftServer$Division: 11d6e79b-1aa6-4571-843b-2e54970fb159@group-FC70F0AEDA8F: set configuration 0: peers:[11d6e79b-1aa6-4571-843b-2e54970fb159|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-07-16 05:07:19,083 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,517 [grpc-default-executor-1] INFO server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm_1               | 2023-07-16 05:06:21,479 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5 to datanode:301767aa-b1e5-4681-b115-67455213d3e0
datanode_3          | 2023-07-16 05:14:18,806 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-07-16 05:15:18,807 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,518 [grpc-default-executor-1] INFO leader.FollowerInfo: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
scm_1               | 2023-07-16 05:06:21,495 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_5          | 2023-07-16 05:07:48,176 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-11d6e79b-1aa6-4571-843b-2e54970fb159: Detected pause in JVM or host machine approximately 0.213s with 0.324s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=324ms
scm_1               | 2023-07-16 05:06:21,532 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: f02ad586-a085-4098-91a1-a5d1edc638c5, Nodes: 301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.479239Z[UTC]]
datanode_5          | 2023-07-16 05:08:19,084 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,560 [grpc-default-executor-1] INFO server.GrpcLogAppender: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5->f02a32cc-4f60-4ab6-a6b0-1a6b831becc3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm_1               | 2023-07-16 05:06:21,540 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 to datanode:11d6e79b-1aa6-4571-843b-2e54970fb159
datanode_5          | 2023-07-16 05:09:19,085 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:10:19,085 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,541 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 to datanode:333b8eb3-808f-40e6-a1dc-b9ce55f887b5
datanode_5          | 2023-07-16 05:11:19,085 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-07-16 05:12:19,086 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,545 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15 to datanode:301767aa-b1e5-4681-b115-67455213d3e0
datanode_5          | 2023-07-16 05:13:19,086 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,818 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44F4E9684BB5 with new leaderId: 131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_2          | 2023-07-16 05:06:45,819 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread1] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: change Leader from null to 131a1d93-dd5a-481f-884e-0521d7ee6f91 at term 2 for appendEntries, leader elected after 935ms
datanode_5          | 2023-07-16 05:14:19,087 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,552 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_2          | 2023-07-16 05:06:45,874 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO server.RaftServer$Division: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5: set configuration 1: peers:[333b8eb3-808f-40e6-a1dc-b9ce55f887b5|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, f02a32cc-4f60-4ab6-a6b0-1a6b831becc3|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, 131a1d93-dd5a-481f-884e-0521d7ee6f91|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-07-16 05:06:21,559 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 20604c91-9021-41ef-9746-c9411bc9fd15, Nodes: 11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15)333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.540847Z[UTC]]
datanode_5          | 2023-07-16 05:15:19,088 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-07-16 05:06:45,875 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5-server-thread2] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-07-16 05:06:21,564 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1855ec52-aa87-492c-936f-1d9760a6fae4 to datanode:131a1d93-dd5a-481f-884e-0521d7ee6f91
datanode_2          | 2023-07-16 05:06:45,877 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_0 to /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_0-0
datanode_2          | 2023-07-16 05:06:45,879 [333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5@group-44F4E9684BB5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/024cbf87-fe18-4fec-a3ac-44f4e9684bb5/current/log_inprogress_1
scm_1               | 2023-07-16 05:06:21,571 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_2          | 2023-07-16 05:07:19,042 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,578 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 1855ec52-aa87-492c-936f-1d9760a6fae4, Nodes: 131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.563980Z[UTC]]
datanode_2          | 2023-07-16 05:08:19,044 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,580 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
datanode_2          | 2023-07-16 05:09:19,044 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,606 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1fa08159-4631-48c1-a393-59ba0c080380 to datanode:333b8eb3-808f-40e6-a1dc-b9ce55f887b5
scm_1               | 2023-07-16 05:06:21,628 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_2          | 2023-07-16 05:10:19,045 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,635 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 1fa08159-4631-48c1-a393-59ba0c080380, Nodes: 333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:06:21.606286Z[UTC]]
datanode_2          | 2023-07-16 05:11:19,046 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:21,639 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
datanode_2          | 2023-07-16 05:12:19,047 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:25,604 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_2          | 2023-07-16 05:13:19,048 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:25,619 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=f02ad586-a085-4098-91a1-a5d1edc638c5
datanode_2          | 2023-07-16 05:14:19,049 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:25,688 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_2          | 2023-07-16 05:15:19,051 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-07-16 05:06:26,173 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:06:31,129 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:06:33,879 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:06:34,170 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-07-16 05:06:34,197 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=20604c91-9021-41ef-9746-c9411bc9fd15
scm_1               | 2023-07-16 05:06:34,200 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-07-16 05:06:34,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-07-16 05:06:34,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-07-16 05:06:34,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-07-16 05:06:34,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-07-16 05:06:34,203 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-07-16 05:06:34,207 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-07-16 05:06:34,213 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-07-16 05:06:34,275 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=024cbf87-fe18-4fec-a3ac-44f4e9684bb5
scm_1               | 2023-07-16 05:06:34,416 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-07-16 05:06:34,423 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-07-16 05:06:36,141 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=ad8a4b11-a3a4-4244-9a6b-518a42fd211d
scm_1               | 2023-07-16 05:06:36,184 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=1fa08159-4631-48c1-a393-59ba0c080380
scm_1               | 2023-07-16 05:06:37,463 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=5ce31707-4feb-46a0-9df8-8c72950665a4
scm_1               | 2023-07-16 05:06:52,426 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=0285a48e-d76e-4b3f-b1b5-fc70f0aeda8f
scm_1               | 2023-07-16 05:06:53,143 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=1855ec52-aa87-492c-936f-1d9760a6fae4
scm_1               | 2023-07-16 05:07:31,196 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-07-16 05:07:31,264 [bf1223b9-84ee-4a14-b347-7ad6126fbc27@group-757BF1EEF498-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-07-16 05:07:31,281 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-07-16 05:07:57,028 [IPC Server handler 0 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: b4a536fb-1835-4523-a02b-76c2dca715c4, Nodes: 131a1d93-dd5a-481f-884e-0521d7ee6f91(xcompat_datanode_4.xcompat_default/172.24.0.10)301767aa-b1e5-4681-b115-67455213d3e0(xcompat_datanode_1.xcompat_default/172.24.0.11)f02a32cc-4f60-4ab6-a6b0-1a6b831becc3(xcompat_datanode_3.xcompat_default/172.24.0.8)333b8eb3-808f-40e6-a1dc-b9ce55f887b5(xcompat_datanode_2.xcompat_default/172.24.0.12)11d6e79b-1aa6-4571-843b-2e54970fb159(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-16T05:07:56.982855Z[UTC]]
scm_1               | 2023-07-16 05:08:21,641 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-16 05:09:55,848 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm_1               | 2023-07-16 05:10:21,643 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-16 05:11:06,445 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-07-16 05:12:21,644 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-07-16 05:14:21,648 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
Attaching to 
